topic,paper_ID,sentence_ID,sentence,words,BIO,POS,length
machine-translation,8,2,Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE,"['Published', 'as', 'a', 'conference', 'paper', 'at', 'ICLR', '2015', 'NEURAL', 'MACHINE', 'TRANSLATION', 'BY', 'JOINTLY', 'LEARNING', 'TO', 'ALIGN', 'AND', 'TRANSLATE']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'IN', 'DT', 'NN', 'NN', 'IN', 'NNP', 'CD', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",18
machine-translation,8,7,"In this paper , we conjecture that the use of a fixed - length vector is a bottleneck in improving the performance of this basic encoder - decoder architecture , and propose to extend this by allowing a model to automatically ( soft - ) search for parts of a source sentence that are relevant to predicting a target word , without having to form these parts as a hard segment explicitly .","['In', 'this', 'paper', ',', 'we', 'conjecture', 'that', 'the', 'use', 'of', 'a', 'fixed', '-', 'length', 'vector', 'is', 'a', 'bottleneck', 'in', 'improving', 'the', 'performance', 'of', 'this', 'basic', 'encoder', '-', 'decoder', 'architecture', ',', 'and', 'propose', 'to', 'extend', 'this', 'by', 'allowing', 'a', 'model', 'to', 'automatically', '(', 'soft', '-', ')', 'search', 'for', 'parts', 'of', 'a', 'source', 'sentence', 'that', 'are', 'relevant', 'to', 'predicting', 'a', 'target', 'word', ',', 'without', 'having', 'to', 'form', 'these', 'parts', 'as', 'a', 'hard', 'segment', 'explicitly', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', ':', 'NN', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'NN', 'NN', ',', 'CC', 'VB', 'TO', 'VB', 'DT', 'IN', 'VBG', 'DT', 'NN', 'TO', 'RB', '(', 'JJ', ':', ')', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'WDT', 'VBP', 'JJ', 'TO', 'VBG', 'DT', 'NN', 'NN', ',', 'IN', 'VBG', 'TO', 'VB', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'RB', '.']",73
machine-translation,8,11,"Neural machine translation is a newly emerging approach to machine translation , recently proposed by , and .","['Neural', 'machine', 'translation', 'is', 'a', 'newly', 'emerging', 'approach', 'to', 'machine', 'translation', ',', 'recently', 'proposed', 'by', ',', 'and', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', 'VBZ', 'DT', 'RB', 'VBG', 'NN', 'TO', 'NN', 'NN', ',', 'RB', 'VBN', 'IN', ',', 'CC', '.']",18
machine-translation,8,20,"In order to address this issue , we introduce an extension to the encoder - decoder model which learns to align and translate jointly .","['In', 'order', 'to', 'address', 'this', 'issue', ',', 'we', 'introduce', 'an', 'extension', 'to', 'the', 'encoder', '-', 'decoder', 'model', 'which', 'learns', 'to', 'align', 'and', 'translate', 'jointly', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'TO', 'DT', 'NN', ':', 'NN', 'NN', 'WDT', 'VBZ', 'TO', 'VB', 'CC', 'VB', 'RB', '.']",25
machine-translation,8,21,"Each time the proposed model generates a word in a translation , it ( soft - ) searches for a set of positions in a source sentence where the most relevant information is concentrated .","['Each', 'time', 'the', 'proposed', 'model', 'generates', 'a', 'word', 'in', 'a', 'translation', ',', 'it', '(', 'soft', '-', ')', 'searches', 'for', 'a', 'set', 'of', 'positions', 'in', 'a', 'source', 'sentence', 'where', 'the', 'most', 'relevant', 'information', 'is', 'concentrated', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'DT', 'VBN', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'PRP', '(', 'JJ', ':', ')', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'WRB', 'DT', 'RBS', 'JJ', 'NN', 'VBZ', 'VBN', '.']",35
machine-translation,8,22,The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words .,"['The', 'model', 'then', 'predicts', 'a', 'target', 'word', 'based', 'on', 'the', 'context', 'vectors', 'associated', 'with', 'these', 'source', 'positions', 'and', 'all', 'the', 'previous', 'generated', 'target', 'words', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'RB', 'VBZ', 'DT', 'NN', 'NN', 'VBN', 'IN', 'DT', 'NN', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'NNS', 'CC', 'PDT', 'DT', 'JJ', 'VBD', 'NN', 'NNS', '.']",25
machine-translation,8,118,We train two types of models .,"['We', 'train', 'two', 'types', 'of', 'models', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'NNS', 'IN', 'NNS', '.']",7
machine-translation,8,119,"The first one is an RNN Encoder - Decoder ( RNNencdec , , and the other is the proposed model , to which we refer as RNNsearch .","['The', 'first', 'one', 'is', 'an', 'RNN', 'Encoder', '-', 'Decoder', '(', 'RNNencdec', ',', ',', 'and', 'the', 'other', 'is', 'the', 'proposed', 'model', ',', 'to', 'which', 'we', 'refer', 'as', 'RNNsearch', '.']","['O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'NNP', 'NNP', ':', 'NN', '(', 'NNP', ',', ',', 'CC', 'DT', 'JJ', 'VBZ', 'DT', 'JJ', 'NN', ',', 'TO', 'WDT', 'PRP', 'VBP', 'IN', 'NNP', '.']",28
machine-translation,8,120,"We train each model twice : first with the sentences of length up to 30 words ( RNNencdec - 30 , RNNsearch - 30 ) and then with the sentences of length up to 50 word ( RNNencdec - 50 , RNNsearch - 50 ) .","['We', 'train', 'each', 'model', 'twice', ':', 'first', 'with', 'the', 'sentences', 'of', 'length', 'up', 'to', '30', 'words', '(', 'RNNencdec', '-', '30', ',', 'RNNsearch', '-', '30', ')', 'and', 'then', 'with', 'the', 'sentences', 'of', 'length', 'up', 'to', '50', 'word', '(', 'RNNencdec', '-', '50', ',', 'RNNsearch', '-', '50', ')', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'RB', ':', 'JJ', 'IN', 'DT', 'NNS', 'IN', 'NN', 'IN', 'TO', 'CD', 'NNS', '(', 'NNP', ':', 'CD', ',', 'NNP', ':', 'CD', ')', 'CC', 'RB', 'IN', 'DT', 'NNS', 'IN', 'NN', 'IN', 'TO', 'CD', 'NN', '(', 'NNP', ':', 'CD', ',', 'NNP', ':', 'CD', ')', '.']",46
machine-translation,8,121,The encoder and decoder of the RNNencdec have 1000 hidden units each .,"['The', 'encoder', 'and', 'decoder', 'of', 'the', 'RNNencdec', 'have', '1000', 'hidden', 'units', 'each', '.']","['O', 'B-p', 'I-p', 'I-p', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'CC', 'NN', 'IN', 'DT', 'NNP', 'VBP', 'CD', 'JJ', 'NNS', 'DT', '.']",13
machine-translation,8,122,The encoder of the RNNsearch consists of forward and backward recurrent neural networks ( RNN ) each having 1000 hidden units .,"['The', 'encoder', 'of', 'the', 'RNNsearch', 'consists', 'of', 'forward', 'and', 'backward', 'recurrent', 'neural', 'networks', '(', 'RNN', ')', 'each', 'having', '1000', 'hidden', 'units', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'DT', 'NNP', 'VBZ', 'IN', 'NN', 'CC', 'NN', 'NN', 'JJ', 'NNS', '(', 'NNP', ')', 'DT', 'VBG', 'CD', 'JJ', 'NNS', '.']",22
machine-translation,8,123,It s decoder has 1000 hidden units .,"['It', 's', 'decoder', 'has', '1000', 'hidden', 'units', '.']","['O', 'O', 'B-n', 'O', 'B-n', 'B-p', 'I-p', 'O']","['PRP', 'VBZ', 'NN', 'VBZ', 'CD', 'JJ', 'NNS', '.']",8
machine-translation,8,124,"In both cases , we use a multilayer network with a single maxout hidden layer to compute the conditional probability of each target word .","['In', 'both', 'cases', ',', 'we', 'use', 'a', 'multilayer', 'network', 'with', 'a', 'single', 'maxout', 'hidden', 'layer', 'to', 'compute', 'the', 'conditional', 'probability', 'of', 'each', 'target', 'word', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",25
machine-translation,8,141,"In , we see that the performance of RNNencdec dramatically drops as the length of the sentences increases .","['In', ',', 'we', 'see', 'that', 'the', 'performance', 'of', 'RNNencdec', 'dramatically', 'drops', 'as', 'the', 'length', 'of', 'the', 'sentences', 'increases', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'IN', 'NNP', 'RB', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNS', 'VBZ', '.']",19
machine-translation,8,142,"On the other hand , both RNNsearch - 30 and RNNsearch - 50 are more robust to the length of the sentences .","['On', 'the', 'other', 'hand', ',', 'both', 'RNNsearch', '-', '30', 'and', 'RNNsearch', '-', '50', 'are', 'more', 'robust', 'to', 'the', 'length', 'of', 'the', 'sentences', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'DT', 'NNP', ':', 'CD', 'CC', 'NNP', ':', 'CD', 'VBP', 'JJR', 'JJ', 'TO', 'DT', 'NN', 'IN', 'DT', 'NNS', '.']",23
machine-translation,8,143,"RNNsearch - 50 , especially , shows no performance deterioration even with sentences of length 50 or more .","['RNNsearch', '-', '50', ',', 'especially', ',', 'shows', 'no', 'performance', 'deterioration', 'even', 'with', 'sentences', 'of', 'length', '50', 'or', 'more', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'CD', ',', 'RB', ',', 'VBZ', 'DT', 'NN', 'NN', 'RB', 'IN', 'NNS', 'IN', 'NN', 'CD', 'CC', 'JJR', '.']",19
machine-translation,8,144,This superiority of the proposed model over the basic encoder - decoder is further confirmed by the fact that the RNNsearch - 30 even outperforms RNNencdec - 50 ( see ) .,"['This', 'superiority', 'of', 'the', 'proposed', 'model', 'over', 'the', 'basic', 'encoder', '-', 'decoder', 'is', 'further', 'confirmed', 'by', 'the', 'fact', 'that', 'the', 'RNNsearch', '-', '30', 'even', 'outperforms', 'RNNencdec', '-', '50', '(', 'see', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'DT', 'VBN', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNP', ':', 'CD', 'RB', 'NNS', 'NNP', ':', 'CD', '(', 'NN', ')', '.']",32
machine-translation,9,2,Under review as a conference paper at ICLR 2018 COMPRESSING WORD EMBEDDINGS VIA DEEP COMPOSITIONAL CODE LEARNING,"['Under', 'review', 'as', 'a', 'conference', 'paper', 'at', 'ICLR', '2018', 'COMPRESSING', 'WORD', 'EMBEDDINGS', 'VIA', 'DEEP', 'COMPOSITIONAL', 'CODE', 'LEARNING']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'NNP', 'CD', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",17
machine-translation,9,5,Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance .,"['Deploying', 'neural', 'NLP', 'models', 'to', 'mobile', 'devices', 'requires', 'compressing', 'the', 'word', 'embeddings', 'without', 'any', 'significant', 'sacrifices', 'in', 'performance', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'JJ', 'NNP', 'NNS', 'TO', 'VB', 'NNS', 'VBZ', 'VBG', 'DT', 'NN', 'VBZ', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NN', '.']",19
machine-translation,9,24,"Thus , it is becoming more important to compress the size of NLP models for deployment to devices with limited memory or storage capacity .","['Thus', ',', 'it', 'is', 'becoming', 'more', 'important', 'to', 'compress', 'the', 'size', 'of', 'NLP', 'models', 'for', 'deployment', 'to', 'devices', 'with', 'limited', 'memory', 'or', 'storage', 'capacity', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBZ', 'VBG', 'RBR', 'JJ', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNP', 'NNS', 'IN', 'NN', 'TO', 'NNS', 'IN', 'JJ', 'NN', 'CC', 'NN', 'NN', '.']",25
machine-translation,9,32,"Following the intuition of creating partially shared embeddings , instead of assigning each word a unique ID , we represent each word w with a code C w = ( C 1 w , C 2 w , ... , C M w ) .","['Following', 'the', 'intuition', 'of', 'creating', 'partially', 'shared', 'embeddings', ',', 'instead', 'of', 'assigning', 'each', 'word', 'a', 'unique', 'ID', ',', 'we', 'represent', 'each', 'word', 'w', 'with', 'a', 'code', 'C', 'w', '=', '(', 'C', '1', 'w', ',', 'C', '2', 'w', ',', '...', ',', 'C', 'M', 'w', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'DT', 'NN', 'IN', 'VBG', 'RB', 'VBN', 'NNS', ',', 'RB', 'IN', 'VBG', 'DT', 'NN', 'DT', 'JJ', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NNP', 'NN', 'NNP', '(', 'NNP', 'CD', 'NN', ',', 'NNP', 'CD', 'NN', ',', ':', ',', 'NNP', 'NNP', 'NN', ')', '.']",45
machine-translation,9,37,"Once we have obtained such compact codes for all words in the vocabulary , we use embedding vectors to represent the codes rather than the unique words .","['Once', 'we', 'have', 'obtained', 'such', 'compact', 'codes', 'for', 'all', 'words', 'in', 'the', 'vocabulary', ',', 'we', 'use', 'embedding', 'vectors', 'to', 'represent', 'the', 'codes', 'rather', 'than', 'the', 'unique', 'words', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['RB', 'PRP', 'VBP', 'VBN', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'DT', 'JJ', ',', 'PRP', 'VBP', 'VBG', 'NNS', 'TO', 'VB', 'DT', 'NNS', 'RB', 'IN', 'DT', 'JJ', 'NNS', '.']",28
machine-translation,9,38,"More specifically , we create M codebooks E 1 , E 2 , ... , EM , each containing K codeword vectors .","['More', 'specifically', ',', 'we', 'create', 'M', 'codebooks', 'E', '1', ',', 'E', '2', ',', '...', ',', 'EM', ',', 'each', 'containing', 'K', 'codeword', 'vectors', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['RBR', 'RB', ',', 'PRP', 'VBP', 'NNP', 'VBZ', 'NNP', 'CD', ',', 'NNP', 'CD', ',', ':', ',', 'NNP', ',', 'DT', 'VBG', 'NNP', 'NN', 'NNS', '.']",23
machine-translation,9,39,The embedding of a word is computed by summing up the codewords corresponding to all the components in the code as,"['The', 'embedding', 'of', 'a', 'word', 'is', 'computed', 'by', 'summing', 'up', 'the', 'codewords', 'corresponding', 'to', 'all', 'the', 'components', 'in', 'the', 'code', 'as']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'VBG', 'RP', 'DT', 'NNS', 'VBG', 'TO', 'PDT', 'DT', 'NNS', 'IN', 'DT', 'NN', 'IN']",21
machine-translation,9,55,"In this work , we utilize such codes for a different purpose , that is , constructing word embeddings with drastically fewer parameters .","['In', 'this', 'work', ',', 'we', 'utilize', 'such', 'codes', 'for', 'a', 'different', 'purpose', ',', 'that', 'is', ',', 'constructing', 'word', 'embeddings', 'with', 'drastically', 'fewer', 'parameters', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', ',', 'WDT', 'VBZ', ',', 'VBG', 'NN', 'NNS', 'IN', 'RB', 'JJR', 'NNS', '.']",24
machine-translation,9,59,We utilize the Gumbel - softmax trick to find the best discrete codes that minimize the loss .,"['We', 'utilize', 'the', 'Gumbel', '-', 'softmax', 'trick', 'to', 'find', 'the', 'best', 'discrete', 'codes', 'that', 'minimize', 'the', 'loss', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', ':', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJS', 'JJ', 'NNS', 'WDT', 'VBP', 'DT', 'NN', '.']",18
machine-translation,9,165,CODE LEARNING,"['CODE', 'LEARNING']","['B-n', 'I-n']","['NNP', 'NNP']",2
machine-translation,9,171,"In each iteration , a small batch of the embeddings is sampled uniformly from the baseline embedding matrix .","['In', 'each', 'iteration', ',', 'a', 'small', 'batch', 'of', 'the', 'embeddings', 'is', 'sampled', 'uniformly', 'from', 'the', 'baseline', 'embedding', 'matrix', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNS', 'VBZ', 'VBN', 'RB', 'IN', 'DT', 'NN', 'VBG', 'NN', '.']",19
machine-translation,9,173,"In our experiments , the batch size is set to 128 .","['In', 'our', 'experiments', ',', 'the', 'batch', 'size', 'is', 'set', 'to', '128', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'O']","['IN', 'PRP$', 'NNS', ',', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",12
machine-translation,9,174,We use Adam optimizer with a fixed learning rate of 0.0001 .,"['We', 'use', 'Adam', 'optimizer', 'with', 'a', 'fixed', 'learning', 'rate', 'of', '0.0001', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'VBG', 'NN', 'IN', 'CD', '.']",12
machine-translation,9,175,The training is run for 200K iterations .,"['The', 'training', 'is', 'run', 'for', '200K', 'iterations', '.']","['O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'IN', 'CD', 'NNS', '.']",8
machine-translation,9,177,"We evenly distribute the model training to 4 GPUs using the nccl package , so that one round of code learning takes around 15 minutes to complete .","['We', 'evenly', 'distribute', 'the', 'model', 'training', 'to', '4', 'GPUs', 'using', 'the', 'nccl', 'package', ',', 'so', 'that', 'one', 'round', 'of', 'code', 'learning', 'takes', 'around', '15', 'minutes', 'to', 'complete', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', 'NN', 'TO', 'CD', 'NNP', 'VBG', 'DT', 'JJ', 'NN', ',', 'IN', 'DT', 'CD', 'NN', 'IN', 'NN', 'VBG', 'VBZ', 'IN', 'CD', 'NNS', 'TO', 'VB', '.']",28
machine-translation,9,178,SENTIMENT ANALYSIS,"['SENTIMENT', 'ANALYSIS']","['B-n', 'I-n']","['NN', 'NN']",2
machine-translation,9,193,The models are trained with Adam optimizer for 15 epochs with a fixed learning rate of 0.0001 .,"['The', 'models', 'are', 'trained', 'with', 'Adam', 'optimizer', 'for', '15', 'epochs', 'with', 'a', 'fixed', 'learning', 'rate', 'of', '0.0001', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'NN', 'IN', 'CD', 'NNS', 'IN', 'DT', 'JJ', 'VBG', 'NN', 'IN', 'CD', '.']",18
machine-translation,9,206,"For our proposed methods , the maximum loss - free compression rate is achieved by a 16 32 coding scheme .","['For', 'our', 'proposed', 'methods', ',', 'the', 'maximum', 'loss', '-', 'free', 'compression', 'rate', 'is', 'achieved', 'by', 'a', '16', '32', 'coding', 'scheme', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'PRP$', 'VBN', 'NNS', ',', 'DT', 'JJ', 'NN', ':', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'CD', 'CD', 'VBG', 'NN', '.']",21
machine-translation,9,208,We also found the classification accuracy can be substantially improved with a slightly lower compression rate .,"['We', 'also', 'found', 'the', 'classification', 'accuracy', 'can', 'be', 'substantially', 'improved', 'with', 'a', 'slightly', 'lower', 'compression', 'rate', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBD', 'DT', 'NN', 'NN', 'MD', 'VB', 'RB', 'VBN', 'IN', 'DT', 'RB', 'JJR', 'NN', 'NN', '.']",17
machine-translation,9,210,MACHINE TRANSLATION,"['MACHINE', 'TRANSLATION']","['B-n', 'I-n']","['NNP', 'NNP']",2
machine-translation,9,233,All models are trained by Nesterov 's accelerated gradient with an initial learning rate of 0.25 .,"['All', 'models', 'are', 'trained', 'by', 'Nesterov', ""'s"", 'accelerated', 'gradient', 'with', 'an', 'initial', 'learning', 'rate', 'of', '0.25', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'POS', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', '.']",17
machine-translation,9,237,"Similar to the code learning , the training is distributed to 4 GPUs , each GPU computes a mini-batch of 16 samples .","['Similar', 'to', 'the', 'code', 'learning', ',', 'the', 'training', 'is', 'distributed', 'to', '4', 'GPUs', ',', 'each', 'GPU', 'computes', 'a', 'mini-batch', 'of', '16', 'samples', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['JJ', 'TO', 'DT', 'NN', 'NN', ',', 'DT', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'NNP', ',', 'DT', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NNS', '.']",23
machine-translation,9,246,The loss - free compression rate reaches 92 % on ASPEC dataset by pruning 90 % of the connections .,"['The', 'loss', '-', 'free', 'compression', 'rate', 'reaches', '92', '%', 'on', 'ASPEC', 'dataset', 'by', 'pruning', '90', '%', 'of', 'the', 'connections', '.']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', ':', 'JJ', 'NN', 'NN', 'VBZ', 'CD', 'NN', 'IN', 'NNP', 'NN', 'IN', 'VBG', 'CD', 'NN', 'IN', 'DT', 'NNS', '.']",20
machine-translation,9,247,"However , with the same pruning ratio , a modest performance loss is observed in IWSLT14 dataset .","['However', ',', 'with', 'the', 'same', 'pruning', 'ratio', ',', 'a', 'modest', 'performance', 'loss', 'is', 'observed', 'in', 'IWSLT14', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'NNP', 'NN', '.']",18
machine-translation,9,248,"For the models using compositional coding , the loss - free compression rate is 94 % for the IWSLT14 dataset and 99 % for the ASPEC dataset .","['For', 'the', 'models', 'using', 'compositional', 'coding', ',', 'the', 'loss', '-', 'free', 'compression', 'rate', 'is', '94', '%', 'for', 'the', 'IWSLT14', 'dataset', 'and', '99', '%', 'for', 'the', 'ASPEC', 'dataset', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNS', 'VBG', 'JJ', 'NN', ',', 'DT', 'NN', ':', 'JJ', 'NN', 'NN', 'VBZ', 'CD', 'NN', 'IN', 'DT', 'NNP', 'NN', 'CC', 'CD', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",28
machine-translation,1,2,Neural Machine Translation in Linear Time,"['Neural', 'Machine', 'Translation', 'in', 'Linear', 'Time']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",6
machine-translation,1,10,The ByteNet decoder attains state - of - the - art performance on character - level language modelling and outperforms the previous best results obtained with recurrent networks .,"['The', 'ByteNet', 'decoder', 'attains', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'on', 'character', '-', 'level', 'language', 'modelling', 'and', 'outperforms', 'the', 'previous', 'best', 'results', 'obtained', 'with', 'recurrent', 'networks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NN', 'NNS', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'NN', ':', 'NN', 'NN', 'NN', 'CC', 'VBZ', 'DT', 'JJ', 'JJS', 'NNS', 'VBD', 'IN', 'JJ', 'NNS', '.']",29
machine-translation,1,11,"The ByteNet also achieves state - of - the - art performance on character - to - character machine translation on the English - to - German WMT translation task , surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time .","['The', 'ByteNet', 'also', 'achieves', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'on', 'character', '-', 'to', '-', 'character', 'machine', 'translation', 'on', 'the', 'English', '-', 'to', '-', 'German', 'WMT', 'translation', 'task', ',', 'surpassing', 'comparable', 'neural', 'translation', 'models', 'that', 'are', 'based', 'on', 'recurrent', 'networks', 'with', 'attentional', 'pooling', 'and', 'run', 'in', 'quadratic', 'time', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'RB', 'VBZ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'NN', ':', 'TO', ':', 'NN', 'NN', 'NN', 'IN', 'DT', 'NNP', ':', 'TO', ':', 'JJ', 'NNP', 'NN', 'NN', ',', 'VBG', 'JJ', 'JJ', 'NN', 'NNS', 'WDT', 'VBP', 'VBN', 'IN', 'NN', 'NNS', 'IN', 'JJ', 'NN', 'CC', 'VB', 'IN', 'JJ', 'NN', '.']",51
machine-translation,1,14,"In neural language modelling , a neural network estimates a distribution over sequences of words or characters that belong to a given language .","['In', 'neural', 'language', 'modelling', ',', 'a', 'neural', 'network', 'estimates', 'a', 'distribution', 'over', 'sequences', 'of', 'words', 'or', 'characters', 'that', 'belong', 'to', 'a', 'given', 'language', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', 'NN', 'NN', ',', 'DT', 'JJ', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'NNS', 'IN', 'NNS', 'CC', 'NNS', 'WDT', 'VBP', 'TO', 'DT', 'VBN', 'NN', '.']",24
machine-translation,1,15,"In neural machine translation , the network estimates a distribution over sequences in the target language conditioned on a given sequence in the source language .","['In', 'neural', 'machine', 'translation', ',', 'the', 'network', 'estimates', 'a', 'distribution', 'over', 'sequences', 'in', 'the', 'target', 'language', 'conditioned', 'on', 'a', 'given', 'sequence', 'in', 'the', 'source', 'language', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', 'NN', 'NN', ',', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'VBN', 'IN', 'DT', 'VBN', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",26
machine-translation,1,31,The ByteNet is the instance within this family of models that uses one - dimensional convolutional neural networks ( CNN ) of fixed depth for both the encoder and the decoder ) .,"['The', 'ByteNet', 'is', 'the', 'instance', 'within', 'this', 'family', 'of', 'models', 'that', 'uses', 'one', '-', 'dimensional', 'convolutional', 'neural', 'networks', '(', 'CNN', ')', 'of', 'fixed', 'depth', 'for', 'both', 'the', 'encoder', 'and', 'the', 'decoder', ')', '.']","['O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['DT', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNS', 'WDT', 'VBZ', 'CD', ':', 'JJ', 'JJ', 'JJ', 'NNS', '(', 'NNP', ')', 'IN', 'VBN', 'NN', 'IN', 'DT', 'DT', 'NN', 'CC', 'DT', 'NN', ')', '.']",33
machine-translation,1,32,The two CNNs use increasing factors of dilation to rapidly grow the receptive fields ; a similar technique is also used in .,"['The', 'two', 'CNNs', 'use', 'increasing', 'factors', 'of', 'dilation', 'to', 'rapidly', 'grow', 'the', 'receptive', 'fields', ';', 'a', 'similar', 'technique', 'is', 'also', 'used', 'in', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'CD', 'NNP', 'NN', 'VBG', 'NNS', 'IN', 'NN', 'TO', 'RB', 'VB', 'DT', 'JJ', 'NNS', ':', 'DT', 'JJ', 'NN', 'VBZ', 'RB', 'VBN', 'IN', '.']",23
machine-translation,1,33,The convolutions in the decoder CNN are masked to prevent the network from seeing future tokens in the target sequence .,"['The', 'convolutions', 'in', 'the', 'decoder', 'CNN', 'are', 'masked', 'to', 'prevent', 'the', 'network', 'from', 'seeing', 'future', 'tokens', 'in', 'the', 'target', 'sequence', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'DT', 'NN', 'NNP', 'VBP', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",21
machine-translation,1,34,The network has beneficial computational and learning properties .,"['The', 'network', 'has', 'beneficial', 'computational', 'and', 'learning', 'properties', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'JJ', 'NN', 'CC', 'NN', 'NNS', '.']",9
machine-translation,1,35,"From a computational perspective , the network has a running time that is linear in the length of the source and target sequences ( up to a constant c ?","['From', 'a', 'computational', 'perspective', ',', 'the', 'network', 'has', 'a', 'running', 'time', 'that', 'is', 'linear', 'in', 'the', 'length', 'of', 'the', 'source', 'and', 'target', 'sequences', '(', 'up', 'to', 'a', 'constant', 'c', '?']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'JJ', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'CC', 'NN', 'NNS', '(', 'RB', 'TO', 'DT', 'JJ', 'NN', '.']",30
machine-translation,1,38,"From a learning perspective , the representation of the source sequence in the ByteNet is resolution preserving ; the representation sidesteps the need for memorization and allows for maximal bandwidth between encoder and decoder .","['From', 'a', 'learning', 'perspective', ',', 'the', 'representation', 'of', 'the', 'source', 'sequence', 'in', 'the', 'ByteNet', 'is', 'resolution', 'preserving', ';', 'the', 'representation', 'sidesteps', 'the', 'need', 'for', 'memorization', 'and', 'allows', 'for', 'maximal', 'bandwidth', 'between', 'encoder', 'and', 'decoder', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'VBG', 'NN', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NNP', 'VBZ', 'JJ', 'NN', ':', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'NN', 'CC', 'VBZ', 'IN', 'JJ', 'NN', 'IN', 'NN', 'CC', 'NN', '.']",35
machine-translation,1,152,Character Prediction,"['Character', 'Prediction']","['B-n', 'I-n']","['NNP', 'NNP']",2
machine-translation,1,156,"The ByteNet Decoder that we use for the result has 30 residual blocks split into six sets of five blocks each ; for the five blocks in each set the dilation rates are , respectively , 1 , 2 , 4 , 8 and 16 .","['The', 'ByteNet', 'Decoder', 'that', 'we', 'use', 'for', 'the', 'result', 'has', '30', 'residual', 'blocks', 'split', 'into', 'six', 'sets', 'of', 'five', 'blocks', 'each', ';', 'for', 'the', 'five', 'blocks', 'in', 'each', 'set', 'the', 'dilation', 'rates', 'are', ',', 'respectively', ',', '1', ',', '2', ',', '4', ',', '8', 'and', '16', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'NNP', 'IN', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'VBZ', 'CD', 'JJ', 'NNS', 'VBD', 'IN', 'CD', 'NNS', 'IN', 'CD', 'NNS', 'DT', ':', 'IN', 'DT', 'CD', 'NNS', 'IN', 'DT', 'NN', 'DT', 'NN', 'NNS', 'VBP', ',', 'RB', ',', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', 'CC', 'CD', '.']",46
machine-translation,1,157,The masked kernel has size 3 .,"['The', 'masked', 'kernel', 'has', 'size', '3', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'NN', 'CD', '.']",7
machine-translation,1,159,The number of hidden units dis 512 .,"['The', 'number', 'of', 'hidden', 'units', 'dis', '512', '.']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'JJ', 'NNS', 'VBP', 'CD', '.']",8
machine-translation,1,161,For the optimization we use Adam with a learning rate of 0.0003 and a weight decay term of 0.0001 .,"['For', 'the', 'optimization', 'we', 'use', 'Adam', 'with', 'a', 'learning', 'rate', 'of', '0.0003', 'and', 'a', 'weight', 'decay', 'term', 'of', '0.0001', '.']","['O', 'O', 'B-p', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'JJ', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', 'CC', 'DT', 'NN', 'JJ', 'NN', 'IN', 'CD', '.']",20
machine-translation,1,162,We apply dropout to the last ReLU layer before the softmax dropping units with a probability of 0.1 .,"['We', 'apply', 'dropout', 'to', 'the', 'last', 'ReLU', 'layer', 'before', 'the', 'softmax', 'dropping', 'units', 'with', 'a', 'probability', 'of', '0.1', '.']","['O', 'B-p', 'B-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'RB', 'TO', 'DT', 'JJ', 'NNP', 'NN', 'IN', 'DT', 'NN', 'VBG', 'NNS', 'IN', 'DT', 'NN', 'IN', 'CD', '.']",19
machine-translation,1,164,"At each step we sample a batch of sequences of 500 characters each , use the first 100 characters as the minimum context and predict the latter 400 characters .","['At', 'each', 'step', 'we', 'sample', 'a', 'batch', 'of', 'sequences', 'of', '500', 'characters', 'each', ',', 'use', 'the', 'first', '100', 'characters', 'as', 'the', 'minimum', 'context', 'and', 'predict', 'the', 'latter', '400', 'characters', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NNS', 'IN', 'CD', 'NNS', 'DT', ',', 'VBP', 'DT', 'JJ', 'CD', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'CC', 'VBP', 'DT', 'JJ', 'CD', 'NNS', '.']",30
machine-translation,1,167,The ByteNet decoder achieves 1.31 bits / character on the test set .,"['The', 'ByteNet', 'decoder', 'achieves', '1.31', 'bits', '/', 'character', 'on', 'the', 'test', 'set', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNP', 'NN', 'VBZ', 'CD', 'NNS', 'VBP', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",13
machine-translation,1,168,Character - Level Machine Translation,"['Character', '-', 'Level', 'Machine', 'Translation']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NN', 'NNP', 'NNP']",5
machine-translation,1,174,The ByteNet used in the experiments has 30 residual blocks in the encoder and 30 residual blocks in the decoder .,"['The', 'ByteNet', 'used', 'in', 'the', 'experiments', 'has', '30', 'residual', 'blocks', 'in', 'the', 'encoder', 'and', '30', 'residual', 'blocks', 'in', 'the', 'decoder', '.']","['O', 'B-n', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'VBN', 'IN', 'DT', 'NNS', 'VBZ', 'CD', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CC', 'CD', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']",21
machine-translation,1,175,"As in the ByteNet Decoder , the residual blocks are arranged in sets of five with corresponding dilation rates of 1 , 2 , 4 , 8 and 16 .","['As', 'in', 'the', 'ByteNet', 'Decoder', ',', 'the', 'residual', 'blocks', 'are', 'arranged', 'in', 'sets', 'of', 'five', 'with', 'corresponding', 'dilation', 'rates', 'of', '1', ',', '2', ',', '4', ',', '8', 'and', '16', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'IN', 'DT', 'NNP', 'NNP', ',', 'DT', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'NNS', 'IN', 'CD', 'IN', 'VBG', 'NN', 'NNS', 'IN', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', 'CC', 'CD', '.']",30
machine-translation,1,176,For this task we use the residual blocks with ReLUs ( .,"['For', 'this', 'task', 'we', 'use', 'the', 'residual', 'blocks', 'with', 'ReLUs', '(', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'O', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'DT', 'JJ', 'NNS', 'IN', 'NNP', '(', '.']",12
machine-translation,1,177,The number of hidden units dis 800 .,"['The', 'number', 'of', 'hidden', 'units', 'dis', '800', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'JJ', 'NNS', 'VBP', 'CD', '.']",8
machine-translation,1,178,"The size of the kernel in the source network is 3 , whereas the size of the masked kernel in the target network is 3 .","['The', 'size', 'of', 'the', 'kernel', 'in', 'the', 'source', 'network', 'is', '3', ',', 'whereas', 'the', 'size', 'of', 'the', 'masked', 'kernel', 'in', 'the', 'target', 'network', 'is', '3', '.']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'CD', ',', 'VBD', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'CD', '.']",26
machine-translation,1,179,For the optimization we use Adam with a learning rate of 0.0003 .,"['For', 'the', 'optimization', 'we', 'use', 'Adam', 'with', 'a', 'learning', 'rate', 'of', '0.0003', '.']","['O', 'O', 'B-p', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'JJ', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', '.']",13
machine-translation,1,180,Each sentence is padded with special characters to the nearest greater multiple of 50 ; 20 % of further padding is ap - plied to each source sentence as apart of dynamic unfolding ( eq. 2 ) .,"['Each', 'sentence', 'is', 'padded', 'with', 'special', 'characters', 'to', 'the', 'nearest', 'greater', 'multiple', 'of', '50', ';', '20', '%', 'of', 'further', 'padding', 'is', 'ap', '-', 'plied', 'to', 'each', 'source', 'sentence', 'as', 'apart', 'of', 'dynamic', 'unfolding', '(', 'eq.', '2', ')', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', 'NNS', 'TO', 'DT', 'JJS', 'JJR', 'NN', 'IN', 'CD', ':', 'CD', 'NN', 'IN', 'JJ', 'NN', 'VBZ', 'JJ', ':', 'VBN', 'TO', 'DT', 'NN', 'NN', 'IN', 'NN', 'IN', 'JJ', 'JJ', '(', 'JJ', 'CD', ')', '.']",38
machine-translation,1,181,Each pair of sentences is mapped to a bucket based on the pair of padded lengths for efficient batching during training .,"['Each', 'pair', 'of', 'sentences', 'is', 'mapped', 'to', 'a', 'bucket', 'based', 'on', 'the', 'pair', 'of', 'padded', 'lengths', 'for', 'efficient', 'batching', 'during', 'training', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'NNS', 'VBZ', 'VBN', 'TO', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'VBG', 'IN', 'NN', '.']",22
machine-translation,1,182,We use vanilla beam search according to the total likelihood of the generated candidate and accept only candidates which end in a end -of - sentence token .,"['We', 'use', 'vanilla', 'beam', 'search', 'according', 'to', 'the', 'total', 'likelihood', 'of', 'the', 'generated', 'candidate', 'and', 'accept', 'only', 'candidates', 'which', 'end', 'in', 'a', 'end', '-of', '-', 'sentence', 'token', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'NN', 'VBG', 'TO', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'CC', 'VB', 'RB', 'NNS', 'WDT', 'VBP', 'IN', 'DT', 'NN', 'SYM', ':', 'NN', 'NN', '.']",28
machine-translation,1,183,We use a beam of size 12 .,"['We', 'use', 'a', 'beam', 'of', 'size', '12', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'CD', '.']",8
machine-translation,1,186,"On NewsTest 2014 the ByteNet achieves the highest performance in character - level and subword - level neural machine translation , and compared to the word - level systems it is second only to the version of GNMT that uses word - pieces .","['On', 'NewsTest', '2014', 'the', 'ByteNet', 'achieves', 'the', 'highest', 'performance', 'in', 'character', '-', 'level', 'and', 'subword', '-', 'level', 'neural', 'machine', 'translation', ',', 'and', 'compared', 'to', 'the', 'word', '-', 'level', 'systems', 'it', 'is', 'second', 'only', 'to', 'the', 'version', 'of', 'GNMT', 'that', 'uses', 'word', '-', 'pieces', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', 'CD', 'DT', 'NNP', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'NN', ':', 'NN', 'CC', 'JJ', ':', 'NN', 'JJ', 'NN', 'NN', ',', 'CC', 'VBN', 'TO', 'DT', 'NN', ':', 'NN', 'NNS', 'PRP', 'VBZ', 'JJ', 'RB', 'TO', 'DT', 'NN', 'IN', 'NNP', 'WDT', 'VBZ', 'NN', ':', 'NNS', '.']",44
machine-translation,1,187,"On NewsTest 2015 , to our knowledge , ByteNet achieves the best published results to date .","['On', 'NewsTest', '2015', ',', 'to', 'our', 'knowledge', ',', 'ByteNet', 'achieves', 'the', 'best', 'published', 'results', 'to', 'date', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', 'CD', ',', 'TO', 'PRP$', 'NN', ',', 'NNP', 'VBZ', 'DT', 'JJS', 'VBN', 'NNS', 'TO', 'NN', '.']",17
machine-translation,5,2,Tilde 's Machine Translation Systems for WMT 2018,"['Tilde', ""'s"", 'Machine', 'Translation', 'Systems', 'for', 'WMT', '2018']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['NNP', 'POS', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'CD']",8
machine-translation,5,4,The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation .,"['The', 'paper', 'describes', 'the', 'development', 'process', 'of', 'the', 'Tilde', ""'s"", 'NMT', 'systems', 'that', 'were', 'submitted', 'for', 'the', 'WMT', '2018', 'shared', 'task', 'on', 'news', 'translation', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'DT', 'NNP', 'POS', 'NNP', 'NNS', 'WDT', 'VBD', 'VBN', 'IN', 'DT', 'NNP', 'CD', 'VBD', 'NN', 'IN', 'NN', 'NN', '.']",25
machine-translation,5,9,Neural machine translation ( NMT ) is a rapidly changing research area .,"['Neural', 'machine', 'translation', '(', 'NMT', ')', 'is', 'a', 'rapidly', 'changing', 'research', 'area', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'DT', 'RB', 'VBG', 'NN', 'NN', '.']",13
machine-translation,5,25,"For the WMT 2018 shared task on news translation , Tilde submitted both constrained and unconstrained NMT systems ( 7 in total ) .","['For', 'the', 'WMT', '2018', 'shared', 'task', 'on', 'news', 'translation', ',', 'Tilde', 'submitted', 'both', 'constrained', 'and', 'unconstrained', 'NMT', 'systems', '(', '7', 'in', 'total', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNP', 'CD', 'VBD', 'NN', 'IN', 'NN', 'NN', ',', 'NNP', 'VBD', 'DT', 'VBN', 'CC', 'JJ', 'NNP', 'NNS', '(', 'CD', 'IN', 'JJ', ')', '.']",24
machine-translation,5,26,The following is a list of the five MT systems submitted :,"['The', 'following', 'is', 'a', 'list', 'of', 'the', 'five', 'MT', 'systems', 'submitted', ':']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O']","['DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'CD', 'NNP', 'NNS', 'VBD', ':']",12
machine-translation,5,27,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c-nmt ) that were deployed as ensembles of averaged factored data ( see Section 3 ) Transformer models .,"['Constrained', 'English', '-', 'Estonian', 'and', 'Estonian', '-', 'English', 'NMT', 'systems', '(', 'tilde', '-', 'c-nmt', ')', 'that', 'were', 'deployed', 'as', 'ensembles', 'of', 'averaged', 'factored', 'data', '(', 'see', 'Section', '3', ')', 'Transformer', 'models', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'JJ', ':', 'JJ', 'CC', 'JJ', ':', 'JJ', 'NNP', 'NNS', '(', 'JJ', ':', 'NN', ')', 'WDT', 'VBD', 'VBN', 'IN', 'NNS', 'IN', 'JJ', 'VBN', 'NNS', '(', 'VB', 'NNP', 'CD', ')', 'JJR', 'NNS', '.']",32
machine-translation,5,28,The models were trained using parallel data and back - translated data in a 1 - to - 1 proportion .,"['The', 'models', 'were', 'trained', 'using', 'parallel', 'data', 'and', 'back', '-', 'translated', 'data', 'in', 'a', '1', '-', 'to', '-', '1', 'proportion', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBD', 'VBN', 'VBG', 'JJ', 'NNS', 'CC', 'SYM', ':', 'VBN', 'NNS', 'IN', 'DT', 'CD', ':', 'TO', ':', 'CD', 'NN', '.']",21
machine-translation,5,29,Unconstrained English - Estonian and Estonian - English NMT systems ( tilde - nc - nmt ) that were deployed as averaged Transformer models .,"['Unconstrained', 'English', '-', 'Estonian', 'and', 'Estonian', '-', 'English', 'NMT', 'systems', '(', 'tilde', '-', 'nc', '-', 'nmt', ')', 'that', 'were', 'deployed', 'as', 'averaged', 'Transformer', 'models', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'JJ', ':', 'JJ', 'CC', 'JJ', ':', 'JJ', 'NNP', 'NNS', '(', 'JJ', ':', 'JJ', ':', 'NN', ')', 'WDT', 'VBD', 'VBN', 'IN', 'JJ', 'NNP', 'NNS', '.']",25
machine-translation,5,30,"These models were also trained using back - translated data similarly to the constrained systems , however , the data , taking into account their relatively large size , were not factored .","['These', 'models', 'were', 'also', 'trained', 'using', 'back', '-', 'translated', 'data', 'similarly', 'to', 'the', 'constrained', 'systems', ',', 'however', ',', 'the', 'data', ',', 'taking', 'into', 'account', 'their', 'relatively', 'large', 'size', ',', 'were', 'not', 'factored', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NNS', 'VBD', 'RB', 'VBN', 'VBG', 'RB', ':', 'VBN', 'NNS', 'RB', 'TO', 'DT', 'VBN', 'NNS', ',', 'RB', ',', 'DT', 'NN', ',', 'VBG', 'IN', 'PDT', 'PRP$', 'RB', 'JJ', 'NN', ',', 'VBD', 'RB', 'VBN', '.']",33
machine-translation,5,31,A constrained Estonian - English NMT system ( tilde - c - nmt - comb ) that is a system combination of six factored data NMT systems .,"['A', 'constrained', 'Estonian', '-', 'English', 'NMT', 'system', '(', 'tilde', '-', 'c', '-', 'nmt', '-', 'comb', ')', 'that', 'is', 'a', 'system', 'combination', 'of', 'six', 'factored', 'data', 'NMT', 'systems', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NNP', ':', 'JJ', 'NNP', 'NN', '(', 'JJ', ':', 'SYM', ':', 'JJ', ':', 'NN', ')', 'WDT', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'CD', 'JJ', 'NNS', 'NNP', 'NNS', '.']",28
machine-translation,5,32,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c - nmt - 2 bt ) averaged from multiple best NMT models .,"['Constrained', 'English', '-', 'Estonian', 'and', 'Estonian', '-', 'English', 'NMT', 'systems', '(', 'tilde', '-', 'c', '-', 'nmt', '-', '2', 'bt', ')', 'averaged', 'from', 'multiple', 'best', 'NMT', 'models', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'JJ', ':', 'JJ', 'CC', 'JJ', ':', 'JJ', 'NNP', 'NNS', '(', 'JJ', ':', 'SYM', ':', 'JJ', ':', 'CD', 'NN', ')', 'VBD', 'IN', 'JJ', 'JJS', 'NN', 'NNS', '.']",27
machine-translation,5,33,The models were trained using two sets of back - translated data in a 1 - to - 1 proportion to the clean parallel data - one set was backtranslated using a system trained on parallelonly data and the other set -using an NMT system trained on parallel data and the first set of back - translated data .,"['The', 'models', 'were', 'trained', 'using', 'two', 'sets', 'of', 'back', '-', 'translated', 'data', 'in', 'a', '1', '-', 'to', '-', '1', 'proportion', 'to', 'the', 'clean', 'parallel', 'data', '-', 'one', 'set', 'was', 'backtranslated', 'using', 'a', 'system', 'trained', 'on', 'parallelonly', 'data', 'and', 'the', 'other', 'set', '-using', 'an', 'NMT', 'system', 'trained', 'on', 'parallel', 'data', 'and', 'the', 'first', 'set', 'of', 'back', '-', 'translated', 'data', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'VBD', 'VBN', 'VBG', 'CD', 'NNS', 'IN', 'JJ', ':', 'VBN', 'NNS', 'IN', 'DT', 'CD', ':', 'TO', ':', 'CD', 'NN', 'TO', 'DT', 'JJ', 'NN', 'NNS', ':', 'CD', 'NN', 'VBD', 'VBN', 'VBG', 'DT', 'NN', 'VBN', 'IN', 'RB', 'NNS', 'CC', 'DT', 'JJ', 'NN', 'VBG', 'DT', 'NNP', 'NN', 'VBD', 'IN', 'JJ', 'NNS', 'CC', 'DT', 'JJ', 'NN', 'IN', 'JJ', ':', 'VBN', 'NNS', '.']",59
machine-translation,5,129,The results ( see ) show that the Transformer models achieved better results than the MLSTM - based models .,"['The', 'results', '(', 'see', ')', 'show', 'that', 'the', 'Transformer', 'models', 'achieved', 'better', 'results', 'than', 'the', 'MLSTM', '-', 'based', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', '(', 'VB', ')', 'VB', 'IN', 'DT', 'NNP', 'NNS', 'VBN', 'RBR', 'NNS', 'IN', 'DT', 'NNP', ':', 'VBN', 'NNS', '.']",20
machine-translation,5,130,"For the constrained scenarios , both ensembles of averaged models achieved higher scores than each individual averaged model .","['For', 'the', 'constrained', 'scenarios', ',', 'both', 'ensembles', 'of', 'averaged', 'models', 'achieved', 'higher', 'scores', 'than', 'each', 'individual', 'averaged', 'model', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NNS', ',', 'DT', 'NNS', 'IN', 'JJ', 'NNS', 'VBN', 'JJR', 'NNS', 'IN', 'DT', 'NN', 'VBD', 'NN', '.']",19
machine-translation,5,131,It is also evident that the unconstrained models ( tilde - nc - nmt ) achieved the best results .,"['It', 'is', 'also', 'evident', 'that', 'the', 'unconstrained', 'models', '(', 'tilde', '-', 'nc', '-', 'nmt', ')', 'achieved', 'the', 'best', 'results', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBZ', 'RB', 'JJ', 'IN', 'DT', 'JJ', 'NNS', '(', 'JJ', ':', 'JJ', ':', 'NN', ')', 'VBD', 'DT', 'JJS', 'NNS', '.']",20
machine-translation,7,5,"Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation .","['Conditional', 'computation', ',', 'where', 'parts', 'of', 'the', 'network', 'are', 'active', 'on', 'a', 'per-example', 'basis', ',', 'has', 'been', 'proposed', 'in', 'theory', 'as', 'away', 'of', 'dramatically', 'increasing', 'model', 'capacity', 'without', 'a', 'proportional', 'increase', 'in', 'computation', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'NN', ',', 'WRB', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'JJ', 'IN', 'DT', 'JJ', 'NN', ',', 'VBZ', 'VBN', 'VBN', 'IN', 'NN', 'IN', 'RB', 'IN', 'RB', 'VBG', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', '.']",34
machine-translation,7,16,Exploiting scale in both training data and model size has been central to the success of deep learning .,"['Exploiting', 'scale', 'in', 'both', 'training', 'data', 'and', 'model', 'size', 'has', 'been', 'central', 'to', 'the', 'success', 'of', 'deep', 'learning', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NN', 'IN', 'DT', 'NN', 'NNS', 'CC', 'NN', 'NN', 'VBZ', 'VBN', 'JJ', 'TO', 'DT', 'NN', 'IN', 'JJ', 'NN', '.']",19
machine-translation,7,45,Our approach to conditional computation is to introduce a new type of general purpose neural network component : a Sparsely - Gated Mixture - of - Experts Layer ( MoE ) .,"['Our', 'approach', 'to', 'conditional', 'computation', 'is', 'to', 'introduce', 'a', 'new', 'type', 'of', 'general', 'purpose', 'neural', 'network', 'component', ':', 'a', 'Sparsely', '-', 'Gated', 'Mixture', '-', 'of', '-', 'Experts', 'Layer', '(', 'MoE', ')', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'TO', 'JJ', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'JJ', 'JJ', 'NN', 'NN', ':', 'DT', 'NNP', ':', 'VBN', 'NNP', ':', 'IN', ':', 'NNS', 'NNP', '(', 'NNP', ')', '.']",32
machine-translation,7,46,"The MoE consists of a number of experts , each a simple feed - forward neural network , and a trainable gating network which selects a sparse combination of the experts to process each input ( see ) .","['The', 'MoE', 'consists', 'of', 'a', 'number', 'of', 'experts', ',', 'each', 'a', 'simple', 'feed', '-', 'forward', 'neural', 'network', ',', 'and', 'a', 'trainable', 'gating', 'network', 'which', 'selects', 'a', 'sparse', 'combination', 'of', 'the', 'experts', 'to', 'process', 'each', 'input', '(', 'see', ')', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'NNS', ',', 'DT', 'DT', 'JJ', 'NN', ':', 'RB', 'JJ', 'NN', ',', 'CC', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNS', 'TO', 'VB', 'DT', 'NN', '(', 'VB', ')', '.']",39
machine-translation,7,47,All parts of the network are trained jointly by back - propagation .,"['All', 'parts', 'of', 'the', 'network', 'are', 'trained', 'jointly', 'by', 'back', '-', 'propagation', '.']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'VBN', 'RB', 'IN', 'JJ', ':', 'NN', '.']",13
machine-translation,7,183,100 BILLION WORD GOOGLE NEWS CORPUS,"['100', 'BILLION', 'WORD', 'GOOGLE', 'NEWS', 'CORPUS']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['CD', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",6
machine-translation,7,186,"In addition to a baseline LSTM model , we trained models augmented with MoE layers containing 32 , experts .","['In', 'addition', 'to', 'a', 'baseline', 'LSTM', 'model', ',', 'we', 'trained', 'models', 'augmented', 'with', 'MoE', 'layers', 'containing', '32', ',', 'experts', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'DT', 'NN', 'NNP', 'NN', ',', 'PRP', 'VBD', 'NNS', 'VBN', 'IN', 'NNP', 'NNS', 'VBG', 'CD', ',', 'NNS', '.']",20
machine-translation,7,190,"When training over the full 100 billion words , test perplexity improves significantly up to 65536 experts ( 68 billion parameters ) , dropping 39 % lower than the computationally matched baseline , but degrades at 131072 experts , possibly a result of too much sparsity .","['When', 'training', 'over', 'the', 'full', '100', 'billion', 'words', ',', 'test', 'perplexity', 'improves', 'significantly', 'up', 'to', '65536', 'experts', '(', '68', 'billion', 'parameters', ')', ',', 'dropping', '39', '%', 'lower', 'than', 'the', 'computationally', 'matched', 'baseline', ',', 'but', 'degrades', 'at', '131072', 'experts', ',', 'possibly', 'a', 'result', 'of', 'too', 'much', 'sparsity', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['WRB', 'NN', 'IN', 'DT', 'JJ', 'CD', 'CD', 'NNS', ',', 'NN', 'NN', 'VBZ', 'RB', 'IN', 'TO', 'CD', 'NNS', '(', 'CD', 'CD', 'NNS', ')', ',', 'VBG', 'CD', 'NN', 'JJR', 'IN', 'DT', 'RB', 'JJ', 'NN', ',', 'CC', 'NNS', 'IN', 'CD', 'NNS', ',', 'RB', 'DT', 'NN', 'IN', 'RB', 'JJ', 'NN', '.']",47
machine-translation,7,193,MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR ),"['MACHINE', 'TRANSLATION', '(', 'SINGLE', 'LANGUAGE', 'PAIR', ')']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', '(', 'NNP', 'NNP', 'NNP', ')']",7
machine-translation,7,195,Our model was a modified version of the GNMT model described in .,"['Our', 'model', 'was', 'a', 'modified', 'version', 'of', 'the', 'GNMT', 'model', 'described', 'in', '.']","['O', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['PRP$', 'NN', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', 'VBD', 'IN', '.']",13
machine-translation,7,196,"To reduce computation , we decreased the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively .","['To', 'reduce', 'computation', ',', 'we', 'decreased', 'the', 'number', 'of', 'LSTM', 'layers', 'in', 'the', 'encoder', 'and', 'decoder', 'from', '9', 'and', '8', 'to', '3', 'and', '2', 'respectively', '.']","['B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'NN', ',', 'PRP', 'VBD', 'DT', 'NN', 'IN', 'NNP', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NN', 'IN', 'CD', 'CC', 'CD', 'TO', 'CD', 'CC', 'CD', 'RB', '.']",26
machine-translation,7,197,We inserted MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .,"['We', 'inserted', 'MoE', 'layers', 'in', 'both', 'the', 'encoder', '(', 'between', 'layers', '2', 'and', '3', ')', 'and', 'the', 'decoder', '(', 'between', 'layers', '1', 'and', '2', ')', '.']","['O', 'B-p', 'B-p', 'I-p', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBD', 'NNP', 'NNS', 'IN', 'DT', 'DT', 'NN', '(', 'IN', 'NNS', 'CD', 'CC', 'CD', ')', 'CC', 'DT', 'NN', '(', 'IN', 'NNS', 'CD', 'CC', 'CD', ')', '.']",26
machine-translation,7,198,"Each MoE layer contained up to 2048 experts each with about two million parameters , adding a total of about 8 billion parameters to the models .","['Each', 'MoE', 'layer', 'contained', 'up', 'to', '2048', 'experts', 'each', 'with', 'about', 'two', 'million', 'parameters', ',', 'adding', 'a', 'total', 'of', 'about', '8', 'billion', 'parameters', 'to', 'the', 'models', '.']","['B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NN', 'VBD', 'RB', 'TO', 'CD', 'NNS', 'DT', 'IN', 'RB', 'CD', 'CD', 'NNS', ',', 'VBG', 'DT', 'NN', 'IN', 'IN', 'CD', 'CD', 'NNS', 'TO', 'DT', 'NNS', '.']",27
machine-translation,7,206,Our approach achieved BLEU scores of 40.56 and 26.03 on the WMT ' 14 En?Fr and En ? De benchmarks .,"['Our', 'approach', 'achieved', 'BLEU', 'scores', 'of', '40.56', 'and', '26.03', 'on', 'the', 'WMT', ""'"", '14', 'En?Fr', 'and', 'En', '?', 'De', 'benchmarks', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBD', 'NNP', 'NNS', 'IN', 'CD', 'CC', 'CD', 'IN', 'DT', 'NNP', 'POS', 'CD', 'NNP', 'CC', 'NNP', '.', 'NNP', 'NN', '.']",21
machine-translation,7,210,"On the Google Production dataset , our model achieved 1.01 higher test BLEU score even after training for only one sixth of the time .","['On', 'the', 'Google', 'Production', 'dataset', ',', 'our', 'model', 'achieved', '1.01', 'higher', 'test', 'BLEU', 'score', 'even', 'after', 'training', 'for', 'only', 'one', 'sixth', 'of', 'the', 'time', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNP', 'NNP', 'NN', ',', 'PRP$', 'NN', 'VBD', 'CD', 'JJR', 'NN', 'NNP', 'VBD', 'RB', 'IN', 'VBG', 'IN', 'RB', 'CD', 'NN', 'IN', 'DT', 'NN', '.']",25
machine-translation,7,211,MULTILINGUAL MACHINE TRANSLATION,"['MULTILINGUAL', 'MACHINE', 'TRANSLATION']","['B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP']",3
machine-translation,7,214,The MoE model achieves 19 % lower perplexity on the dev set than the multilingual GNMT model .,"['The', 'MoE', 'model', 'achieves', '19', '%', 'lower', 'perplexity', 'on', 'the', 'dev', 'set', 'than', 'the', 'multilingual', 'GNMT', 'model', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'NN', 'VBZ', 'CD', 'NN', 'JJR', 'NN', 'IN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'JJ', 'NNP', 'NN', '.']",18
machine-translation,7,215,"On BLEU score , the MoE model significantly beats the multilingual GNMT model on 11 of the 12 language pairs ( by as much as 5.84 points ) , and even beats the monolingual GNMT models on 8 of 12 language pairs .","['On', 'BLEU', 'score', ',', 'the', 'MoE', 'model', 'significantly', 'beats', 'the', 'multilingual', 'GNMT', 'model', 'on', '11', 'of', 'the', '12', 'language', 'pairs', '(', 'by', 'as', 'much', 'as', '5.84', 'points', ')', ',', 'and', 'even', 'beats', 'the', 'monolingual', 'GNMT', 'models', 'on', '8', 'of', '12', 'language', 'pairs', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', 'NN', ',', 'DT', 'NNP', 'NN', 'RB', 'VBZ', 'DT', 'JJ', 'NNP', 'NN', 'IN', 'CD', 'IN', 'DT', 'CD', 'NN', 'NNS', '(', 'IN', 'RB', 'JJ', 'IN', 'CD', 'NNS', ')', ',', 'CC', 'RB', 'VBZ', 'DT', 'JJ', 'NNP', 'NNS', 'IN', 'CD', 'IN', 'CD', 'NN', 'NNS', '.']",43
machine-translation,4,4,Unsupervised neural machine translation ( NMT ) is a recently proposed approach for machine translation which aims to train the model without using any labeled data .,"['Unsupervised', 'neural', 'machine', 'translation', '(', 'NMT', ')', 'is', 'a', 'recently', 'proposed', 'approach', 'for', 'machine', 'translation', 'which', 'aims', 'to', 'train', 'the', 'model', 'without', 'using', 'any', 'labeled', 'data', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'DT', 'RB', 'VBN', 'NN', 'IN', 'NN', 'NN', 'WDT', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'IN', 'VBG', 'DT', 'VBN', 'NNS', '.']",27
machine-translation,4,5,"The models proposed for unsupervised NMT often use only one shared encoder to map the pairs of sentences from different languages to a shared - latent space , which is weak in keeping the unique and internal characteristics of each language , such as the style , terminology , and sentence structure .","['The', 'models', 'proposed', 'for', 'unsupervised', 'NMT', 'often', 'use', 'only', 'one', 'shared', 'encoder', 'to', 'map', 'the', 'pairs', 'of', 'sentences', 'from', 'different', 'languages', 'to', 'a', 'shared', '-', 'latent', 'space', ',', 'which', 'is', 'weak', 'in', 'keeping', 'the', 'unique', 'and', 'internal', 'characteristics', 'of', 'each', 'language', ',', 'such', 'as', 'the', 'style', ',', 'terminology', ',', 'and', 'sentence', 'structure', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'VBN', 'IN', 'JJ', 'NNP', 'RB', 'VBP', 'RB', 'CD', 'VBD', 'NN', 'TO', 'VB', 'DT', 'NNS', 'IN', 'NNS', 'IN', 'JJ', 'NNS', 'TO', 'DT', 'VBN', ':', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'JJ', 'IN', 'VBG', 'DT', 'NN', 'CC', 'JJ', 'NNS', 'IN', 'DT', 'NN', ',', 'JJ', 'IN', 'DT', 'NN', ',', 'NN', ',', 'CC', 'NN', 'NN', '.']",53
machine-translation,4,26,"In order to address this issue , we extend the encoder - shared model , i.e. , the model with one shared encoder , by leveraging two independent encoders with each for one language .","['In', 'order', 'to', 'address', 'this', 'issue', ',', 'we', 'extend', 'the', 'encoder', '-', 'shared', 'model', ',', 'i.e.', ',', 'the', 'model', 'with', 'one', 'shared', 'encoder', ',', 'by', 'leveraging', 'two', 'independent', 'encoders', 'with', 'each', 'for', 'one', 'language', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', ':', 'VBN', 'NN', ',', 'FW', ',', 'DT', 'NN', 'IN', 'CD', 'VBN', 'NN', ',', 'IN', 'VBG', 'CD', 'JJ', 'NNS', 'IN', 'DT', 'IN', 'CD', 'NN', '.']",35
machine-translation,4,27,"Similarly , two independent decoders are utilized .","['Similarly', ',', 'two', 'independent', 'decoders', 'are', 'utilized', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O']","['RB', ',', 'CD', 'JJ', 'NNS', 'VBP', 'JJ', '.']",8
machine-translation,4,28,"For each language , the encoder and its corresponding decoder perform an AE , where the encoder generates the latent representations from the perturbed input sentences and the decoder reconstructs the sentences from the latent representations .","['For', 'each', 'language', ',', 'the', 'encoder', 'and', 'its', 'corresponding', 'decoder', 'perform', 'an', 'AE', ',', 'where', 'the', 'encoder', 'generates', 'the', 'latent', 'representations', 'from', 'the', 'perturbed', 'input', 'sentences', 'and', 'the', 'decoder', 'reconstructs', 'the', 'sentences', 'from', 'the', 'latent', 'representations', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'DT', 'NN', 'CC', 'PRP$', 'JJ', 'NN', 'NN', 'DT', 'NNP', ',', 'WRB', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', 'NNS', 'CC', 'DT', 'NN', 'VBZ', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NNS', '.']",37
machine-translation,4,29,"To map the latent representations from different languages to a shared - latent space , we propose the weightsharing constraint to the two AEs .","['To', 'map', 'the', 'latent', 'representations', 'from', 'different', 'languages', 'to', 'a', 'shared', '-', 'latent', 'space', ',', 'we', 'propose', 'the', 'weightsharing', 'constraint', 'to', 'the', 'two', 'AEs', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', 'TO', 'DT', 'VBN', ':', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'DT', 'CD', 'NNP', '.']",25
machine-translation,4,33,"For cross - language translation , we utilize the backtranslation following .","['For', 'cross', '-', 'language', 'translation', ',', 'we', 'utilize', 'the', 'backtranslation', 'following', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O']","['IN', 'NN', ':', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBG', '.']",12
machine-translation,4,34,"Additionally , two different generative adversarial networks ( GAN ) , namely the local and global GAN , are proposed to further improve the cross - language translation .","['Additionally', ',', 'two', 'different', 'generative', 'adversarial', 'networks', '(', 'GAN', ')', ',', 'namely', 'the', 'local', 'and', 'global', 'GAN', ',', 'are', 'proposed', 'to', 'further', 'improve', 'the', 'cross', '-', 'language', 'translation', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'CD', 'JJ', 'JJ', 'NN', 'NNS', '(', 'NNP', ')', ',', 'RB', 'DT', 'JJ', 'CC', 'JJ', 'NNP', ',', 'VBP', 'VBN', 'TO', 'RBR', 'VB', 'DT', 'NN', ':', 'NN', 'NN', '.']",29
machine-translation,4,35,"We utilize the local GAN to constrain the source and target latent representations to have the same distribution , whereby the encoder tries to fool a local discriminator which is simultaneously trained to distinguish the language of a given latent representation .","['We', 'utilize', 'the', 'local', 'GAN', 'to', 'constrain', 'the', 'source', 'and', 'target', 'latent', 'representations', 'to', 'have', 'the', 'same', 'distribution', ',', 'whereby', 'the', 'encoder', 'tries', 'to', 'fool', 'a', 'local', 'discriminator', 'which', 'is', 'simultaneously', 'trained', 'to', 'distinguish', 'the', 'language', 'of', 'a', 'given', 'latent', 'representation', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NNP', 'TO', 'VB', 'DT', 'NN', 'CC', 'NN', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NN', ',', 'WRB', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'RB', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'VBN', 'JJ', 'NN', '.']",42
machine-translation,4,36,"We apply the global GAN to finetune the corresponding generator , i.e. , the composition of the encoder and decoder of the other language , where a global discriminator is leveraged to guide the training of the generator by assessing how far the generated sentence is from the true data distribution 1 .","['We', 'apply', 'the', 'global', 'GAN', 'to', 'finetune', 'the', 'corresponding', 'generator', ',', 'i.e.', ',', 'the', 'composition', 'of', 'the', 'encoder', 'and', 'decoder', 'of', 'the', 'other', 'language', ',', 'where', 'a', 'global', 'discriminator', 'is', 'leveraged', 'to', 'guide', 'the', 'training', 'of', 'the', 'generator', 'by', 'assessing', 'how', 'far', 'the', 'generated', 'sentence', 'is', 'from', 'the', 'true', 'data', 'distribution', '1', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NNP', 'TO', 'VB', 'DT', 'JJ', 'NN', ',', 'NN', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'CC', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'WRB', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'VBG', 'WRB', 'RB', 'DT', 'JJ', 'NN', 'VBZ', 'IN', 'DT', 'JJ', 'NNS', 'NN', 'CD', '.']",53
machine-translation,4,168,"Following the base model in , we set the dimension of word embedding as 512 , dropout rate as 0.1 and the head number as 8 .","['Following', 'the', 'base', 'model', 'in', ',', 'we', 'set', 'the', 'dimension', 'of', 'word', 'embedding', 'as', '512', ',', 'dropout', 'rate', 'as', '0.1', 'and', 'the', 'head', 'number', 'as', '8', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['VBG', 'DT', 'JJ', 'NN', 'IN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'NN', 'IN', 'CD', ',', 'NN', 'NN', 'IN', 'CD', 'CC', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",27
machine-translation,4,169,We use beam search with a beam size of 4 and length penalty ? = 0.6 .,"['We', 'use', 'beam', 'search', 'with', 'a', 'beam', 'size', 'of', '4', 'and', 'length', 'penalty', '?', '=', '0.6', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', 'CC', 'VB', 'NN', '.', '$', 'CD', '.']",17
machine-translation,4,170,The model is implemented in TensorFlow and trained on up to four K80 GPUs synchronously in a multi - GPU setup on a single machine .,"['The', 'model', 'is', 'implemented', 'in', 'TensorFlow', 'and', 'trained', 'on', 'up', 'to', 'four', 'K80', 'GPUs', 'synchronously', 'in', 'a', 'multi', '-', 'GPU', 'setup', 'on', 'a', 'single', 'machine', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'IN', 'NNP', 'CC', 'VBN', 'IN', 'IN', 'TO', 'CD', 'NNP', 'NNP', 'RB', 'IN', 'DT', 'NN', ':', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",26
machine-translation,4,179,Word - by - word translation ( WBW ) The first baseline we consider is a system that performs word - by - word translations using the inferred bilingual dictionary .,"['Word', '-', 'by', '-', 'word', 'translation', '(', 'WBW', ')', 'The', 'first', 'baseline', 'we', 'consider', 'is', 'a', 'system', 'that', 'performs', 'word', '-', 'by', '-', 'word', 'translations', 'using', 'the', 'inferred', 'bilingual', 'dictionary', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'IN', ':', 'NN', 'NN', '(', 'NNP', ')', 'DT', 'JJ', 'NN', 'PRP', 'VBP', 'VBZ', 'DT', 'NN', 'WDT', 'VBZ', 'NN', ':', 'IN', ':', 'NN', 'NNS', 'VBG', 'DT', 'JJ', 'JJ', 'NN', '.']",31
machine-translation,4,181,Lample et al .,"['Lample', 'et', 'al', '.']","['B-n', 'I-n', 'I-n', 'I-n']","['NNP', 'CC', 'NN', '.']",4
machine-translation,4,188,Supervised training,"['Supervised', 'training']","['B-n', 'I-n']","['JJ', 'NN']",2
machine-translation,4,192,Number of weight - sharing layers,"['Number', 'of', 'weight', '-', 'sharing', 'layers']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'IN', 'JJ', ':', 'NN', 'NNS']",6
machine-translation,4,199,And the best translation performance is achieved when only one layer is shared in our system .,"['And', 'the', 'best', 'translation', 'performance', 'is', 'achieved', 'when', 'only', 'one', 'layer', 'is', 'shared', 'in', 'our', 'system', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['CC', 'DT', 'JJS', 'NN', 'NN', 'VBZ', 'VBN', 'WRB', 'RB', 'CD', 'NN', 'VBZ', 'VBN', 'IN', 'PRP$', 'NN', '.']",17
machine-translation,4,200,"When all of the four layers are shared , i.e. , only one shared encoder is utilized , we get poor translation performance in all of the three translation tasks .","['When', 'all', 'of', 'the', 'four', 'layers', 'are', 'shared', ',', 'i.e.', ',', 'only', 'one', 'shared', 'encoder', 'is', 'utilized', ',', 'we', 'get', 'poor', 'translation', 'performance', 'in', 'all', 'of', 'the', 'three', 'translation', 'tasks', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['WRB', 'DT', 'IN', 'DT', 'CD', 'NNS', 'VBP', 'VBN', ',', 'FW', ',', 'RB', 'CD', 'VBN', 'NN', 'VBZ', 'JJ', ',', 'PRP', 'VBP', 'JJ', 'NN', 'NN', 'IN', 'DT', 'IN', 'DT', 'CD', 'NN', 'NNS', '.']",31
machine-translation,4,220,"The most critical component is the weight - sharing constraint , which is vital to map sentences of different languages to the shared - latent space .","['The', 'most', 'critical', 'component', 'is', 'the', 'weight', '-', 'sharing', 'constraint', ',', 'which', 'is', 'vital', 'to', 'map', 'sentences', 'of', 'different', 'languages', 'to', 'the', 'shared', '-', 'latent', 'space', '.']","['O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'RBS', 'JJ', 'NN', 'VBZ', 'DT', 'NN', ':', 'NN', 'NN', ',', 'WDT', 'VBZ', 'JJ', 'TO', 'VB', 'NNS', 'IN', 'JJ', 'NNS', 'TO', 'DT', 'VBN', ':', 'JJ', 'NN', '.']",27
machine-translation,4,221,The embedding - reinforced encoder also brings some improvement on all of the translation tasks .,"['The', 'embedding', '-', 'reinforced', 'encoder', 'also', 'brings', 'some', 'improvement', 'on', 'all', 'of', 'the', 'translation', 'tasks', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBG', ':', 'VBN', 'NN', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'IN', 'DT', 'NN', 'NNS', '.']",16
machine-translation,4,222,"When we remove the directional self - attention , we getup to - 0.3 BLEU points decline .","['When', 'we', 'remove', 'the', 'directional', 'self', '-', 'attention', ',', 'we', 'getup', 'to', '-', '0.3', 'BLEU', 'points', 'decline', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['WRB', 'PRP', 'VBP', 'DT', 'JJ', 'NN', ':', 'NN', ',', 'PRP', 'VBP', 'TO', ':', 'CD', 'NNP', 'NNS', 'NN', '.']",18
machine-translation,4,224,The GANs also significantly improve the translation performance of our system .,"['The', 'GANs', 'also', 'significantly', 'improve', 'the', 'translation', 'performance', 'of', 'our', 'system', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNP', 'RB', 'RB', 'VB', 'DT', 'NN', 'NN', 'IN', 'PRP$', 'NN', '.']",12
machine-translation,6,2,FRAGE : Frequency - Agnostic Word Representation,"['FRAGE', ':', 'Frequency', '-', 'Agnostic', 'Word', 'Representation']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NN', ':', 'NNP', ':', 'JJ', 'NNP', 'NN']",7
machine-translation,6,5,"Although it is widely accepted that words with similar semantics should be close to each other in the embedding space , we find that word embeddings learned in several tasks are biased towards word frequency : the embeddings of highfrequency and low - frequency words lie in different subregions of the embedding space , and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar .","['Although', 'it', 'is', 'widely', 'accepted', 'that', 'words', 'with', 'similar', 'semantics', 'should', 'be', 'close', 'to', 'each', 'other', 'in', 'the', 'embedding', 'space', ',', 'we', 'find', 'that', 'word', 'embeddings', 'learned', 'in', 'several', 'tasks', 'are', 'biased', 'towards', 'word', 'frequency', ':', 'the', 'embeddings', 'of', 'highfrequency', 'and', 'low', '-', 'frequency', 'words', 'lie', 'in', 'different', 'subregions', 'of', 'the', 'embedding', 'space', ',', 'and', 'the', 'embedding', 'of', 'a', 'rare', 'word', 'and', 'a', 'popular', 'word', 'can', 'be', 'far', 'from', 'each', 'other', 'even', 'if', 'they', 'are', 'semantically', 'similar', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PRP', 'VBZ', 'RB', 'VBN', 'IN', 'NNS', 'IN', 'JJ', 'NNS', 'MD', 'VB', 'RB', 'TO', 'DT', 'JJ', 'IN', 'DT', 'VBG', 'NN', ',', 'PRP', 'VBP', 'IN', 'NN', 'NNS', 'VBN', 'IN', 'JJ', 'NNS', 'VBP', 'VBN', 'NNS', 'NN', 'NN', ':', 'DT', 'NNS', 'IN', 'NN', 'CC', 'JJ', ':', 'NN', 'NNS', 'VBP', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'VBG', 'NN', ',', 'CC', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'CC', 'DT', 'JJ', 'NN', 'MD', 'VB', 'RB', 'IN', 'DT', 'JJ', 'RB', 'IN', 'PRP', 'VBP', 'RB', 'JJ', '.']",78
machine-translation,6,22,"Interestingly , the learned embeddings of rare words and popular words behave differently .","['Interestingly', ',', 'the', 'learned', 'embeddings', 'of', 'rare', 'words', 'and', 'popular', 'words', 'behave', 'differently', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', 'CC', 'JJ', 'NNS', 'VBP', 'RB', '.']",14
machine-translation,6,26,"As shown in ( a ) and 1 ( b ) , the embeddings of rare words and popular words actually lie in different subregions of the space .","['As', 'shown', 'in', '(', 'a', ')', 'and', '1', '(', 'b', ')', ',', 'the', 'embeddings', 'of', 'rare', 'words', 'and', 'popular', 'words', 'actually', 'lie', 'in', 'different', 'subregions', 'of', 'the', 'space', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'IN', '(', 'DT', ')', 'CC', 'CD', '(', 'NN', ')', ',', 'DT', 'NNS', 'IN', 'JJ', 'NNS', 'CC', 'JJ', 'NNS', 'RB', 'VBP', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']",29
machine-translation,6,28,We argue that the different behaviors of the embeddings of popular words and rare words are problematic .,"['We', 'argue', 'that', 'the', 'different', 'behaviors', 'of', 'the', 'embeddings', 'of', 'popular', 'words', 'and', 'rare', 'words', 'are', 'problematic', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'JJ', 'NNS', 'CC', 'JJ', 'NNS', 'VBP', 'JJ', '.']",18
machine-translation,6,36,"To address this problem , in this paper , we propose an adversarial training method to learn FRequency - AGnostic word Embedding ( FRAGE ) .","['To', 'address', 'this', 'problem', ',', 'in', 'this', 'paper', ',', 'we', 'propose', 'an', 'adversarial', 'training', 'method', 'to', 'learn', 'FRequency', '-', 'AGnostic', 'word', 'Embedding', '(', 'FRAGE', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'NNP', ':', 'JJ', 'NN', 'NNP', '(', 'NNP', ')', '.']",26
machine-translation,6,37,"For a given NLP task , in addition to minimize the task - specific loss by optimizing the task - specific parameters together with word embeddings , we introduce another discriminator , which takes a word embedding as input and classifies whether it is a popular / rare word .","['For', 'a', 'given', 'NLP', 'task', ',', 'in', 'addition', 'to', 'minimize', 'the', 'task', '-', 'specific', 'loss', 'by', 'optimizing', 'the', 'task', '-', 'specific', 'parameters', 'together', 'with', 'word', 'embeddings', ',', 'we', 'introduce', 'another', 'discriminator', ',', 'which', 'takes', 'a', 'word', 'embedding', 'as', 'input', 'and', 'classifies', 'whether', 'it', 'is', 'a', 'popular', '/', 'rare', 'word', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'VBN', 'NNP', 'NN', ',', 'IN', 'NN', 'TO', 'VB', 'DT', 'NN', ':', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'NN', ':', 'JJ', 'NNS', 'RB', 'IN', 'NN', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'NN', 'CC', 'VBZ', 'IN', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', 'JJ', 'NN', '.']",50
machine-translation,6,38,"The discriminator optimizes its parameters to maximize its classification accuracy , while word embeddings are optimized towards a low task - dependent loss as well as fooling the discriminator to mis-classify the popular and rare words .","['The', 'discriminator', 'optimizes', 'its', 'parameters', 'to', 'maximize', 'its', 'classification', 'accuracy', ',', 'while', 'word', 'embeddings', 'are', 'optimized', 'towards', 'a', 'low', 'task', '-', 'dependent', 'loss', 'as', 'well', 'as', 'fooling', 'the', 'discriminator', 'to', 'mis-classify', 'the', 'popular', 'and', 'rare', 'words', '.']","['O', 'B-n', 'B-p', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'PRP$', 'NNS', 'TO', 'VB', 'PRP$', 'NN', 'NN', ',', 'IN', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NN', ':', 'JJ', 'NN', 'RB', 'RB', 'IN', 'VBG', 'DT', 'NN', 'TO', 'VB', 'DT', 'JJ', 'CC', 'JJ', 'NNS', '.']",37
machine-translation,6,39,"When the whole training process converges and the system achieves an equilibrium , the discriminator can not well differentiate popular words from rare words .","['When', 'the', 'whole', 'training', 'process', 'converges', 'and', 'the', 'system', 'achieves', 'an', 'equilibrium', ',', 'the', 'discriminator', 'can', 'not', 'well', 'differentiate', 'popular', 'words', 'from', 'rare', 'words', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['WRB', 'DT', 'JJ', 'NN', 'NN', 'NNS', 'CC', 'DT', 'NN', 'VBZ', 'DT', 'NN', ',', 'DT', 'NN', 'MD', 'RB', 'RB', 'VB', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', '.']",25
machine-translation,6,177,"Word Similarity evaluates the performance of the learned word embeddings by calculating the word similarity : it evaluates whether the most similar words of a given word in the embedding space are consistent with the ground - truth , in terms of Spearman 's rank correlation .","['Word', 'Similarity', 'evaluates', 'the', 'performance', 'of', 'the', 'learned', 'word', 'embeddings', 'by', 'calculating', 'the', 'word', 'similarity', ':', 'it', 'evaluates', 'whether', 'the', 'most', 'similar', 'words', 'of', 'a', 'given', 'word', 'in', 'the', 'embedding', 'space', 'are', 'consistent', 'with', 'the', 'ground', '-', 'truth', ',', 'in', 'terms', 'of', 'Spearman', ""'s"", 'rank', 'correlation', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNS', 'IN', 'VBG', 'DT', 'NN', 'NN', ':', 'PRP', 'VBZ', 'IN', 'DT', 'RBS', 'JJ', 'NNS', 'IN', 'DT', 'VBN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBP', 'JJ', 'IN', 'DT', 'NN', ':', 'NN', ',', 'IN', 'NNS', 'IN', 'NNP', 'POS', 'NN', 'NN', '.']",47
machine-translation,6,178,"We use the skip - gram model as our baseline model , and train the embeddings using Enwik9 6 .","['We', 'use', 'the', 'skip', '-', 'gram', 'model', 'as', 'our', 'baseline', 'model', ',', 'and', 'train', 'the', 'embeddings', 'using', 'Enwik9', '6', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', ':', 'NN', 'NN', 'IN', 'PRP$', 'NN', 'NN', ',', 'CC', 'VB', 'DT', 'NNS', 'VBG', 'NNP', 'CD', '.']",20
machine-translation,6,179,"We test the baseline and our method on three datasets : RG65 , WS and RW .","['We', 'test', 'the', 'baseline', 'and', 'our', 'method', 'on', 'three', 'datasets', ':', 'RG65', ',', 'WS', 'and', 'RW', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'CC', 'PRP$', 'NN', 'IN', 'CD', 'NNS', ':', 'NNP', ',', 'NNP', 'CC', 'NNP', '.']",17
machine-translation,6,182,Language Modeling is a basic task in natural language processing .,"['Language', 'Modeling', 'is', 'a', 'basic', 'task', 'in', 'natural', 'language', 'processing', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",11
machine-translation,6,183,The goal is to predict the next word conditioned on previous words and the task is evaluated by perplexity .,"['The', 'goal', 'is', 'to', 'predict', 'the', 'next', 'word', 'conditioned', 'on', 'previous', 'words', 'and', 'the', 'task', 'is', 'evaluated', 'by', 'perplexity', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'JJ', 'NN', 'VBN', 'IN', 'JJ', 'NNS', 'CC', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'NN', '.']",20
machine-translation,6,184,"We do experiments on two widely used datasets , Penn Treebank ( PTB ) and WikiText - 2 ( WT2 ) .","['We', 'do', 'experiments', 'on', 'two', 'widely', 'used', 'datasets', ',', 'Penn', 'Treebank', '(', 'PTB', ')', 'and', 'WikiText', '-', '2', '(', 'WT2', ')', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNS', 'IN', 'CD', 'RB', 'VBN', 'NNS', ',', 'NNP', 'NNP', '(', 'NNP', ')', 'CC', 'NNP', ':', 'CD', '(', 'NNP', ')', '.']",22
machine-translation,6,185,"We choose two recent works as our baselines : the AWD - LSTM model and the AWD - LSTM - MoS model , which achieves state - of - the - art performance .","['We', 'choose', 'two', 'recent', 'works', 'as', 'our', 'baselines', ':', 'the', 'AWD', '-', 'LSTM', 'model', 'and', 'the', 'AWD', '-', 'LSTM', '-', 'MoS', 'model', ',', 'which', 'achieves', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'CD', 'JJ', 'NNS', 'IN', 'PRP$', 'NNS', ':', 'DT', 'NNP', ':', 'NNP', 'NN', 'CC', 'DT', 'NNP', ':', 'NNP', ':', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', '.']",34
machine-translation,6,186,Machine Translation is a popular task in both deep learning and natural language processing .,"['Machine', 'Translation', 'is', 'a', 'popular', 'task', 'in', 'both', 'deep', 'learning', 'and', 'natural', 'language', 'processing', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'CC', 'JJ', 'NN', 'NN', '.']",15
machine-translation,6,187,"We choose two datasets : WMT14 English - German and IWSLT14 German - English datasets , which are evaluated in terms of BLEU score .","['We', 'choose', 'two', 'datasets', ':', 'WMT14', 'English', '-', 'German', 'and', 'IWSLT14', 'German', '-', 'English', 'datasets', ',', 'which', 'are', 'evaluated', 'in', 'terms', 'of', 'BLEU', 'score', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'CD', 'NNS', ':', 'NNP', 'NNP', ':', 'JJ', 'CC', 'JJ', 'NNP', ':', 'JJ', 'NNS', ',', 'WDT', 'VBP', 'VBN', 'IN', 'NNS', 'IN', 'NNP', 'NN', '.']",25
machine-translation,6,188,"We use Transformer as the baseline model , which achieves state - of - the - art accuracy on multiple translation datasets .","['We', 'use', 'Transformer', 'as', 'the', 'baseline', 'model', ',', 'which', 'achieves', 'state', '-', 'of', '-', 'the', '-', 'art', 'accuracy', 'on', 'multiple', 'translation', 'datasets', '.']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'PRP', 'IN', 'DT', 'NN', 'NN', ',', 'WDT', 'VBZ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NNS', '.']",23
machine-translation,6,190,Text Classification is a conventional machine learning task and is evaluated by accuracy .,"['Text', 'Classification', 'is', 'a', 'conventional', 'machine', 'learning', 'task', 'and', 'is', 'evaluated', 'by', 'accuracy', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'VBG', 'NN', 'CC', 'VBZ', 'VBN', 'IN', 'NN', '.']",14
machine-translation,6,191,"Following the setting in , we implement a Recurrent CNN - based model and test it on AG 's news corpus ( AGs ) , IMDB movie review dataset ( IMDB ) and 20 Newsgroups ( 20 NG ) .","['Following', 'the', 'setting', 'in', ',', 'we', 'implement', 'a', 'Recurrent', 'CNN', '-', 'based', 'model', 'and', 'test', 'it', 'on', 'AG', ""'s"", 'news', 'corpus', '(', 'AGs', ')', ',', 'IMDB', 'movie', 'review', 'dataset', '(', 'IMDB', ')', 'and', '20', 'Newsgroups', '(', '20', 'NG', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'DT', 'NN', 'IN', ',', 'PRP', 'VBP', 'DT', 'NNP', 'NNP', ':', 'VBN', 'NN', 'CC', 'NN', 'PRP', 'IN', 'NNP', 'POS', 'NN', 'NN', '(', 'NNP', ')', ',', 'NNP', 'NN', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'CD', 'NNP', '(', 'CD', 'NNP', ')', '.']",40
machine-translation,6,214,Language Modeling,"['Language', 'Modeling']","['B-n', 'I-n']","['NN', 'VBG']",2
machine-translation,6,217,"In all these settings , our method outperforms the two baselines .","['In', 'all', 'these', 'settings', ',', 'our', 'method', 'outperforms', 'the', 'two', 'baselines', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'PDT', 'DT', 'NNS', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'CD', 'NNS', '.']",12
machine-translation,6,218,"On PTB dataset , our method improves the AWD - LSTM and AWD - LSTM - MoS baseline by 0.8/1.2/1.0 and 0.76/1.13/1.15 points in test set at different checkpoints .","['On', 'PTB', 'dataset', ',', 'our', 'method', 'improves', 'the', 'AWD', '-', 'LSTM', 'and', 'AWD', '-', 'LSTM', '-', 'MoS', 'baseline', 'by', '0.8/1.2/1.0', 'and', '0.76/1.13/1.15', 'points', 'in', 'test', 'set', 'at', 'different', 'checkpoints', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NNP', ':', 'NNP', 'NN', 'IN', 'CD', 'CC', 'CD', 'NNS', 'IN', 'NN', 'VBN', 'IN', 'JJ', 'NNS', '.']",30
machine-translation,6,219,"On WT2 dataset , which contains more rare words , our method achieves larger improvements .","['On', 'WT2', 'dataset', ',', 'which', 'contains', 'more', 'rare', 'words', ',', 'our', 'method', 'achieves', 'larger', 'improvements', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'JJR', 'JJ', 'NNS', ',', 'PRP$', 'NN', 'VBZ', 'JJR', 'NNS', '.']",16
machine-translation,6,220,"We improve the results of AWD - LSTM and AWD - LSTM - MoS by 2.3/2.4/2.7 and 1.15/1.72/1.54 in terms of test perplexity , respectively .","['We', 'improve', 'the', 'results', 'of', 'AWD', '-', 'LSTM', 'and', 'AWD', '-', 'LSTM', '-', 'MoS', 'by', '2.3/2.4/2.7', 'and', '1.15/1.72/1.54', 'in', 'terms', 'of', 'test', 'perplexity', ',', 'respectively', '.']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NNS', 'IN', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NNP', ':', 'NN', 'IN', 'CD', 'CC', 'CD', 'IN', 'NNS', 'IN', 'NN', 'NN', ',', 'RB', '.']",26
machine-translation,6,221,Machine Translation,"['Machine', 'Translation']","['B-n', 'I-n']","['NN', 'NN']",2
machine-translation,6,223,We outperform the baselines for 1.06/0.71 in the term of BLEU in transformer_base and transformer_big settings in WMT14 English - German : BLEU scores on test set on WMT2014 English - German and IWSLT German - English tasks .,"['We', 'outperform', 'the', 'baselines', 'for', '1.06/0.71', 'in', 'the', 'term', 'of', 'BLEU', 'in', 'transformer_base', 'and', 'transformer_big', 'settings', 'in', 'WMT14', 'English', '-', 'German', ':', 'BLEU', 'scores', 'on', 'test', 'set', 'on', 'WMT2014', 'English', '-', 'German', 'and', 'IWSLT', 'German', '-', 'English', 'tasks', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NNS', 'IN', 'CD', 'IN', 'DT', 'NN', 'IN', 'NNP', 'IN', 'NN', 'CC', 'JJ', 'NNS', 'IN', 'NNP', 'NNP', ':', 'JJ', ':', 'NNP', 'VBZ', 'IN', 'NN', 'VBN', 'IN', 'NNP', 'NNP', ':', 'JJ', 'CC', 'JJ', 'NNP', ':', 'JJ', 'NNS', '.']",39
machine-translation,6,225,The model learned from adversarial training also outperforms original one in IWSLT14 German - English task by 0.85 .,"['The', 'model', 'learned', 'from', 'adversarial', 'training', 'also', 'outperforms', 'original', 'one', 'in', 'IWSLT14', 'German', '-', 'English', 'task', 'by', '0.85', '.']","['O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'VBN', 'IN', 'JJ', 'NN', 'RB', 'VBZ', 'JJ', 'CD', 'IN', 'JJ', 'NNP', ':', 'JJ', 'NN', 'IN', 'CD', '.']",19
machine-translation,6,227,Text Classification,"['Text', 'Classification']","['B-n', 'I-n']","['NNP', 'NNP']",2
machine-translation,6,229,Our method outperforms the baseline method for 1.26%/0.66%/0.44 % on three different datasets .,"['Our', 'method', 'outperforms', 'the', 'baseline', 'method', 'for', '1.26%/0.66%/0.44', '%', 'on', 'three', 'different', 'datasets', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'CD', 'NN', 'IN', 'CD', 'JJ', 'NNS', '.']",14
machine-translation,2,6,"We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .","['We', 'propose', 'a', 'new', 'simple', 'network', 'architecture', ',', 'the', 'Transformer', ',', 'based', 'solely', 'on', 'attention', 'mechanisms', ',', 'dispensing', 'with', 'recurrence', 'and', 'convolutions', 'entirely', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', ',', 'DT', 'NNP', ',', 'VBN', 'RB', 'IN', 'NN', 'NNS', ',', 'VBG', 'IN', 'NN', 'CC', 'NNS', 'RB', '.']",24
machine-translation,2,22,Numerous efforts have since continued to push the boundaries of recurrent language models and encoder - decoder architectures .,"['Numerous', 'efforts', 'have', 'since', 'continued', 'to', 'push', 'the', 'boundaries', 'of', 'recurrent', 'language', 'models', 'and', 'encoder', '-', 'decoder', 'architectures', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'NNS', 'VBP', 'IN', 'VBN', 'TO', 'VB', 'DT', 'NNS', 'IN', 'JJ', 'NN', 'NNS', 'CC', 'VB', ':', 'NN', 'NNS', '.']",19
machine-translation,2,30,"In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .","['In', 'this', 'work', 'we', 'propose', 'the', 'Transformer', ',', 'a', 'model', 'architecture', 'eschewing', 'recurrence', 'and', 'instead', 'relying', 'entirely', 'on', 'an', 'attention', 'mechanism', 'to', 'draw', 'global', 'dependencies', 'between', 'input', 'and', 'output', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'DT', 'NNP', ',', 'DT', 'NN', 'NN', 'VBG', 'NN', 'CC', 'RB', 'VBG', 'RB', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'NN', 'CC', 'NN', '.']",30
machine-translation,2,48,Encoder and Decoder Stacks,"['Encoder', 'and', 'Decoder', 'Stacks']","['B-n', 'I-n', 'I-n', 'I-n']","['NN', 'CC', 'NNP', 'NNP']",4
machine-translation,2,49,Encoder :,"['Encoder', ':']","['B-n', 'O']","['NN', ':']",2
machine-translation,2,50,The encoder is composed of a stack of N = 6 identical layers .,"['The', 'encoder', 'is', 'composed', 'of', 'a', 'stack', 'of', 'N', '=', '6', 'identical', 'layers', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'CD', 'JJ', 'NNS', '.']",14
machine-translation,2,51,Each layer has two sub-layers .,"['Each', 'layer', 'has', 'two', 'sub-layers', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'CD', 'NNS', '.']",6
machine-translation,2,52,"The first is a multi-head self - attention mechanism , and the second is a simple , positionwise fully connected feed - forward network .","['The', 'first', 'is', 'a', 'multi-head', 'self', '-', 'attention', 'mechanism', ',', 'and', 'the', 'second', 'is', 'a', 'simple', ',', 'positionwise', 'fully', 'connected', 'feed', '-', 'forward', 'network', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'VBZ', 'DT', 'JJ', 'NN', ':', 'NN', 'NN', ',', 'CC', 'DT', 'JJ', 'VBZ', 'DT', 'JJ', ',', 'VB', 'RB', 'VBN', 'NN', ':', 'NN', 'NN', '.']",25
machine-translation,2,53,"We employ a residual connection around each of the two sub-layers , followed by layer normalization .","['We', 'employ', 'a', 'residual', 'connection', 'around', 'each', 'of', 'the', 'two', 'sub-layers', ',', 'followed', 'by', 'layer', 'normalization', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'IN', 'DT', 'CD', 'NNS', ',', 'VBN', 'IN', 'NN', 'NN', '.']",17
machine-translation,2,56,Decoder :,"['Decoder', ':']","['B-n', 'O']","['NN', ':']",2
machine-translation,2,57,The decoder is also composed of a stack of N = 6 identical layers .,"['The', 'decoder', 'is', 'also', 'composed', 'of', 'a', 'stack', 'of', 'N', '=', '6', 'identical', 'layers', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'CD', 'JJ', 'NNS', '.']",15
machine-translation,2,58,"In addition to the two sub-layers in each encoder layer , the decoder inserts a third sub - layer , which performs multi-head attention over the output of the encoder stack .","['In', 'addition', 'to', 'the', 'two', 'sub-layers', 'in', 'each', 'encoder', 'layer', ',', 'the', 'decoder', 'inserts', 'a', 'third', 'sub', '-', 'layer', ',', 'which', 'performs', 'multi-head', 'attention', 'over', 'the', 'output', 'of', 'the', 'encoder', 'stack', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'DT', 'CD', 'NNS', 'IN', 'DT', 'NN', 'NN', ',', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', ':', 'NN', ',', 'WDT', 'VBZ', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",32
machine-translation,2,59,"Similar to the encoder , we employ residual connections around each of the sub-layers , followed by layer normalization .","['Similar', 'to', 'the', 'encoder', ',', 'we', 'employ', 'residual', 'connections', 'around', 'each', 'of', 'the', 'sub-layers', ',', 'followed', 'by', 'layer', 'normalization', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['JJ', 'TO', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'IN', 'DT', 'IN', 'DT', 'NNS', ',', 'VBN', 'IN', 'NN', 'NN', '.']",20
machine-translation,2,60,We also modify the self - attention sub - layer in the decoder stack to prevent positions from attending to subsequent positions .,"['We', 'also', 'modify', 'the', 'self', '-', 'attention', 'sub', '-', 'layer', 'in', 'the', 'decoder', 'stack', 'to', 'prevent', 'positions', 'from', 'attending', 'to', 'subsequent', 'positions', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', ':', 'NN', 'SYM', ':', 'NN', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', 'NNS', 'IN', 'VBG', 'TO', 'JJ', 'NNS', '.']",23
machine-translation,2,62,Attention,['Attention'],['B-n'],['NN'],1
machine-translation,2,63,"An attention function can be described as mapping a query and a set of key - value pairs to an output , where the query , keys , values , and output are all vectors .","['An', 'attention', 'function', 'can', 'be', 'described', 'as', 'mapping', 'a', 'query', 'and', 'a', 'set', 'of', 'key', '-', 'value', 'pairs', 'to', 'an', 'output', ',', 'where', 'the', 'query', ',', 'keys', ',', 'values', ',', 'and', 'output', 'are', 'all', 'vectors', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', 'MD', 'VB', 'VBN', 'IN', 'VBG', 'DT', 'NN', 'CC', 'DT', 'NN', 'IN', 'JJ', ':', 'NN', 'VBZ', 'TO', 'DT', 'NN', ',', 'WRB', 'DT', 'NN', ',', 'NNS', ',', 'NNS', ',', 'CC', 'NN', 'VBP', 'DT', 'NNS', '.']",36
machine-translation,2,64,"The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .","['The', 'output', 'is', 'computed', 'as', 'a', 'weighted', 'sum', 'of', 'the', 'values', ',', 'where', 'the', 'weight', 'assigned', 'to', 'each', 'value', 'is', 'computed', 'by', 'a', 'compatibility', 'function', 'of', 'the', 'query', 'with', 'the', 'corresponding', 'key', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNS', ',', 'WRB', 'DT', 'NN', 'VBN', 'TO', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",33
machine-translation,2,65,Scaled Dot - Product Attention,"['Scaled', 'Dot', '-', 'Product', 'Attention']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['VBN', 'NNP', ':', 'NN', 'NN']",5
machine-translation,2,80,Multi - Head Attention,"['Multi', '-', 'Head', 'Attention']","['B-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NN', 'NN']",4
machine-translation,2,101,Position - wise Feed - Forward Networks,"['Position', '-', 'wise', 'Feed', '-', 'Forward', 'Networks']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NN', 'NNP', ':', 'NNP', 'NNP']",7
machine-translation,2,102,"In addition to attention sub - layers , each of the layers in our encoder and decoder contains a fully connected feed - forward network , which is applied to each position separately and identically .","['In', 'addition', 'to', 'attention', 'sub', '-', 'layers', ',', 'each', 'of', 'the', 'layers', 'in', 'our', 'encoder', 'and', 'decoder', 'contains', 'a', 'fully', 'connected', 'feed', '-', 'forward', 'network', ',', 'which', 'is', 'applied', 'to', 'each', 'position', 'separately', 'and', 'identically', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'NN', 'SYM', ':', 'NNS', ',', 'DT', 'IN', 'DT', 'NNS', 'IN', 'PRP$', 'NN', 'CC', 'NN', 'VBZ', 'DT', 'RB', 'VBN', 'NN', ':', 'NN', 'NN', ',', 'WDT', 'VBZ', 'VBN', 'TO', 'DT', 'NN', 'RB', 'CC', 'RB', '.']",36
machine-translation,2,103,This consists of two linear transformations with a ReLU activation in between .,"['This', 'consists', 'of', 'two', 'linear', 'transformations', 'with', 'a', 'ReLU', 'activation', 'in', 'between', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['DT', 'VBZ', 'IN', 'CD', 'JJ', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'IN', 'IN', '.']",13
machine-translation,2,108,Embeddings and Softmax,"['Embeddings', 'and', 'Softmax']","['B-n', 'I-n', 'I-n']","['NNS', 'CC', 'NNP']",3
machine-translation,2,109,"Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model .","['Similarly', 'to', 'other', 'sequence', 'transduction', 'models', ',', 'we', 'use', 'learned', 'embeddings', 'to', 'convert', 'the', 'input', 'tokens', 'and', 'output', 'tokens', 'to', 'vectors', 'of', 'dimension', 'd', 'model', '.']","['O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', 'TO', 'JJ', 'NN', 'NN', 'NNS', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'NN', 'NNS', 'CC', 'NN', 'NNS', 'TO', 'NNS', 'IN', 'NN', 'NN', 'NN', '.']",26
machine-translation,2,110,We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next - token probabilities .,"['We', 'also', 'use', 'the', 'usual', 'learned', 'linear', 'transformation', 'and', 'softmax', 'function', 'to', 'convert', 'the', 'decoder', 'output', 'to', 'predicted', 'next', '-', 'token', 'probabilities', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'JJ', 'VBD', 'JJ', 'NN', 'CC', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'TO', 'VBN', 'JJ', ':', 'JJ', 'NNS', '.']",23
machine-translation,2,113,Positional Encoding,"['Positional', 'Encoding']","['B-n', 'I-n']","['JJ', 'NN']",2
machine-translation,2,114,"Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the : Maximum path lengths , per-layer complexity and minimum number of sequential operations for different layer types .","['Since', 'our', 'model', 'contains', 'no', 'recurrence', 'and', 'no', 'convolution', ',', 'in', 'order', 'for', 'the', 'model', 'to', 'make', 'use', 'of', 'the', 'order', 'of', 'the', 'sequence', ',', 'we', 'must', 'inject', 'some', 'information', 'about', 'the', 'relative', 'or', 'absolute', 'position', 'of', 'the', ':', 'Maximum', 'path', 'lengths', ',', 'per-layer', 'complexity', 'and', 'minimum', 'number', 'of', 'sequential', 'operations', 'for', 'different', 'layer', 'types', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PRP$', 'NN', 'VBZ', 'DT', 'NN', 'CC', 'DT', 'NN', ',', 'IN', 'NN', 'IN', 'DT', 'NN', 'TO', 'VB', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'PRP', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'CC', 'JJ', 'NN', 'IN', 'DT', ':', 'JJ', 'NN', 'NNS', ',', 'JJ', 'NN', 'CC', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NNS', '.']",56
machine-translation,2,117,tokens in the sequence .,"['tokens', 'in', 'the', 'sequence', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'IN', 'DT', 'NN', '.']",5
machine-translation,2,118,"To this end , we add "" positional encodings "" to the input embeddings at the bottoms of the encoder and decoder stacks .","['To', 'this', 'end', ',', 'we', 'add', '""', 'positional', 'encodings', '""', 'to', 'the', 'input', 'embeddings', 'at', 'the', 'bottoms', 'of', 'the', 'encoder', 'and', 'decoder', 'stacks', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'JJ', 'NNS', 'VBP', 'TO', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NN', 'NNS', '.']",24
machine-translation,2,159,Hardware and Schedule,"['Hardware', 'and', 'Schedule']","['B-n', 'I-n', 'I-n']","['NNP', 'CC', 'NNP']",3
machine-translation,2,160,We trained our models on one machine with 8 NVIDIA P100 GPUs .,"['We', 'trained', 'our', 'models', 'on', 'one', 'machine', 'with', '8', 'NVIDIA', 'P100', 'GPUs', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'PRP$', 'NNS', 'IN', 'CD', 'NN', 'IN', 'CD', 'NNP', 'NNP', 'NNP', '.']",13
machine-translation,2,162,"We trained the base models for a total of 100,000 steps or 12 hours .","['We', 'trained', 'the', 'base', 'models', 'for', 'a', 'total', 'of', '100,000', 'steps', 'or', '12', 'hours', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'CD', 'NNS', 'CC', 'CD', 'NNS', '.']",15
machine-translation,2,164,"The big models were trained for 300,000 steps ( 3.5 days ) .","['The', 'big', 'models', 'were', 'trained', 'for', '300,000', 'steps', '(', '3.5', 'days', ')', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NNS', 'VBD', 'VBN', 'IN', 'CD', 'NNS', '(', 'CD', 'NNS', ')', '.']",13
machine-translation,2,165,Optimizer,['Optimizer'],['B-n'],['NN'],1
machine-translation,2,166,"We used the Adam optimizer with ? 1 = 0.9 , ? 2 = 0.98 and = 10 ?9 .","['We', 'used', 'the', 'Adam', 'optimizer', 'with', '?', '1', '=', '0.9', ',', '?', '2', '=', '0.98', 'and', '=', '10', '?9', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'DT', 'NNP', 'NN', 'IN', '.', 'CD', 'NN', 'CD', ',', '.', 'CD', 'JJ', 'CD', 'CC', '$', 'CD', 'NN', '.']",20
machine-translation,2,169,We used warmup_steps = 4000 .,"['We', 'used', 'warmup_steps', '=', '4000', '.']","['O', 'O', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'VBD', 'NNS', 'JJ', 'CD', '.']",6
machine-translation,2,170,Regularization,['Regularization'],['B-n'],['NN'],1
machine-translation,2,172,Residual Dropout,"['Residual', 'Dropout']","['B-n', 'I-n']","['JJ', 'NN']",2
machine-translation,2,173,"We apply dropout to the output of each sub - layer , before it is added to the sub - layer input and normalized .","['We', 'apply', 'dropout', 'to', 'the', 'output', 'of', 'each', 'sub', '-', 'layer', ',', 'before', 'it', 'is', 'added', 'to', 'the', 'sub', '-', 'layer', 'input', 'and', 'normalized', '.']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'RB', 'TO', 'DT', 'NN', 'IN', 'DT', 'NN', ':', 'NN', ',', 'IN', 'PRP', 'VBZ', 'VBN', 'TO', 'DT', 'NN', ':', 'NN', 'NN', 'CC', 'VBN', '.']",25
machine-translation,2,174,"In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .","['In', 'addition', ',', 'we', 'apply', 'dropout', 'to', 'the', 'sums', 'of', 'the', 'embeddings', 'and', 'the', 'positional', 'encodings', 'in', 'both', 'the', 'encoder', 'and', 'decoder', 'stacks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'RB', 'TO', 'DT', 'NNS', 'IN', 'DT', 'NNS', 'CC', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'DT', 'NN', 'CC', 'NN', 'NNS', '.']",24
machine-translation,2,176,Label Smoothing,"['Label', 'Smoothing']","['B-n', 'I-n']","['NNP', 'VBG']",2
machine-translation,2,177,"During training , we employed label smoothing of value ls = 0.1 .","['During', 'training', ',', 'we', 'employed', 'label', 'smoothing', 'of', 'value', 'ls', '=', '0.1', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBD', 'JJ', 'NN', 'IN', 'NN', 'NN', 'VBD', 'CD', '.']",13
machine-translation,2,180,Machine Translation,"['Machine', 'Translation']","['B-n', 'I-n']","['NN', 'NN']",2
machine-translation,2,181,"On the WMT 2014 English - to - German translation task , the big transformer model ( Transformer ( big ) in ) outperforms the best previously reported models ( including ensembles ) by more than 2.0 BLEU , establishing a new state - of - the - art BLEU score of 28.4 .","['On', 'the', 'WMT', '2014', 'English', '-', 'to', '-', 'German', 'translation', 'task', ',', 'the', 'big', 'transformer', 'model', '(', 'Transformer', '(', 'big', ')', 'in', ')', 'outperforms', 'the', 'best', 'previously', 'reported', 'models', '(', 'including', 'ensembles', ')', 'by', 'more', 'than', '2.0', 'BLEU', ',', 'establishing', 'a', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'BLEU', 'score', 'of', '28.4', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['IN', 'DT', 'NNP', 'CD', 'NNP', ':', 'TO', ':', 'JJ', 'NN', 'NN', ',', 'DT', 'JJ', 'NN', 'NN', '(', 'NNP', '(', 'JJ', ')', 'IN', ')', 'VBZ', 'DT', 'JJS', 'RB', 'VBN', 'NNS', '(', 'VBG', 'NNS', ')', 'IN', 'JJR', 'IN', 'CD', 'NNP', ',', 'VBG', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNP', 'NN', 'IN', 'CD', '.']",54
machine-translation,2,185,"On the WMT 2014 English - to - French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 / 4 the training cost of the previous state - of - the - art model .","['On', 'the', 'WMT', '2014', 'English', '-', 'to', '-', 'French', 'translation', 'task', ',', 'our', 'big', 'model', 'achieves', 'a', 'BLEU', 'score', 'of', '41.0', ',', 'outperforming', 'all', 'of', 'the', 'previously', 'published', 'single', 'models', ',', 'at', 'less', 'than', '1', '/', '4', 'the', 'training', 'cost', 'of', 'the', 'previous', 'state', '-', 'of', '-', 'the', '-', 'art', 'model', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNP', 'CD', 'NNP', ':', 'TO', ':', 'JJ', 'NN', 'NN', ',', 'PRP$', 'JJ', 'NN', 'VBZ', 'DT', 'NNP', 'NN', 'IN', 'CD', ',', 'VBG', 'DT', 'IN', 'DT', 'RB', 'VBN', 'JJ', 'NNS', ',', 'IN', 'JJR', 'IN', 'CD', 'JJ', 'CD', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', '.']",52
machine-translation,2,204,English Constituency Parsing,"['English', 'Constituency', 'Parsing']","['B-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NNP']",3
machine-translation,2,214,"Our results in show that despite the lack of task - specific tuning our model performs surprisingly well , yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar .","['Our', 'results', 'in', 'show', 'that', 'despite', 'the', 'lack', 'of', 'task', '-', 'specific', 'tuning', 'our', 'model', 'performs', 'surprisingly', 'well', ',', 'yielding', 'better', 'results', 'than', 'all', 'previously', 'reported', 'models', 'with', 'the', 'exception', 'of', 'the', 'Recurrent', 'Neural', 'Network', 'Grammar', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NNS', 'IN', 'NN', 'IN', 'IN', 'DT', 'NN', 'IN', 'NN', ':', 'JJ', 'VBG', 'PRP$', 'NN', 'NNS', 'RB', 'RB', ',', 'VBG', 'JJR', 'NNS', 'IN', 'DT', 'RB', 'VBN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', '.']",37
machine-translation,3,4,Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years .,"['Neural', 'machine', 'translation', '(', 'NMT', ')', 'aims', 'at', 'solving', 'machine', 'translation', '(', 'MT', ')', 'problems', 'using', 'neural', 'networks', 'and', 'has', 'exhibited', 'promising', 'results', 'in', 'recent', 'years', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'IN', 'VBG', 'NN', 'NN', '(', 'NNP', ')', 'NNS', 'VBG', 'JJ', 'NNS', 'CC', 'VBZ', 'VBN', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', '.']",27
machine-translation,3,31,"In this work , we introduce a new type of linear connections for multi - layer recurrent networks .","['In', 'this', 'work', ',', 'we', 'introduce', 'a', 'new', 'type', 'of', 'linear', 'connections', 'for', 'multi', '-', 'layer', 'recurrent', 'networks', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NN', ':', 'NN', 'NN', 'NNS', '.']",19
machine-translation,3,32,"These connections , which are called fast - forward connections , play an essential role in building a deep topology with depth of 16 .","['These', 'connections', ',', 'which', 'are', 'called', 'fast', '-', 'forward', 'connections', ',', 'play', 'an', 'essential', 'role', 'in', 'building', 'a', 'deep', 'topology', 'with', 'depth', 'of', '16', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', ',', 'WDT', 'VBP', 'VBN', 'RB', ':', 'NN', 'NNS', ',', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'CD', '.']",25
machine-translation,3,33,"In addition , we introduce an interleaved bi-directional architecture to stack LSTM layers in the encoder .","['In', 'addition', ',', 'we', 'introduce', 'an', 'interleaved', 'bi-directional', 'architecture', 'to', 'stack', 'LSTM', 'layers', 'in', 'the', 'encoder', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'TO', 'VB', 'NNP', 'NNS', 'IN', 'DT', 'NN', '.']",17
machine-translation,3,204,We use 256 dimensional word embeddings for both the source and target languages .,"['We', 'use', '256', 'dimensional', 'word', 'embeddings', 'for', 'both', 'the', 'source', 'and', 'target', 'languages', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'DT', 'NN', 'CC', 'NN', 'NNS', '.']",14
machine-translation,3,205,"All LSTM layers , including the 2n e layers in the encoder and then d layers in the decoder , have 512 memory cells .","['All', 'LSTM', 'layers', ',', 'including', 'the', '2n', 'e', 'layers', 'in', 'the', 'encoder', 'and', 'then', 'd', 'layers', 'in', 'the', 'decoder', ',', 'have', '512', 'memory', 'cells', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'NNS', ',', 'VBG', 'DT', 'CD', 'NN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'RB', 'VB', 'NNS', 'IN', 'DT', 'NN', ',', 'VBP', 'CD', 'NN', 'NNS', '.']",25
machine-translation,3,208,"For each LSTM layer , the activation functions for gates , inputs and outputs are sigmoid , tanh , and tanh respectively .","['For', 'each', 'LSTM', 'layer', ',', 'the', 'activation', 'functions', 'for', 'gates', ',', 'inputs', 'and', 'outputs', 'are', 'sigmoid', ',', 'tanh', ',', 'and', 'tanh', 'respectively', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'DT', 'NNP', 'NN', ',', 'DT', 'NN', 'NNS', 'IN', 'NNS', ',', 'NNS', 'CC', 'NNS', 'VBP', 'JJ', ',', 'NN', ',', 'CC', 'NN', 'RB', '.']",23
machine-translation,3,214,"Since there are non-linear activations in the recurrent computation , a larger learning rate l r = 5 10 ? 4 is used , while for the feed - forward computation a smaller learning rate l f = 4 10 ? 5 is used .","['Since', 'there', 'are', 'non-linear', 'activations', 'in', 'the', 'recurrent', 'computation', ',', 'a', 'larger', 'learning', 'rate', 'l', 'r', '=', '5', '10', '?', '4', 'is', 'used', ',', 'while', 'for', 'the', 'feed', '-', 'forward', 'computation', 'a', 'smaller', 'learning', 'rate', 'l', 'f', '=', '4', '10', '?', '5', 'is', 'used', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-p', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O']","['IN', 'EX', 'VBP', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', ',', 'DT', 'JJR', 'NN', 'NN', 'NN', 'NN', 'VBD', 'CD', 'CD', '.', 'CD', 'VBZ', 'VBN', ',', 'IN', 'IN', 'DT', 'NN', ':', 'NN', 'NN', 'DT', 'JJR', 'NN', 'NN', 'NN', 'NN', 'VBD', 'CD', 'CD', '.', 'CD', 'VBZ', 'VBN', '.']",45
machine-translation,3,224,The dropout ratio pd is 0.1 .,"['The', 'dropout', 'ratio', 'pd', 'is', '0.1', '.']","['O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'O']","['DT', 'NN', 'NN', 'NN', 'VBZ', 'CD', '.']",7
machine-translation,3,229,We use 4 ? 8 GPU machines ( each has 4 K40 GPU cards ) running for 10 days to train the full model with parallelization at the data batch level .,"['We', 'use', '4', '?', '8', 'GPU', 'machines', '(', 'each', 'has', '4', 'K40', 'GPU', 'cards', ')', 'running', 'for', '10', 'days', 'to', 'train', 'the', 'full', 'model', 'with', 'parallelization', 'at', 'the', 'data', 'batch', 'level', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', '.', 'CD', 'NNP', 'NNS', '(', 'DT', 'VBZ', 'CD', 'NNP', 'NNP', 'NNS', ')', 'VBG', 'IN', 'CD', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'DT', 'NNS', 'NN', 'NN', '.']",32
machine-translation,3,237,Single models,"['Single', 'models']","['B-n', 'I-n']","['NNP', 'NNS']",2
machine-translation,3,238,English - to - French : First we list our single model results on the English - to - French task in Tab .,"['English', '-', 'to', '-', 'French', ':', 'First', 'we', 'list', 'our', 'single', 'model', 'results', 'on', 'the', 'English', '-', 'to', '-', 'French', 'task', 'in', 'Tab', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', ':', 'TO', ':', 'NN', ':', 'NNP', 'PRP', 'VBP', 'PRP$', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'NNP', ':', 'TO', ':', 'JJ', 'NN', 'IN', 'NNP', '.']",24
machine-translation,3,241,"From Deep - ED , we obtain the BLEU score of 36.3 , which outperforms Enc - Dec model by 4.8 BLEU points .","['From', 'Deep', '-', 'ED', ',', 'we', 'obtain', 'the', 'BLEU', 'score', 'of', '36.3', ',', 'which', 'outperforms', 'Enc', '-', 'Dec', 'model', 'by', '4.8', 'BLEU', 'points', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', ':', 'NNP', ',', 'PRP', 'VB', 'DT', 'NNP', 'NN', 'IN', 'CD', ',', 'WDT', 'VBZ', 'NNP', ':', 'NNP', 'NN', 'IN', 'CD', 'NNP', 'NNS', '.']",24
machine-translation,3,244,"For Deep - Att , the performance is further improved to 37.7 .","['For', 'Deep', '-', 'Att', ',', 'the', 'performance', 'is', 'further', 'improved', 'to', '37.7', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['IN', 'NNP', ':', 'NNP', ',', 'DT', 'NN', 'VBZ', 'RB', 'VBN', 'TO', 'CD', '.']",13
machine-translation,0,2,Learning Phrase Representations using RNN Encoder - Decoder for Statistical Machine Translation,"['Learning', 'Phrase', 'Representations', 'using', 'RNN', 'Encoder', '-', 'Decoder', 'for', 'Statistical', 'Machine', 'Translation']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['VBG', 'NNP', 'NNP', 'VBG', 'NNP', 'NNP', ':', 'NN', 'IN', 'JJ', 'NNP', 'NN']",12
machine-translation,0,15,"Along this line of research on using neural networks for SMT , this paper focuses on a novel neural network architecture that can be used as apart of the conventional phrase - based SMT system .","['Along', 'this', 'line', 'of', 'research', 'on', 'using', 'neural', 'networks', 'for', 'SMT', ',', 'this', 'paper', 'focuses', 'on', 'a', 'novel', 'neural', 'network', 'architecture', 'that', 'can', 'be', 'used', 'as', 'apart', 'of', 'the', 'conventional', 'phrase', '-', 'based', 'SMT', 'system', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'DT', 'NN', 'IN', 'NN', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'NNP', ',', 'DT', 'NN', 'VBZ', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'WDT', 'MD', 'VB', 'VBN', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'VBN', 'NNP', 'NN', '.']",36
machine-translation,0,16,"The proposed neural network architecture , which we will refer to as an RNN Encoder - Decoder , consists of two recurrent neural networks ( RNN ) that act as an encoder and a decoder pair .","['The', 'proposed', 'neural', 'network', 'architecture', ',', 'which', 'we', 'will', 'refer', 'to', 'as', 'an', 'RNN', 'Encoder', '-', 'Decoder', ',', 'consists', 'of', 'two', 'recurrent', 'neural', 'networks', '(', 'RNN', ')', 'that', 'act', 'as', 'an', 'encoder', 'and', 'a', 'decoder', 'pair', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-n', 'O', 'O']","['DT', 'VBN', 'JJ', 'NN', 'NN', ',', 'WDT', 'PRP', 'MD', 'VB', 'TO', 'IN', 'DT', 'NNP', 'NNP', ':', 'NN', ',', 'VBZ', 'IN', 'CD', 'JJ', 'JJ', 'NNS', '(', 'NNP', ')', 'WDT', 'NN', 'IN', 'DT', 'NN', 'CC', 'DT', 'NN', 'NN', '.']",37
machine-translation,0,17,"The encoder maps a variable - length source sequence to a fixed - length vector , and the decoder maps the vector representation back to a variable - length target sequence .","['The', 'encoder', 'maps', 'a', 'variable', '-', 'length', 'source', 'sequence', 'to', 'a', 'fixed', '-', 'length', 'vector', ',', 'and', 'the', 'decoder', 'maps', 'the', 'vector', 'representation', 'back', 'to', 'a', 'variable', '-', 'length', 'target', 'sequence', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', ':', 'NN', 'NN', 'NN', 'TO', 'DT', 'VBN', ':', 'NN', 'NN', ',', 'CC', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'RB', 'TO', 'DT', 'JJ', ':', 'NN', 'NN', 'NN', '.']",32
machine-translation,0,18,The two networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence .,"['The', 'two', 'networks', 'are', 'trained', 'jointly', 'to', 'maximize', 'the', 'conditional', 'probability', 'of', 'the', 'target', 'sequence', 'given', 'a', 'source', 'sequence', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'CD', 'NNS', 'VBP', 'VBN', 'RB', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBN', 'DT', 'NN', 'NN', '.']",20
machine-translation,0,19,"Additionally , we propose to use a rather sophisticated hidden unit in order to improve both the memory capacity and the ease of training .","['Additionally', ',', 'we', 'propose', 'to', 'use', 'a', 'rather', 'sophisticated', 'hidden', 'unit', 'in', 'order', 'to', 'improve', 'both', 'the', 'memory', 'capacity', 'and', 'the', 'ease', 'of', 'training', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'TO', 'VB', 'DT', 'RB', 'JJ', 'JJ', 'NN', 'IN', 'NN', 'TO', 'VB', 'PDT', 'DT', 'NN', 'NN', 'CC', 'DT', 'NN', 'IN', 'NN', '.']",25
machine-translation,0,135,The baseline phrase - based SMT system was built using Moses with default settings .,"['The', 'baseline', 'phrase', '-', 'based', 'SMT', 'system', 'was', 'built', 'using', 'Moses', 'with', 'default', 'settings', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', ':', 'VBN', 'NNP', 'NN', 'VBD', 'VBN', 'VBG', 'NNS', 'IN', 'NN', 'NNS', '.']",15
machine-translation,0,159,"As expected , adding features computed by neural networks consistently improves the performance over the baseline performance .","['As', 'expected', ',', 'adding', 'features', 'computed', 'by', 'neural', 'networks', 'consistently', 'improves', 'the', 'performance', 'over', 'the', 'baseline', 'performance', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', ',', 'VBG', 'NNS', 'VBN', 'IN', 'JJ', 'NNS', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",18
machine-translation,0,160,The best performance was achieved when we used both CSLM and the phrase scores from the RNN Encoder - Decoder .,"['The', 'best', 'performance', 'was', 'achieved', 'when', 'we', 'used', 'both', 'CSLM', 'and', 'the', 'phrase', 'scores', 'from', 'the', 'RNN', 'Encoder', '-', 'Decoder', '.']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJS', 'NN', 'VBD', 'VBN', 'WRB', 'PRP', 'VBD', 'DT', 'NNP', 'CC', 'DT', 'NN', 'VBZ', 'IN', 'DT', 'NNP', 'NNP', ':', 'NN', '.']",21
text-classification,8,2,Baseline Needs More Love : On Simple Word - Embedding - Based Models and Associated Pooling Mechanisms,"['Baseline', 'Needs', 'More', 'Love', ':', 'On', 'Simple', 'Word', '-', 'Embedding', '-', 'Based', 'Models', 'and', 'Associated', 'Pooling', 'Mechanisms']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', 'NNP', ':', 'IN', 'NNP', 'NNP', ':', 'VBG', ':', 'VBN', 'NNP', 'CC', 'NNP', 'NNP', 'NNP']",17
text-classification,8,4,"Many deep learning architectures have been proposed to model the compositionality in text sequences , requiring a substantial number of parameters and expensive computations .","['Many', 'deep', 'learning', 'architectures', 'have', 'been', 'proposed', 'to', 'model', 'the', 'compositionality', 'in', 'text', 'sequences', ',', 'requiring', 'a', 'substantial', 'number', 'of', 'parameters', 'and', 'expensive', 'computations', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'JJ', 'NN', 'NNS', 'VBP', 'VBN', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', 'NNS', ',', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'CC', 'JJ', 'NNS', '.']",25
text-classification,8,10,The source code and datasets can be obtained from https://github.com/dinghanshen/SWEM .,"['The', 'source', 'code', 'and', 'datasets', 'can', 'be', 'obtained', 'from', 'https://github.com/dinghanshen/SWEM', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']","['DT', 'NN', 'NN', 'CC', 'NNS', 'MD', 'VB', 'VBN', 'IN', 'NN', '.']",11
text-classification,8,14,"Leveraging the word - embedding construct , many deep architectures have been proposed to model the compositionality in variable - length text sequences .","['Leveraging', 'the', 'word', '-', 'embedding', 'construct', ',', 'many', 'deep', 'architectures', 'have', 'been', 'proposed', 'to', 'model', 'the', 'compositionality', 'in', 'variable', '-', 'length', 'text', 'sequences', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'DT', 'NN', ':', 'VBG', 'NN', ',', 'JJ', 'JJ', 'NNS', 'VBP', 'VBN', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'JJ', ':', 'NN', 'NN', 'NNS', '.']",24
text-classification,8,21,"In this paper , we conduct an extensive experimental investigation to understand when , and why , simple pooling strategies , operated over word embeddings alone , already carry sufficient information for natural language understanding .","['In', 'this', 'paper', ',', 'we', 'conduct', 'an', 'extensive', 'experimental', 'investigation', 'to', 'understand', 'when', ',', 'and', 'why', ',', 'simple', 'pooling', 'strategies', ',', 'operated', 'over', 'word', 'embeddings', 'alone', ',', 'already', 'carry', 'sufficient', 'information', 'for', 'natural', 'language', 'understanding', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'TO', 'VB', 'WRB', ',', 'CC', 'WRB', ',', 'JJ', 'NN', 'NNS', ',', 'VBD', 'IN', 'NN', 'NNS', 'RB', ',', 'RB', 'VB', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",36
text-classification,8,22,"To ac- count for the distinct nature of various NLP tasks that may require different semantic features , we compare SWEM - based models with existing recurrent and convolutional networks in a pointby - point manner .","['To', 'ac-', 'count', 'for', 'the', 'distinct', 'nature', 'of', 'various', 'NLP', 'tasks', 'that', 'may', 'require', 'different', 'semantic', 'features', ',', 'we', 'compare', 'SWEM', '-', 'based', 'models', 'with', 'existing', 'recurrent', 'and', 'convolutional', 'networks', 'in', 'a', 'pointby', '-', 'point', 'manner', '.']","['B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNP', 'NNS', 'WDT', 'MD', 'VB', 'JJ', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'NNP', ':', 'VBN', 'NNS', 'IN', 'VBG', 'NN', 'CC', 'JJ', 'NNS', 'IN', 'DT', 'JJ', ':', 'NN', 'NN', '.']",37
text-classification,8,81,"Interestingly , for the sentiment analysis tasks , both CNN and LSTM compositional functions perform better than SWEM , suggesting that wordorder information maybe required for analyzing sentiment orientations .","['Interestingly', ',', 'for', 'the', 'sentiment', 'analysis', 'tasks', ',', 'both', 'CNN', 'and', 'LSTM', 'compositional', 'functions', 'perform', 'better', 'than', 'SWEM', ',', 'suggesting', 'that', 'wordorder', 'information', 'maybe', 'required', 'for', 'analyzing', 'sentiment', 'orientations', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'IN', 'DT', 'NN', 'NN', 'NNS', ',', 'DT', 'NNP', 'CC', 'NNP', 'JJ', 'NNS', 'VBP', 'JJR', 'IN', 'NNP', ',', 'VBG', 'IN', 'NN', 'NN', 'RB', 'VBN', 'IN', 'VBG', 'JJ', 'NNS', '.']",30
text-classification,8,117,We use Glo Ve word embeddings with K = 300 as initialization for all our models .,"['We', 'use', 'Glo', 'Ve', 'word', 'embeddings', 'with', 'K', '=', '300', 'as', 'initialization', 'for', 'all', 'our', 'models', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NNP', 'NN', 'NNS', 'IN', 'NNP', 'NNP', 'CD', 'IN', 'NN', 'IN', 'DT', 'PRP$', 'NNS', '.']",17
text-classification,8,118,"Out - Of - Vocabulary ( OOV ) words are initialized from a uniform distribution with range [ ? 0.01 , 0.01 ] .","['Out', '-', 'Of', '-', 'Vocabulary', '(', 'OOV', ')', 'words', 'are', 'initialized', 'from', 'a', 'uniform', 'distribution', 'with', 'range', '[', '?', '0.01', ',', '0.01', ']', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', ':', 'IN', ':', 'NNP', '(', 'NNP', ')', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NN', '.', 'CD', ',', 'CD', 'NN', '.']",24
text-classification,8,119,"The Glo Ve embeddings are employed in two ways to learn refined word embeddings : ( i ) directly updating each word embedding during training ; and ( ii ) training a 300 dimensional Multilayer Perceptron ( MLP ) layer with ReLU activation , with Glo Ve embeddings as input to the MLP and with output defining the refined word embeddings .","['The', 'Glo', 'Ve', 'embeddings', 'are', 'employed', 'in', 'two', 'ways', 'to', 'learn', 'refined', 'word', 'embeddings', ':', '(', 'i', ')', 'directly', 'updating', 'each', 'word', 'embedding', 'during', 'training', ';', 'and', '(', 'ii', ')', 'training', 'a', '300', 'dimensional', 'Multilayer', 'Perceptron', '(', 'MLP', ')', 'layer', 'with', 'ReLU', 'activation', ',', 'with', 'Glo', 'Ve', 'embeddings', 'as', 'input', 'to', 'the', 'MLP', 'and', 'with', 'output', 'defining', 'the', 'refined', 'word', 'embeddings', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'NNP', 'NNS', 'VBP', 'VBN', 'IN', 'CD', 'NNS', 'TO', 'VB', 'JJ', 'NN', 'NNS', ':', '(', 'NN', ')', 'RB', 'VBG', 'DT', 'NN', 'VBG', 'IN', 'NN', ':', 'CC', '(', 'NN', ')', 'VBG', 'DT', 'CD', 'JJ', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', 'IN', 'NNP', 'NN', ',', 'IN', 'NNP', 'NNP', 'NNS', 'IN', 'NN', 'TO', 'DT', 'NNP', 'CC', 'IN', 'NN', 'VBG', 'DT', 'VBN', 'NN', 'NNS', '.']",62
text-classification,8,124,"Adam ) is used to optimize all models , with learning rate selected from .","['Adam', ')', 'is', 'used', 'to', 'optimize', 'all', 'models', ',', 'with', 'learning', 'rate', 'selected', 'from', '.']","['B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ')', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NNS', ',', 'IN', 'VBG', 'NN', 'VBN', 'IN', '.']",15
text-classification,8,125,"Surprisingly , on topic prediction tasks , our SWEM model exhibits stronger performances , relative to both LSTM and CNN compositional architectures , this by leveraging both the average and max - pooling features from word embeddings .","['Surprisingly', ',', 'on', 'topic', 'prediction', 'tasks', ',', 'our', 'SWEM', 'model', 'exhibits', 'stronger', 'performances', ',', 'relative', 'to', 'both', 'LSTM', 'and', 'CNN', 'compositional', 'architectures', ',', 'this', 'by', 'leveraging', 'both', 'the', 'average', 'and', 'max', '-', 'pooling', 'features', 'from', 'word', 'embeddings', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'IN', 'NN', 'NN', 'NNS', ',', 'PRP$', 'NNP', 'NN', 'NNS', 'JJR', 'NNS', ',', 'JJ', 'TO', 'DT', 'NNP', 'CC', 'NNP', 'JJ', 'NNS', ',', 'DT', 'IN', 'VBG', 'CC', 'DT', 'NN', 'CC', 'JJ', ':', 'NN', 'NNS', 'IN', 'NN', 'NNS', '.']",38
text-classification,8,127,"On the ontology classification problem ( DBpedia dataset ) , we observe the same trend , that SWEM exhibits comparable or even superior results , relative to CNN or LSTM models .","['On', 'the', 'ontology', 'classification', 'problem', '(', 'DBpedia', 'dataset', ')', ',', 'we', 'observe', 'the', 'same', 'trend', ',', 'that', 'SWEM', 'exhibits', 'comparable', 'or', 'even', 'superior', 'results', ',', 'relative', 'to', 'CNN', 'or', 'LSTM', 'models', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'NN', 'NN', '(', 'NNP', 'NN', ')', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', ',', 'WDT', 'NNP', 'VBZ', 'JJ', 'CC', 'RB', 'JJ', 'NNS', ',', 'JJ', 'TO', 'NNP', 'CC', 'NNP', 'NNS', '.']",32
text-classification,8,149,Text Sequence Matching,"['Text', 'Sequence', 'Matching']","['B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP']",3
text-classification,8,152,"Surprisingly , on most of the datasets considered ( except WikiQA ) , SWEM demonstrates the best results compared with those with CNN or the LSTM encoder .","['Surprisingly', ',', 'on', 'most', 'of', 'the', 'datasets', 'considered', '(', 'except', 'WikiQA', ')', ',', 'SWEM', 'demonstrates', 'the', 'best', 'results', 'compared', 'with', 'those', 'with', 'CNN', 'or', 'the', 'LSTM', 'encoder', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'IN', 'JJS', 'IN', 'DT', 'NNS', 'VBN', '(', 'IN', 'NNP', ')', ',', 'NNP', 'VBZ', 'DT', 'JJS', 'NNS', 'VBN', 'IN', 'DT', 'IN', 'NNP', 'CC', 'DT', 'NNP', 'NN', '.']",28
text-classification,8,153,"Notably , on SNLI dataset , we observe that SWEM - max performs the best among all SWEM variants , consistent with the findings in Nie and Bansal ( 2017 ) ; , that max - pooling over BiLSTM hidden units outperforms average pooling operation on SNLI dataset .","['Notably', ',', 'on', 'SNLI', 'dataset', ',', 'we', 'observe', 'that', 'SWEM', '-', 'max', 'performs', 'the', 'best', 'among', 'all', 'SWEM', 'variants', ',', 'consistent', 'with', 'the', 'findings', 'in', 'Nie', 'and', 'Bansal', '(', '2017', ')', ';', ',', 'that', 'max', '-', 'pooling', 'over', 'BiLSTM', 'hidden', 'units', 'outperforms', 'average', 'pooling', 'operation', 'on', 'SNLI', 'dataset', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'IN', 'NNP', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', ':', 'NN', 'VBZ', 'DT', 'JJS', 'IN', 'DT', 'NNP', 'NNS', ',', 'NN', 'IN', 'DT', 'NNS', 'IN', 'NNP', 'CC', 'NNP', '(', 'CD', ')', ':', ',', 'IN', 'SYM', ':', 'NN', 'IN', 'NNP', 'JJ', 'NNS', 'NNS', 'JJ', 'JJ', 'NN', 'IN', 'NNP', 'NN', '.']",49
text-classification,8,176,SWEM - hier for sentiment analysis,"['SWEM', '-', 'hier', 'for', 'sentiment', 'analysis']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NN', 'IN', 'NN', 'NN']",6
text-classification,8,177,"As demonstrated in Section 4.2.1 , word - order information plays a vital role for sentiment analysis tasks .","['As', 'demonstrated', 'in', 'Section', '4.2.1', ',', 'word', '-', 'order', 'information', 'plays', 'a', 'vital', 'role', 'for', 'sentiment', 'analysis', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'IN', 'NN', 'CD', ',', 'NN', ':', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NN', 'NNS', '.']",19
text-classification,8,178,"However , according to the case study above , the most important features for sentiment prediction maybe some key n-gram phrase / words from Negative :","['However', ',', 'according', 'to', 'the', 'case', 'study', 'above', ',', 'the', 'most', 'important', 'features', 'for', 'sentiment', 'prediction', 'maybe', 'some', 'key', 'n-gram', 'phrase', '/', 'words', 'from', 'Negative', ':']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['RB', ',', 'VBG', 'TO', 'DT', 'NN', 'NN', 'IN', ',', 'DT', 'RBS', 'JJ', 'NNS', 'IN', 'NN', 'NN', 'RB', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'NNS', 'IN', 'JJ', ':']",26
text-classification,8,189,"SWEM - hier greatly outperforms the other three SWEM variants , and the corresponding accuracies are comparable to the results of CNN or LSTM ) .","['SWEM', '-', 'hier', 'greatly', 'outperforms', 'the', 'other', 'three', 'SWEM', 'variants', ',', 'and', 'the', 'corresponding', 'accuracies', 'are', 'comparable', 'to', 'the', 'results', 'of', 'CNN', 'or', 'LSTM', ')', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['NNP', ':', 'NN', 'RB', 'VBZ', 'DT', 'JJ', 'CD', 'NNP', 'NNS', ',', 'CC', 'DT', 'JJ', 'NNS', 'VBP', 'JJ', 'TO', 'DT', 'NNS', 'IN', 'NNP', 'CC', 'NNP', ')', '.']",26
text-classification,8,191,Short Sentence Processing,"['Short', 'Sentence', 'Processing']","['B-n', 'I-n', 'I-n']","['JJ', 'NN', 'NN']",3
text-classification,8,195,"Compared with CNN / LSTM compositional functions , SWEM yields inferior accuracies on sentiment analysis datasets , consistent with our observation in the case of document categorization .","['Compared', 'with', 'CNN', '/', 'LSTM', 'compositional', 'functions', ',', 'SWEM', 'yields', 'inferior', 'accuracies', 'on', 'sentiment', 'analysis', 'datasets', ',', 'consistent', 'with', 'our', 'observation', 'in', 'the', 'case', 'of', 'document', 'categorization', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'IN', 'NNP', 'NNP', 'NNP', 'JJ', 'NNS', ',', 'NNP', 'NNS', 'JJ', 'NNS', 'IN', 'NN', 'NN', 'NNS', ',', 'NN', 'IN', 'PRP$', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', '.']",28
text-classification,8,196,"However , SWEM exhibits comparable performance on the other two tasks , again with much less parameters and faster training .","['However', ',', 'SWEM', 'exhibits', 'comparable', 'performance', 'on', 'the', 'other', 'two', 'tasks', ',', 'again', 'with', 'much', 'less', 'parameters', 'and', 'faster', 'training', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'NNP', 'VBZ', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'CD', 'NNS', ',', 'RB', 'IN', 'JJ', 'JJR', 'NNS', 'CC', 'JJR', 'NN', '.']",21
text-classification,9,2,Translations as Additional Contexts for Sentence Classification,"['Translations', 'as', 'Additional', 'Contexts', 'for', 'Sentence', 'Classification']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNS', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",7
text-classification,9,12,"One of the primary tasks in natural language processing ( NLP ) is sentence classification , where given a sentence ( e.g. a sentence of a review ) as input , we are tasked to classify it into one of multiple classes ( e.g. into positive or negative ) .","['One', 'of', 'the', 'primary', 'tasks', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'sentence', 'classification', ',', 'where', 'given', 'a', 'sentence', '(', 'e.g.', 'a', 'sentence', 'of', 'a', 'review', ')', 'as', 'input', ',', 'we', 'are', 'tasked', 'to', 'classify', 'it', 'into', 'one', 'of', 'multiple', 'classes', '(', 'e.g.', 'into', 'positive', 'or', 'negative', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'JJ', 'NN', ',', 'WRB', 'VBN', 'DT', 'NN', '(', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', ')', 'IN', 'NN', ',', 'PRP', 'VBP', 'VBN', 'TO', 'VB', 'PRP', 'IN', 'CD', 'IN', 'JJ', 'NNS', '(', 'VB', 'IN', 'JJ', 'CC', 'JJ', ')', '.']",50
text-classification,9,42,"Based on these observations , we present a neural attentionbased multiple context fixing attachment ( MCFA ) .","['Based', 'on', 'these', 'observations', ',', 'we', 'present', 'a', 'neural', 'attentionbased', 'multiple', 'context', 'fixing', 'attachment', '(', 'MCFA', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O']","['VBN', 'IN', 'DT', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'VBG', 'NN', '(', 'NNP', ')', '.']",18
text-classification,9,43,"MCFA is a series of modules that uses all the sentence vectors ( e.g. Arabic , English , Korean , etc. ) as context to fix a sentence vector ( e.g. Korean ) .","['MCFA', 'is', 'a', 'series', 'of', 'modules', 'that', 'uses', 'all', 'the', 'sentence', 'vectors', '(', 'e.g.', 'Arabic', ',', 'English', ',', 'Korean', ',', 'etc.', ')', 'as', 'context', 'to', 'fix', 'a', 'sentence', 'vector', '(', 'e.g.', 'Korean', ')', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'IN', 'NNS', 'WDT', 'VBZ', 'PDT', 'DT', 'NN', 'NNS', '(', 'JJ', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NN', ')', 'IN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', '(', 'JJ', 'NNP', ')', '.']",34
text-classification,9,44,"Fixing the vectors is done by selectively moving the vectors to a location in the same vector space that better separates the class , as shown in .","['Fixing', 'the', 'vectors', 'is', 'done', 'by', 'selectively', 'moving', 'the', 'vectors', 'to', 'a', 'location', 'in', 'the', 'same', 'vector', 'space', 'that', 'better', 'separates', 'the', 'class', ',', 'as', 'shown', 'in', '.']","['B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['VBG', 'DT', 'NNS', 'VBZ', 'VBN', 'IN', 'RB', 'VBG', 'DT', 'NNS', 'TO', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'JJR', 'VBZ', 'DT', 'NN', ',', 'IN', 'VBN', 'IN', '.']",28
text-classification,9,46,MCFA computes two sentence usability metrics to control the noise when fixing vectors : ( a ) self usability ? i ( a ) weighs the confidence of using sentence a in solving the task .,"['MCFA', 'computes', 'two', 'sentence', 'usability', 'metrics', 'to', 'control', 'the', 'noise', 'when', 'fixing', 'vectors', ':', '(', 'a', ')', 'self', 'usability', '?', 'i', '(', 'a', ')', 'weighs', 'the', 'confidence', 'of', 'using', 'sentence', 'a', 'in', 'solving', 'the', 'task', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'CD', 'NN', 'NN', 'NNS', 'TO', 'VB', 'DT', 'NN', 'WRB', 'VBG', 'NNS', ':', '(', 'DT', ')', 'NN', 'NN', '.', 'NN', '(', 'DT', ')', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'NN', 'DT', 'IN', 'VBG', 'DT', 'NN', '.']",36
text-classification,9,47,"( b ) relative usability ? r ( a , b ) weighs the confidence of using sentence a in fixing sentence b.","['(', 'b', ')', 'relative', 'usability', '?', 'r', '(', 'a', ',', 'b', ')', 'weighs', 'the', 'confidence', 'of', 'using', 'sentence', 'a', 'in', 'fixing', 'sentence', 'b.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'NN', ')', 'NN', 'NN', '.', 'NN', '(', 'DT', ',', 'NN', ')', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'NN', 'DT', 'IN', 'VBG', 'NN', 'NN']",23
text-classification,9,153,"For our CNN , we use rectified linear units and three filters with different window sizes h = 3 , 4 , 5 with 100 feature maps each , following .","['For', 'our', 'CNN', ',', 'we', 'use', 'rectified', 'linear', 'units', 'and', 'three', 'filters', 'with', 'different', 'window', 'sizes', 'h', '=', '3', ',', '4', ',', '5', 'with', '100', 'feature', 'maps', 'each', ',', 'following', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['IN', 'PRP$', 'NNP', ',', 'PRP', 'VBP', 'JJ', 'JJ', 'NNS', 'CC', 'CD', 'NNS', 'IN', 'JJ', 'NN', 'NNS', 'VBP', 'JJ', 'CD', ',', 'CD', ',', 'CD', 'IN', 'CD', 'NN', 'NNS', 'DT', ',', 'VBG', '.']",31
text-classification,9,154,"For the final sentence vector , we concatenate the feature maps to get a 300 - dimension vector .","['For', 'the', 'final', 'sentence', 'vector', ',', 'we', 'concatenate', 'the', 'feature', 'maps', 'to', 'get', 'a', '300', '-', 'dimension', 'vector', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'CD', ':', 'NN', 'NN', '.']",19
text-classification,9,155,We use dropout on all nonlinear connections with a dropout rate of 0.5 .,"['We', 'use', 'dropout', 'on', 'all', 'nonlinear', 'connections', 'with', 'a', 'dropout', 'rate', 'of', '0.5', '.']","['O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'IN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",14
text-classification,9,158,"During training , we use mini-batch size of 50 .","['During', 'training', ',', 'we', 'use', 'mini-batch', 'size', 'of', '50', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'IN', 'CD', '.']",10
text-classification,9,159,Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule .,"['Training', 'is', 'done', 'via', 'stochastic', 'gradient', 'descent', 'over', 'shuffled', 'mini-batches', 'with', 'the', 'Adadelta', 'update', 'rule', '.']","['B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NN', 'VBZ', 'VBN', 'IN', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'NN', '.']",16
text-classification,9,160,We perform early stopping using a random 10 % of the training set as the development set .,"['We', 'perform', 'early', 'stopping', 'using', 'a', 'random', '10', '%', 'of', 'the', 'training', 'set', 'as', 'the', 'development', 'set', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'VBG', 'DT', 'JJ', 'CD', 'NN', 'IN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",18
text-classification,9,169,We show that CNN + MCFA achieves state of the art performance on three of the four data sets and performs competitively on one data set .,"['We', 'show', 'that', 'CNN', '+', 'MCFA', 'achieves', 'state', 'of', 'the', 'art', 'performance', 'on', 'three', 'of', 'the', 'four', 'data', 'sets', 'and', 'performs', 'competitively', 'on', 'one', 'data', 'set', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'NNP', 'NNP', 'NNP', 'VBZ', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', 'IN', 'DT', 'CD', 'NNS', 'NNS', 'CC', 'NNS', 'RB', 'IN', 'CD', 'NN', 'NN', '.']",27
text-classification,9,170,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .","['When', 'N', '=', '1', ',', 'MCFA', 'increases', 'the', 'performance', 'of', 'a', 'normal', 'CNN', 'from', '85.0', 'to', '87.6', ',', 'beating', 'the', 'current', 'state', 'of', 'the', 'art', 'on', 'the', 'CR', 'data', 'set', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['WRB', 'NNP', 'VBZ', 'CD', ',', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'IN', 'CD', 'TO', 'CD', ',', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NN', '.']",31
text-classification,9,171,"When N = 10 , MCFA additionally beats the state of the art on the TREC data set .","['When', 'N', '=', '10', ',', 'MCFA', 'additionally', 'beats', 'the', 'state', 'of', 'the', 'art', 'on', 'the', 'TREC', 'data', 'set', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['WRB', 'NNP', 'VBZ', 'CD', ',', 'NNP', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NN', '.']",19
text-classification,9,172,"Finally , our ensemble classifier additionally outperforms all competing models on the MR data set .","['Finally', ',', 'our', 'ensemble', 'classifier', 'additionally', 'outperforms', 'all', 'competing', 'models', 'on', 'the', 'MR', 'data', 'set', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'JJ', 'NN', 'RB', 'VBZ', 'DT', 'VBG', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'NN', '.']",16
text-classification,1,2,Supervised and Semi- Supervised Text Categorization using LSTM for Region Embeddings,"['Supervised', 'and', 'Semi-', 'Supervised', 'Text', 'Categorization', 'using', 'LSTM', 'for', 'Region', 'Embeddings']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['VBN', 'CC', 'NNP', 'VBD', 'NNP', 'NNP', 'VBG', 'NNP', 'IN', 'NNP', 'NNS']",11
text-classification,1,4,"One - hot CNN ( convolutional neural network ) has been shown to be effective for text categorization ( Johnson & Zhang , 2015a ; b ) .","['One', '-', 'hot', 'CNN', '(', 'convolutional', 'neural', 'network', ')', 'has', 'been', 'shown', 'to', 'be', 'effective', 'for', 'text', 'categorization', '(', 'Johnson', '&', 'Zhang', ',', '2015a', ';', 'b', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['CD', ':', 'JJ', 'NNP', '(', 'JJ', 'JJ', 'NN', ')', 'VBZ', 'VBN', 'VBN', 'TO', 'VB', 'JJ', 'IN', 'JJ', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ':', 'NN', ')', '.']",28
text-classification,1,21,"In this work , we consider a more general framework ( subsuming one - hot CNN ) which jointly trains a feature generator and a linear model , where the feature generator consists of ' region embedding + pooling ' .","['In', 'this', 'work', ',', 'we', 'consider', 'a', 'more', 'general', 'framework', '(', 'subsuming', 'one', '-', 'hot', 'CNN', ')', 'which', 'jointly', 'trains', 'a', 'feature', 'generator', 'and', 'a', 'linear', 'model', ',', 'where', 'the', 'feature', 'generator', 'consists', 'of', ""'"", 'region', 'embedding', '+', 'pooling', ""'"", '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'RBR', 'JJ', 'NN', '(', 'VBG', 'CD', ':', 'JJ', 'NNP', ')', 'WDT', 'RB', 'VBZ', 'DT', 'NN', 'NN', 'CC', 'DT', 'JJ', 'NN', ',', 'WRB', 'DT', 'NN', 'NN', 'VBZ', 'IN', 'POS', 'NN', 'VBG', 'JJ', 'VBG', 'POS', '.']",41
text-classification,1,31,"In this work , we build on the general framework of ' region embedding + pooling ' and explore a more sophisticated region embedding via Long Short - Term Memory ( LSTM ) , seeking to overcome the shortcomings above , in the supervised and semi-supervised settings .","['In', 'this', 'work', ',', 'we', 'build', 'on', 'the', 'general', 'framework', 'of', ""'"", 'region', 'embedding', '+', 'pooling', ""'"", 'and', 'explore', 'a', 'more', 'sophisticated', 'region', 'embedding', 'via', 'Long', 'Short', '-', 'Term', 'Memory', '(', 'LSTM', ')', ',', 'seeking', 'to', 'overcome', 'the', 'shortcomings', 'above', ',', 'in', 'the', 'supervised', 'and', 'semi-supervised', 'settings', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'IN', 'DT', 'JJ', 'NN', 'IN', 'POS', 'NN', 'VBG', 'JJ', 'VBG', 'POS', 'CC', 'VB', 'DT', 'RBR', 'JJ', 'NN', 'VBG', 'IN', 'NNP', 'NNP', ':', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'VBG', 'TO', 'VB', 'DT', 'NNS', 'IN', ',', 'IN', 'DT', 'JJ', 'CC', 'JJ', 'NNS', '.']",48
text-classification,1,37,"We pursue the best use of LSTM for our purpose , and then compare the resulting model with the previous best methods including one - hot CNN and previous LSTM .","['We', 'pursue', 'the', 'best', 'use', 'of', 'LSTM', 'for', 'our', 'purpose', ',', 'and', 'then', 'compare', 'the', 'resulting', 'model', 'with', 'the', 'previous', 'best', 'methods', 'including', 'one', '-', 'hot', 'CNN', 'and', 'previous', 'LSTM', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJS', 'NN', 'IN', 'NNP', 'IN', 'PRP$', 'NN', ',', 'CC', 'RB', 'VB', 'DT', 'VBG', 'NN', 'IN', 'DT', 'JJ', 'JJS', 'NNS', 'VBG', 'CD', ':', 'JJ', 'NNP', 'CC', 'JJ', 'NNP', '.']",31
text-classification,1,38,"Our strategy is to simplify the model as much as possible , including elimination of a word embedding layer routinely used to produce input to LSTM .","['Our', 'strategy', 'is', 'to', 'simplify', 'the', 'model', 'as', 'much', 'as', 'possible', ',', 'including', 'elimination', 'of', 'a', 'word', 'embedding', 'layer', 'routinely', 'used', 'to', 'produce', 'input', 'to', 'LSTM', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'RB', 'RB', 'IN', 'JJ', ',', 'VBG', 'NN', 'IN', 'DT', 'NN', 'VBG', 'NN', 'RB', 'VBN', 'TO', 'VB', 'NN', 'TO', 'NNP', '.']",27
text-classification,1,124,Experiments ( supervised ),"['Experiments', '(', 'supervised', ')']","['B-n', 'I-n', 'I-n', 'I-n']","['NNS', '(', 'VBN', ')']",4
text-classification,1,139,"Comparing the two types of LSTM in , we see that our one - hot bidirectional LSTM with pooling ( oh - 2 LSTMp ) outperforms word - vector LSTM ( wv - LSTM ) on all the datasets , confirming the effectiveness of our approach .","['Comparing', 'the', 'two', 'types', 'of', 'LSTM', 'in', ',', 'we', 'see', 'that', 'our', 'one', '-', 'hot', 'bidirectional', 'LSTM', 'with', 'pooling', '(', 'oh', '-', '2', 'LSTMp', ')', 'outperforms', 'word', '-', 'vector', 'LSTM', '(', 'wv', '-', 'LSTM', ')', 'on', 'all', 'the', 'datasets', ',', 'confirming', 'the', 'effectiveness', 'of', 'our', 'approach', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'DT', 'CD', 'NNS', 'IN', 'NNP', 'IN', ',', 'PRP', 'VBP', 'IN', 'PRP$', 'CD', ':', 'JJ', 'JJ', 'NNP', 'IN', 'NN', '(', 'UH', ':', 'CD', 'NNP', ')', 'VBZ', 'NN', ':', 'NN', 'NNP', '(', 'SYM', ':', 'NNP', ')', 'IN', 'PDT', 'DT', 'NNS', ',', 'VBG', 'DT', 'NN', 'IN', 'PRP$', 'NN', '.']",47
text-classification,1,143,"In , on three out of the four datasets , oh - 2 LSTMp outperforms SVM and the CNN .","['In', ',', 'on', 'three', 'out', 'of', 'the', 'four', 'datasets', ',', 'oh', '-', '2', 'LSTMp', 'outperforms', 'SVM', 'and', 'the', 'CNN', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', ',', 'IN', 'CD', 'IN', 'IN', 'DT', 'CD', 'NNS', ',', 'SYM', ':', 'CD', 'NNP', 'NNS', 'NNP', 'CC', 'DT', 'NNP', '.']",20
text-classification,1,195,Semi-supervised experiments,"['Semi-supervised', 'experiments']","['B-n', 'I-n']","['JJ', 'NNS']",2
text-classification,1,212,"Therefore , we tested wv - 2 LSTMp ( word - vector bidirectional LSTM with pooling ) , whose only difference from oh - 2 LSTMp is that the input to the LSTM layers is the pre-trained word vectors .","['Therefore', ',', 'we', 'tested', 'wv', '-', '2', 'LSTMp', '(', 'word', '-', 'vector', 'bidirectional', 'LSTM', 'with', 'pooling', ')', ',', 'whose', 'only', 'difference', 'from', 'oh', '-', '2', 'LSTMp', 'is', 'that', 'the', 'input', 'to', 'the', 'LSTM', 'layers', 'is', 'the', 'pre-trained', 'word', 'vectors', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBD', 'SYM', ':', 'CD', 'NNP', '(', 'NN', ':', 'NN', 'JJ', 'NNP', 'IN', 'VBG', ')', ',', 'WP$', 'JJ', 'NN', 'IN', 'JJ', ':', 'CD', 'NNP', 'VBZ', 'IN', 'DT', 'NN', 'TO', 'DT', 'NNP', 'NNS', 'VBZ', 'DT', 'JJ', 'NN', 'NNS', '.']",40
text-classification,1,220,"Now we review the performance of one - hot CNN with one 200 - dim CNN tv-embedding row # 5 ) , which is comparable with our LSTM with two 100 - dim LSTM tv-embeddings ( row # 4 ) in terms of the dimensionality of tv-embeddings .","['Now', 'we', 'review', 'the', 'performance', 'of', 'one', '-', 'hot', 'CNN', 'with', 'one', '200', '-', 'dim', 'CNN', 'tv-embedding', 'row', '#', '5', ')', ',', 'which', 'is', 'comparable', 'with', 'our', 'LSTM', 'with', 'two', '100', '-', 'dim', 'LSTM', 'tv-embeddings', '(', 'row', '#', '4', ')', 'in', 'terms', 'of', 'the', 'dimensionality', 'of', 'tv-embeddings', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'CD', ':', 'JJ', 'NNP', 'IN', 'CD', 'CD', ':', 'NN', 'NNP', 'NN', 'NN', '#', 'CD', ')', ',', 'WDT', 'VBZ', 'JJ', 'IN', 'PRP$', 'NNP', 'IN', 'CD', 'CD', ':', 'NN', 'NNP', 'NNS', '(', 'VB', '#', 'CD', ')', 'IN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NNS', '.']",48
text-classification,5,2,Universal Language Model Fine - tuning for Text Classification,"['Universal', 'Language', 'Model', 'Fine', '-', 'tuning', 'for', 'Text', 'Classification']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', 'NNP', 'NNP', 'NNP', ':', 'NN', 'IN', 'NNP', 'NNP']",9
text-classification,5,31,"1 ) We propose Universal Language Model Fine - tuning ( ULMFiT ) , a method that can be used to achieve CV - like transfer learning for any task for NLP .","['1', ')', 'We', 'propose', 'Universal', 'Language', 'Model', 'Fine', '-', 'tuning', '(', 'ULMFiT', ')', ',', 'a', 'method', 'that', 'can', 'be', 'used', 'to', 'achieve', 'CV', '-', 'like', 'transfer', 'learning', 'for', 'any', 'task', 'for', 'NLP', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', ')', 'PRP', 'VBP', 'JJ', 'NNP', 'NNP', 'NNP', ':', 'NN', '(', 'NNP', ')', ',', 'DT', 'NN', 'WDT', 'MD', 'VB', 'VBN', 'TO', 'VB', 'NNP', ':', 'IN', 'NN', 'VBG', 'IN', 'DT', 'NN', 'IN', 'NNP', '.']",33
text-classification,5,32,"2 ) We propose discriminative fine - tuning , slanted triangular learning rates , and gradual unfreezing , novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine - tuning .","['2', ')', 'We', 'propose', 'discriminative', 'fine', '-', 'tuning', ',', 'slanted', 'triangular', 'learning', 'rates', ',', 'and', 'gradual', 'unfreezing', ',', 'novel', 'techniques', 'to', 'retain', 'previous', 'knowledge', 'and', 'avoid', 'catastrophic', 'forgetting', 'during', 'fine', '-', 'tuning', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['CD', ')', 'PRP', 'VBP', 'JJ', 'JJ', ':', 'NN', ',', 'VBD', 'JJ', 'NN', 'NNS', ',', 'CC', 'JJ', 'NN', ',', 'NN', 'NNS', 'TO', 'VB', 'JJ', 'NN', 'CC', 'VB', 'JJ', 'VBG', 'IN', 'JJ', ':', 'NN', '.']",33
text-classification,5,162,"We use the AWD - LSTM language model with an embedding size of 400 , 3 layers , 1150 hidden activations per layer , and a BPTT batch size of 70 .","['We', 'use', 'the', 'AWD', '-', 'LSTM', 'language', 'model', 'with', 'an', 'embedding', 'size', 'of', '400', ',', '3', 'layers', ',', '1150', 'hidden', 'activations', 'per', 'layer', ',', 'and', 'a', 'BPTT', 'batch', 'size', 'of', '70', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', ':', 'NNP', 'NN', 'NN', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', ',', 'CD', 'NNS', ',', 'CD', 'JJ', 'NNS', 'IN', 'NN', ',', 'CC', 'DT', 'NNP', 'NN', 'NN', 'IN', 'CD', '.']",32
text-classification,5,163,"We apply dropout of 0.4 to layers , 0.3 to RNN layers , 0.4 to input embedding layers , 0.05 to embedding layers , and weight dropout of 0.5 to the RNN hidden - to - hidden matrix .","['We', 'apply', 'dropout', 'of', '0.4', 'to', 'layers', ',', '0.3', 'to', 'RNN', 'layers', ',', '0.4', 'to', 'input', 'embedding', 'layers', ',', '0.05', 'to', 'embedding', 'layers', ',', 'and', 'weight', 'dropout', 'of', '0.5', 'to', 'the', 'RNN', 'hidden', '-', 'to', '-', 'hidden', 'matrix', '.']","['O', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'IN', 'CD', 'TO', 'NNS', ',', 'CD', 'TO', 'NNP', 'NNS', ',', 'CD', 'TO', 'VB', 'VBG', 'NNS', ',', 'CD', 'TO', 'VBG', 'NNS', ',', 'CC', 'VBD', 'NN', 'IN', 'CD', 'TO', 'DT', 'NNP', 'SYM', ':', 'TO', ':', 'NN', 'NN', '.']",39
text-classification,5,164,The classifier has a hidden layer of size 50 .,"['The', 'classifier', 'has', 'a', 'hidden', 'layer', 'of', 'size', '50', '.']","['O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'CD', '.']",10
text-classification,5,165,"We use Adam with ? 1 = 0.7 instead of the default ? 1 = 0.9 and ? 2 = 0.99 , similar to .","['We', 'use', 'Adam', 'with', '?', '1', '=', '0.7', 'instead', 'of', 'the', 'default', '?', '1', '=', '0.9', 'and', '?', '2', '=', '0.99', ',', 'similar', 'to', '.']","['O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'FW', 'IN', '.', 'CD', '$', 'CD', 'RB', 'IN', 'DT', 'NN', '.', 'CD', 'JJ', 'CD', 'CC', '.', 'CD', 'NN', 'CD', ',', 'JJ', 'TO', '.']",25
text-classification,5,166,"We use a batch size of 64 , a base learning rate of 0.004 and 0.01 for finetuning the LM and the classifier respectively , and tune the number of epochs on the validation set of each task 7 .","['We', 'use', 'a', 'batch', 'size', 'of', '64', ',', 'a', 'base', 'learning', 'rate', 'of', '0.004', 'and', '0.01', 'for', 'finetuning', 'the', 'LM', 'and', 'the', 'classifier', 'respectively', ',', 'and', 'tune', 'the', 'number', 'of', 'epochs', 'on', 'the', 'validation', 'set', 'of', 'each', 'task', '7', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-p', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CD', ',', 'DT', 'NN', 'VBG', 'NN', 'IN', 'CD', 'CC', 'CD', 'IN', 'VBG', 'DT', 'NNP', 'CC', 'DT', 'NN', 'RB', ',', 'CC', 'VBZ', 'DT', 'NN', 'IN', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'CD', '.']",40
text-classification,5,175,"Our method outperforms both CoVe , a state - of - the - art transfer learning method based on hypercolumns , as well as the state - of - the - art on both datasets .","['Our', 'method', 'outperforms', 'both', 'CoVe', ',', 'a', 'state', '-', 'of', '-', 'the', '-', 'art', 'transfer', 'learning', 'method', 'based', 'on', 'hypercolumns', ',', 'as', 'well', 'as', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'on', 'both', 'datasets', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'DT', 'NNP', ',', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'VBG', 'NNS', 'VBN', 'IN', 'NN', ',', 'RB', 'RB', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'DT', 'NNS', '.']",36
text-classification,5,176,"On IMDb , we reduce the error dramatically by 43.9 % and 22 % with regard to CoVe and the state - of - the - art respectively .","['On', 'IMDb', ',', 'we', 'reduce', 'the', 'error', 'dramatically', 'by', '43.9', '%', 'and', '22', '%', 'with', 'regard', 'to', 'CoVe', 'and', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'respectively', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ',', 'PRP', 'VB', 'DT', 'NN', 'RB', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'NN', 'TO', 'NNP', 'CC', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'RB', '.']",29
text-classification,5,181,"On TREC - 6 , our improvement - similar as the improvements of state - of - the - art approaches - is not statistically significant , due to the small size of the 500 - examples test set .","['On', 'TREC', '-', '6', ',', 'our', 'improvement', '-', 'similar', 'as', 'the', 'improvements', 'of', 'state', '-', 'of', '-', 'the', '-', 'art', 'approaches', '-', 'is', 'not', 'statistically', 'significant', ',', 'due', 'to', 'the', 'small', 'size', 'of', 'the', '500', '-', 'examples', 'test', 'set', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', ':', 'CD', ',', 'PRP$', 'NN', ':', 'JJ', 'IN', 'DT', 'NNS', 'IN', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', ':', 'VBZ', 'RB', 'RB', 'JJ', ',', 'JJ', 'TO', 'DT', 'JJ', 'NN', 'IN', 'DT', 'CD', ':', 'NNS', 'VBP', 'NN', '.']",40
text-classification,5,186,"On AG , we observe a similarly dramatic error reduction by 23.7 % compared to the state - of - the - art .","['On', 'AG', ',', 'we', 'observe', 'a', 'similarly', 'dramatic', 'error', 'reduction', 'by', '23.7', '%', 'compared', 'to', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', ',', 'PRP', 'VBP', 'DT', 'RB', 'JJ', 'NN', 'NN', 'IN', 'CD', 'NN', 'VBN', 'TO', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', '.']",24
text-classification,5,187,"On DBpedia , Yelp - bi , and Yelp - full , we reduce the error by 4.8 % , 18.2 % , 2.0 % respectively .","['On', 'DBpedia', ',', 'Yelp', '-', 'bi', ',', 'and', 'Yelp', '-', 'full', ',', 'we', 'reduce', 'the', 'error', 'by', '4.8', '%', ',', '18.2', '%', ',', '2.0', '%', 'respectively', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'NNP', ',', 'NNP', ':', 'NN', ',', 'CC', 'NNP', ':', 'JJ', ',', 'PRP', 'VB', 'DT', 'NN', 'IN', 'CD', 'NN', ',', 'CD', 'NN', ',', 'CD', 'NN', 'RB', '.']",27
text-classification,5,193,Low - shot learning,"['Low', '-', 'shot', 'learning']","['B-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NN', 'NN']",4
text-classification,5,202,"On IMDb and AG , supervised ULMFiT with only 100 labeled examples matches the performance of training from scratch with 10 and 20 more data respectively , clearly demonstrating the benefit of general - domain LM pretraining .","['On', 'IMDb', 'and', 'AG', ',', 'supervised', 'ULMFiT', 'with', 'only', '100', 'labeled', 'examples', 'matches', 'the', 'performance', 'of', 'training', 'from', 'scratch', 'with', '10', 'and', '20', 'more', 'data', 'respectively', ',', 'clearly', 'demonstrating', 'the', 'benefit', 'of', 'general', '-', 'domain', 'LM', 'pretraining', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', 'CC', 'NNP', ',', 'VBD', 'NNP', 'IN', 'RB', 'CD', 'VBD', 'NNS', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'IN', 'NN', 'IN', 'CD', 'CC', 'CD', 'JJR', 'NNS', 'RB', ',', 'RB', 'VBG', 'DT', 'NN', 'IN', 'JJ', ':', 'NN', 'NNP', 'NN', '.']",38
text-classification,5,204,"On TREC - 6 , ULMFiT significantly improves upon training from scratch ; as examples are shorter and fewer , supervised and semi-supervised ULMFiT achieve similar results .","['On', 'TREC', '-', '6', ',', 'ULMFiT', 'significantly', 'improves', 'upon', 'training', 'from', 'scratch', ';', 'as', 'examples', 'are', 'shorter', 'and', 'fewer', ',', 'supervised', 'and', 'semi-supervised', 'ULMFiT', 'achieve', 'similar', 'results', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ':', 'CD', ',', 'NNP', 'RB', 'VBZ', 'IN', 'VBG', 'IN', 'NN', ':', 'IN', 'NNS', 'VBP', 'JJR', 'CC', 'JJR', ',', 'JJ', 'CC', 'JJ', 'NNP', 'NN', 'JJ', 'NNS', '.']",28
text-classification,5,207,"Pretraining is most useful for small and medium - sized datasets , which are most common in commercial applications .","['Pretraining', 'is', 'most', 'useful', 'for', 'small', 'and', 'medium', '-', 'sized', 'datasets', ',', 'which', 'are', 'most', 'common', 'in', 'commercial', 'applications', '.']","['B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'VBZ', 'RBS', 'JJ', 'IN', 'JJ', 'CC', 'JJ', ':', 'JJ', 'NNS', ',', 'WDT', 'VBP', 'RBS', 'JJ', 'IN', 'JJ', 'NNS', '.']",20
text-classification,5,209,Impact of LM quality,"['Impact', 'of', 'LM', 'quality']","['B-n', 'I-n', 'I-n', 'I-n']","['NN', 'IN', 'NNP', 'NN']",4
text-classification,5,211,"Using our fine - tuning techniques , even a regular LM reaches surprisingly good performance on the larger datasets .","['Using', 'our', 'fine', '-', 'tuning', 'techniques', ',', 'even', 'a', 'regular', 'LM', 'reaches', 'surprisingly', 'good', 'performance', 'on', 'the', 'larger', 'datasets', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['VBG', 'PRP$', 'JJ', ':', 'NN', 'NNS', ',', 'RB', 'DT', 'JJ', 'NNP', 'VBZ', 'RB', 'JJ', 'NN', 'IN', 'DT', 'JJR', 'NNS', '.']",20
text-classification,5,212,"On the smaller TREC - 6 , a vanilla LM without dropout runs the risk of overfitting , which decreases performance .","['On', 'the', 'smaller', 'TREC', '-', '6', ',', 'a', 'vanilla', 'LM', 'without', 'dropout', 'runs', 'the', 'risk', 'of', 'overfitting', ',', 'which', 'decreases', 'performance', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJR', 'NNP', ':', 'CD', ',', 'DT', 'NN', 'NNP', 'IN', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'NN', ',', 'WDT', 'VBZ', 'NN', '.']",22
text-classification,5,215,Fine - tuning the LM is most beneficial for larger datasets .,"['Fine', '-', 'tuning', 'the', 'LM', 'is', 'most', 'beneficial', 'for', 'larger', 'datasets', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O']","['NNP', ':', 'VBG', 'DT', 'NNP', 'VBZ', 'JJS', 'JJ', 'IN', 'JJR', 'NNS', '.']",12
text-classification,5,225,"Fine - tuning the classifier significantly improves over training from scratch , particularly on the small TREC - 6 . ' Last ' , the standard fine - tuning method in CV , severely underfits and is never able to lower the training error to 0 . ' Chainthaw ' achieves competitive performance on the smaller datasets , but is outperformed significantly on the large AG .","['Fine', '-', 'tuning', 'the', 'classifier', 'significantly', 'improves', 'over', 'training', 'from', 'scratch', ',', 'particularly', 'on', 'the', 'small', 'TREC', '-', '6', '.', ""'"", 'Last', ""'"", ',', 'the', 'standard', 'fine', '-', 'tuning', 'method', 'in', 'CV', ',', 'severely', 'underfits', 'and', 'is', 'never', 'able', 'to', 'lower', 'the', 'training', 'error', 'to', '0', '.', ""'"", 'Chainthaw', ""'"", 'achieves', 'competitive', 'performance', 'on', 'the', 'smaller', 'datasets', ',', 'but', 'is', 'outperformed', 'significantly', 'on', 'the', 'large', 'AG', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'VBG', 'DT', 'NN', 'RB', 'VBZ', 'RP', 'NN', 'IN', 'NN', ',', 'RB', 'IN', 'DT', 'JJ', 'NNP', ':', 'CD', '.', ""''"", 'JJ', ""''"", ',', 'DT', 'JJ', 'JJ', ':', 'VBG', 'NN', 'IN', 'NNP', ',', 'RB', 'JJ', 'CC', 'VBZ', 'RB', 'JJ', 'TO', 'VB', 'DT', 'NN', 'NN', 'TO', 'CD', '.', ""''"", 'NNP', 'POS', 'NNS', 'JJ', 'NN', 'IN', 'DT', 'JJR', 'NNS', ',', 'CC', 'VBZ', 'VBN', 'RB', 'IN', 'DT', 'JJ', 'NNP', '.']",67
text-classification,5,238,"At the cost of training a second model , ensembling the predictions of a forward and backwards LM - classifier brings a performance boost of around 0.5 - 0.7 .","['At', 'the', 'cost', 'of', 'training', 'a', 'second', 'model', ',', 'ensembling', 'the', 'predictions', 'of', 'a', 'forward', 'and', 'backwards', 'LM', '-', 'classifier', 'brings', 'a', 'performance', 'boost', 'of', 'around', '0.5', '-', '0.7', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', ',', 'VBG', 'DT', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NNS', 'NNP', ':', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'IN', 'CD', ':', 'CD', '.']",30
text-classification,5,239,On IMD b we lower the test error from 5.30 of a single model to 4.58 for the bidirectional model .,"['On', 'IMD', 'b', 'we', 'lower', 'the', 'test', 'error', 'from', '5.30', 'of', 'a', 'single', 'model', 'to', '4.58', 'for', 'the', 'bidirectional', 'model', '.']","['B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NNP', 'NN', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CD', 'IN', 'DT', 'JJ', 'NN', 'TO', 'CD', 'IN', 'DT', 'JJ', 'NN', '.']",21
text-classification,7,2,Investigating Capsule Networks with Dynamic Routing for Text Classification,"['Investigating', 'Capsule', 'Networks', 'with', 'Dynamic', 'Routing', 'for', 'Text', 'Classification']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['VBG', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",9
text-classification,7,10,1 Codes are publicly available at : https://github.com/andyweizhao/capsule_text_classification .,"['1', 'Codes', 'are', 'publicly', 'available', 'at', ':', 'https://github.com/andyweizhao/capsule_text_classification', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']","['CD', 'NNS', 'VBP', 'RB', 'JJ', 'IN', ':', 'NN', '.']",9
text-classification,7,12,Modeling articles or sentences computationally is a fundamental topic in natural language processing .,"['Modeling', 'articles', 'or', 'sentences', 'computationally', 'is', 'a', 'fundamental', 'topic', 'in', 'natural', 'language', 'processing', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NNS', 'CC', 'NNS', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",14
text-classification,7,16,"Earlier efforts in modeling texts have achieved limited success on text categorization using a simple bag - of - words classifier , implying understanding the meaning of the individual word or n-gram is a necessary step towards more sophisticated models .","['Earlier', 'efforts', 'in', 'modeling', 'texts', 'have', 'achieved', 'limited', 'success', 'on', 'text', 'categorization', 'using', 'a', 'simple', 'bag', '-', 'of', '-', 'words', 'classifier', ',', 'implying', 'understanding', 'the', 'meaning', 'of', 'the', 'individual', 'word', 'or', 'n-gram', 'is', 'a', 'necessary', 'step', 'towards', 'more', 'sophisticated', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJR', 'NNS', 'IN', 'VBG', 'NNS', 'VBP', 'VBN', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'VBG', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'NNS', 'JJR', ',', 'VBG', 'VBG', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'CC', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NNS', 'RBR', 'JJ', 'NNS', '.']",41
text-classification,7,40,"Our capsule network , depicted in , is a variant of the capsule networks proposed in .","['Our', 'capsule', 'network', ',', 'depicted', 'in', ',', 'is', 'a', 'variant', 'of', 'the', 'capsule', 'networks', 'proposed', 'in', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'NN', ',', 'VBN', 'IN', ',', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'NNS', 'VBN', 'IN', '.']",17
text-classification,7,41,"It consists of four layers : ngram convolutional layer , primary capsule layer , convolutional capsule layer , and fully connected capsule layer .","['It', 'consists', 'of', 'four', 'layers', ':', 'ngram', 'convolutional', 'layer', ',', 'primary', 'capsule', 'layer', ',', 'convolutional', 'capsule', 'layer', ',', 'and', 'fully', 'connected', 'capsule', 'layer', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'IN', 'CD', 'NNS', ':', 'JJ', 'JJ', 'NN', ',', 'JJ', 'NN', 'NN', ',', 'JJ', 'NN', 'NN', ',', 'CC', 'RB', 'VBN', 'NN', 'NN', '.']",24
text-classification,7,42,"In addition , we explore two capsule frameworks to integrate these four components in different ways .","['In', 'addition', ',', 'we', 'explore', 'two', 'capsule', 'frameworks', 'to', 'integrate', 'these', 'four', 'components', 'in', 'different', 'ways', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'CD', 'NN', 'NNS', 'TO', 'VB', 'DT', 'CD', 'NNS', 'IN', 'JJ', 'NNS', '.']",17
text-classification,7,44,N - gram Convolutional Layer,"['N', '-', 'gram', 'Convolutional', 'Layer']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NN', 'NNP', 'NNP']",5
text-classification,7,45,This layer is a standard convolutional layer which extracts n-gram features at different positions of a sentence through various convolutional filters .,"['This', 'layer', 'is', 'a', 'standard', 'convolutional', 'layer', 'which', 'extracts', 'n-gram', 'features', 'at', 'different', 'positions', 'of', 'a', 'sentence', 'through', 'various', 'convolutional', 'filters', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'JJ', 'JJ', 'NNS', '.']",22
text-classification,7,58,Primary Capsule Layer,"['Primary', 'Capsule', 'Layer']","['B-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NNP']",3
text-classification,7,59,This is the first capsule layer in which the capsules replace the scalar - output feature detectors of CNNs with vector- output capsules to preserve the instantiated parameters such as the local order of words and semantic representations of words .,"['This', 'is', 'the', 'first', 'capsule', 'layer', 'in', 'which', 'the', 'capsules', 'replace', 'the', 'scalar', '-', 'output', 'feature', 'detectors', 'of', 'CNNs', 'with', 'vector-', 'output', 'capsules', 'to', 'preserve', 'the', 'instantiated', 'parameters', 'such', 'as', 'the', 'local', 'order', 'of', 'words', 'and', 'semantic', 'representations', 'of', 'words', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'WDT', 'DT', 'NNS', 'VB', 'DT', 'JJ', ':', 'NN', 'NN', 'NNS', 'IN', 'NNP', 'IN', 'JJ', 'NN', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'JJ', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'CC', 'JJ', 'NNS', 'IN', 'NNS', '.']",41
text-classification,7,84,Dynamic Routing,"['Dynamic', 'Routing']","['B-n', 'I-n']","['NNP', 'NNP']",2
text-classification,7,85,The basic idea of dynamic routing is to construct a non-linear map in an iterative manner ensuring that the output of each capsule gets sent to an appropriate parent in the subsequent layer :,"['The', 'basic', 'idea', 'of', 'dynamic', 'routing', 'is', 'to', 'construct', 'a', 'non-linear', 'map', 'in', 'an', 'iterative', 'manner', 'ensuring', 'that', 'the', 'output', 'of', 'each', 'capsule', 'gets', 'sent', 'to', 'an', 'appropriate', 'parent', 'in', 'the', 'subsequent', 'layer', ':']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBG', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'VBN', 'TO', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', ':']",34
text-classification,7,107,Convolutional Capsule Layer,"['Convolutional', 'Capsule', 'Layer']","['B-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NNP']",3
text-classification,7,108,"In this layer , each capsule is connected only to a local region K 2 C spatially in the layer below .","['In', 'this', 'layer', ',', 'each', 'capsule', 'is', 'connected', 'only', 'to', 'a', 'local', 'region', 'K', '2', 'C', 'spatially', 'in', 'the', 'layer', 'below', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'DT', 'NN', 'VBZ', 'VBN', 'RB', 'TO', 'DT', 'JJ', 'NN', 'NNP', 'CD', 'NNP', 'RB', 'IN', 'DT', 'NN', 'IN', '.']",22
text-classification,7,109,Those capsules in the region multiply transformation matrices to learn child - parent relationships followed by routing by agreement to produce parent capsules in the layer above .,"['Those', 'capsules', 'in', 'the', 'region', 'multiply', 'transformation', 'matrices', 'to', 'learn', 'child', '-', 'parent', 'relationships', 'followed', 'by', 'routing', 'by', 'agreement', 'to', 'produce', 'parent', 'capsules', 'in', 'the', 'layer', 'above', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'DT', 'NN', 'NN', 'NN', 'NNS', 'TO', 'VB', 'NN', ':', 'NN', 'NNS', 'VBN', 'IN', 'VBG', 'IN', 'NN', 'TO', 'VB', 'NN', 'NNS', 'IN', 'DT', 'NN', 'IN', '.']",28
text-classification,7,117,Fully Connected Capsule Layer,"['Fully', 'Connected', 'Capsule', 'Layer']","['B-n', 'I-n', 'I-n', 'I-n']","['RB', 'VBN', 'NNP', 'NNP']",4
text-classification,7,118,The capsules in the layer below are flattened into a list of capsules and fed into fully connected capsule layer in which capsules are multiplied by transformation matrix W d 1 ?,"['The', 'capsules', 'in', 'the', 'layer', 'below', 'are', 'flattened', 'into', 'a', 'list', 'of', 'capsules', 'and', 'fed', 'into', 'fully', 'connected', 'capsule', 'layer', 'in', 'which', 'capsules', 'are', 'multiplied', 'by', 'transformation', 'matrix', 'W', 'd', '1', '?']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'DT', 'NN', 'IN', 'VBP', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NNS', 'CC', 'VBN', 'IN', 'RB', 'VBN', 'NN', 'NN', 'IN', 'WDT', 'NNS', 'VBP', 'VBN', 'IN', 'NN', 'NN', 'NNP', 'VBZ', 'CD', '.']",32
text-classification,7,124,The Architectures of Capsule Network,"['The', 'Architectures', 'of', 'Capsule', 'Network']","['O', 'B-n', 'I-n', 'I-n', 'I-n']","['DT', 'NNP', 'IN', 'NNP', 'NNP']",5
text-classification,7,125,We explore two capsule architectures ( denoted as Capsule - A and Capsule - B ) to integrate these four,"['We', 'explore', 'two', 'capsule', 'architectures', '(', 'denoted', 'as', 'Capsule', '-', 'A', 'and', 'Capsule', '-', 'B', ')', 'to', 'integrate', 'these', 'four']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'CD', 'NN', 'NNS', '(', 'VBN', 'IN', 'NNP', ':', 'DT', 'CC', 'NNP', ':', 'NN', ')', 'TO', 'VB', 'DT', 'CD']",20
text-classification,7,139,"In the experiments , we use 300 - dimensional word2vec vectors to initialize embedding vectors .","['In', 'the', 'experiments', ',', 'we', 'use', '300', '-', 'dimensional', 'word2vec', 'vectors', 'to', 'initialize', 'embedding', 'vectors', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'PRP', 'VBP', 'CD', ':', 'JJ', 'NN', 'NNS', 'TO', 'VB', 'VBG', 'NNS', '.']",16
text-classification,7,140,We conduct mini-batch with size 50 for AG 's news and size 25 for other datasets .,"['We', 'conduct', 'mini-batch', 'with', 'size', '50', 'for', 'AG', ""'s"", 'news', 'and', 'size', '25', 'for', 'other', 'datasets', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'IN', 'NN', 'CD', 'IN', 'NNP', 'POS', 'NN', 'CC', 'NN', 'CD', 'IN', 'JJ', 'NNS', '.']",17
text-classification,7,141,We use Adam optimization algorithm with 1e - 3 learning rate to train the model .,"['We', 'use', 'Adam', 'optimization', 'algorithm', 'with', '1e', '-', '3', 'learning', 'rate', 'to', 'train', 'the', 'model', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'NN', 'IN', 'CD', ':', 'CD', 'VBG', 'NN', 'TO', 'VB', 'DT', 'NN', '.']",16
text-classification,7,142,We use 3 iteration of routing for all datasets since it optimizes the loss faster and converges to a lower loss at the end .,"['We', 'use', '3', 'iteration', 'of', 'routing', 'for', 'all', 'datasets', 'since', 'it', 'optimizes', 'the', 'loss', 'faster', 'and', 'converges', 'to', 'a', 'lower', 'loss', 'at', 'the', 'end', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'CD', 'NN', 'IN', 'VBG', 'IN', 'DT', 'NNS', 'IN', 'PRP', 'VBZ', 'DT', 'NN', 'RBR', 'CC', 'NNS', 'TO', 'DT', 'JJR', 'NN', 'IN', 'DT', 'NN', '.']",25
text-classification,7,144,"In the experiments , we evaluate and compare our model with several strong baseline methods including : LSTM / Bi - LSTM , tree - structured LSTM ( Tree - LSTM ) , LSTM regularized by linguistic knowledge ( LR - LSTM ) , CNNrand / CNN - static / CNN - non-static ( Kim , 2014 ) , very deep convolutional network ( VD - CNN ) , and character - level convolutional network ( CL - CNN ) .","['In', 'the', 'experiments', ',', 'we', 'evaluate', 'and', 'compare', 'our', 'model', 'with', 'several', 'strong', 'baseline', 'methods', 'including', ':', 'LSTM', '/', 'Bi', '-', 'LSTM', ',', 'tree', '-', 'structured', 'LSTM', '(', 'Tree', '-', 'LSTM', ')', ',', 'LSTM', 'regularized', 'by', 'linguistic', 'knowledge', '(', 'LR', '-', 'LSTM', ')', ',', 'CNNrand', '/', 'CNN', '-', 'static', '/', 'CNN', '-', 'non-static', '(', 'Kim', ',', '2014', ')', ',', 'very', 'deep', 'convolutional', 'network', '(', 'VD', '-', 'CNN', ')', ',', 'and', 'character', '-', 'level', 'convolutional', 'network', '(', 'CL', '-', 'CNN', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'PRP', 'VBP', 'CC', 'VBP', 'PRP$', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'NNS', 'VBG', ':', 'NNP', 'NNP', 'NNP', ':', 'NNP', ',', 'JJ', ':', 'VBN', 'NNP', '(', 'NNP', ':', 'NNP', ')', ',', 'NNP', 'VBN', 'IN', 'JJ', 'NN', '(', 'NNP', ':', 'NNP', ')', ',', 'NNP', 'NNP', 'NNP', ':', 'JJ', 'NN', 'NNP', ':', 'JJ', '(', 'NNP', ',', 'CD', ')', ',', 'RB', 'JJ', 'NN', 'NN', '(', 'NNP', ':', 'NN', ')', ',', 'CC', 'SYM', ':', 'NN', 'JJ', 'NN', '(', 'NNP', ':', 'NN', ')', '.']",81
text-classification,7,149,"From the results , we observe that the capsule networks achieve best results on 4 out of 6 benchmarks , which verifies the effectiveness of the capsule networks .","['From', 'the', 'results', ',', 'we', 'observe', 'that', 'the', 'capsule', 'networks', 'achieve', 'best', 'results', 'on', '4', 'out', 'of', '6', 'benchmarks', ',', 'which', 'verifies', 'the', 'effectiveness', 'of', 'the', 'capsule', 'networks', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNS', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'NNS', 'VBP', 'JJS', 'NNS', 'IN', 'CD', 'IN', 'IN', 'CD', 'NNS', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'NNS', '.']",29
text-classification,7,154,"Generally , all three proposed dynamic routing strategies contribute to the effectiveness of Capsule - B by alleviating the disturbance of some noise capsules which may contain "" background "" information such as stop words and the words that are unrelated to specific categories .","['Generally', ',', 'all', 'three', 'proposed', 'dynamic', 'routing', 'strategies', 'contribute', 'to', 'the', 'effectiveness', 'of', 'Capsule', '-', 'B', 'by', 'alleviating', 'the', 'disturbance', 'of', 'some', 'noise', 'capsules', 'which', 'may', 'contain', '""', 'background', '""', 'information', 'such', 'as', 'stop', 'words', 'and', 'the', 'words', 'that', 'are', 'unrelated', 'to', 'specific', 'categories', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'DT', 'CD', 'VBN', 'JJ', 'VBG', 'NNS', 'VBP', 'TO', 'DT', 'NN', 'IN', 'NNP', ':', 'NN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'NNS', 'WDT', 'MD', 'VB', 'JJ', 'NN', 'NNP', 'NN', 'JJ', 'IN', 'JJ', 'NNS', 'CC', 'DT', 'NNS', 'WDT', 'VBP', 'JJ', 'TO', 'JJ', 'NNS', '.']",45
text-classification,4,2,Learning Context - Sensitive Convolutional Filters for Text Processing,"['Learning', 'Context', '-', 'Sensitive', 'Convolutional', 'Filters', 'for', 'Text', 'Processing']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['VBG', 'NNP', ':', 'JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",9
text-classification,4,6,"In this paper , we consider an approach of using a small meta network to learn contextsensitive convolutional filters for text processing .","['In', 'this', 'paper', ',', 'we', 'consider', 'an', 'approach', 'of', 'using', 'a', 'small', 'meta', 'network', 'to', 'learn', 'contextsensitive', 'convolutional', 'filters', 'for', 'text', 'processing', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'JJ', 'JJ', 'NNS', 'IN', 'NN', 'NN', '.']",23
text-classification,4,26,"In this paper , we propose a generic approach to learn context - sensitive convolutional filters for natural language understanding .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'generic', 'approach', 'to', 'learn', 'context', '-', 'sensitive', 'convolutional', 'filters', 'for', 'natural', 'language', 'understanding', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'TO', 'VB', 'JJ', ':', 'JJ', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NN', '.']",21
text-classification,4,27,"In contrast to traditional CNNs , the convolution operation in our framework does not have a fixed set of filters , and thus provides the network with stronger modeling flexibility and capacity .","['In', 'contrast', 'to', 'traditional', 'CNNs', ',', 'the', 'convolution', 'operation', 'in', 'our', 'framework', 'does', 'not', 'have', 'a', 'fixed', 'set', 'of', 'filters', ',', 'and', 'thus', 'provides', 'the', 'network', 'with', 'stronger', 'modeling', 'flexibility', 'and', 'capacity', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'JJ', 'NNP', ',', 'DT', 'NN', 'NN', 'IN', 'PRP$', 'NN', 'VBZ', 'RB', 'VB', 'DT', 'VBN', 'NN', 'IN', 'NNS', ',', 'CC', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'JJR', 'VBG', 'NN', 'CC', 'NN', '.']",33
text-classification,4,28,"Specifically , we introduce a meta network to generate a set of contextsensitive filters , conditioned on specific input sentences ; these filters are adaptively applied to either the same ( Section 3.2 ) or different ( Section 3.3 ) text sequences .","['Specifically', ',', 'we', 'introduce', 'a', 'meta', 'network', 'to', 'generate', 'a', 'set', 'of', 'contextsensitive', 'filters', ',', 'conditioned', 'on', 'specific', 'input', 'sentences', ';', 'these', 'filters', 'are', 'adaptively', 'applied', 'to', 'either', 'the', 'same', '(', 'Section', '3.2', ')', 'or', 'different', '(', 'Section', '3.3', ')', 'text', 'sequences', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', ',', 'VBN', 'IN', 'JJ', 'NN', 'NNS', ':', 'DT', 'NNS', 'VBP', 'RB', 'VBN', 'TO', 'CC', 'DT', 'JJ', '(', 'NNP', 'CD', ')', 'CC', 'JJ', '(', 'NNP', 'CD', ')', 'NN', 'NNS', '.']",43
text-classification,4,29,"In this manner , the learned filters vary from sentence to sentence and allow for more fine - grained feature abstraction .","['In', 'this', 'manner', ',', 'the', 'learned', 'filters', 'vary', 'from', 'sentence', 'to', 'sentence', 'and', 'allow', 'for', 'more', 'fine', '-', 'grained', 'feature', 'abstraction', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'DT', 'JJ', 'NNS', 'VBP', 'IN', 'NN', 'TO', 'NN', 'CC', 'VB', 'IN', 'JJR', 'JJ', ':', 'JJ', 'NN', 'NN', '.']",22
text-classification,4,31,"In this regard , we propose a novel bidirectional filter generation mechanism to allow interactions between sentence pairs while constructing context - sensitive representations .","['In', 'this', 'regard', ',', 'we', 'propose', 'a', 'novel', 'bidirectional', 'filter', 'generation', 'mechanism', 'to', 'allow', 'interactions', 'between', 'sentence', 'pairs', 'while', 'constructing', 'context', '-', 'sensitive', 'representations', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'NN', 'TO', 'VB', 'NNS', 'IN', 'NN', 'NNS', 'IN', 'VBG', 'JJ', ':', 'JJ', 'NNS', '.']",25
text-classification,4,152,"For the document classification experiments , we randomly initialize the word embeddings uniformly within [ ? 0.001 , 0.001 ] and update them during training .","['For', 'the', 'document', 'classification', 'experiments', ',', 'we', 'randomly', 'initialize', 'the', 'word', 'embeddings', 'uniformly', 'within', '[', '?', '0.001', ',', '0.001', ']', 'and', 'update', 'them', 'during', 'training', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'NN', 'NNS', ',', 'PRP', 'VBP', 'VB', 'DT', 'NN', 'VBZ', 'RB', 'IN', 'NNP', '.', 'CD', ',', 'CD', 'NN', 'CC', 'VB', 'PRP', 'IN', 'NN', '.']",26
text-classification,4,153,"For the generated filters , we set the window size as h = 5 , with K = 100 feature maps ( the dimension of z is set as 100 ) .","['For', 'the', 'generated', 'filters', ',', 'we', 'set', 'the', 'window', 'size', 'as', 'h', '=', '5', ',', 'with', 'K', '=', '100', 'feature', 'maps', '(', 'the', 'dimension', 'of', 'z', 'is', 'set', 'as', '100', ')', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'NN', 'NN', 'CD', ',', 'IN', 'NNP', 'NNP', 'CD', 'NN', 'NNS', '(', 'DT', 'NN', 'IN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', ')', '.']",32
text-classification,4,155,"A one - layer architec - ture is utilized for both the CNN baseline and the ACNN model , since we did not observe significant performance gains with a multilayer architecture .","['A', 'one', '-', 'layer', 'architec', '-', 'ture', 'is', 'utilized', 'for', 'both', 'the', 'CNN', 'baseline', 'and', 'the', 'ACNN', 'model', ',', 'since', 'we', 'did', 'not', 'observe', 'significant', 'performance', 'gains', 'with', 'a', 'multilayer', 'architecture', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'CD', ':', 'NN', 'JJ', ':', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'DT', 'NNP', 'NN', 'CC', 'DT', 'NNP', 'NN', ',', 'IN', 'PRP', 'VBD', 'RB', 'VB', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",32
text-classification,4,156,"The minibatch size is set as 128 , and a dropout rate of 0.2 is utilized on the embedding layer .","['The', 'minibatch', 'size', 'is', 'set', 'as', '128', ',', 'and', 'a', 'dropout', 'rate', 'of', '0.2', 'is', 'utilized', 'on', 'the', 'embedding', 'layer', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', ',', 'CC', 'DT', 'NN', 'NN', 'IN', 'CD', 'VBZ', 'VBN', 'IN', 'DT', 'VBG', 'NN', '.']",21
text-classification,4,158,"For the sentence matching tasks , we initialized the word embeddings with 50 - dimensional Glove word vectors pretrained from Wikipedia 2014 and Gigaword 5 for all model variants .","['For', 'the', 'sentence', 'matching', 'tasks', ',', 'we', 'initialized', 'the', 'word', 'embeddings', 'with', '50', '-', 'dimensional', 'Glove', 'word', 'vectors', 'pretrained', 'from', 'Wikipedia', '2014', 'and', 'Gigaword', '5', 'for', 'all', 'model', 'variants', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'VBG', 'NNS', ',', 'PRP', 'VBD', 'DT', 'NN', 'VBZ', 'IN', 'CD', ':', 'JJ', 'NNP', 'NN', 'NNS', 'VBD', 'IN', 'NNP', 'CD', 'CC', 'NNP', 'CD', 'IN', 'DT', 'NN', 'NNS', '.']",30
text-classification,4,159,"As for the filters , we set the window size as h = 5 , with K = 300 feature maps .","['As', 'for', 'the', 'filters', ',', 'we', 'set', 'the', 'window', 'size', 'as', 'h', '=', '5', ',', 'with', 'K', '=', '300', 'feature', 'maps', '.']","['O', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'IN', 'DT', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'NN', 'NN', 'CD', ',', 'IN', 'NNP', 'NNP', 'CD', 'NN', 'NNS', '.']",22
text-classification,4,161,"We use Adam to train the models , with a learning rate of 3 10 ?4 .","['We', 'use', 'Adam', 'to', 'train', 'the', 'models', ',', 'with', 'a', 'learning', 'rate', 'of', '3', '10', '?4', '.']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'TO', 'VB', 'DT', 'NNS', ',', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', 'CD', 'NN', '.']",17
text-classification,4,162,"Dropout , with a rate of 0.5 , is employed on the word embedding layer .","['Dropout', ',', 'with', 'a', 'rate', 'of', '0.5', ',', 'is', 'employed', 'on', 'the', 'word', 'embedding', 'layer', '.']","['B-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NN', ',', 'IN', 'DT', 'NN', 'IN', 'CD', ',', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'VBG', 'NN', '.']",16
text-classification,4,164,All models are implemented with TensorFlow and are trained using one NVIDIA GeForce GTX TITAN X GPU with 12 GB memory .,"['All', 'models', 'are', 'implemented', 'with', 'TensorFlow', 'and', 'are', 'trained', 'using', 'one', 'NVIDIA', 'GeForce', 'GTX', 'TITAN', 'X', 'GPU', 'with', '12', 'GB', 'memory', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'CC', 'VBP', 'VBN', 'VBG', 'CD', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'CD', 'NNP', 'NN', '.']",22
text-classification,4,166,"For document classification , we consider several baseline models : ( i ) ngrams , a bag - of - means method based on TFIDF representations built by choosing the 500,000 most frequent n-grams ( up to 5 - grams ) from the training set and use their corresponding counts as features ; ( ii ) small / large word CNN : 6 layer word - based convolutional networks , with 256/1024 features at each layer , denoted as small / large , respectively ; ( iii ) deep CNN : deep convolutional neural networks with 9/17 /29 layers .","['For', 'document', 'classification', ',', 'we', 'consider', 'several', 'baseline', 'models', ':', '(', 'i', ')', 'ngrams', ',', 'a', 'bag', '-', 'of', '-', 'means', 'method', 'based', 'on', 'TFIDF', 'representations', 'built', 'by', 'choosing', 'the', '500,000', 'most', 'frequent', 'n-grams', '(', 'up', 'to', '5', '-', 'grams', ')', 'from', 'the', 'training', 'set', 'and', 'use', 'their', 'corresponding', 'counts', 'as', 'features', ';', '(', 'ii', ')', 'small', '/', 'large', 'word', 'CNN', ':', '6', 'layer', 'word', '-', 'based', 'convolutional', 'networks', ',', 'with', '256/1024', 'features', 'at', 'each', 'layer', ',', 'denoted', 'as', 'small', '/', 'large', ',', 'respectively', ';', '(', 'iii', ')', 'deep', 'CNN', ':', 'deep', 'convolutional', 'neural', 'networks', 'with', '9/17', '/29', 'layers', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'NNS', ':', '(', 'NN', ')', 'NN', ',', 'DT', 'JJ', ':', 'IN', ':', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'NNS', 'VBN', 'IN', 'VBG', 'DT', 'CD', 'JJS', 'JJ', 'NNS', '(', 'IN', 'TO', 'CD', ':', 'NNS', ')', 'IN', 'DT', 'NN', 'NN', 'CC', 'VB', 'PRP$', 'NN', 'NNS', 'IN', 'NNS', ':', '(', 'NN', ')', 'JJ', 'NNP', 'JJ', 'NN', 'NNP', ':', 'CD', 'NN', 'NN', ':', 'VBN', 'JJ', 'NNS', ',', 'IN', 'CD', 'NNS', 'IN', 'DT', 'NN', ',', 'VBD', 'IN', 'JJ', 'NNP', 'JJ', ',', 'RB', ':', '(', 'NN', ')', 'JJ', 'NN', ':', 'JJ', 'JJ', 'JJ', 'NNS', 'IN', 'CD', 'JJ', 'NNS', '.']",100
text-classification,4,173,Document Classification,"['Document', 'Classification']","['B-n', 'I-n']","['NNP', 'NNP']",2
text-classification,4,175,"As illustrated in , S - ACNN significantly outperforms S - CNN on both datasets , demonstrating the advantage of the filtergeneration module in our ACNN framework .","['As', 'illustrated', 'in', ',', 'S', '-', 'ACNN', 'significantly', 'outperforms', 'S', '-', 'CNN', 'on', 'both', 'datasets', ',', 'demonstrating', 'the', 'advantage', 'of', 'the', 'filtergeneration', 'module', 'in', 'our', 'ACNN', 'framework', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'IN', ',', 'NNP', ':', 'NNP', 'RB', 'VBZ', 'NNP', ':', 'NN', 'IN', 'DT', 'NNS', ',', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'PRP$', 'NNP', 'NN', '.']",28
text-classification,4,179,"Although we only use one convolution layer for our ACNN model , it already outperforms other CNN baseline methods with much deeper architectures .","['Although', 'we', 'only', 'use', 'one', 'convolution', 'layer', 'for', 'our', 'ACNN', 'model', ',', 'it', 'already', 'outperforms', 'other', 'CNN', 'baseline', 'methods', 'with', 'much', 'deeper', 'architectures', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'PRP', 'RB', 'VBP', 'CD', 'NN', 'NN', 'IN', 'PRP$', 'NNP', 'NN', ',', 'PRP', 'RB', 'VBZ', 'JJ', 'NNP', 'NN', 'NNS', 'IN', 'JJ', 'JJR', 'NNS', '.']",24
text-classification,4,182,"Moreover , our M - ACNN also achieves slightly better performance than self - attentive sentence embeddings proposed in , which requires significant more parameters than our method .","['Moreover', ',', 'our', 'M', '-', 'ACNN', 'also', 'achieves', 'slightly', 'better', 'performance', 'than', 'self', '-', 'attentive', 'sentence', 'embeddings', 'proposed', 'in', ',', 'which', 'requires', 'significant', 'more', 'parameters', 'than', 'our', 'method', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'NNP', ':', 'NNP', 'RB', 'VBZ', 'RB', 'JJR', 'NN', 'IN', 'PRP', ':', 'JJ', 'NN', 'NNS', 'VBN', 'IN', ',', 'WDT', 'VBZ', 'JJ', 'JJR', 'NNS', 'IN', 'PRP$', 'NN', '.']",29
text-classification,4,188,Answer Sentence Selection,"['Answer', 'Sentence', 'Selection']","['B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP']",3
text-classification,4,200,"Notably , our model yields significantly better results than an attentive pooling network and ABCNN ( attention - based CNN ) baselines .","['Notably', ',', 'our', 'model', 'yields', 'significantly', 'better', 'results', 'than', 'an', 'attentive', 'pooling', 'network', 'and', 'ABCNN', '(', 'attention', '-', 'based', 'CNN', ')', 'baselines', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'NN', 'NNS', 'RB', 'JJR', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', 'CC', 'NNP', '(', 'NN', ':', 'VBN', 'NNP', ')', 'NNS', '.']",23
text-classification,6,2,Universal Sentence Encoder,"['Universal', 'Sentence', 'Encoder']","['B-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NN']",3
text-classification,6,4,We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks .,"['We', 'present', 'models', 'for', 'encoding', 'sentences', 'into', 'embedding', 'vectors', 'that', 'specifically', 'target', 'transfer', 'learning', 'to', 'other', 'NLP', 'tasks', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'NNS', 'IN', 'VBG', 'NNS', 'IN', 'VBG', 'NNS', 'WDT', 'RB', 'VBP', 'VB', 'NN', 'TO', 'JJ', 'NNP', 'NNS', '.']",19
text-classification,6,9,We find that transfer learning using sentence embeddings tends to outperform word level transfer .,"['We', 'find', 'that', 'transfer', 'learning', 'using', 'sentence', 'embeddings', 'tends', 'to', 'outperform', 'word', 'level', 'transfer', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'VBG', 'VBG', 'NN', 'NNS', 'VBZ', 'TO', 'VB', 'NN', 'NN', 'NN', '.']",15
text-classification,6,10,"With transfer learning via sentence embeddings , we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task .","['With', 'transfer', 'learning', 'via', 'sentence', 'embeddings', ',', 'we', 'observe', 'surprisingly', 'good', 'performance', 'with', 'minimal', 'amounts', 'of', 'supervised', 'training', 'data', 'for', 'a', 'transfer', 'task', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', 'VBG', 'IN', 'NN', 'NNS', ',', 'PRP', 'VBP', 'RB', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",24
text-classification,6,32,Both models are implemented in TensorFlow and are available to download from TF Hub : 1 https://tfhub.dev/google/universal-sentence-encoder/1,"['Both', 'models', 'are', 'implemented', 'in', 'TensorFlow', 'and', 'are', 'available', 'to', 'download', 'from', 'TF', 'Hub', ':', '1', 'https://tfhub.dev/google/universal-sentence-encoder/1']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'CC', 'VBP', 'JJ', 'TO', 'VB', 'IN', 'NNP', 'NNP', ':', 'CD', 'NN']",17
text-classification,6,44,The transformer based sentence encoding model constructs sentence embeddings using the encoding sub - graph of the transformer architecture .,"['The', 'transformer', 'based', 'sentence', 'encoding', 'model', 'constructs', 'sentence', 'embeddings', 'using', 'the', 'encoding', 'sub', '-', 'graph', 'of', 'the', 'transformer', 'architecture', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBN', 'NN', 'VBG', 'NN', 'NNS', 'NN', 'NNS', 'VBG', 'DT', 'VBG', 'JJ', ':', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",20
text-classification,6,45,This sub - graph uses attention to compute context aware representations of words in a sentence that take into account both the ordering and identity of all the other words .,"['This', 'sub', '-', 'graph', 'uses', 'attention', 'to', 'compute', 'context', 'aware', 'representations', 'of', 'words', 'in', 'a', 'sentence', 'that', 'take', 'into', 'account', 'both', 'the', 'ordering', 'and', 'identity', 'of', 'all', 'the', 'other', 'words', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', ':', 'NN', 'VBZ', 'NN', 'TO', 'VB', 'NN', 'JJ', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'NN', 'WDT', 'VBP', 'IN', 'NN', 'CC', 'DT', 'NN', 'CC', 'NN', 'IN', 'PDT', 'DT', 'JJ', 'NNS', '.']",31
text-classification,6,46,The context aware word representations are converted to a fixed length sentence encoding vector by computing the element - wise sum of the representations at each word position .,"['The', 'context', 'aware', 'word', 'representations', 'are', 'converted', 'to', 'a', 'fixed', 'length', 'sentence', 'encoding', 'vector', 'by', 'computing', 'the', 'element', '-', 'wise', 'sum', 'of', 'the', 'representations', 'at', 'each', 'word', 'position', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'JJ', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'DT', 'JJ', 'NN', 'NN', 'VBG', 'NN', 'IN', 'VBG', 'DT', 'NN', ':', 'NN', 'NN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",29
text-classification,6,48,The encoding model is designed to be as general purpose as possible .,"['The', 'encoding', 'model', 'is', 'designed', 'to', 'be', 'as', 'general', 'purpose', 'as', 'possible', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['DT', 'VBG', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'IN', 'JJ', 'NN', 'IN', 'JJ', '.']",13
text-classification,6,49,This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks .,"['This', 'is', 'accomplished', 'by', 'using', 'multi-task', 'learning', 'whereby', 'a', 'single', 'encoding', 'model', 'is', 'used', 'to', 'feed', 'multiple', 'downstream', 'tasks', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'VBN', 'IN', 'VBG', 'JJ', 'NN', 'WRB', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'JJ', 'NN', 'NNS', '.']",20
text-classification,6,54,Deep Averaging Network ( DAN ),"['Deep', 'Averaging', 'Network', '(', 'DAN', ')']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', '(', 'NNP', ')']",6
text-classification,6,55,The second encoding model makes use of a deep averaging network ( DAN ) whereby input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural network ( DNN ) to produce sentence embeddings .,"['The', 'second', 'encoding', 'model', 'makes', 'use', 'of', 'a', 'deep', 'averaging', 'network', '(', 'DAN', ')', 'whereby', 'input', 'embeddings', 'for', 'words', 'and', 'bi-grams', 'are', 'first', 'averaged', 'together', 'and', 'then', 'passed', 'through', 'a', 'feedforward', 'deep', 'neural', 'network', '(', 'DNN', ')', 'to', 'produce', 'sentence', 'embeddings', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'JJ', 'NN', 'VBZ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'WRB', 'JJ', 'NNS', 'IN', 'NNS', 'CC', 'NNS', 'VBP', 'RB', 'VBN', 'RB', 'CC', 'RB', 'VBD', 'IN', 'DT', 'JJ', 'JJ', 'JJ', 'NN', '(', 'NNP', ')', 'TO', 'VB', 'NN', 'NNS', '.']",42
text-classification,6,56,"Similar to the Transformer encoder , the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding .","['Similar', 'to', 'the', 'Transformer', 'encoder', ',', 'the', 'DAN', 'encoder', 'takes', 'as', 'input', 'a', 'lowercased', 'PTB', 'tokenized', 'string', 'and', 'outputs', 'a', '512', 'dimensional', 'sentence', 'embedding', '.']","['O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'TO', 'DT', 'NNP', 'NN', ',', 'DT', 'NNP', 'NN', 'VBZ', 'IN', 'NN', 'DT', 'JJ', 'NNP', 'VBD', 'NN', 'CC', 'VBZ', 'DT', 'CD', 'JJ', 'NN', 'NN', '.']",25
text-classification,6,58,We make use of mul-titask learning whereby a single DAN encoder is used to supply sentence embeddings for multiple downstream tasks .,"['We', 'make', 'use', 'of', 'mul-titask', 'learning', 'whereby', 'a', 'single', 'DAN', 'encoder', 'is', 'used', 'to', 'supply', 'sentence', 'embeddings', 'for', 'multiple', 'downstream', 'tasks', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NN', 'IN', 'NN', 'NN', 'WRB', 'DT', 'JJ', 'NNP', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'NN', 'NNS', 'IN', 'JJ', 'NN', 'NNS', '.']",22
text-classification,6,59,The primary advantage of the DAN encoder is that compute time is linear in the length of the input sequence .,"['The', 'primary', 'advantage', 'of', 'the', 'DAN', 'encoder', 'is', 'that', 'compute', 'time', 'is', 'linear', 'in', 'the', 'length', 'of', 'the', 'input', 'sequence', '.']","['O', 'B-p', 'I-p', 'B-p', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', 'VBZ', 'IN', 'JJ', 'NN', 'VBZ', 'JJ', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",21
text-classification,6,69,MR : Movie review snippet sentiment on a five star scale .,"['MR', ':', 'Movie', 'review', 'snippet', 'sentiment', 'on', 'a', 'five', 'star', 'scale', '.']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NN', ':', 'NNP', 'NN', 'NN', 'NN', 'IN', 'DT', 'CD', 'NN', 'NN', '.']",12
text-classification,6,70,CR : Sentiment of sentences mined from customer reviews .,"['CR', ':', 'Sentiment', 'of', 'sentences', 'mined', 'from', 'customer', 'reviews', '.']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['NN', ':', 'NN', 'IN', 'NNS', 'VBN', 'IN', 'NN', 'NNS', '.']",10
text-classification,6,71,SUBJ : Subjectivity of sentences from movie reviews and plot summaries .,"['SUBJ', ':', 'Subjectivity', 'of', 'sentences', 'from', 'movie', 'reviews', 'and', 'plot', 'summaries', '.']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', ':', 'NN', 'IN', 'NNS', 'IN', 'NN', 'NNS', 'CC', 'NN', 'NNS', '.']",12
text-classification,6,72,MPQA : Phrase level opinion polarity from news data .,"['MPQA', ':', 'Phrase', 'level', 'opinion', 'polarity', 'from', 'news', 'data', '.']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NN', ':', 'NNP', 'NN', 'NN', 'NN', 'IN', 'NN', 'NNS', '.']",10
text-classification,6,73,TREC : Fine grained question classification sourced from TREC .,"['TREC', ':', 'Fine', 'grained', 'question', 'classification', 'sourced', 'from', 'TREC', '.']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['NN', ':', 'NNP', 'VBD', 'NN', 'NN', 'VBD', 'IN', 'NNP', '.']",10
text-classification,6,74,SST : Binary phrase level sentiment classification .,"['SST', ':', 'Binary', 'phrase', 'level', 'sentiment', 'classification', '.']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', ':', 'JJ', 'NN', 'NN', 'NN', 'NN', '.']",8
text-classification,6,75,STS Benchmark : Semantic textual similarity ( STS ) between sentence pairs scored by Pearson correlation with human judgments .,"['STS', 'Benchmark', ':', 'Semantic', 'textual', 'similarity', '(', 'STS', ')', 'between', 'sentence', 'pairs', 'scored', 'by', 'Pearson', 'correlation', 'with', 'human', 'judgments', '.']","['B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', ':', 'JJ', 'JJ', 'NN', '(', 'NNP', ')', 'IN', 'NN', 'NNS', 'VBN', 'IN', 'NNP', 'NN', 'IN', 'JJ', 'NNS', '.']",20
text-classification,6,76,WEAT : Word pairs from the psychology literature on implicit association tests ( IAT ) that are used to characterize model bias .,"['WEAT', ':', 'Word', 'pairs', 'from', 'the', 'psychology', 'literature', 'on', 'implicit', 'association', 'tests', '(', 'IAT', ')', 'that', 'are', 'used', 'to', 'characterize', 'model', 'bias', '.']","['B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', ':', 'NNP', 'VBZ', 'IN', 'DT', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NNS', '(', 'NNP', ')', 'WDT', 'VBP', 'VBN', 'TO', 'VB', 'NN', 'NN', '.']",23
text-classification,6,84,"For word level transfer , we use word embeddings from a word2 vec skip - gram model trained on a corpus of news data .","['For', 'word', 'level', 'transfer', ',', 'we', 'use', 'word', 'embeddings', 'from', 'a', 'word2', 'vec', 'skip', '-', 'gram', 'model', 'trained', 'on', 'a', 'corpus', 'of', 'news', 'data', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'NN', 'NN', 'NN', ',', 'PRP', 'VBP', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'SYM', ':', 'NN', 'NN', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', '.']",25
text-classification,6,85,The pretrained word embeddings are included as input to two model types : a convolutional neural network models ( CNN ) ; a DAN .,"['The', 'pretrained', 'word', 'embeddings', 'are', 'included', 'as', 'input', 'to', 'two', 'model', 'types', ':', 'a', 'convolutional', 'neural', 'network', 'models', '(', 'CNN', ')', ';', 'a', 'DAN', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'O']","['DT', 'JJ', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'NN', 'TO', 'CD', 'NN', 'NNS', ':', 'DT', 'JJ', 'JJ', 'NN', 'NNS', '(', 'NNP', ')', ':', 'DT', 'NNP', '.']",25
text-classification,6,87,Additional baseline CNN and DAN models are trained without using any pretrained word or sentence embeddings .,"['Additional', 'baseline', 'CNN', 'and', 'DAN', 'models', 'are', 'trained', 'without', 'using', 'any', 'pretrained', 'word', 'or', 'sentence', 'embeddings', '.']","['B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'NN', 'NNP', 'CC', 'NNP', 'NNS', 'VBP', 'VBN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'CC', 'NN', 'NNS', '.']",17
text-classification,6,112,We observe that transfer learning from the transformer based sentence encoder usually performs as good or better than transfer learning from the DAN encoder .,"['We', 'observe', 'that', 'transfer', 'learning', 'from', 'the', 'transformer', 'based', 'sentence', 'encoder', 'usually', 'performs', 'as', 'good', 'or', 'better', 'than', 'transfer', 'learning', 'from', 'the', 'DAN', 'encoder', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'VBP', 'VBG', 'IN', 'DT', 'NN', 'VBN', 'NN', 'RBR', 'RB', 'VBZ', 'IN', 'JJ', 'CC', 'JJR', 'IN', 'VB', 'VBG', 'IN', 'DT', 'NNP', 'NN', '.']",25
text-classification,6,114,Models that make use of sentence level transfer learning tend to perform better than models that only use word level transfer .,"['Models', 'that', 'make', 'use', 'of', 'sentence', 'level', 'transfer', 'learning', 'tend', 'to', 'perform', 'better', 'than', 'models', 'that', 'only', 'use', 'word', 'level', 'transfer', '.']","['B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'WDT', 'VBP', 'NN', 'IN', 'NN', 'NN', 'NN', 'VBG', 'VBP', 'TO', 'VB', 'JJR', 'IN', 'NNS', 'IN', 'RB', 'VBP', 'NN', 'NN', 'NN', '.']",22
text-classification,6,117,"We observe that , for smaller quantities of data , sentence level transfer learning can achieve surprisingly good task performance .","['We', 'observe', 'that', ',', 'for', 'smaller', 'quantities', 'of', 'data', ',', 'sentence', 'level', 'transfer', 'learning', 'can', 'achieve', 'surprisingly', 'good', 'task', 'performance', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', ',', 'IN', 'JJR', 'NNS', 'IN', 'NNS', ',', 'NN', 'NN', 'NN', 'NN', 'MD', 'VB', 'RB', 'JJ', 'NN', 'NN', '.']",21
text-classification,6,118,"As the training set size increases , models that do not make use of transfer learning approach the performance of the other models .","['As', 'the', 'training', 'set', 'size', 'increases', ',', 'models', 'that', 'do', 'not', 'make', 'use', 'of', 'transfer', 'learning', 'approach', 'the', 'performance', 'of', 'the', 'other', 'models', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'VBN', 'NN', 'NNS', ',', 'NNS', 'WDT', 'VBP', 'RB', 'VB', 'NN', 'IN', 'NN', 'VBG', 'VBP', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNS', '.']",24
text-classification,2,2,Bag of Tricks for Efficient Text Classification,"['Bag', 'of', 'Tricks', 'for', 'Efficient', 'Text', 'Classification']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', 'IN', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",7
text-classification,2,4,This paper explores a simple and efficient baseline for text classification .,"['This', 'paper', 'explores', 'a', 'simple', 'and', 'efficient', 'baseline', 'for', 'text', 'classification', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', 'CC', 'JJ', 'NN', 'IN', 'JJ', 'NN', '.']",12
text-classification,2,22,shows a simple linear model with rank constraint .,"['shows', 'a', 'simple', 'linear', 'model', 'with', 'rank', 'constraint', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['VBZ', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'JJ', 'NN', '.']",9
text-classification,2,23,The first weight matrix A is a look - up table over the words .,"['The', 'first', 'weight', 'matrix', 'A', 'is', 'a', 'look', '-', 'up', 'table', 'over', 'the', 'words', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'NNP', 'VBZ', 'DT', 'NN', ':', 'RB', 'NN', 'IN', 'DT', 'NNS', '.']",15
text-classification,2,24,"The word representations are then averaged into a text representation , which is in turn fed to a linear classifier .","['The', 'word', 'representations', 'are', 'then', 'averaged', 'into', 'a', 'text', 'representation', ',', 'which', 'is', 'in', 'turn', 'fed', 'to', 'a', 'linear', 'classifier', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'DT', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'IN', 'NN', 'VBN', 'TO', 'DT', 'JJ', 'NN', '.']",21
text-classification,2,27,We use the softmax function f to compute the probability distribution over the predefined classes .,"['We', 'use', 'the', 'softmax', 'function', 'f', 'to', 'compute', 'the', 'probability', 'distribution', 'over', 'the', 'predefined', 'classes', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNS', '.']",16
text-classification,2,34,"In order to improve our running time , we use a hierarchical softmax ) based on the Huffman coding tree .","['In', 'order', 'to', 'improve', 'our', 'running', 'time', ',', 'we', 'use', 'a', 'hierarchical', 'softmax', ')', 'based', 'on', 'the', 'Huffman', 'coding', 'tree', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'VB', 'PRP$', 'VBG', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', ')', 'VBN', 'IN', 'DT', 'NNP', 'VBG', 'NN', '.']",21
text-classification,2,45,"Instead , we use a bag of n-grams as additional features to capture some partial information about the local word order .","['Instead', ',', 'we', 'use', 'a', 'bag', 'of', 'n-grams', 'as', 'additional', 'features', 'to', 'capture', 'some', 'partial', 'information', 'about', 'the', 'local', 'word', 'order', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'IN', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",22
text-classification,2,46,This is very efficient in practice while achieving comparable results to methods that explicitly use the order .,"['This', 'is', 'very', 'efficient', 'in', 'practice', 'while', 'achieving', 'comparable', 'results', 'to', 'methods', 'that', 'explicitly', 'use', 'the', 'order', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['DT', 'VBZ', 'RB', 'JJ', 'IN', 'NN', 'IN', 'VBG', 'JJ', 'NNS', 'TO', 'NNS', 'WDT', 'RB', 'VBP', 'DT', 'NN', '.']",18
text-classification,2,47,"We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .","['We', 'maintain', 'a', 'fast', 'and', 'memory', 'efficient', 'mapping', 'of', 'the', 'n-grams', 'by', 'using', 'the', 'hashing', 'trick', 'with', 'the', 'same', 'hashing', 'function', 'as', 'in', 'and', '10M', 'bins', 'if', 'we', 'only', 'used', 'bigrams', ',', 'and', '100M', 'otherwise', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'CC', 'NN', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'IN', 'VBG', 'DT', 'VBG', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'IN', 'CC', 'CD', 'NNS', 'IN', 'PRP', 'RB', 'VBD', 'NNS', ',', 'CC', 'CD', 'RB', '.']",36
text-classification,2,53,Sentiment analysis,"['Sentiment', 'analysis']","['B-n', 'I-n']","['NN', 'NN']",2
text-classification,2,60,"We use 10 hidden units and run fastText for 5 epochs with a learning rate selected on a validation set from { 0.05 , 0.1 , 0.25 , 0.5 } .","['We', 'use', '10', 'hidden', 'units', 'and', 'run', 'fastText', 'for', '5', 'epochs', 'with', 'a', 'learning', 'rate', 'selected', 'on', 'a', 'validation', 'set', 'from', '{', '0.05', ',', '0.1', ',', '0.25', ',', '0.5', '}', '.']","['O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'JJ', 'NNS', 'CC', 'VB', 'NN', 'IN', 'CD', 'NNS', 'IN', 'DT', 'VBG', 'NN', 'VBN', 'IN', 'DT', 'NN', 'VBN', 'IN', '(', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ')', '.']",31
text-classification,2,61,"On this task , adding bigram information improves the performance by 1 - 4 % .","['On', 'this', 'task', ',', 'adding', 'bigram', 'information', 'improves', 'the', 'performance', 'by', '1', '-', '4', '%', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'VBG', 'JJ', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'CD', ':', 'CD', 'NN', '.']",16
text-classification,2,62,"Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .","['Overall', 'our', 'accuracy', 'is', 'slightly', 'better', 'than', 'char', '-', 'CNN', 'and', 'char', '-', 'CRNN', 'and', ',', 'a', 'bit', 'worse', 'than', 'VDCNN', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['JJ', 'PRP$', 'NN', 'VBZ', 'RB', 'JJR', 'IN', 'JJR', ':', 'NN', 'CC', 'NN', ':', 'NN', 'CC', ',', 'DT', 'NN', 'JJR', 'IN', 'NNP', '.']",22
text-classification,2,63,"Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .","['Note', 'that', 'we', 'can', 'increase', 'the', 'accuracy', 'slightly', 'by', 'using', 'more', 'n-grams', ',', 'for', 'example', 'with', 'trigrams', ',', 'the', 'performance', 'on', 'Sogou', 'goes', 'up', 'to', '97.1', '%', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['NN', 'IN', 'PRP', 'MD', 'VB', 'DT', 'NN', 'RB', 'IN', 'VBG', 'JJR', 'JJ', ',', 'IN', 'NN', 'IN', 'NNS', ',', 'DT', 'NN', 'IN', 'NNP', 'VBZ', 'RB', 'TO', 'CD', 'NN', '.']",28
text-classification,2,69,Tag prediction,"['Tag', 'prediction']","['B-n', 'I-n']","['NNP', 'NN']",2
text-classification,2,84,"At test time , Tagspace needs to compute the scores for all the classes which makes it relatively slow , while our fast inference gives a significant speed - up when the number of classes is large ( more than 300 K here ) .","['At', 'test', 'time', ',', 'Tagspace', 'needs', 'to', 'compute', 'the', 'scores', 'for', 'all', 'the', 'classes', 'which', 'makes', 'it', 'relatively', 'slow', ',', 'while', 'our', 'fast', 'inference', 'gives', 'a', 'significant', 'speed', '-', 'up', 'when', 'the', 'number', 'of', 'classes', 'is', 'large', '(', 'more', 'than', '300', 'K', 'here', ')', '.']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'NN', ',', 'NNP', 'VBZ', 'TO', 'VB', 'DT', 'NNS', 'IN', 'PDT', 'DT', 'NNS', 'WDT', 'VBZ', 'PRP', 'RB', 'JJ', ',', 'IN', 'PRP$', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', ':', 'RB', 'WRB', 'DT', 'NN', 'IN', 'NNS', 'VBZ', 'JJ', '(', 'JJR', 'IN', 'CD', 'NNP', 'RB', ')', '.']",45
text-classification,2,85,"Overall , we are more than an order of magnitude faster to obtain model with a better quality .","['Overall', ',', 'we', 'are', 'more', 'than', 'an', 'order', 'of', 'magnitude', 'faster', 'to', 'obtain', 'model', 'with', 'a', 'better', 'quality', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['JJ', ',', 'PRP', 'VBP', 'JJR', 'IN', 'DT', 'NN', 'IN', 'NN', 'RBR', 'TO', 'VB', 'NN', 'IN', 'DT', 'JJR', 'NN', '.']",19
text-classification,3,2,On the Role of Text Preprocessing in Neural Network Architectures : An Evaluation Study on Text Categorization and Sentiment Analysis,"['On', 'the', 'Role', 'of', 'Text', 'Preprocessing', 'in', 'Neural', 'Network', 'Architectures', ':', 'An', 'Evaluation', 'Study', 'on', 'Text', 'Categorization', 'and', 'Sentiment', 'Analysis']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', ':', 'DT', 'NN', 'NNP', 'IN', 'NNP', 'NNP', 'CC', 'NNP', 'NNP']",20
text-classification,3,4,"Text preprocessing is often the first step in the pipeline of a Natural Language Processing ( NLP ) system , with potential impact in its final performance .","['Text', 'preprocessing', 'is', 'often', 'the', 'first', 'step', 'in', 'the', 'pipeline', 'of', 'a', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'system', ',', 'with', 'potential', 'impact', 'in', 'its', 'final', 'performance', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'VBZ', 'RB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', ',', 'IN', 'JJ', 'NN', 'IN', 'PRP$', 'JJ', 'NN', '.']",28
text-classification,3,23,"In this paper we focus on the role of preprocessing the input text , particularly in how it is split into individual ( meaning - bearing ) tokens and how it affects the performance of standard neural text classification models based on .","['In', 'this', 'paper', 'we', 'focus', 'on', 'the', 'role', 'of', 'preprocessing', 'the', 'input', 'text', ',', 'particularly', 'in', 'how', 'it', 'is', 'split', 'into', 'individual', '(', 'meaning', '-', 'bearing', ')', 'tokens', 'and', 'how', 'it', 'affects', 'the', 'performance', 'of', 'standard', 'neural', 'text', 'classification', 'models', 'based', 'on', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'IN', 'VBG', 'DT', 'NN', 'NN', ',', 'RB', 'IN', 'WRB', 'PRP', 'VBZ', 'VBN', 'IN', 'JJ', '(', 'VBG', ':', 'NN', ')', 'NNS', 'CC', 'WRB', 'PRP', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'NN', 'NNS', 'VBN', 'IN', '.']",43
text-classification,3,32,The accompanying materials of this submission can be downloaded at the following repository : github.com/pedrada88/preproc-textclassification .,"['The', 'accompanying', 'materials', 'of', 'this', 'submission', 'can', 'be', 'downloaded', 'at', 'the', 'following', 'repository', ':', 'github.com/pedrada88/preproc-textclassification', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']","['DT', 'VBG', 'NNS', 'IN', 'DT', 'NN', 'MD', 'VB', 'VBN', 'IN', 'DT', 'JJ', 'NN', ':', 'NN', '.']",16
text-classification,3,63,We tried with two classification models .,"['We', 'tried', 'with', 'two', 'classification', 'models', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'IN', 'CD', 'NN', 'NNS', '.']",7
text-classification,3,64,"The first one is a standard CNN model similar to that of , using ReLU as non-linear activation function .","['The', 'first', 'one', 'is', 'a', 'standard', 'CNN', 'model', 'similar', 'to', 'that', 'of', ',', 'using', 'ReLU', 'as', 'non-linear', 'activation', 'function', '.']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NNP', 'NN', 'JJ', 'TO', 'DT', 'IN', ',', 'VBG', 'NNP', 'IN', 'JJ', 'NN', 'NN', '.']",20
text-classification,3,65,"In the second model , we add a recurrent layer ( specifically an LSTM ) before passing the pooled features directly to the fully connected softmax layer .","['In', 'the', 'second', 'model', ',', 'we', 'add', 'a', 'recurrent', 'layer', '(', 'specifically', 'an', 'LSTM', ')', 'before', 'passing', 'the', 'pooled', 'features', 'directly', 'to', 'the', 'fully', 'connected', 'softmax', 'layer', '.']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', '(', 'RB', 'DT', 'NNP', ')', 'IN', 'VBG', 'DT', 'JJ', 'NNS', 'RB', 'TO', 'DT', 'RB', 'VBN', 'JJ', 'NN', '.']",28
text-classification,3,68,The embedding layer was initialized using 300 - dimensional CBOW Word2vec embeddings trained on the 3B - word UMBC WebBase corpus with standard hyperparameters,"['The', 'embedding', 'layer', 'was', 'initialized', 'using', '300', '-', 'dimensional', 'CBOW', 'Word2vec', 'embeddings', 'trained', 'on', 'the', '3B', '-', 'word', 'UMBC', 'WebBase', 'corpus', 'with', 'standard', 'hyperparameters']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n']","['DT', 'VBG', 'NN', 'VBD', 'VBN', 'VBG', 'CD', ':', 'JJ', 'NNP', 'NNP', 'NNS', 'VBD', 'IN', 'DT', 'CD', ':', 'NN', 'NNP', 'NNP', 'NN', 'IN', 'JJ', 'NNS']",24
text-classification,3,94,Experiment 1 : Preprocessing effect,"['Experiment', '1', ':', 'Preprocessing', 'effect']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['JJ', 'CD', ':', 'NN', 'NN']",5
text-classification,3,95,"Nevertheless , the use of more complex preprocessing techniques such as lemmatization and multiword grouping does not help in general .","['Nevertheless', ',', 'the', 'use', 'of', 'more', 'complex', 'preprocessing', 'techniques', 'such', 'as', 'lemmatization', 'and', 'multiword', 'grouping', 'does', 'not', 'help', 'in', 'general', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O']","['RB', ',', 'DT', 'NN', 'IN', 'RBR', 'JJ', 'VBG', 'NNS', 'JJ', 'IN', 'NN', 'CC', 'NN', 'NN', 'VBZ', 'RB', 'VB', 'IN', 'JJ', '.']",21
text-classification,3,97,Experiment 2 : Cross-preprocessing,"['Experiment', '2', ':', 'Cross-preprocessing']","['B-n', 'I-n', 'I-n', 'I-n']","['JJ', 'CD', ':', 'NN']",4
text-classification,3,100,"In this experiment we observe a different trend , with multiwordenhanced vectors exhibiting a better performance both on the single CNN model ( best over all performance in seven of the nine datasets ) and on the CNN + LSTM model ( best performance in four datasets and in the same ballpark as the best results in four of the remaining five datasets ) .","['In', 'this', 'experiment', 'we', 'observe', 'a', 'different', 'trend', ',', 'with', 'multiwordenhanced', 'vectors', 'exhibiting', 'a', 'better', 'performance', 'both', 'on', 'the', 'single', 'CNN', 'model', '(', 'best', 'over', 'all', 'performance', 'in', 'seven', 'of', 'the', 'nine', 'datasets', ')', 'and', 'on', 'the', 'CNN', '+', 'LSTM', 'model', '(', 'best', 'performance', 'in', 'four', 'datasets', 'and', 'in', 'the', 'same', 'ballpark', 'as', 'the', 'best', 'results', 'in', 'four', 'of', 'the', 'remaining', 'five', 'datasets', ')', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'DT', 'JJ', 'NN', ',', 'IN', 'JJ', 'NNS', 'VBG', 'DT', 'JJR', 'NN', 'DT', 'IN', 'DT', 'JJ', 'NNP', 'NN', '(', 'JJS', 'IN', 'DT', 'NN', 'IN', 'CD', 'IN', 'DT', 'CD', 'NNS', ')', 'CC', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NN', '(', 'JJS', 'NN', 'IN', 'CD', 'NNS', 'CC', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJS', 'NNS', 'IN', 'CD', 'IN', 'DT', 'VBG', 'CD', 'NNS', ')', '.']",65
text-classification,3,109,"Interestingly , using multiword - wise embeddings on the vanilla setting leads to consistently better results than using them on the same multiwordgrouped preprocessed dataset in eight of the nine datasets .","['Interestingly', ',', 'using', 'multiword', '-', 'wise', 'embeddings', 'on', 'the', 'vanilla', 'setting', 'leads', 'to', 'consistently', 'better', 'results', 'than', 'using', 'them', 'on', 'the', 'same', 'multiwordgrouped', 'preprocessed', 'dataset', 'in', 'eight', 'of', 'the', 'nine', 'datasets', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'VBG', 'SYM', ':', 'NN', 'NNS', 'IN', 'DT', 'NN', 'VBG', 'NNS', 'TO', 'RB', 'JJR', 'NNS', 'IN', 'VBG', 'PRP', 'IN', 'DT', 'JJ', 'VBD', 'JJ', 'NN', 'IN', 'CD', 'IN', 'DT', 'CD', 'NNS', '.']",32
text-classification,3,111,"Apart from this somewhat surprising finding , the use of the embeddings trained on a simple tokenized corpus ( i.e. vanilla ) proved again competitive , as different preprocessing techniques such as lowercasing and lemmatizing do not seem to help .","['Apart', 'from', 'this', 'somewhat', 'surprising', 'finding', ',', 'the', 'use', 'of', 'the', 'embeddings', 'trained', 'on', 'a', 'simple', 'tokenized', 'corpus', '(', 'i.e.', 'vanilla', ')', 'proved', 'again', 'competitive', ',', 'as', 'different', 'preprocessing', 'techniques', 'such', 'as', 'lowercasing', 'and', 'lemmatizing', 'do', 'not', 'seem', 'to', 'help', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'IN', 'DT', 'RB', 'JJ', 'NN', ',', 'DT', 'NN', 'IN', 'DT', 'NNS', 'VBN', 'IN', 'DT', 'JJ', 'JJ', 'NN', '(', 'JJ', 'NN', ')', 'VBD', 'RB', 'JJ', ',', 'IN', 'JJ', 'VBG', 'NNS', 'JJ', 'IN', 'NN', 'CC', 'VBG', 'VBP', 'RB', 'VB', 'TO', 'VB', '.']",41
text-classification,0,2,Character - level Convolutional Networks for Text Classification,"['Character', '-', 'level', 'Convolutional', 'Networks', 'for', 'Text', 'Classification']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', ':', 'NN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",8
text-classification,0,13,"Text classification is a classic topic for natural language processing , in which one needs to assign predefined categories to free - text documents .","['Text', 'classification', 'is', 'a', 'classic', 'topic', 'for', 'natural', 'language', 'processing', ',', 'in', 'which', 'one', 'needs', 'to', 'assign', 'predefined', 'categories', 'to', 'free', '-', 'text', 'documents', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', ',', 'IN', 'WDT', 'CD', 'VBZ', 'TO', 'VB', 'JJ', 'NNS', 'TO', 'VB', ':', 'NN', 'NNS', '.']",25
text-classification,0,18,"In this article we explore treating text as a kind of raw signal at character level , and applying temporal ( one-dimensional ) ConvNets to it .","['In', 'this', 'article', 'we', 'explore', 'treating', 'text', 'as', 'a', 'kind', 'of', 'raw', 'signal', 'at', 'character', 'level', ',', 'and', 'applying', 'temporal', '(', 'one-dimensional', ')', 'ConvNets', 'to', 'it', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'VBG', 'NN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NN', 'IN', 'JJR', 'NN', ',', 'CC', 'VBG', 'JJ', '(', 'JJ', ')', 'NNPS', 'TO', 'PRP', '.']",27
text-classification,0,19,For this article we only used a classification task as away to exemplify ConvNets ' ability to understand texts .,"['For', 'this', 'article', 'we', 'only', 'used', 'a', 'classification', 'task', 'as', 'away', 'to', 'exemplify', 'ConvNets', ""'"", 'ability', 'to', 'understand', 'texts', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'DT', 'NN', 'PRP', 'RB', 'VBD', 'DT', 'NN', 'NN', 'IN', 'RB', 'TO', 'VB', 'NNP', 'POS', 'NN', 'TO', 'VB', 'NN', '.']",20
text-classification,0,195,The most important conclusion from our experiments is that character - level ConvNets could work for text classification without the need for words .,"['The', 'most', 'important', 'conclusion', 'from', 'our', 'experiments', 'is', 'that', 'character', '-', 'level', 'ConvNets', 'could', 'work', 'for', 'text', 'classification', 'without', 'the', 'need', 'for', 'words', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O']","['DT', 'RBS', 'JJ', 'NN', 'IN', 'PRP$', 'NNS', 'VBZ', 'IN', 'NN', ':', 'NN', 'NNS', 'MD', 'VB', 'IN', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNS', '.']",24
text-classification,0,199,The most obvious trend coming from all the plots in is that the larger datasets tend to perform better .,"['The', 'most', 'obvious', 'trend', 'coming', 'from', 'all', 'the', 'plots', 'in', 'is', 'that', 'the', 'larger', 'datasets', 'tend', 'to', 'perform', 'better', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['DT', 'RBS', 'JJ', 'NN', 'VBG', 'IN', 'PDT', 'DT', 'NNS', 'IN', 'VBZ', 'IN', 'DT', 'JJR', 'NNS', 'VBP', 'TO', 'VB', 'JJR', '.']",20
text-classification,0,207,"However , further analysis is needed to validate the hypothesis that ConvNets are truly good at identifying exotic character combinations such as misspellings and emoticons , as our experiments alone do not show any explicit evidence .","['However', ',', 'further', 'analysis', 'is', 'needed', 'to', 'validate', 'the', 'hypothesis', 'that', 'ConvNets', 'are', 'truly', 'good', 'at', 'identifying', 'exotic', 'character', 'combinations', 'such', 'as', 'misspellings', 'and', 'emoticons', ',', 'as', 'our', 'experiments', 'alone', 'do', 'not', 'show', 'any', 'explicit', 'evidence', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNS', 'VBP', 'RB', 'JJ', 'IN', 'VBG', 'JJ', 'NN', 'NNS', 'JJ', 'IN', 'NNS', 'CC', 'NNS', ',', 'IN', 'PRP$', 'NNS', 'RB', 'VBP', 'RB', 'VB', 'DT', 'JJ', 'NN', '.']",37
text-classification,0,217,"Comparing with traditional models , this suggests such a simple use of a distributed word representation may not give us an advantage to text classification .","['Comparing', 'with', 'traditional', 'models', ',', 'this', 'suggests', 'such', 'a', 'simple', 'use', 'of', 'a', 'distributed', 'word', 'representation', 'may', 'not', 'give', 'us', 'an', 'advantage', 'to', 'text', 'classification', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'IN', 'JJ', 'NNS', ',', 'DT', 'VBZ', 'PDT', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'MD', 'RB', 'VB', 'PRP', 'DT', 'NN', 'TO', 'VB', 'NN', '.']",26
question-answering,8,2,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,"['MACHINE', 'COMPREHENSION', 'USING', 'MATCH', '-', 'LSTM', 'AND', 'ANSWER', 'POINTER']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'NNP']",9
question-answering,8,4,Machine comprehension of text is an important problem in natural language processing .,"['Machine', 'comprehension', 'of', 'text', 'is', 'an', 'important', 'problem', 'in', 'natural', 'language', 'processing', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'IN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",13
question-answering,8,38,"In this paper , we propose a new end - to - end neural architecture to address the machine comprehension problem as defined in the SQuAD dataset .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'new', 'end', '-', 'to', '-', 'end', 'neural', 'architecture', 'to', 'address', 'the', 'machine', 'comprehension', 'problem', 'as', 'defined', 'in', 'the', 'SQuAD', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', ':', 'TO', ':', 'VB', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'NN', 'IN', 'VBN', 'IN', 'DT', 'NNP', 'NN', '.']",28
question-answering,8,39,"Specifically , observing that in the SQuAD dataset many questions are paraphrases of sentences from the original text , we adopt a match - LSTM model that we developed earlier for textual entailment .","['Specifically', ',', 'observing', 'that', 'in', 'the', 'SQuAD', 'dataset', 'many', 'questions', 'are', 'paraphrases', 'of', 'sentences', 'from', 'the', 'original', 'text', ',', 'we', 'adopt', 'a', 'match', '-', 'LSTM', 'model', 'that', 'we', 'developed', 'earlier', 'for', 'textual', 'entailment', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'VBG', 'IN', 'IN', 'DT', 'NNP', 'VBD', 'JJ', 'NNS', 'VBP', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', ':', 'NNP', 'NN', 'IN', 'PRP', 'VBD', 'JJR', 'IN', 'JJ', 'NN', '.']",34
question-answering,8,40,"We further adopt the Pointer Net ( Ptr - Net ) model developed by , which enables the predictions of tokens from the input sequence only rather than from a larger fixed vocabulary and thus allows us to generate answers that consist of multiple tokens from the original text .","['We', 'further', 'adopt', 'the', 'Pointer', 'Net', '(', 'Ptr', '-', 'Net', ')', 'model', 'developed', 'by', ',', 'which', 'enables', 'the', 'predictions', 'of', 'tokens', 'from', 'the', 'input', 'sequence', 'only', 'rather', 'than', 'from', 'a', 'larger', 'fixed', 'vocabulary', 'and', 'thus', 'allows', 'us', 'to', 'generate', 'answers', 'that', 'consist', 'of', 'multiple', 'tokens', 'from', 'the', 'original', 'text', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'VB', 'DT', 'NNP', 'NNP', '(', 'NNP', ':', 'NN', ')', 'NN', 'VBN', 'IN', ',', 'WDT', 'VBZ', 'DT', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'RB', 'RB', 'IN', 'IN', 'DT', 'JJR', 'JJ', 'NN', 'CC', 'RB', 'VBZ', 'PRP', 'TO', 'VB', 'NNS', 'IN', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",50
question-answering,8,41,We propose two ways to apply the Ptr - Net model for our task : a sequence model and a boundary model .,"['We', 'propose', 'two', 'ways', 'to', 'apply', 'the', 'Ptr', '-', 'Net', 'model', 'for', 'our', 'task', ':', 'a', 'sequence', 'model', 'and', 'a', 'boundary', 'model', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'NNS', 'TO', 'VB', 'DT', 'NNP', ':', 'JJ', 'NN', 'IN', 'PRP$', 'NN', ':', 'DT', 'NN', 'NN', 'CC', 'DT', 'JJ', 'NN', '.']",23
question-answering,8,42,We also further extend the boundary model with a search mechanism .,"['We', 'also', 'further', 'extend', 'the', 'boundary', 'model', 'with', 'a', 'search', 'mechanism', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'RBR', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",12
question-answering,8,176,We use word embeddings from GloVe to initialize the model .,"['We', 'use', 'word', 'embeddings', 'from', 'GloVe', 'to', 'initialize', 'the', 'model', '.']","['O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NN', 'NNS', 'IN', 'NNP', 'TO', 'VB', 'DT', 'NN', '.']",11
question-answering,8,179,The dimensionality l of the hidden layers is set to be 150 or 300 .,"['The', 'dimensionality', 'l', 'of', 'the', 'hidden', 'layers', 'is', 'set', 'to', 'be', '150', 'or', '300', '.']","['O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'VBZ', 'VBN', 'TO', 'VB', 'CD', 'CC', 'CD', '.']",15
question-answering,8,180,We use ADAMAX with the coefficients ? 1 = 0.9 and ? 2 = 0.999 to optimize the model .,"['We', 'use', 'ADAMAX', 'with', 'the', 'coefficients', '?', '1', '=', '0.9', 'and', '?', '2', '=', '0.999', 'to', 'optimize', 'the', 'model', '.']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'IN', 'DT', 'NNS', '.', 'CD', 'JJ', 'CD', 'CC', '.', 'CD', '$', 'CD', 'TO', 'VB', 'DT', 'NN', '.']",20
question-answering,8,181,Each update is computed through a minibatch of 30 instances .,"['Each', 'update', 'is', 'computed', 'through', 'a', 'minibatch', 'of', '30', 'instances', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'IN', 'CD', 'NNS', '.']",11
question-answering,8,191,"outperformed the logistic regression model by , which relies on carefully designed features .","['outperformed', 'the', 'logistic', 'regression', 'model', 'by', ',', 'which', 'relies', 'on', 'carefully', 'designed', 'features', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['VBD', 'DT', 'JJ', 'NN', 'NN', 'IN', ',', 'WDT', 'VBZ', 'IN', 'RB', 'VBN', 'NNS', '.']",14
question-answering,8,192,"Furthermore , our boundary model has outperformed the sequence model , achieving an exact match score of 61.1 % and an F1 score of 71.2 % .","['Furthermore', ',', 'our', 'boundary', 'model', 'has', 'outperformed', 'the', 'sequence', 'model', ',', 'achieving', 'an', 'exact', 'match', 'score', 'of', '61.1', '%', 'and', 'an', 'F1', 'score', 'of', '71.2', '%', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'JJ', 'NN', 'VBZ', 'VBN', 'DT', 'NN', 'NN', ',', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'NN', 'CC', 'DT', 'NNP', 'NN', 'IN', 'CD', 'NN', '.']",27
question-answering,8,200,"While by adding Bi - Ans - Ptr with bi-directional pre-processing LSTM , we can get 1.2 % improvement in F1 .","['While', 'by', 'adding', 'Bi', '-', 'Ans', '-', 'Ptr', 'with', 'bi-directional', 'pre-processing', 'LSTM', ',', 'we', 'can', 'get', '1.2', '%', 'improvement', 'in', 'F1', '.']","['O', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'IN', 'VBG', 'NNP', ':', 'NNP', ':', 'NN', 'IN', 'JJ', 'JJ', 'NNP', ',', 'PRP', 'MD', 'VB', 'CD', 'NN', 'NN', 'IN', 'NNP', '.']",22
question-answering,9,2,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,"['LEARNING', 'RECURRENT', 'SPAN', 'REPRESENTATIONS', 'FOR', 'EXTRACTIVE', 'QUESTION', 'ANSWERING']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",8
question-answering,9,4,"The reading comprehension task , that asks questions about a given evidence document , is a central problem in natural language understanding .","['The', 'reading', 'comprehension', 'task', ',', 'that', 'asks', 'questions', 'about', 'a', 'given', 'evidence', 'document', ',', 'is', 'a', 'central', 'problem', 'in', 'natural', 'language', 'understanding', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', 'NN', ',', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'VBN', 'NN', 'NN', ',', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",23
question-answering,9,17,A primary goal of natural language processing is to develop systems that can answer questions about the contents of documents .,"['A', 'primary', 'goal', 'of', 'natural', 'language', 'processing', 'is', 'to', 'develop', 'systems', 'that', 'can', 'answer', 'questions', 'about', 'the', 'contents', 'of', 'documents', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', 'VBZ', 'TO', 'VB', 'NNS', 'WDT', 'MD', 'VB', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'NNS', '.']",21
question-answering,9,24,"To overcome this , we present a novel neural architecture called RASOR that builds fixed - length span representations , reusing recurrent computations for shared substructures .","['To', 'overcome', 'this', ',', 'we', 'present', 'a', 'novel', 'neural', 'architecture', 'called', 'RASOR', 'that', 'builds', 'fixed', '-', 'length', 'span', 'representations', ',', 'reusing', 'recurrent', 'computations', 'for', 'shared', 'substructures', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['TO', 'VB', 'DT', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'VBN', 'NNP', 'IN', 'VBZ', 'VBN', ':', 'NN', 'NN', 'NNS', ',', 'VBG', 'JJ', 'NNS', 'IN', 'VBN', 'NNS', '.']",27
question-answering,9,25,"We demonstrate that directly classifying each of the competing spans , and training with global normalization over all possible spans , leads to a significant increase in performance .","['We', 'demonstrate', 'that', 'directly', 'classifying', 'each', 'of', 'the', 'competing', 'spans', ',', 'and', 'training', 'with', 'global', 'normalization', 'over', 'all', 'possible', 'spans', ',', 'leads', 'to', 'a', 'significant', 'increase', 'in', 'performance', '.']","['O', 'B-p', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'RB', 'VBG', 'DT', 'IN', 'DT', 'VBG', 'NNS', ',', 'CC', 'VBG', 'IN', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NNS', ',', 'VBZ', 'TO', 'DT', 'JJ', 'NN', 'IN', 'NN', '.']",29
question-answering,9,106,We represent each of the words in the question and document using 300 dimensional GloVe embeddings trained on a corpus of 840 bn words .,"['We', 'represent', 'each', 'of', 'the', 'words', 'in', 'the', 'question', 'and', 'document', 'using', '300', 'dimensional', 'GloVe', 'embeddings', 'trained', 'on', 'a', 'corpus', 'of', '840', 'bn', 'words', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NN', 'VBG', 'CD', 'JJ', 'NNP', 'NNS', 'VBD', 'IN', 'DT', 'NN', 'IN', 'CD', 'NN', 'NNS', '.']",25
question-answering,9,108,"We couple the input and forget gates in our LSTMs , as described in , and we use a single dropout mask to apply dropout across all LSTM time - steps as proposed by .","['We', 'couple', 'the', 'input', 'and', 'forget', 'gates', 'in', 'our', 'LSTMs', ',', 'as', 'described', 'in', ',', 'and', 'we', 'use', 'a', 'single', 'dropout', 'mask', 'to', 'apply', 'dropout', 'across', 'all', 'LSTM', 'time', '-', 'steps', 'as', 'proposed', 'by', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'CC', 'VB', 'NNS', 'IN', 'PRP$', 'NNP', ',', 'IN', 'VBN', 'IN', ',', 'CC', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'RP', 'IN', 'DT', 'NNP', 'NN', ':', 'NNS', 'IN', 'VBN', 'IN', '.']",35
question-answering,9,109,Hidden layers in the feed forward neural networks use rectified linear units .,"['Hidden', 'layers', 'in', 'the', 'feed', 'forward', 'neural', 'networks', 'use', 'rectified', 'linear', 'units', '.']","['B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNS', 'IN', 'DT', 'NN', 'RB', 'JJ', 'NNS', 'VBP', 'JJ', 'JJ', 'NNS', '.']",13
question-answering,9,111,"To choose the final model configuration , we ran grid searches over : the dimensionality of the LSTM hidden states ; the width and depth of the feed forward neural networks ; dropout for the LSTMs ; the number of stacked LSTM layers ; and the decay multiplier [ 0.9 , 0.95 , 1.0 ] with which we multiply the learning rate every 10 k steps .","['To', 'choose', 'the', 'final', 'model', 'configuration', ',', 'we', 'ran', 'grid', 'searches', 'over', ':', 'the', 'dimensionality', 'of', 'the', 'LSTM', 'hidden', 'states', ';', 'the', 'width', 'and', 'depth', 'of', 'the', 'feed', 'forward', 'neural', 'networks', ';', 'dropout', 'for', 'the', 'LSTMs', ';', 'the', 'number', 'of', 'stacked', 'LSTM', 'layers', ';', 'and', 'the', 'decay', 'multiplier', '[', '0.9', ',', '0.95', ',', '1.0', ']', 'with', 'which', 'we', 'multiply', 'the', 'learning', 'rate', 'every', '10', 'k', 'steps', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'JJ', 'NN', 'NN', ',', 'PRP', 'VBD', 'JJ', 'NNS', 'IN', ':', 'DT', 'NN', 'IN', 'DT', 'NNP', 'JJ', 'NNS', ':', 'DT', 'NN', 'CC', 'NN', 'IN', 'DT', 'NN', 'RB', 'JJ', 'NNS', ':', 'NN', 'IN', 'DT', 'NNP', ':', 'DT', 'NN', 'IN', 'JJ', 'NNP', 'NNS', ':', 'CC', 'DT', 'NN', 'NN', 'VBZ', 'CD', ',', 'CD', ',', 'CD', 'NN', 'IN', 'WDT', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'DT', 'CD', 'NN', 'NNS', '.']",67
question-answering,9,112,The best model uses 50d LSTM states ; two - layer BiLSTMs for the span encoder and the passage - independent question representation ; dropout of 0.1 throughout ; and a learning rate decay of 5 % every 10 k steps .,"['The', 'best', 'model', 'uses', '50d', 'LSTM', 'states', ';', 'two', '-', 'layer', 'BiLSTMs', 'for', 'the', 'span', 'encoder', 'and', 'the', 'passage', '-', 'independent', 'question', 'representation', ';', 'dropout', 'of', '0.1', 'throughout', ';', 'and', 'a', 'learning', 'rate', 'decay', 'of', '5', '%', 'every', '10', 'k', 'steps', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJS', 'NN', 'VBZ', 'CD', 'NNP', 'NNS', ':', 'CD', ':', 'NN', 'NNP', 'IN', 'DT', 'NN', 'NN', 'CC', 'DT', 'NN', ':', 'JJ', 'NN', 'NN', ':', 'NN', 'IN', 'CD', 'IN', ':', 'CC', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'NN', 'DT', 'CD', 'NN', 'NNS', '.']",42
question-answering,9,113,All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the ADAM optimizer with a mini-batch size of 4 and trained using 10 asynchronous training threads on a single machine .,"['All', 'models', 'are', 'implemented', 'using', 'TensorFlow', '3', 'and', 'trained', 'on', 'the', 'SQUAD', 'training', 'set', 'using', 'the', 'ADAM', 'optimizer', 'with', 'a', 'mini-batch', 'size', 'of', '4', 'and', 'trained', 'using', '10', 'asynchronous', 'training', 'threads', 'on', 'a', 'single', 'machine', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'VBG', 'NNP', 'CD', 'CC', 'VBD', 'IN', 'DT', 'NNP', 'NN', 'NN', 'VBG', 'DT', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', 'CC', 'VBD', 'VBG', 'CD', 'JJ', 'VBG', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",36
question-answering,9,121,"Despite not having access to any external representation of linguistic structure , RASOR achieves an error reduction of more than 50 % over this baseline , both in terms of exact match and F1 , relative to the human performance upper bound .","['Despite', 'not', 'having', 'access', 'to', 'any', 'external', 'representation', 'of', 'linguistic', 'structure', ',', 'RASOR', 'achieves', 'an', 'error', 'reduction', 'of', 'more', 'than', '50', '%', 'over', 'this', 'baseline', ',', 'both', 'in', 'terms', 'of', 'exact', 'match', 'and', 'F1', ',', 'relative', 'to', 'the', 'human', 'performance', 'upper', 'bound', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'RB', 'VBG', 'NN', 'TO', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', ',', 'NNP', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'JJR', 'IN', 'CD', 'NN', 'IN', 'DT', 'NN', ',', 'DT', 'IN', 'NNS', 'IN', 'JJ', 'NN', 'CC', 'NNP', ',', 'JJ', 'TO', 'DT', 'JJ', 'NN', 'JJ', 'NN', '.']",43
question-answering,9,125,"In contrast , RASOR can efficiently and explicitly model the quadratic number of possible answers , which leads to a 14 % error reduction over the best performing Match - LSTM model .","['In', 'contrast', ',', 'RASOR', 'can', 'efficiently', 'and', 'explicitly', 'model', 'the', 'quadratic', 'number', 'of', 'possible', 'answers', ',', 'which', 'leads', 'to', 'a', '14', '%', 'error', 'reduction', 'over', 'the', 'best', 'performing', 'Match', '-', 'LSTM', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'NNP', 'MD', 'RB', 'CC', 'RB', 'VB', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', ',', 'WDT', 'VBZ', 'TO', 'DT', 'CD', 'NN', 'NN', 'NN', 'IN', 'DT', 'JJS', 'NN', 'NNP', ':', 'NNP', 'NN', '.']",33
question-answering,9,131,"The passage - aligned question representation is crucial , since lexically similar regions of the passage provide strong signal for relevant answer spans .","['The', 'passage', '-', 'aligned', 'question', 'representation', 'is', 'crucial', ',', 'since', 'lexically', 'similar', 'regions', 'of', 'the', 'passage', 'provide', 'strong', 'signal', 'for', 'relevant', 'answer', 'spans', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', ':', 'VBN', 'NN', 'NN', 'VBZ', 'JJ', ',', 'IN', 'RB', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NNS', '.']",24
question-answering,9,157,"First , we observe general improvements when using labels that closely align with the task .","['First', ',', 'we', 'observe', 'general', 'improvements', 'when', 'using', 'labels', 'that', 'closely', 'align', 'with', 'the', 'task', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'WRB', 'VBG', 'NNS', 'WDT', 'RB', 'VBZ', 'IN', 'DT', 'NN', '.']",16
question-answering,9,162,"Second , we observe the importance of allowing interactions between the endpoints using the spanlevel FFNN .","['Second', ',', 'we', 'observe', 'the', 'importance', 'of', 'allowing', 'interactions', 'between', 'the', 'endpoints', 'using', 'the', 'spanlevel', 'FFNN', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['JJ', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'NNS', 'IN', 'DT', 'NNS', 'VBG', 'DT', 'NN', 'NNP', '.']",17
question-answering,9,163,"RASOR outperforms the endpoint prediction model by 1.1 in exact match , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .","['RASOR', 'outperforms', 'the', 'endpoint', 'prediction', 'model', 'by', '1.1', 'in', 'exact', 'match', ',', 'The', 'interaction', 'between', 'endpoints', 'enables', 'RASOR', 'to', 'enforce', 'consistency', 'across', 'its', 'two', 'substructures', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'VBZ', 'DT', 'NN', 'NN', 'NN', 'IN', 'CD', 'IN', 'JJ', 'NN', ',', 'DT', 'NN', 'IN', 'NNS', 'NNS', 'NNP', 'TO', 'VB', 'NN', 'IN', 'PRP$', 'CD', 'NNS', '.']",26
question-answering,1,2,Convolutional Neural Network Architectures for Matching Natural Language Sentences,"['Convolutional', 'Neural', 'Network', 'Architectures', 'for', 'Matching', 'Natural', 'Language', 'Sentences']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'NNS']",9
question-answering,1,4,"Semantic matching is of central importance to many natural language tasks [ 2,28 ] .","['Semantic', 'matching', 'is', 'of', 'central', 'importance', 'to', 'many', 'natural', 'language', 'tasks', '[', '2,28', ']', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'VBZ', 'IN', 'JJ', 'NN', 'TO', 'JJ', 'JJ', 'NN', 'NNS', 'VBP', 'CD', 'NN', '.']",15
question-answering,1,11,Matching two potentially heterogenous language objects is central to many natural language applications .,"['Matching', 'two', 'potentially', 'heterogenous', 'language', 'objects', 'is', 'central', 'to', 'many', 'natural', 'language', 'applications', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'CD', 'RB', 'JJ', 'NN', 'NNS', 'VBZ', 'JJ', 'TO', 'JJ', 'JJ', 'NN', 'NNS', '.']",14
question-answering,1,15,A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .,"['A', 'successful', 'sentence', '-', 'matching', 'algorithm', 'therefore', 'needs', 'to', 'capture', 'not', 'only', 'the', 'internal', 'structures', 'of', 'sentences', 'but', 'also', 'the', 'rich', 'patterns', 'in', 'their', 'interactions', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', ':', 'NN', 'NN', 'RB', 'VBZ', 'TO', 'VB', 'RB', 'RB', 'DT', 'JJ', 'NNS', 'IN', 'NNS', 'CC', 'RB', 'DT', 'JJ', 'NNS', 'IN', 'PRP$', 'NNS', '.']",26
question-answering,1,16,"Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .","['Towards', 'this', 'end', ',', 'we', 'propose', 'deep', 'neural', 'network', 'models', ',', 'which', 'adapt', 'the', 'convolutional', 'strategy', '(', 'proven', 'successful', 'on', 'image', 'and', 'speech', ')', 'to', 'natural', 'language', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['NNS', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'JJ', 'NN', 'NNS', ',', 'WDT', 'VBP', 'DT', 'JJ', 'NN', '(', 'RB', 'JJ', 'IN', 'NN', 'CC', 'NN', ')', 'TO', 'JJ', 'NN', '.']",28
question-answering,1,17,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .","['To', 'further', 'explore', 'the', 'relation', 'between', 'representing', 'sentences', 'and', 'matching', 'them', ',', 'we', 'devise', 'a', 'novel', 'model', 'that', 'can', 'naturally', 'host', 'both', 'the', 'hierarchical', 'composition', 'for', 'sentences', 'and', 'the', 'simple', '-', 'to', '-', 'comprehensive', 'fusion', 'of', 'matching', 'patterns', 'with', 'the', 'same', 'convolutional', 'architecture', '.']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'RBR', 'VB', 'DT', 'NN', 'IN', 'VBG', 'NNS', 'CC', 'VBG', 'PRP', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'WDT', 'MD', 'RB', 'VB', 'DT', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'CC', 'DT', 'JJ', ':', 'TO', ':', 'JJ', 'NN', 'IN', 'VBG', 'NNS', 'IN', 'DT', 'JJ', 'JJ', 'NN', '.']",44
question-answering,1,127,"In other words , We use stochastic gradient descent for the optimization of models .","['In', 'other', 'words', ',', 'We', 'use', 'stochastic', 'gradient', 'descent', 'for', 'the', 'optimization', 'of', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNS', '.']",15
question-answering,1,128,All the proposed models perform better with mini-batch ( 100 ? 200 in sizes ) which can be easily parallelized on single machine with multi-cores .,"['All', 'the', 'proposed', 'models', 'perform', 'better', 'with', 'mini-batch', '(', '100', '?', '200', 'in', 'sizes', ')', 'which', 'can', 'be', 'easily', 'parallelized', 'on', 'single', 'machine', 'with', 'multi-cores', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PDT', 'DT', 'VBN', 'NNS', 'VB', 'JJR', 'IN', 'NN', '(', 'CD', '.', 'CD', 'IN', 'NNS', ')', 'WDT', 'MD', 'VB', 'RB', 'VBN', 'IN', 'JJ', 'NN', 'IN', 'NNS', '.']",26
question-answering,1,129,"For regularization , we find that for both architectures , early stopping is enough for models with medium size and large training sets ( with over 500K instances ) .","['For', 'regularization', ',', 'we', 'find', 'that', 'for', 'both', 'architectures', ',', 'early', 'stopping', 'is', 'enough', 'for', 'models', 'with', 'medium', 'size', 'and', 'large', 'training', 'sets', '(', 'with', 'over', '500K', 'instances', ')', '.']","['B-p', 'I-p', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'IN', 'IN', 'DT', 'NNS', ',', 'JJ', 'NN', 'VBZ', 'JJ', 'IN', 'NNS', 'IN', 'NN', 'NN', 'CC', 'JJ', 'VBG', 'NNS', '(', 'IN', 'IN', 'CD', 'NNS', ')', '.']",30
question-answering,1,131,"We use 50 - dimensional word embedding trained with the Word2 Vec : the embedding for English words ( Section 5.2 & 5.4 ) is learnt on Wikipedia ( ?1B words ) , while that for Chinese words ( Section 5.3 ) is learnt on Weibo data (? 300 M words ) .","['We', 'use', '50', '-', 'dimensional', 'word', 'embedding', 'trained', 'with', 'the', 'Word2', 'Vec', ':', 'the', 'embedding', 'for', 'English', 'words', '(', 'Section', '5.2', '&', '5.4', ')', 'is', 'learnt', 'on', 'Wikipedia', '(', '?1B', 'words', ')', ',', 'while', 'that', 'for', 'Chinese', 'words', '(', 'Section', '5.3', ')', 'is', 'learnt', 'on', 'Weibo', 'data', '(?', '300', 'M', 'words', ')', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'CD', ':', 'JJ', 'NN', 'VBG', 'VBN', 'IN', 'DT', 'NNP', 'NNP', ':', 'DT', 'VBG', 'IN', 'NNP', 'NNS', '(', 'NNP', 'CD', 'CC', 'CD', ')', 'VBZ', 'VBN', 'IN', 'NNP', '(', 'NNP', 'NNS', ')', ',', 'IN', 'DT', 'IN', 'JJ', 'NNS', '(', 'NNP', 'CD', ')', 'VBZ', 'VBN', 'IN', 'NNP', 'NNS', 'VBD', 'CD', 'NNP', 'NNS', ')', '.']",53
question-answering,1,134,"We use 3 - word window throughout all experiments 2 , but test various numbers of feature maps ( typically from 200 to 500 ) , for optimal performance .","['We', 'use', '3', '-', 'word', 'window', 'throughout', 'all', 'experiments', '2', ',', 'but', 'test', 'various', 'numbers', 'of', 'feature', 'maps', '(', 'typically', 'from', '200', 'to', '500', ')', ',', 'for', 'optimal', 'performance', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', ':', 'NN', 'NN', 'IN', 'DT', 'NNS', 'CD', ',', 'CC', 'RB', 'JJ', 'NNS', 'IN', 'NN', 'NNS', '(', 'RB', 'IN', 'CD', 'TO', 'CD', ')', ',', 'IN', 'JJ', 'NN', '.']",30
question-answering,1,136,"We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .","['We', 'use', 'ReLu', 'as', 'the', 'activation', 'function', 'for', 'all', 'of', 'models', '(', 'convolution', 'and', 'MLP', ')', ',', 'which', 'yields', 'comparable', 'or', 'better', 'results', 'to', 'sigmoid', '-', 'like', 'functions', ',', 'but', 'converges', 'faster', '.']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'IN', 'NNS', '(', 'NN', 'CC', 'NNP', ')', ',', 'WDT', 'VBZ', 'JJ', 'CC', 'JJR', 'NNS', 'TO', 'VB', ':', 'IN', 'NNS', ',', 'CC', 'NNS', 'RBR', '.']",33
question-answering,1,142,WORDEMBED : We first represent each short - text as the sum of the embedding of the words it contains .,"['WORDEMBED', ':', 'We', 'first', 'represent', 'each', 'short', '-', 'text', 'as', 'the', 'sum', 'of', 'the', 'embedding', 'of', 'the', 'words', 'it', 'contains', '.']","['B-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', ':', 'PRP', 'RB', 'VBP', 'DT', 'JJ', ':', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNS', 'PRP', 'VBZ', '.']",21
question-answering,1,143,"The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DEEPMATCH : We take the matching model in and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer ; URAE+ MLP :","['The', 'matching', 'score', 'of', 'two', 'short', '-', 'texts', 'are', 'calculated', 'with', 'an', 'MLP', 'with', 'the', 'embedding', 'of', 'the', 'two', 'documents', 'as', 'input', ';', 'DEEPMATCH', ':', 'We', 'take', 'the', 'matching', 'model', 'in', 'and', 'train', 'it', 'on', 'our', 'datasets', 'with', '3', 'hidden', 'layers', 'and', '1,000', 'hidden', 'nodes', 'in', 'the', 'first', 'hidden', 'layer', ';', 'URAE+', 'MLP', ':']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['DT', 'VBG', 'NN', 'IN', 'CD', 'JJ', ':', 'NN', 'VBP', 'VBN', 'IN', 'DT', 'NNP', 'IN', 'DT', 'NN', 'IN', 'DT', 'CD', 'NNS', 'IN', 'NN', ':', 'NNP', ':', 'PRP', 'VBP', 'DT', 'VBG', 'NN', 'IN', 'CC', 'VB', 'PRP', 'IN', 'PRP$', 'NNS', 'IN', 'CD', 'JJ', 'NNS', 'CC', 'CD', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'JJ', 'NN', ':', 'NNP', 'NNP', ':']",54
question-answering,1,144,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :","['We', 'use', 'the', 'Unfolding', 'Recursive', 'Autoencoder', 'to', 'get', 'a', '100', 'dimensional', 'vector', 'representation', 'of', 'each', 'sentence', ',', 'and', 'put', 'an', 'MLP', 'on', 'the', 'top', 'as', 'in', 'WORDEMBED', ';', 'SENNA', '+', 'MLP', '/', 'SIM', ':']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NNP', 'NNP', 'TO', 'VB', 'DT', 'CD', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', ',', 'CC', 'VBD', 'DT', 'NNP', 'IN', 'DT', 'JJ', 'IN', 'IN', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', ':']",34
question-answering,1,145,We use the SENNA - type sentence model for sentence representation ;,"['We', 'use', 'the', 'SENNA', '-', 'type', 'sentence', 'model', 'for', 'sentence', 'representation', ';']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', ':', 'NN', 'NN', 'NN', 'IN', 'NN', 'NN', ':']",12
question-answering,1,146,"SENMLP : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .","['SENMLP', ':', 'We', 'take', 'the', 'whole', 'sentence', 'as', 'input', '(', 'with', 'word', 'embedding', 'aligned', 'sequentially', ')', ',', 'and', 'use', 'an', 'MLP', 'to', 'obtain', 'the', 'score', 'of', 'coherence', '.']","['B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NN', ':', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NN', '(', 'IN', 'NN', 'VBG', 'VBN', 'RB', ')', ',', 'CC', 'VB', 'DT', 'NNP', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', '.']",28
question-answering,1,173,ARC - II outperforms others significantly when the training instances are relatively abundant ( as in Experiment I & II ) .,"['ARC', '-', 'II', 'outperforms', 'others', 'significantly', 'when', 'the', 'training', 'instances', 'are', 'relatively', 'abundant', '(', 'as', 'in', 'Experiment', 'I', '&', 'II', ')', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', 'VBZ', 'NNS', 'RB', 'WRB', 'DT', 'NN', 'NNS', 'VBP', 'RB', 'JJ', '(', 'IN', 'IN', 'NNP', 'PRP', 'CC', 'NNP', ')', '.']",22
question-answering,1,176,"As another important observation , convolutional models ( ARC - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .","['As', 'another', 'important', 'observation', ',', 'convolutional', 'models', '(', 'ARC', '-', 'I', '&', 'II', ',', 'SENNA', '+', 'MLP', ')', 'perform', 'favorably', 'over', 'bag', '-', 'of', '-', 'words', 'models', ',', 'indicating', 'the', 'importance', 'of', 'utilizing', 'sequential', 'structures', 'in', 'understanding', 'and', 'matching', 'sentences', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'JJ', 'NNS', '(', 'NNP', ':', 'PRP', 'CC', 'NNP', ',', 'NNP', 'NNP', 'NNP', ')', 'NN', 'RB', 'IN', 'JJ', ':', 'IN', ':', 'NNS', 'NNS', ',', 'VBG', 'DT', 'NN', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'JJ', 'CC', 'JJ', 'NNS', '.']",41
question-answering,1,177,"Quite interestingly , as shown by our other experiments , ARC - I and ARC - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .","['Quite', 'interestingly', ',', 'as', 'shown', 'by', 'our', 'other', 'experiments', ',', 'ARC', '-', 'I', 'and', 'ARC', '-', 'II', 'trained', 'purely', 'with', 'random', 'negatives', 'automatically', 'gain', 'some', 'ability', 'in', 'telling', 'whether', 'the', 'words', 'in', 'a', 'given', 'sentence', 'are', 'in', 'right', 'sequential', 'order', '(', 'with', 'around', '60', '%', 'accuracy', 'for', 'both', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'RB', ',', 'IN', 'VBN', 'IN', 'PRP$', 'JJ', 'NNS', ',', 'NNP', ':', 'PRP', 'CC', 'NNP', ':', 'NNP', 'VBD', 'RB', 'IN', 'JJ', 'NNS', 'RB', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'IN', 'DT', 'NNS', 'IN', 'DT', 'VBN', 'NN', 'VBP', 'IN', 'JJ', 'JJ', 'NN', '(', 'IN', 'RB', 'CD', 'NN', 'NN', 'IN', 'DT', ')', '.']",50
question-answering,1,179,We noticed that simple sum of embedding learned via Word2 Vec yields reasonably good results on all three tasks .,"['We', 'noticed', 'that', 'simple', 'sum', 'of', 'embedding', 'learned', 'via', 'Word2', 'Vec', 'yields', 'reasonably', 'good', 'results', 'on', 'all', 'three', 'tasks', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'IN', 'JJ', 'NN', 'IN', 'VBG', 'VBN', 'IN', 'NNP', 'NNP', 'NNS', 'RB', 'JJ', 'NNS', 'IN', 'DT', 'CD', 'NNS', '.']",20
question-answering,5,2,Iterative Alternating Neural Attention for Machine Reading,"['Iterative', 'Alternating', 'Neural', 'Attention', 'for', 'Machine', 'Reading']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['JJ', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",7
question-answering,5,4,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document .","['We', 'propose', 'a', 'novel', 'neural', 'attention', 'architecture', 'to', 'tackle', 'machine', 'comprehension', 'tasks', ',', 'such', 'as', 'answering', 'Cloze', '-', 'style', 'queries', 'with', 'respect', 'to', 'a', 'document', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'TO', 'VB', 'NN', 'NN', 'NNS', ',', 'JJ', 'IN', 'VBG', 'NNP', ':', 'NN', 'NNS', 'IN', 'NN', 'TO', 'DT', 'NN', '.']",26
question-answering,5,19,"Encouraged by the recent success of deep learning attention architectures , we propose a novel neural attention - based inference model designed to perform machine reading comprehension tasks .","['Encouraged', 'by', 'the', 'recent', 'success', 'of', 'deep', 'learning', 'attention', 'architectures', ',', 'we', 'propose', 'a', 'novel', 'neural', 'attention', '-', 'based', 'inference', 'model', 'designed', 'to', 'perform', 'machine', 'reading', 'comprehension', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'VBG', 'NN', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', ':', 'VBN', 'NN', 'NN', 'VBN', 'TO', 'VB', 'NN', 'VBG', 'NN', 'NNS', '.']",29
question-answering,5,20,The model first reads the document and the query using a recurrent neural network .,"['The', 'model', 'first', 'reads', 'the', 'document', 'and', 'the', 'query', 'using', 'a', 'recurrent', 'neural', 'network', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'RB', 'VBZ', 'DT', 'NN', 'CC', 'DT', 'NN', 'VBG', 'DT', 'JJ', 'JJ', 'NN', '.']",15
question-answering,5,21,"Then , it deploys an iterative inference process to uncover the inferential links that exist between the missing query word , the query , and the document .","['Then', ',', 'it', 'deploys', 'an', 'iterative', 'inference', 'process', 'to', 'uncover', 'the', 'inferential', 'links', 'that', 'exist', 'between', 'the', 'missing', 'query', 'word', ',', 'the', 'query', ',', 'and', 'the', 'document', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'WDT', 'VBP', 'IN', 'DT', 'VBG', 'NN', 'NN', ',', 'DT', 'NN', ',', 'CC', 'DT', 'NN', '.']",28
question-answering,5,22,"This phase involves a novel alternating attention mechanism ; it first attends to some parts of the query , then finds their corresponding matches by attending to the document .","['This', 'phase', 'involves', 'a', 'novel', 'alternating', 'attention', 'mechanism', ';', 'it', 'first', 'attends', 'to', 'some', 'parts', 'of', 'the', 'query', ',', 'then', 'finds', 'their', 'corresponding', 'matches', 'by', 'attending', 'to', 'the', 'document', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'NN', 'VBG', 'NN', 'NN', ':', 'PRP', 'RB', 'VBZ', 'TO', 'DT', 'NNS', 'IN', 'DT', 'NN', ',', 'RB', 'VBZ', 'PRP$', 'JJ', 'NNS', 'IN', 'VBG', 'TO', 'DT', 'NN', '.']",30
question-answering,5,23,The result of this alternating search is fed back into the iterative inference process to seed the next search step .,"['The', 'result', 'of', 'this', 'alternating', 'search', 'is', 'fed', 'back', 'into', 'the', 'iterative', 'inference', 'process', 'to', 'seed', 'the', 'next', 'search', 'step', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'DT', 'VBG', 'NN', 'VBZ', 'VBN', 'RB', 'IN', 'DT', 'JJ', 'NN', 'NN', 'TO', 'NN', 'DT', 'JJ', 'NN', 'NN', '.']",21
question-answering,5,25,"After a fixed number of iterations , the model uses a summary of its inference process to predict the answer .","['After', 'a', 'fixed', 'number', 'of', 'iterations', ',', 'the', 'model', 'uses', 'a', 'summary', 'of', 'its', 'inference', 'process', 'to', 'predict', 'the', 'answer', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', ',', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', '.']",21
question-answering,5,118,"To train our model , we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .","['To', 'train', 'our', 'model', ',', 'we', 'used', 'stochastic', 'gradient', 'descent', 'with', 'the', 'ADAM', 'optimizer', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', ',', 'with', 'an', 'initial', 'learning', 'rate', 'of', '0.001', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['TO', 'VB', 'PRP$', 'NN', ',', 'PRP', 'VBD', 'JJ', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', ',', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', '.']",30
question-answering,5,119,"We set the batch size to 32 and we decay the learning rate by 0.8 if the accuracy on the validation set does not increase after a half - epoch , i.e. 2000 batches ( for CBT ) and 5000 batches for ( CNN ) .","['We', 'set', 'the', 'batch', 'size', 'to', '32', 'and', 'we', 'decay', 'the', 'learning', 'rate', 'by', '0.8', 'if', 'the', 'accuracy', 'on', 'the', 'validation', 'set', 'does', 'not', 'increase', 'after', 'a', 'half', '-', 'epoch', ',', 'i.e.', '2000', 'batches', '(', 'for', 'CBT', ')', 'and', '5000', 'batches', 'for', '(', 'CNN', ')', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'CD', 'CC', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'RB', 'VB', 'IN', 'DT', 'JJ', ':', 'NN', ',', 'JJ', 'CD', 'NNS', '(', 'IN', 'NNP', ')', 'CC', 'CD', 'NNS', 'IN', '(', 'NNP', ')', '.']",46
question-answering,5,120,"We initialize all weights of our model by sampling from the normal distribution N ( 0 , 0.05 ) .","['We', 'initialize', 'all', 'weights', 'of', 'our', 'model', 'by', 'sampling', 'from', 'the', 'normal', 'distribution', 'N', '(', '0', ',', '0.05', ')', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNS', 'IN', 'PRP$', 'NN', 'IN', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NNP', '(', 'CD', ',', 'CD', ')', '.']",20
question-answering,5,121,"Following , the GRU recurrent weights are initialized to be orthogonal and biases are initialized to zero .","['Following', ',', 'the', 'GRU', 'recurrent', 'weights', 'are', 'initialized', 'to', 'be', 'orthogonal', 'and', 'biases', 'are', 'initialized', 'to', 'zero', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'O']","['VBG', ',', 'DT', 'NNP', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'VB', 'JJ', 'CC', 'NNS', 'VBP', 'VBN', 'TO', 'CD', '.']",18
question-answering,5,122,"In order to stabilize the learning , we clip the gradients if their norm is greater than 5 and those marked with 2 are from .","['In', 'order', 'to', 'stabilize', 'the', 'learning', ',', 'we', 'clip', 'the', 'gradients', 'if', 'their', 'norm', 'is', 'greater', 'than', '5', 'and', 'those', 'marked', 'with', '2', 'are', 'from', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', 'TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNS', 'IN', 'PRP$', 'NN', 'VBZ', 'JJR', 'IN', 'CD', 'CC', 'DT', 'VBN', 'IN', 'CD', 'VBP', 'IN', '.']",26
question-answering,5,125,"Our model is implemented in Theano , using the Keras library .","['Our', 'model', 'is', 'implemented', 'in', 'Theano', ',', 'using', 'the', 'Keras', 'library', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'VBN', 'IN', 'NNP', ',', 'VBG', 'DT', 'NNP', 'NN', '.']",12
question-answering,5,133,CBT,['CBT'],['B-n'],['NN'],1
question-answering,5,135,Our model ( line 7 ) sets a new stateof - the - art on the common noun category by gaining 3.6 and 5.6 points in validation and test over the best baseline AS Reader ( line 5 ) .,"['Our', 'model', '(', 'line', '7', ')', 'sets', 'a', 'new', 'stateof', '-', 'the', '-', 'art', 'on', 'the', 'common', 'noun', 'category', 'by', 'gaining', '3.6', 'and', '5.6', 'points', 'in', 'validation', 'and', 'test', 'over', 'the', 'best', 'baseline', 'AS', 'Reader', '(', 'line', '5', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', '(', 'NN', 'CD', ')', 'VBZ', 'DT', 'JJ', 'NN', ':', 'DT', ':', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'VBG', 'CD', 'CC', 'CD', 'NNS', 'IN', 'NN', 'CC', 'NN', 'IN', 'DT', 'JJS', 'NN', 'IN', 'NNP', '(', 'NN', 'CD', ')', '.']",40
question-answering,5,149,CNN,['CNN'],['B-n'],['NN'],1
question-answering,5,151,The results show that our model ( line 8 ) improves state - of - the - art accuracy by 4 percent absolute on validation and 3.4 on test with respect to the most recent published result ( AS Reader ) ( line 7 ) .,"['The', 'results', 'show', 'that', 'our', 'model', '(', 'line', '8', ')', 'improves', 'state', '-', 'of', '-', 'the', '-', 'art', 'accuracy', 'by', '4', 'percent', 'absolute', 'on', 'validation', 'and', '3.4', 'on', 'test', 'with', 'respect', 'to', 'the', 'most', 'recent', 'published', 'result', '(', 'AS', 'Reader', ')', '(', 'line', '7', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'VBP', 'IN', 'PRP$', 'NN', '(', 'NN', 'CD', ')', 'NNS', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'CD', 'NN', 'NN', 'IN', 'NN', 'CC', 'CD', 'IN', 'NN', 'IN', 'NN', 'TO', 'DT', 'RBS', 'JJ', 'VBN', 'NN', '(', 'IN', 'NNP', ')', '(', 'NN', 'CD', ')', '.']",46
question-answering,5,159,"Similarly to the single model case , our ensembles achieve state - of - the - art test performance of 75.2 and 76.1 on validation and test respectively , outperforming previously published results .","['Similarly', 'to', 'the', 'single', 'model', 'case', ',', 'our', 'ensembles', 'achieve', 'state', '-', 'of', '-', 'the', '-', 'art', 'test', 'performance', 'of', '75.2', 'and', '76.1', 'on', 'validation', 'and', 'test', 'respectively', ',', 'outperforming', 'previously', 'published', 'results', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', 'TO', 'DT', 'JJ', 'NN', 'NN', ',', 'PRP$', 'NNS', 'VBP', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'NN', 'IN', 'CD', 'CC', 'CD', 'IN', 'NN', 'CC', 'NN', 'RB', ',', 'VBG', 'RB', 'VBN', 'NNS', '.']",34
question-answering,7,4,We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders .,"['We', 'present', 'a', 'memory', 'augmented', 'neural', 'network', 'for', 'natural', 'language', 'understanding', ':', 'Neural', 'Semantic', 'Encoders', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'VBN', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', ':', 'JJ', 'NNP', 'NNP', '.']",16
question-answering,7,18,In this paper we propose a novel class of memory augmented neural networks called Neural Semantic Encoders ( NSE ) for natural language understanding .,"['In', 'this', 'paper', 'we', 'propose', 'a', 'novel', 'class', 'of', 'memory', 'augmented', 'neural', 'networks', 'called', 'Neural', 'Semantic', 'Encoders', '(', 'NSE', ')', 'for', 'natural', 'language', 'understanding', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NN', 'VBN', 'JJ', 'NNS', 'VBD', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'IN', 'JJ', 'NN', 'NN', '.']",25
question-answering,7,20,NSE has a variable sized encoding memory which allows the model to access entire input sequence during the reading process ; therefore efficiently delivering long - term dependencies overtime .,"['NSE', 'has', 'a', 'variable', 'sized', 'encoding', 'memory', 'which', 'allows', 'the', 'model', 'to', 'access', 'entire', 'input', 'sequence', 'during', 'the', 'reading', 'process', ';', 'therefore', 'efficiently', 'delivering', 'long', '-', 'term', 'dependencies', 'overtime', '.']","['B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'JJ', 'VBG', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'TO', 'NN', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', ':', 'RB', 'RB', 'VBG', 'JJ', ':', 'NN', 'NNS', 'VBP', '.']",30
question-answering,7,21,"The encoding memory evolves overtime and maintains the memory of the input sequence through read , compose and write operations .","['The', 'encoding', 'memory', 'evolves', 'overtime', 'and', 'maintains', 'the', 'memory', 'of', 'the', 'input', 'sequence', 'through', 'read', ',', 'compose', 'and', 'write', 'operations', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBG', 'NN', 'NNS', 'RB', 'CC', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'NN', ',', 'NN', 'CC', 'JJ', 'NNS', '.']",21
question-answering,7,22,NSE sequentially processes the input and supports word compositionality inheriting both temporal and hierarchical nature of human language .,"['NSE', 'sequentially', 'processes', 'the', 'input', 'and', 'supports', 'word', 'compositionality', 'inheriting', 'both', 'temporal', 'and', 'hierarchical', 'nature', 'of', 'human', 'language', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'RB', 'VBZ', 'DT', 'NN', 'CC', 'NNS', 'NN', 'NN', 'VBG', 'DT', 'JJ', 'CC', 'JJ', 'NN', 'IN', 'JJ', 'NN', '.']",19
question-answering,7,23,NSE can read from and write to a set of relevant encoding memories simultaneously or multiple NSEs can access a shared encoding memory effectively supporting knowledge and representation sharing .,"['NSE', 'can', 'read', 'from', 'and', 'write', 'to', 'a', 'set', 'of', 'relevant', 'encoding', 'memories', 'simultaneously', 'or', 'multiple', 'NSEs', 'can', 'access', 'a', 'shared', 'encoding', 'memory', 'effectively', 'supporting', 'knowledge', 'and', 'representation', 'sharing', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'MD', 'VB', 'IN', 'CC', 'VB', 'TO', 'DT', 'NN', 'IN', 'JJ', 'VBG', 'NNS', 'RB', 'CC', 'JJ', 'NNP', 'MD', 'NN', 'DT', 'VBN', 'VBG', 'NN', 'RB', 'VBG', 'NN', 'CC', 'NN', 'NN', '.']",30
question-answering,7,24,"NSE is flexible , robust and suitable for practical NLU tasks and can be trained easily by any gradient descent optimizer .","['NSE', 'is', 'flexible', ',', 'robust', 'and', 'suitable', 'for', 'practical', 'NLU', 'tasks', 'and', 'can', 'be', 'trained', 'easily', 'by', 'any', 'gradient', 'descent', 'optimizer', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'JJ', ',', 'JJ', 'CC', 'JJ', 'IN', 'JJ', 'NNP', 'NNS', 'CC', 'MD', 'VB', 'VBN', 'RB', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",22
question-answering,7,127,The models are trained using Adam with hyperparameters selected on development set .,"['The', 'models', 'are', 'trained', 'using', 'Adam', 'with', 'hyperparameters', 'selected', 'on', 'development', 'set', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'VBG', 'NNP', 'IN', 'NNS', 'VBN', 'IN', 'NN', 'VBN', '.']",13
question-answering,7,128,We chose two one - layer LSTM for read / write modules on the tasks other than QA on which we used two - layer LSTM .,"['We', 'chose', 'two', 'one', '-', 'layer', 'LSTM', 'for', 'read', '/', 'write', 'modules', 'on', 'the', 'tasks', 'other', 'than', 'QA', 'on', 'which', 'we', 'used', 'two', '-', 'layer', 'LSTM', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-p', 'I-p', 'I-p', 'I-p', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'CD', 'CD', ':', 'NN', 'NNP', 'IN', 'NN', 'NNP', 'WDT', 'NNS', 'IN', 'DT', 'NNS', 'JJ', 'IN', 'NNP', 'IN', 'WDT', 'PRP', 'VBD', 'CD', ':', 'NN', 'NNP', '.']",27
question-answering,7,129,The pre-trained 300 - D Glove 840B vectors and 100 - D Glove 6B vectors were obtained for the word embeddings .,"['The', 'pre-trained', '300', '-', 'D', 'Glove', '840B', 'vectors', 'and', '100', '-', 'D', 'Glove', '6B', 'vectors', 'were', 'obtained', 'for', 'the', 'word', 'embeddings', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-p', 'I-p', 'O']","['DT', 'JJ', 'CD', ':', 'NNP', 'NNP', 'CD', 'NNS', 'CC', 'CD', ':', 'NNP', 'NNP', 'CD', 'NNS', 'VBD', 'VBN', 'IN', 'DT', 'NN', 'NNS', '.']",22
question-answering,7,132,We crop or pad the input sequence to a fixed length .,"['We', 'crop', 'or', 'pad', 'the', 'input', 'sequence', 'to', 'a', 'fixed', 'length', '.']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'NN', 'CC', 'VB', 'DT', 'NN', 'NN', 'TO', 'DT', 'JJ', 'NN', '.']",12
question-answering,7,134,The models were regularized by using dropouts and an l 2 weight decay .,"['The', 'models', 'were', 'regularized', 'by', 'using', 'dropouts', 'and', 'an', 'l', '2', 'weight', 'decay', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBD', 'VBN', 'IN', 'VBG', 'NNS', 'CC', 'DT', 'NN', 'CD', 'NN', 'NN', '.']",14
question-answering,7,135,Natural Language Inference,"['Natural', 'Language', 'Inference']","['B-p', 'I-p', 'I-p']","['JJ', 'NNP', 'NN']",3
question-answering,7,142,"In addition , the MLP has a hidden layer with 1024 units with ReLU activation and a sof tmax layer .","['In', 'addition', ',', 'the', 'MLP', 'has', 'a', 'hidden', 'layer', 'with', '1024', 'units', 'with', 'ReLU', 'activation', 'and', 'a', 'sof', 'tmax', 'layer', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'DT', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NNS', 'IN', 'NNP', 'NN', 'CC', 'DT', 'NN', 'NN', 'NN', '.']",21
question-answering,7,143,"We set the batch size to 128 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .","['We', 'set', 'the', 'batch', 'size', 'to', '128', ',', 'the', 'initial', 'learning', 'rate', 'to', '3e', '-', '4', 'and', 'l', '2', 'regularizer', 'strength', 'to', '3', 'e', '-', '5', ',', 'and', 'train', 'each', 'model', 'for', '40', 'epochs', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'CD', ',', 'DT', 'JJ', 'NN', 'NN', 'TO', 'CD', ':', 'CD', 'CC', '$', 'CD', 'NN', 'NN', 'TO', 'CD', 'SYM', ':', 'CD', ',', 'CC', 'VB', 'DT', 'NN', 'IN', 'CD', 'NNS', '.']",35
question-answering,7,158,Our MMA - NSE attention model is similar to the LSTM attention model .,"['Our', 'MMA', '-', 'NSE', 'attention', 'model', 'is', 'similar', 'to', 'the', 'LSTM', 'attention', 'model', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NNP', ':', 'NNP', 'NN', 'NN', 'VBZ', 'JJ', 'TO', 'DT', 'NNP', 'NN', 'NN', '.']",14
question-answering,7,160,This model obtained 85.4 % accuracy score .,"['This', 'model', 'obtained', '85.4', '%', 'accuracy', 'score', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBD', 'CD', 'NN', 'NN', 'NN', '.']",8
question-answering,7,162,Answer Sentence Selection,"['Answer', 'Sentence', 'Selection']","['B-p', 'I-p', 'I-p']","['NNP', 'NNP', 'NNP']",3
question-answering,7,173,"We set the batch size to 4 and the initial learning rate to 1 e - 5 , and train the model for 10 epochs .","['We', 'set', 'the', 'batch', 'size', 'to', '4', 'and', 'the', 'initial', 'learning', 'rate', 'to', '1', 'e', '-', '5', ',', 'and', 'train', 'the', 'model', 'for', '10', 'epochs', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'CD', 'CC', 'DT', 'JJ', 'NN', 'NN', 'TO', 'CD', 'SYM', ':', 'CD', ',', 'CC', 'VB', 'DT', 'NN', 'IN', 'CD', 'NNS', '.']",26
question-answering,7,174,We used 40 % dropouts afterword embeddings and no l 2 weight decay .,"['We', 'used', '40', '%', 'dropouts', 'afterword', 'embeddings', 'and', 'no', 'l', '2', 'weight', 'decay', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'CD', 'NN', 'NNS', 'JJ', 'NNS', 'CC', 'DT', 'NN', 'CD', 'NN', 'NN', '.']",14
question-answering,7,175,The word embeddings are pre-trained 300 - D Glove 840B vectors .,"['The', 'word', 'embeddings', 'are', 'pre-trained', '300', '-', 'D', 'Glove', '840B', 'vectors', '.']","['O', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'JJ', 'CD', ':', 'NNP', 'NNP', 'CD', 'NNS', '.']",12
question-answering,7,176,"For this task , a linear mapping layer transforms the 300 - D word embeddings to the 512- D LSTM inputs .","['For', 'this', 'task', ',', 'a', 'linear', 'mapping', 'layer', 'transforms', 'the', '300', '-', 'D', 'word', 'embeddings', 'to', 'the', '512-', 'D', 'LSTM', 'inputs', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'DT', 'CD', ':', 'NN', 'NN', 'VBZ', 'TO', 'DT', 'JJ', 'NNP', 'NNP', 'NNS', '.']",22
question-answering,7,181,Our MMA - NSE attention model exceeds the NASM by approximately 1 % on MAP and 0.8 % on MRR for this task .,"['Our', 'MMA', '-', 'NSE', 'attention', 'model', 'exceeds', 'the', 'NASM', 'by', 'approximately', '1', '%', 'on', 'MAP', 'and', '0.8', '%', 'on', 'MRR', 'for', 'this', 'task', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['PRP$', 'NNP', ':', 'NNP', 'NN', 'NN', 'VBZ', 'DT', 'NNP', 'IN', 'RB', 'CD', 'NN', 'IN', 'NNP', 'CC', 'CD', 'NN', 'IN', 'NNP', 'IN', 'DT', 'NN', '.']",24
question-answering,7,186,Sentence Classification,"['Sentence', 'Classification']","['B-p', 'I-p']","['NN', 'NN']",2
question-answering,7,191,The first layer of the MLP has ReLU activation and 1024 or 300 units for binary or fine - grained setting .,"['The', 'first', 'layer', 'of', 'the', 'MLP', 'has', 'ReLU', 'activation', 'and', '1024', 'or', '300', 'units', 'for', 'binary', 'or', 'fine', '-', 'grained', 'setting', '.']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'VBZ', 'VBN', 'NN', 'CC', 'CD', 'CC', 'CD', 'NNS', 'IN', 'JJ', 'CC', 'JJ', ':', 'JJ', 'NN', '.']",22
question-answering,7,192,The second layer is a sof tmax layer .,"['The', 'second', 'layer', 'is', 'a', 'sof', 'tmax', 'layer', '.']","['O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', '.']",9
question-answering,7,193,The read / write modules are two one - layer LSTM with 300 hidden units and the word embeddings are the pre-trained 300 - D Glove 840B vectors .,"['The', 'read', '/', 'write', 'modules', 'are', 'two', 'one', '-', 'layer', 'LSTM', 'with', '300', 'hidden', 'units', 'and', 'the', 'word', 'embeddings', 'are', 'the', 'pre-trained', '300', '-', 'D', 'Glove', '840B', 'vectors', '.']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NNP', 'JJ', 'NNS', 'VBP', 'CD', 'CD', ':', 'NN', 'NNP', 'IN', 'CD', 'JJ', 'NNS', 'CC', 'DT', 'NN', 'NNS', 'VBP', 'DT', 'JJ', 'CD', ':', 'NNP', 'NNP', 'CD', 'NNS', '.']",29
question-answering,7,194,"We set the batch size to 64 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 25 epochs .","['We', 'set', 'the', 'batch', 'size', 'to', '64', ',', 'the', 'initial', 'learning', 'rate', 'to', '3e', '-', '4', 'and', 'l', '2', 'regularizer', 'strength', 'to', '3', 'e', '-', '5', ',', 'and', 'train', 'each', 'model', 'for', '25', 'epochs', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'CD', ',', 'DT', 'JJ', 'NN', 'NN', 'TO', 'CD', ':', 'CD', 'CC', '$', 'CD', 'NN', 'NN', 'TO', 'CD', 'SYM', ':', 'CD', ',', 'CC', 'VB', 'DT', 'NN', 'IN', 'CD', 'NNS', '.']",35
question-answering,7,199,Our model outperformed the DMN and set the state - of - the - art results on both subtasks .,"['Our', 'model', 'outperformed', 'the', 'DMN', 'and', 'set', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'on', 'both', 'subtasks', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBD', 'DT', 'NNP', 'CC', 'VBD', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'NNS', '.']",20
question-answering,7,200,Document Sentiment Analysis,"['Document', 'Sentiment', 'Analysis']","['B-p', 'I-p', 'I-p']","['NNP', 'NNP', 'NN']",3
question-answering,7,204,We stack a NSE or LSTM on the top of another NSE for document modeling .,"['We', 'stack', 'a', 'NSE', 'or', 'LSTM', 'on', 'the', 'top', 'of', 'another', 'NSE', 'for', 'document', 'modeling', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'CC', 'NNP', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNP', 'IN', 'NN', 'NN', '.']",16
question-answering,7,207,The whole network is trained jointly by backpropagating the cross entropy loss .,"['The', 'whole', 'network', 'is', 'trained', 'jointly', 'by', 'backpropagating', 'the', 'cross', 'entropy', 'loss', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'VBN', 'RB', 'IN', 'VBG', 'DT', 'NN', 'NN', 'NN', '.']",13
question-answering,7,208,We used one - layer LSTM with 100 hidden units for the read / write modules and the pre-trained 100 - D Glove 6B vectors for this task .,"['We', 'used', 'one', '-', 'layer', 'LSTM', 'with', '100', 'hidden', 'units', 'for', 'the', 'read', '/', 'write', 'modules', 'and', 'the', 'pre-trained', '100', '-', 'D', 'Glove', '6B', 'vectors', 'for', 'this', 'task', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'CD', ':', 'NN', 'NNP', 'IN', 'CD', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NNP', 'JJ', 'NNS', 'CC', 'DT', 'JJ', 'CD', ':', 'NNP', 'NNP', 'CD', 'NNS', 'IN', 'DT', 'NN', '.']",29
question-answering,7,209,"We set the batch size to 32 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and trained each model for 50 epochs .","['We', 'set', 'the', 'batch', 'size', 'to', '32', ',', 'the', 'initial', 'learning', 'rate', 'to', '3e', '-', '4', 'and', 'l', '2', 'regularizer', 'strength', 'to', '1', 'e', '-', '5', ',', 'and', 'trained', 'each', 'model', 'for', '50', 'epochs', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'CD', ',', 'DT', 'JJ', 'NN', 'NN', 'TO', 'CD', ':', 'CD', 'CC', '$', 'CD', 'NN', 'NN', 'TO', 'CD', 'SYM', ':', 'CD', ',', 'CC', 'VBD', 'DT', 'NN', 'IN', 'CD', 'NNS', '.']",35
question-answering,7,221,Machine Translation,"['Machine', 'Translation']","['B-p', 'I-p']","['NN', 'NN']",2
question-answering,7,241,The models were trained to minimize word - level cross entropy loss and were regularized by 20 % input dropouts and the 30 % output dropouts .,"['The', 'models', 'were', 'trained', 'to', 'minimize', 'word', '-', 'level', 'cross', 'entropy', 'loss', 'and', 'were', 'regularized', 'by', '20', '%', 'input', 'dropouts', 'and', 'the', '30', '%', 'output', 'dropouts', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBD', 'VBN', 'TO', 'VB', 'NN', ':', 'NN', 'NN', 'JJ', 'NN', 'CC', 'VBD', 'VBN', 'IN', 'CD', 'NN', 'NN', 'NNS', 'CC', 'DT', 'CD', 'NN', 'NN', 'NNS', '.']",27
question-answering,7,242,"We set the batch size to 128 , the initial learning rate to 1e - 3 for LSTM - LSTM and 3e - 4 for the other models and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .","['We', 'set', 'the', 'batch', 'size', 'to', '128', ',', 'the', 'initial', 'learning', 'rate', 'to', '1e', '-', '3', 'for', 'LSTM', '-', 'LSTM', 'and', '3e', '-', '4', 'for', 'the', 'other', 'models', 'and', 'l', '2', 'regularizer', 'strength', 'to', '3', 'e', '-', '5', ',', 'and', 'train', 'each', 'model', 'for', '40', 'epochs', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'CD', ',', 'DT', 'JJ', 'NN', 'NN', 'TO', 'CD', ':', 'CD', 'IN', 'NNP', ':', 'NNP', 'CC', 'CD', ':', 'CD', 'IN', 'DT', 'JJ', 'NNS', 'CC', '$', 'CD', 'NN', 'NN', 'TO', 'CD', 'SYM', ':', 'CD', ',', 'CC', 'VB', 'DT', 'NN', 'IN', 'CD', 'NNS', '.']",47
question-answering,4,2,A Parallel - Hierarchical Model for Machine Comprehension on Sparse Data,"['A', 'Parallel', '-', 'Hierarchical', 'Model', 'for', 'Machine', 'Comprehension', 'on', 'Sparse', 'Data']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['DT', 'NNP', ':', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",11
question-answering,4,4,Understanding unstructured text is a major goal within natural language processing .,"['Understanding', 'unstructured', 'text', 'is', 'a', 'major', 'goal', 'within', 'natural', 'language', 'processing', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",12
question-answering,4,6,"In this work , we investigate machine comprehension on the challenging MCTest benchmark .","['In', 'this', 'work', ',', 'we', 'investigate', 'machine', 'comprehension', 'on', 'the', 'challenging', 'MCTest', 'benchmark', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'NN', 'NN', 'IN', 'DT', 'VBG', 'NNP', 'NN', '.']",14
question-answering,4,14,"Comprehension of unstructured text by machines , at a near- human level , is a major goal for natural language processing .","['Comprehension', 'of', 'unstructured', 'text', 'by', 'machines', ',', 'at', 'a', 'near-', 'human', 'level', ',', 'is', 'a', 'major', 'goal', 'for', 'natural', 'language', 'processing', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'IN', 'JJ', 'NN', 'IN', 'NNS', ',', 'IN', 'DT', 'JJ', 'JJ', 'NN', ',', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",22
question-answering,4,16,Machine comprehension ( MC ) is evaluated by posing a set of questions based on a text passage ( akin to the reading tests we all took in school ) .,"['Machine', 'comprehension', '(', 'MC', ')', 'is', 'evaluated', 'by', 'posing', 'a', 'set', 'of', 'questions', 'based', 'on', 'a', 'text', 'passage', '(', 'akin', 'to', 'the', 'reading', 'tests', 'we', 'all', 'took', 'in', 'school', ')', '.']","['B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', '(', 'NNP', ')', 'VBZ', 'VBN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'JJ', 'NN', '(', 'JJ', 'TO', 'DT', 'NN', 'VBZ', 'PRP', 'DT', 'VBD', 'IN', 'NN', ')', '.']",31
question-answering,4,27,The key to our model is that it compares the question and answer candidates to the text using several distinct perspectives .,"['The', 'key', 'to', 'our', 'model', 'is', 'that', 'it', 'compares', 'the', 'question', 'and', 'answer', 'candidates', 'to', 'the', 'text', 'using', 'several', 'distinct', 'perspectives', '.']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'TO', 'PRP$', 'NN', 'VBZ', 'IN', 'PRP', 'VBZ', 'DT', 'NN', 'CC', 'NN', 'NNS', 'TO', 'DT', 'NN', 'VBG', 'JJ', 'JJ', 'NNS', '.']",22
question-answering,4,29,"The semantic perspective compares the hypothesis to sentences in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .","['The', 'semantic', 'perspective', 'compares', 'the', 'hypothesis', 'to', 'sentences', 'in', 'the', 'text', 'viewed', 'as', 'single', ',', 'self', '-', 'contained', 'thoughts', ';', 'these', 'are', 'represented', 'using', 'a', 'sum', 'and', 'transformation', 'of', 'word', 'embedding', 'vectors', ',', 'similarly', 'to', 'in', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'NN', 'TO', 'NNS', 'IN', 'DT', 'NN', 'VBD', 'IN', 'JJ', ',', 'PRP', ':', 'VBN', 'NNS', ':', 'DT', 'VBP', 'VBN', 'VBG', 'DT', 'NN', 'CC', 'NN', 'IN', 'NN', 'VBG', 'NNS', ',', 'RB', 'TO', 'IN', '.']",37
question-answering,4,30,"The word - by - word perspective focuses on similarity matches between individual words from hypothesis and text , at various scales .","['The', 'word', '-', 'by', '-', 'word', 'perspective', 'focuses', 'on', 'similarity', 'matches', 'between', 'individual', 'words', 'from', 'hypothesis', 'and', 'text', ',', 'at', 'various', 'scales', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', ':', 'IN', ':', 'NN', 'NN', 'VBZ', 'IN', 'NN', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'NN', 'CC', 'NN', ',', 'IN', 'JJ', 'NNS', '.']",23
question-answering,4,32,"We also use a sliding window acting on a subsentential scale ( inspired by the work of ) , which implicitly considers the linear distance between matched words .","['We', 'also', 'use', 'a', 'sliding', 'window', 'acting', 'on', 'a', 'subsentential', 'scale', '(', 'inspired', 'by', 'the', 'work', 'of', ')', ',', 'which', 'implicitly', 'considers', 'the', 'linear', 'distance', 'between', 'matched', 'words', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', 'NN', 'VBG', 'IN', 'DT', 'JJ', 'NN', '(', 'VBN', 'IN', 'DT', 'NN', 'IN', ')', ',', 'WDT', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'VBN', 'NNS', '.']",29
question-answering,4,33,"Finally , this word - level sliding window operates on two different views of text sentences : the sequential view , where words appear in their natural order , and the dependency view , where words are reordered based on a linearization of the sentence 's dependency graph .","['Finally', ',', 'this', 'word', '-', 'level', 'sliding', 'window', 'operates', 'on', 'two', 'different', 'views', 'of', 'text', 'sentences', ':', 'the', 'sequential', 'view', ',', 'where', 'words', 'appear', 'in', 'their', 'natural', 'order', ',', 'and', 'the', 'dependency', 'view', ',', 'where', 'words', 'are', 'reordered', 'based', 'on', 'a', 'linearization', 'of', 'the', 'sentence', ""'s"", 'dependency', 'graph', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', ':', 'NN', 'VBG', 'JJ', 'NNS', 'IN', 'CD', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', ':', 'DT', 'JJ', 'NN', ',', 'WRB', 'NNS', 'VBP', 'IN', 'PRP$', 'JJ', 'NN', ',', 'CC', 'DT', 'NN', 'NN', ',', 'WRB', 'NNS', 'VBP', 'VBN', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'POS', 'NN', 'NN', '.']",49
question-answering,4,218,"For word vectors we use Google 's publicly available embeddings , trained with word2vec on the 100 - billion - word News corpus .","['For', 'word', 'vectors', 'we', 'use', 'Google', ""'s"", 'publicly', 'available', 'embeddings', ',', 'trained', 'with', 'word2vec', 'on', 'the', '100', '-', 'billion', '-', 'word', 'News', 'corpus', '.']","['B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'NNS', 'PRP', 'VBP', 'NNP', 'POS', 'RB', 'JJ', 'NNS', ',', 'VBN', 'IN', 'NN', 'IN', 'DT', 'CD', ':', 'CD', ':', 'NN', 'NNP', 'NN', '.']",24
question-answering,4,231,"We found dropout to be particularly effective at improving generalization from the training to the test set , and used 0.5 as the dropout probability .","['We', 'found', 'dropout', 'to', 'be', 'particularly', 'effective', 'at', 'improving', 'generalization', 'from', 'the', 'training', 'to', 'the', 'test', 'set', ',', 'and', 'used', '0.5', 'as', 'the', 'dropout', 'probability', '.']","['O', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBD', 'NN', 'TO', 'VB', 'RB', 'JJ', 'IN', 'VBG', 'NN', 'IN', 'DT', 'NN', 'TO', 'DT', 'NN', 'NN', ',', 'CC', 'VBD', 'CD', 'IN', 'DT', 'NN', 'NN', '.']",26
question-answering,4,235,"We used the Adam optimizer with the standard settings ( Kingma and Ba , 2014 ) and a learning rate of 0.003 .","['We', 'used', 'the', 'Adam', 'optimizer', 'with', 'the', 'standard', 'settings', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', 'and', 'a', 'learning', 'rate', 'of', '0.003', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBD', 'DT', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NNS', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'CC', 'DT', 'JJ', 'NN', 'IN', 'CD', '.']",23
question-answering,4,236,To determine the best hyperparameters we performed a grid search over 150 settings based on validation - set accuracy .,"['To', 'determine', 'the', 'best', 'hyperparameters', 'we', 'performed', 'a', 'grid', 'search', 'over', '150', 'settings', 'based', 'on', 'validation', '-', 'set', 'accuracy', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'JJS', 'NNS', 'PRP', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NNS', 'VBN', 'IN', 'NN', ':', 'NN', 'NN', '.']",20
question-answering,4,247,"On MCTest - 500 , the Parallel Hierarchical model significantly outperforms these methods on single questions ( > 2 % ) and slightly outperforms the latter two on multi questions ( ? 0.3 % ) and over all ( ? 1 % ) .","['On', 'MCTest', '-', '500', ',', 'the', 'Parallel', 'Hierarchical', 'model', 'significantly', 'outperforms', 'these', 'methods', 'on', 'single', 'questions', '(', '>', '2', '%', ')', 'and', 'slightly', 'outperforms', 'the', 'latter', 'two', 'on', 'multi', 'questions', '(', '?', '0.3', '%', ')', 'and', 'over', 'all', '(', '?', '1', '%', ')', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', ':', 'CD', ',', 'DT', 'NNP', 'NNP', 'NN', 'RB', 'VBZ', 'DT', 'NNS', 'IN', 'JJ', 'NNS', '(', 'JJ', 'CD', 'NN', ')', 'CC', 'RB', 'VBZ', 'DT', 'JJ', 'CD', 'IN', 'NNS', 'NNS', '(', '.', 'CD', 'NN', ')', 'CC', 'IN', 'DT', '(', '.', 'CD', 'NN', ')', '.']",44
question-answering,4,259,"Not surprisingly , the n-gram functionality is important , contributing almost 5 % accuracy improvement .","['Not', 'surprisingly', ',', 'the', 'n-gram', 'functionality', 'is', 'important', ',', 'contributing', 'almost', '5', '%', 'accuracy', 'improvement', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', 'RB', ',', 'DT', 'JJ', 'NN', 'VBZ', 'JJ', ',', 'VBG', 'RB', 'CD', 'NN', 'NN', 'NN', '.']",16
question-answering,4,262,"The top N function contributes very little to the over all performance , suggesting that most multi questions have their evidence distributed across contiguous sentences .","['The', 'top', 'N', 'function', 'contributes', 'very', 'little', 'to', 'the', 'over', 'all', 'performance', ',', 'suggesting', 'that', 'most', 'multi', 'questions', 'have', 'their', 'evidence', 'distributed', 'across', 'contiguous', 'sentences', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NNP', 'NN', 'VBZ', 'RB', 'JJ', 'TO', 'DT', 'IN', 'DT', 'NN', ',', 'VBG', 'IN', 'JJS', 'JJ', 'NNS', 'VBP', 'PRP$', 'NN', 'VBN', 'IN', 'JJ', 'NNS', '.']",26
question-answering,4,263,"Ablating the sentential component made the most significant difference , reducing performance by more than 5 % .","['Ablating', 'the', 'sentential', 'component', 'made', 'the', 'most', 'significant', 'difference', ',', 'reducing', 'performance', 'by', 'more', 'than', '5', '%', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'DT', 'JJ', 'NN', 'VBD', 'DT', 'RBS', 'JJ', 'NN', ',', 'VBG', 'NN', 'IN', 'JJR', 'IN', 'CD', 'NN', '.']",18
question-answering,4,264,Simple word - by - word matching is obviously useful on MCTest .,"['Simple', 'word', '-', 'by', '-', 'word', 'matching', 'is', 'obviously', 'useful', 'on', 'MCTest', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'O']","['NN', 'NN', ':', 'IN', ':', 'NN', 'NN', 'VBZ', 'RB', 'JJ', 'IN', 'NNP', '.']",13
question-answering,4,265,"The sequential sliding window makes a 3 % contribution , highlighting the importance of word - distance measures .","['The', 'sequential', 'sliding', 'window', 'makes', 'a', '3', '%', 'contribution', ',', 'highlighting', 'the', 'importance', 'of', 'word', '-', 'distance', 'measures', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'DT', 'CD', 'NN', 'NN', ',', 'VBG', 'DT', 'NN', 'IN', 'NN', ':', 'NN', 'NNS', '.']",19
question-answering,4,266,"On the other hand , the dependency - based sliding window makes only a minor contribution .","['On', 'the', 'other', 'hand', ',', 'the', 'dependency', '-', 'based', 'sliding', 'window', 'makes', 'only', 'a', 'minor', 'contribution', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'DT', 'NN', ':', 'VBN', 'VBG', 'NN', 'VBZ', 'RB', 'DT', 'JJ', 'NN', '.']",17
question-answering,4,269,"Finally , the exogenous word weights make a significant contribution of almost 5 % .","['Finally', ',', 'the', 'exogenous', 'word', 'weights', 'make', 'a', 'significant', 'contribution', 'of', 'almost', '5', '%', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'JJ', 'NN', 'NNS', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'RB', 'CD', 'NN', '.']",15
question-answering,6,2,Published as a conference paper at ICLR 2017 QUERY - REDUCTION NETWORKS FOR QUESTION ANSWERING,"['Published', 'as', 'a', 'conference', 'paper', 'at', 'ICLR', '2017', 'QUERY', '-', 'REDUCTION', 'NETWORKS', 'FOR', 'QUESTION', 'ANSWERING']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['VBN', 'IN', 'DT', 'NN', 'NN', 'IN', 'NNP', 'CD', 'NNP', ':', 'NN', 'NNP', 'NNP', 'NNP', 'NNP']",15
question-answering,6,4,"In this paper , we study the problem of question answering when reasoning over multiple facts is required .","['In', 'this', 'paper', ',', 'we', 'study', 'the', 'problem', 'of', 'question', 'answering', 'when', 'reasoning', 'over', 'multiple', 'facts', 'is', 'required', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'VBG', 'WRB', 'VBG', 'IN', 'JJ', 'NNS', 'VBZ', 'VBN', '.']",19
question-answering,6,24,"Our proposed model , Query - Reduction Network 1 ( QRN ) , is a single recurrent unit that addresses the long - term dependency problem of most RNN - based models by simplifying the recurrent update , while taking the advantage of RNN 's capability to model sequential data ) .","['Our', 'proposed', 'model', ',', 'Query', '-', 'Reduction', 'Network', '1', '(', 'QRN', ')', ',', 'is', 'a', 'single', 'recurrent', 'unit', 'that', 'addresses', 'the', 'long', '-', 'term', 'dependency', 'problem', 'of', 'most', 'RNN', '-', 'based', 'models', 'by', 'simplifying', 'the', 'recurrent', 'update', ',', 'while', 'taking', 'the', 'advantage', 'of', 'RNN', ""'s"", 'capability', 'to', 'model', 'sequential', 'data', ')', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O']","['PRP$', 'VBN', 'NN', ',', 'NNP', ':', 'NN', 'NNP', 'CD', '(', 'NNP', ')', ',', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', ':', 'NN', 'NN', 'NN', 'IN', 'JJS', 'NNP', ':', 'VBN', 'NNS', 'IN', 'VBG', 'DT', 'NN', 'NN', ',', 'IN', 'VBG', 'DT', 'NN', 'IN', 'NNP', 'POS', 'NN', 'TO', 'VB', 'JJ', 'NNS', ')', '.']",52
question-answering,6,25,"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time .","['QRN', 'considers', 'the', 'context', 'sentences', 'as', 'a', 'sequence', 'of', 'state', '-', 'changing', 'triggers', ',', 'and', 'transforms', '(', 'reduces', ')', 'the', 'original', 'query', 'to', 'a', 'more', 'informed', 'query', 'as', 'it', 'observes', 'each', 'trigger', 'through', 'time', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NN', ':', 'VBG', 'NNS', ',', 'CC', 'NNS', '(', 'NNS', ')', 'DT', 'JJ', 'NN', 'TO', 'DT', 'RBR', 'JJ', 'NN', 'IN', 'PRP', 'VBZ', 'DT', 'NN', 'IN', 'NN', '.']",35
question-answering,6,204,We withhold 10 % of the training for development .,"['We', 'withhold', '10', '%', 'of', 'the', 'training', 'for', 'development', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'CD', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', '.']",10
question-answering,6,205,We use the hidden state size of 50 by deafult .,"['We', 'use', 'the', 'hidden', 'state', 'size', 'of', '50', 'by', 'deafult', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'IN', 'NN', '.']",11
question-answering,6,206,"Batch sizes of 32 for bAbI story - based QA 1k , bAb I dialog and DSTC2 dialog , and 128 for bAbI QA 10 k are used .","['Batch', 'sizes', 'of', '32', 'for', 'bAbI', 'story', '-', 'based', 'QA', '1k', ',', 'bAb', 'I', 'dialog', 'and', 'DSTC2', 'dialog', ',', 'and', '128', 'for', 'bAbI', 'QA', '10', 'k', 'are', 'used', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O']","['NNP', 'NNS', 'IN', 'CD', 'IN', 'NN', 'NN', ':', 'VBN', 'NNP', 'CD', ',', 'NN', 'PRP', 'VBP', 'CC', 'NNP', 'NN', ',', 'CC', 'CD', 'IN', 'NN', 'NNP', 'CD', 'NN', 'VBP', 'VBN', '.']",29
question-answering,6,210,L2 weight decay of 0.001 ( 0.0005 for QA 10 k ) is used for all weights .,"['L2', 'weight', 'decay', 'of', '0.001', '(', '0.0005', 'for', 'QA', '10', 'k', ')', 'is', 'used', 'for', 'all', 'weights', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBD', 'NN', 'IN', 'CD', '(', 'CD', 'IN', 'NNP', 'CD', 'NN', ')', 'VBZ', 'VBN', 'IN', 'DT', 'NNS', '.']",18
question-answering,6,211,The loss function is the cross entropy betweenv and the one - hot vector of the true answer .,"['The', 'loss', 'function', 'is', 'the', 'cross', 'entropy', 'betweenv', 'and', 'the', 'one', '-', 'hot', 'vector', 'of', 'the', 'true', 'answer', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'NN', 'CC', 'DT', 'CD', ':', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",19
question-answering,6,212,"The loss is minimized by stochastic gradient descent for maximally 500 epochs , but training is early stopped if the loss on the development data does not decrease for 50 epochs .","['The', 'loss', 'is', 'minimized', 'by', 'stochastic', 'gradient', 'descent', 'for', 'maximally', '500', 'epochs', ',', 'but', 'training', 'is', 'early', 'stopped', 'if', 'the', 'loss', 'on', 'the', 'development', 'data', 'does', 'not', 'decrease', 'for', '50', 'epochs', '.']","['O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', 'NN', 'NN', 'IN', 'RB', 'CD', 'NNS', ',', 'CC', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'RB', 'VB', 'IN', 'CD', 'NNS', '.']",32
question-answering,6,213,The learning rate is controlled by AdaGrad with the initial learning rate of 0.5 ( 0.1 for QA 10 k ) .,"['The', 'learning', 'rate', 'is', 'controlled', 'by', 'AdaGrad', 'with', 'the', 'initial', 'learning', 'rate', 'of', '0.5', '(', '0.1', 'for', 'QA', '10', 'k', ')', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'NNP', 'IN', 'DT', 'JJ', 'VBG', 'NN', 'IN', 'CD', '(', 'CD', 'IN', 'NNP', 'CD', 'NN', ')', '.']",22
question-answering,6,218,Story - based QA .,"['Story', '-', 'based', 'QA', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'VBN', 'NNP', '.']",5
question-answering,6,220,"In 1 k data , QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ) outperforms all other models by a large margin ( 2.8 + % ) .","['In', '1', 'k', 'data', ',', 'QRN', ""'s"", ""'"", '2', ""r'"", '(', '2', 'layers', '+', 'reset', 'gate', '+', 'd', '=', '50', ')', 'outperforms', 'all', 'other', 'models', 'by', 'a', 'large', 'margin', '(', '2.8', '+', '%', ')', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'CD', 'NN', 'NNS', ',', 'NNP', 'POS', 'POS', 'CD', 'NN', '(', 'CD', 'NNS', 'JJ', 'JJ', 'NN', 'NNP', 'NN', 'NNP', 'CD', ')', 'NNS', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', '(', 'CD', 'NNP', 'NN', ')', '.']",35
question-answering,6,221,"In 10 k dataset , the average accuracy of QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model outperforms all previous models by a large margin ( 2.5 + % ) , achieving a nearly perfect score of 99.7 % .","['In', '10', 'k', 'dataset', ',', 'the', 'average', 'accuracy', 'of', 'QRN', ""'s"", ""'"", '6r200', ""'"", '(', '6', 'layers', '+', 'reset', 'gate', '+', 'd', '=', '200', ')', 'model', 'outperforms', 'all', 'previous', 'models', 'by', 'a', 'large', 'margin', '(', '2.5', '+', '%', ')', ',', 'achieving', 'a', 'nearly', 'perfect', 'score', 'of', '99.7', '%', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'CD', 'NN', 'NN', ',', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'POS', 'POS', 'CD', 'POS', '(', 'CD', 'NNS', 'JJ', 'JJ', 'NN', 'NNP', 'NN', 'NNP', 'CD', ')', 'NN', 'VBZ', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', '(', 'CD', 'NNP', 'NN', ')', ',', 'VBG', 'DT', 'RB', 'JJ', 'NN', 'IN', 'CD', 'NN', '.']",49
question-answering,6,222,Dialog . Table 1 ( bottom ) reports the summary of the results of our model ( QRN ) and previous work on bAbI dialog and Task 6 dialog ( task - wise results are shown in in Appendix ) .,"['Dialog', '.', 'Table', '1', '(', 'bottom', ')', 'reports', 'the', 'summary', 'of', 'the', 'results', 'of', 'our', 'model', '(', 'QRN', ')', 'and', 'previous', 'work', 'on', 'bAbI', 'dialog', 'and', 'Task', '6', 'dialog', '(', 'task', '-', 'wise', 'results', 'are', 'shown', 'in', 'in', 'Appendix', ')', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', '.', 'JJ', 'CD', '(', 'NN', ')', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NNS', 'IN', 'PRP$', 'NN', '(', 'NNP', ')', 'CC', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'CC', 'NNP', 'CD', 'NN', '(', 'JJ', ':', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'IN', 'NNP', ')', '.']",41
question-answering,6,225,QRN outperforms previous work by a large margin ( 2.0 + % ) in every comparison .,"['QRN', 'outperforms', 'previous', 'work', 'by', 'a', 'large', 'margin', '(', '2.0', '+', '%', ')', 'in', 'every', 'comparison', '.']","['O', 'B-p', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NN', 'NNS', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', '(', 'CD', 'NNP', 'NN', ')', 'IN', 'DT', 'NN', '.']",17
question-answering,6,229,"According to the ablation results , we infer that : ( a ) When the number of layers is only one , the model lacks reasoning capability .","['According', 'to', 'the', 'ablation', 'results', ',', 'we', 'infer', 'that', ':', '(', 'a', ')', 'When', 'the', 'number', 'of', 'layers', 'is', 'only', 'one', ',', 'the', 'model', 'lacks', 'reasoning', 'capability', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['VBG', 'TO', 'DT', 'NN', 'NNS', ',', 'PRP', 'VBP', 'IN', ':', '(', 'DT', ')', 'WRB', 'DT', 'NN', 'IN', 'NNS', 'VBZ', 'RB', 'CD', ',', 'DT', 'NN', 'VBZ', 'VBG', 'NN', '.']",28
question-answering,6,232,( b ) Adding the reset gate helps .,"['(', 'b', ')', 'Adding', 'the', 'reset', 'gate', 'helps', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O']","['(', 'NN', ')', 'VBG', 'DT', 'NN', 'NN', 'VBZ', '.']",9
question-answering,6,233,"( c ) Including vector gates hurts in 1 k datasets , as the model either overfits to the training data or converges to local minima .","['(', 'c', ')', 'Including', 'vector', 'gates', 'hurts', 'in', '1', 'k', 'datasets', ',', 'as', 'the', 'model', 'either', 'overfits', 'to', 'the', 'training', 'data', 'or', 'converges', 'to', 'local', 'minima', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'NN', ')', 'VBG', 'NN', 'NNS', 'VBZ', 'IN', 'CD', 'NN', 'NNS', ',', 'IN', 'DT', 'NN', 'CC', 'NNS', 'TO', 'DT', 'NN', 'NNS', 'CC', 'NNS', 'TO', 'JJ', 'NN', '.']",27
question-answering,6,234,"On the other hand , vector gates in bAbI story - based QA 10 k dataset sometimes help .","['On', 'the', 'other', 'hand', ',', 'vector', 'gates', 'in', 'bAbI', 'story', '-', 'based', 'QA', '10', 'k', 'dataset', 'sometimes', 'help', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'NN', 'NNS', 'IN', 'JJ', 'NN', ':', 'VBN', 'NNP', 'CD', 'NN', 'NN', 'RB', 'NN', '.']",19
question-answering,2,2,Large - scale Simple Question Answering with Memory Networks,"['Large', '-', 'scale', 'Simple', 'Question', 'Answering', 'with', 'Memory', 'Networks']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['JJ', ':', 'NN', 'JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",9
question-answering,2,4,Training large - scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions .,"['Training', 'large', '-', 'scale', 'question', 'answering', 'systems', 'is', 'complicated', 'because', 'training', 'sources', 'usually', 'cover', 'a', 'small', 'portion', 'of', 'the', 'range', 'of', 'possible', 'questions', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', ':', 'NN', 'NN', 'VBG', 'NNS', 'VBZ', 'VBN', 'IN', 'NN', 'NNS', 'RB', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', '.']",24
question-answering,2,5,"This paper studies the impact of multitask and transfer learning for simple question answering ; a setting for which the reasoning required to answer is quite easy , as long as one can retrieve the correct evidence given a question , which can be difficult in large - scale conditions .","['This', 'paper', 'studies', 'the', 'impact', 'of', 'multitask', 'and', 'transfer', 'learning', 'for', 'simple', 'question', 'answering', ';', 'a', 'setting', 'for', 'which', 'the', 'reasoning', 'required', 'to', 'answer', 'is', 'quite', 'easy', ',', 'as', 'long', 'as', 'one', 'can', 'retrieve', 'the', 'correct', 'evidence', 'given', 'a', 'question', ',', 'which', 'can', 'be', 'difficult', 'in', 'large', '-', 'scale', 'conditions', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NNS', 'DT', 'NN', 'IN', 'NN', 'CC', 'VB', 'VBG', 'IN', 'JJ', 'NN', 'VBG', ':', 'DT', 'NN', 'IN', 'WDT', 'DT', 'NN', 'VBN', 'TO', 'VB', 'VBZ', 'RB', 'JJ', ',', 'RB', 'RB', 'IN', 'CD', 'MD', 'VB', 'DT', 'JJ', 'NN', 'VBN', 'DT', 'NN', ',', 'WDT', 'MD', 'VB', 'JJ', 'IN', 'JJ', ':', 'NN', 'NNS', '.']",51
question-answering,2,19,"First , as an effort to study the coverage of existing systems and the possibility to train jointly on different data sources via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .","['First', ',', 'as', 'an', 'effort', 'to', 'study', 'the', 'coverage', 'of', 'existing', 'systems', 'and', 'the', 'possibility', 'to', 'train', 'jointly', 'on', 'different', 'data', 'sources', 'via', 'multitasking', ',', 'we', 'collected', 'the', 'first', 'large', '-', 'scale', 'dataset', 'of', 'questions', 'and', 'answers', 'based', 'on', 'a', 'KB', ',', 'called', 'SimpleQuestions', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O']","['RB', ',', 'IN', 'DT', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'VBG', 'NNS', 'CC', 'DT', 'NN', 'TO', 'VB', 'RB', 'IN', 'JJ', 'NNS', 'NNS', 'IN', 'NN', ',', 'PRP', 'VBD', 'DT', 'JJ', 'JJ', ':', 'NN', 'NN', 'IN', 'NNS', 'CC', 'NNS', 'VBN', 'IN', 'DT', 'NNP', ',', 'VBD', 'NNS', '.']",45
question-answering,2,24,"Second , in sections 3 and 4 , we present an embedding - based QA system developed under the framework of Memory Networks ( Mem NNs ) .","['Second', ',', 'in', 'sections', '3', 'and', '4', ',', 'we', 'present', 'an', 'embedding', '-', 'based', 'QA', 'system', 'developed', 'under', 'the', 'framework', 'of', 'Memory', 'Networks', '(', 'Mem', 'NNs', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', ',', 'IN', 'NNS', 'CD', 'CC', 'CD', ',', 'PRP', 'VBP', 'DT', 'VBG', ':', 'VBN', 'NNP', 'NN', 'VBD', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NNP', '(', 'NNP', 'NNP', ')', '.']",28
question-answering,2,26,The setting of the simple QA corresponds to the elementary operation of performing a single lookup in the memory .,"['The', 'setting', 'of', 'the', 'simple', 'QA', 'corresponds', 'to', 'the', 'elementary', 'operation', 'of', 'performing', 'a', 'single', 'lookup', 'in', 'the', 'memory', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'NNS', 'TO', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",20
question-answering,2,229,"The embedding dimension and the learning rate were chosen among { 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 } respectively , and the margin ?","['The', 'embedding', 'dimension', 'and', 'the', 'learning', 'rate', 'were', 'chosen', 'among', '{', '64', ',', '128', ',', '256', '}', 'and', '{', '1', ',', '0.1', ',', '...', ',', '1.0e', '?', '4', '}', 'respectively', ',', 'and', 'the', 'margin', '?']","['O', 'B-p', 'I-p', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O']","['DT', 'VBG', 'NN', 'CC', 'DT', 'NN', 'NN', 'VBD', 'VBN', 'IN', '(', 'CD', ',', 'CD', ',', 'CD', ')', 'CC', '(', 'CD', ',', 'CD', ',', ':', ',', 'CD', '.', 'CD', ')', 'RB', ',', 'CC', 'DT', 'NN', '.']",35
question-answering,2,230,was set to 0.1 .,"['was', 'set', 'to', '0.1', '.']","['O', 'O', 'O', 'B-n', 'O']","['VBD', 'VBN', 'TO', 'CD', '.']",5
question-answering,2,242,"On the main benchmark WebQuestions , our best results use all data sources , the bigger extract from Freebase and the CANDS AS NEGS setting .","['On', 'the', 'main', 'benchmark', 'WebQuestions', ',', 'our', 'best', 'results', 'use', 'all', 'data', 'sources', ',', 'the', 'bigger', 'extract', 'from', 'Freebase', 'and', 'the', 'CANDS', 'AS', 'NEGS', 'setting', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', 'NNP', ',', 'PRP$', 'JJS', 'NNS', 'VBP', 'DT', 'NNS', 'NNS', ',', 'DT', 'JJR', 'NN', 'IN', 'NNP', 'CC', 'DT', 'NNP', 'NNP', 'NNP', 'NN', '.']",26
question-answering,2,256,Transfer learning on Reverb,"['Transfer', 'learning', 'on', 'Reverb']","['B-n', 'I-n', 'I-n', 'I-n']","['NN', 'VBG', 'IN', 'NNP']",4
question-answering,2,257,"In this set of experiments , all Reverb facts are added to the memory , without any retraining , and we test our ability to rerank answers on the companion QA set .","['In', 'this', 'set', 'of', 'experiments', ',', 'all', 'Reverb', 'facts', 'are', 'added', 'to', 'the', 'memory', ',', 'without', 'any', 'retraining', ',', 'and', 'we', 'test', 'our', 'ability', 'to', 'rerank', 'answers', 'on', 'the', 'companion', 'QA', 'set', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'IN', 'NNS', ',', 'DT', 'NNP', 'NNS', 'VBP', 'VBN', 'TO', 'DT', 'NN', ',', 'IN', 'DT', 'NN', ',', 'CC', 'PRP', 'VBP', 'PRP$', 'NN', 'TO', 'VB', 'NNS', 'IN', 'DT', 'NN', 'NNP', 'NN', '.']",33
question-answering,2,259,"Our best results are 67 % accuracy ( and 68 % for the ensemble of 5 models ) , which are better than the 54 % of the original paper and close to the stateof - the - art 73 % of .","['Our', 'best', 'results', 'are', '67', '%', 'accuracy', '(', 'and', '68', '%', 'for', 'the', 'ensemble', 'of', '5', 'models', ')', ',', 'which', 'are', 'better', 'than', 'the', '54', '%', 'of', 'the', 'original', 'paper', 'and', 'close', 'to', 'the', 'stateof', '-', 'the', '-', 'art', '73', '%', 'of', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'JJS', 'NNS', 'VBP', 'CD', 'NN', 'NN', '(', 'CC', 'CD', 'NN', 'IN', 'DT', 'NN', 'IN', 'CD', 'NNS', ')', ',', 'WDT', 'VBP', 'JJR', 'IN', 'DT', 'CD', 'NN', 'IN', 'DT', 'JJ', 'NN', 'CC', 'RB', 'TO', 'DT', 'NN', ':', 'DT', ':', 'NN', 'CD', 'NN', 'IN', '.']",43
question-answering,2,262,"We first notice that models trained on a single QA dataset perform poorly on the other datasets ( e.g. 46.6 % accuracy on SimpleQuestions for the model trained on WebQuestions only ) , which shows that the performance on We-bQuestions does not necessarily guarantee high coverage for simple QA .","['We', 'first', 'notice', 'that', 'models', 'trained', 'on', 'a', 'single', 'QA', 'dataset', 'perform', 'poorly', 'on', 'the', 'other', 'datasets', '(', 'e.g.', '46.6', '%', 'accuracy', 'on', 'SimpleQuestions', 'for', 'the', 'model', 'trained', 'on', 'WebQuestions', 'only', ')', ',', 'which', 'shows', 'that', 'the', 'performance', 'on', 'We-bQuestions', 'does', 'not', 'necessarily', 'guarantee', 'high', 'coverage', 'for', 'simple', 'QA', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'JJ', 'NNP', 'NN', 'NN', 'RB', 'IN', 'DT', 'JJ', 'NNS', '(', 'VB', 'CD', 'NN', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'VBD', 'IN', 'NNP', 'RB', ')', ',', 'WDT', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'NNP', 'VBZ', 'RB', 'RB', 'VB', 'JJ', 'NN', 'IN', 'JJ', 'NNP', '.']",50
question-answering,2,263,"On the other hand , training on both datasets only improves performance ; in particular , the model is able to capture all question patterns of the two datasets ; there is no "" negative interaction "" .","['On', 'the', 'other', 'hand', ',', 'training', 'on', 'both', 'datasets', 'only', 'improves', 'performance', ';', 'in', 'particular', ',', 'the', 'model', 'is', 'able', 'to', 'capture', 'all', 'question', 'patterns', 'of', 'the', 'two', 'datasets', ';', 'there', 'is', 'no', '""', 'negative', 'interaction', '""', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'NN', 'IN', 'DT', 'NNS', 'RB', 'VBZ', 'NN', ':', 'IN', 'JJ', ',', 'DT', 'NN', 'VBZ', 'JJ', 'TO', 'VB', 'DT', 'NN', 'NNS', 'IN', 'DT', 'CD', 'NNS', ':', 'EX', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'NN', '.']",38
question-answering,3,2,Sentence Similarity Learning by Lexical Decomposition and Composition,"['Sentence', 'Similarity', 'Learning', 'by', 'Lexical', 'Decomposition', 'and', 'Composition']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'CC', 'NNP']",8
question-answering,3,4,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences .","['Most', 'conventional', 'sentence', 'similarity', 'methods', 'only', 'focus', 'on', 'similar', 'parts', 'of', 'two', 'input', 'sentences', ',', 'and', 'simply', 'ignore', 'the', 'dissimilar', 'parts', ',', 'which', 'usually', 'give', 'us', 'some', 'clues', 'and', 'semantic', 'meanings', 'about', 'the', 'sentences', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJS', 'JJ', 'NN', 'NN', 'NNS', 'RB', 'VB', 'IN', 'JJ', 'NNS', 'IN', 'CD', 'NN', 'NNS', ',', 'CC', 'RB', 'VB', 'DT', 'NN', 'NNS', ',', 'WDT', 'RB', 'VBP', 'PRP', 'DT', 'NNS', 'CC', 'JJ', 'NNS', 'IN', 'DT', 'NNS', '.']",35
question-answering,3,45,"In this paper , we propose a novel model to tackle all these challenges jointly by decomposing and composing lexical semantics over sentences .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', 'model', 'to', 'tackle', 'all', 'these', 'challenges', 'jointly', 'by', 'decomposing', 'and', 'composing', 'lexical', 'semantics', 'over', 'sentences', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'TO', 'VB', 'PDT', 'DT', 'NNS', 'RB', 'IN', 'VBG', 'CC', 'VBG', 'JJ', 'NNS', 'IN', 'NNS', '.']",24
question-answering,3,46,"Given a sentence pair , the model represents each word as a low -dimensional vector ( challenge 1 ) , and calculates a semantic matching vector for each word based on all words in the other sentence ( challenge 2 ) .","['Given', 'a', 'sentence', 'pair', ',', 'the', 'model', 'represents', 'each', 'word', 'as', 'a', 'low', '-dimensional', 'vector', '(', 'challenge', '1', ')', ',', 'and', 'calculates', 'a', 'semantic', 'matching', 'vector', 'for', 'each', 'word', 'based', 'on', 'all', 'words', 'in', 'the', 'other', 'sentence', '(', 'challenge', '2', ')', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['VBN', 'DT', 'NN', 'NN', ',', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NN', '(', 'VB', 'CD', ')', ',', 'CC', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', '(', 'VB', 'CD', ')', '.']",42
question-answering,3,47,"Then based on the semantic matching vector , each word vector is decomposed into two components : a similar component and a dissimilar component ( challenge 3 ) .","['Then', 'based', 'on', 'the', 'semantic', 'matching', 'vector', ',', 'each', 'word', 'vector', 'is', 'decomposed', 'into', 'two', 'components', ':', 'a', 'similar', 'component', 'and', 'a', 'dissimilar', 'component', '(', 'challenge', '3', ')', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['RB', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', 'NNS', ':', 'DT', 'JJ', 'NN', 'CC', 'DT', 'JJ', 'NN', '(', 'VB', 'CD', ')', '.']",29
question-answering,3,48,"We use similar components of all the words to represent the similar parts of the sentence pair , and dissimilar components of every word to model the dissimilar parts explicitly .","['We', 'use', 'similar', 'components', 'of', 'all', 'the', 'words', 'to', 'represent', 'the', 'similar', 'parts', 'of', 'the', 'sentence', 'pair', ',', 'and', 'dissimilar', 'components', 'of', 'every', 'word', 'to', 'model', 'the', 'dissimilar', 'parts', 'explicitly', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NNS', 'IN', 'PDT', 'DT', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', ',', 'CC', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'RB', '.']",31
question-answering,3,49,"After this , a two - channel CNN operation is performed to compose the similar and dissimilar components into a feature vector ( challenge 2 and 3 ) .","['After', 'this', ',', 'a', 'two', '-', 'channel', 'CNN', 'operation', 'is', 'performed', 'to', 'compose', 'the', 'similar', 'and', 'dissimilar', 'components', 'into', 'a', 'feature', 'vector', '(', 'challenge', '2', 'and', '3', ')', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', ',', 'DT', 'CD', ':', 'NN', 'NNP', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'CC', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', '(', 'VB', 'CD', 'CC', 'CD', ')', '.']",29
question-answering,3,50,"Finally , the composed feature vector is utilized to predict the sentence similarity .","['Finally', ',', 'the', 'composed', 'feature', 'vector', 'is', 'utilized', 'to', 'predict', 'the', 'sentence', 'similarity', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NN', 'NN', '.']",14
question-answering,3,177,"We switched the semantic matching functions among { max , global , local - l} , where l ? { 1 , 2 , 3 , 4 } , and fixed the other options as : the linear decomposition , the filter types including {unigram , bigram , trigram } , and 500 filters for each type .","['We', 'switched', 'the', 'semantic', 'matching', 'functions', 'among', '{', 'max', ',', 'global', ',', 'local', '-', 'l}', ',', 'where', 'l', '?', '{', '1', ',', '2', ',', '3', ',', '4', '}', ',', 'and', 'fixed', 'the', 'other', 'options', 'as', ':', 'the', 'linear', 'decomposition', ',', 'the', 'filter', 'types', 'including', '{unigram', ',', 'bigram', ',', 'trigram', '}', ',', 'and', '500', 'filters', 'for', 'each', 'type', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'DT', 'JJ', 'JJ', 'NNS', 'IN', '(', 'NN', ',', 'JJ', ',', 'JJ', ':', 'NN', ',', 'WRB', 'NN', '.', '(', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ')', ',', 'CC', 'VBD', 'DT', 'JJ', 'NNS', 'IN', ':', 'DT', 'JJ', 'NN', ',', 'DT', 'NN', 'NNS', 'VBG', 'NN', ',', 'NN', ',', 'NN', ')', ',', 'CC', 'CD', 'NNS', 'IN', 'DT', 'NN', '.']",58
question-answering,3,179,We found that the max function worked better than the global function on both MAP and MRR .,"['We', 'found', 'that', 'the', 'max', 'function', 'worked', 'better', 'than', 'the', 'global', 'function', 'on', 'both', 'MAP', 'and', 'MRR', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'IN', 'DT', 'NN', 'NN', 'VBD', 'JJR', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'CC', 'NNP', '.']",18
question-answering,3,185,"We varied the decomposition operation among { rigid , linear , orthogonal } , and kept the other options unchanged .","['We', 'varied', 'the', 'decomposition', 'operation', 'among', '{', 'rigid', ',', 'linear', ',', 'orthogonal', '}', ',', 'and', 'kept', 'the', 'other', 'options', 'unchanged', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'DT', 'NN', 'NN', 'IN', '(', 'JJ', ',', 'JJ', ',', 'JJ', ')', ',', 'CC', 'VBD', 'DT', 'JJ', 'NNS', 'JJ', '.']",21
question-answering,3,191,"Third , we tested the influence of various filter types .","['Third', ',', 'we', 'tested', 'the', 'influence', 'of', 'various', 'filter', 'types', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ',', 'PRP', 'VBD', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NNS', '.']",11
question-answering,3,192,"We constructed 5 groups of filters : win - 1 contains only the unigram filters , win - 2 contains both unigram and bigram filters , win - 3 contains all the filters in win - 2 plus trigram filters , win - 4 extends filters in win - 3 with 4 - gram filters , and win - 5 adds 5 - gram filters into win - 4 .","['We', 'constructed', '5', 'groups', 'of', 'filters', ':', 'win', '-', '1', 'contains', 'only', 'the', 'unigram', 'filters', ',', 'win', '-', '2', 'contains', 'both', 'unigram', 'and', 'bigram', 'filters', ',', 'win', '-', '3', 'contains', 'all', 'the', 'filters', 'in', 'win', '-', '2', 'plus', 'trigram', 'filters', ',', 'win', '-', '4', 'extends', 'filters', 'in', 'win', '-', '3', 'with', '4', '-', 'gram', 'filters', ',', 'and', 'win', '-', '5', 'adds', '5', '-', 'gram', 'filters', 'into', 'win', '-', '4', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'CD', 'NNS', 'IN', 'NNS', ':', 'VB', ':', 'CD', 'NNS', 'RB', 'DT', 'JJ', 'NNS', ',', 'VBP', ':', 'CD', 'NNS', 'DT', 'JJ', 'CC', 'JJ', 'NNS', ',', 'VBP', ':', 'CD', 'NNS', 'PDT', 'DT', 'NNS', 'IN', 'JJ', ':', 'CD', 'CC', 'JJ', 'NNS', ',', 'VBP', ':', 'CD', 'VBZ', 'NNS', 'IN', 'JJ', ':', 'CD', 'IN', 'CD', ':', 'NN', 'NNS', ',', 'CC', 'VB', ':', 'CD', 'VBZ', 'CD', ':', 'NN', 'NNS', 'IN', 'JJ', ':', 'CD', '.']",70
question-answering,3,202,QASent dataset .,"['QASent', 'dataset', '.']","['B-n', 'I-n', 'O']","['JJ', 'NN', '.']",3
question-answering,3,214,"We can see that our model ( the last row of ) got the best MAP among all previous work , and a comparable MRR than dos .","['We', 'can', 'see', 'that', 'our', 'model', '(', 'the', 'last', 'row', 'of', ')', 'got', 'the', 'best', 'MAP', 'among', 'all', 'previous', 'work', ',', 'and', 'a', 'comparable', 'MRR', 'than', 'dos', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'MD', 'VB', 'IN', 'PRP$', 'NN', '(', 'DT', 'JJ', 'NN', 'IN', ')', 'VBD', 'DT', 'JJS', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'CC', 'DT', 'JJ', 'NNP', 'IN', 'NN', '.']",28
question-answering,3,215,Wiki QA dataset .,"['Wiki', 'QA', 'dataset', '.']","['B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NN', '.']",4
question-answering,3,224,The last row of shows that our model is more effective than the other models .,"['The', 'last', 'row', 'of', 'shows', 'that', 'our', 'model', 'is', 'more', 'effective', 'than', 'the', 'other', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'PRP$', 'NN', 'VBZ', 'RBR', 'JJ', 'IN', 'DT', 'JJ', 'NNS', '.']",16
question-answering,3,225,MSRP dataset .,"['MSRP', 'dataset', '.']","['B-n', 'I-n', 'O']","['NNP', 'NN', '.']",3
question-answering,3,237,"Comparing to these neural network based models , our model obtained a comparable performance ( the last row of ) without using any sparse features , extra annotated resources and specific training strategies .","['Comparing', 'to', 'these', 'neural', 'network', 'based', 'models', ',', 'our', 'model', 'obtained', 'a', 'comparable', 'performance', '(', 'the', 'last', 'row', 'of', ')', 'without', 'using', 'any', 'sparse', 'features', ',', 'extra', 'annotated', 'resources', 'and', 'specific', 'training', 'strategies', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['VBG', 'TO', 'DT', 'JJ', 'NN', 'VBN', 'NNS', ',', 'PRP$', 'NN', 'VBD', 'DT', 'JJ', 'NN', '(', 'DT', 'JJ', 'NN', 'IN', ')', 'IN', 'VBG', 'DT', 'NN', 'NNS', ',', 'JJ', 'JJ', 'NNS', 'CC', 'JJ', 'NN', 'NNS', '.']",34
question-answering,0,2,Open Question Answering with Weakly Supervised Embedding Models,"['Open', 'Question', 'Answering', 'with', 'Weakly', 'Supervised', 'Embedding', 'Models']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', 'VBD', 'NNP', 'NNP']",8
question-answering,0,4,Building computers able to answer questions on any subject is along standing goal of artificial intelligence .,"['Building', 'computers', 'able', 'to', 'answer', 'questions', 'on', 'any', 'subject', 'is', 'along', 'standing', 'goal', 'of', 'artificial', 'intelligence', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NNS', 'JJ', 'TO', 'VB', 'NNS', 'IN', 'DT', 'NN', 'VBZ', 'IN', 'VBG', 'NN', 'IN', 'JJ', 'NN', '.']",17
question-answering,0,12,"This paper addresses the challenging problem of open - domain question answering , which consists of building systems able to answer questions from any domain .","['This', 'paper', 'addresses', 'the', 'challenging', 'problem', 'of', 'open', '-', 'domain', 'question', 'answering', ',', 'which', 'consists', 'of', 'building', 'systems', 'able', 'to', 'answer', 'questions', 'from', 'any', 'domain', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'DT', 'VBG', 'NN', 'IN', 'JJ', ':', 'NN', 'NN', 'NN', ',', 'WDT', 'VBZ', 'IN', 'VBG', 'NNS', 'JJ', 'TO', 'VB', 'NNS', 'IN', 'DT', 'NN', '.']",26
question-answering,0,23,"In this paper , we instead take the approach of converting questions to ( uninterpretable ) vectorial representations which require no pre-defined grammars or lexicons and can query any KB independent of its schema .","['In', 'this', 'paper', ',', 'we', 'instead', 'take', 'the', 'approach', 'of', 'converting', 'questions', 'to', '(', 'uninterpretable', ')', 'vectorial', 'representations', 'which', 'require', 'no', 'pre-defined', 'grammars', 'or', 'lexicons', 'and', 'can', 'query', 'any', 'KB', 'independent', 'of', 'its', 'schema', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'RB', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'NNS', 'TO', '(', 'JJ', ')', 'NN', 'NNS', 'WDT', 'VBP', 'DT', 'JJ', 'NNS', 'CC', 'NNS', 'CC', 'MD', 'VB', 'DT', 'NNP', 'JJ', 'IN', 'PRP$', 'NN', '.']",35
question-answering,0,31,Our approach is based on learning low - dimensional vector embeddings of words and of KB triples so that representations of questions and corresponding answers end up being similar in the embedding space .,"['Our', 'approach', 'is', 'based', 'on', 'learning', 'low', '-', 'dimensional', 'vector', 'embeddings', 'of', 'words', 'and', 'of', 'KB', 'triples', 'so', 'that', 'representations', 'of', 'questions', 'and', 'corresponding', 'answers', 'end', 'up', 'being', 'similar', 'in', 'the', 'embedding', 'space', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'VBN', 'IN', 'VBG', 'JJ', ':', 'JJ', 'NN', 'NNS', 'IN', 'NNS', 'CC', 'IN', 'NNP', 'NNS', 'RB', 'IN', 'NNS', 'IN', 'NNS', 'CC', 'VBG', 'NNS', 'VBP', 'RP', 'VBG', 'JJ', 'IN', 'DT', 'JJ', 'NN', '.']",34
question-answering,0,33,"In order to avoid transferring the cost of manual intervention to the one of labeling large amounts of data , we make use of weak supervision .","['In', 'order', 'to', 'avoid', 'transferring', 'the', 'cost', 'of', 'manual', 'intervention', 'to', 'the', 'one', 'of', 'labeling', 'large', 'amounts', 'of', 'data', ',', 'we', 'make', 'use', 'of', 'weak', 'supervision', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'VB', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'NN', 'TO', 'DT', 'CD', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'NNS', ',', 'PRP', 'VBP', 'NN', 'IN', 'JJ', 'NN', '.']",27
question-answering,0,35,We end up learning meaningful vectorial representations for questions involving up to 800 k words and for triples of an mostly automatically created KB with 2.4 M entities and 600 k relationships .,"['We', 'end', 'up', 'learning', 'meaningful', 'vectorial', 'representations', 'for', 'questions', 'involving', 'up', 'to', '800', 'k', 'words', 'and', 'for', 'triples', 'of', 'an', 'mostly', 'automatically', 'created', 'KB', 'with', '2.4', 'M', 'entities', 'and', '600', 'k', 'relationships', '.']","['O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'RP', 'VBG', 'JJ', 'JJ', 'NNS', 'IN', 'NNS', 'VBG', 'RB', 'TO', 'CD', 'NN', 'NNS', 'CC', 'IN', 'NNS', 'IN', 'DT', 'RB', 'RB', 'VBN', 'NNP', 'IN', 'CD', 'NNP', 'NNS', 'CC', 'CD', 'NNS', 'NNS', '.']",33
question-answering,0,38,"Thus , we propose a method to fine - tune embedding - based models by carefully optimizing a matrix parameterizing the similarity used in the embedding space , leading to a consistent improvement in performance .","['Thus', ',', 'we', 'propose', 'a', 'method', 'to', 'fine', '-', 'tune', 'embedding', '-', 'based', 'models', 'by', 'carefully', 'optimizing', 'a', 'matrix', 'parameterizing', 'the', 'similarity', 'used', 'in', 'the', 'embedding', 'space', ',', 'leading', 'to', 'a', 'consistent', 'improvement', 'in', 'performance', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'TO', 'VB', ':', 'NN', 'VBG', ':', 'VBN', 'NNS', 'IN', 'RB', 'VBG', 'DT', 'NN', 'VBG', 'DT', 'NN', 'VBN', 'IN', 'DT', 'VBG', 'NN', ',', 'VBG', 'TO', 'DT', 'JJ', 'NN', 'IN', 'NN', '.']",36
question-answering,0,188,Reranking,['Reranking'],['B-n'],['VBG'],1
question-answering,0,201,"First , we can see that multitasking with paraphrase data is essential since it improves F1 from 0.60 to 0.68 .","['First', ',', 'we', 'can', 'see', 'that', 'multitasking', 'with', 'paraphrase', 'data', 'is', 'essential', 'since', 'it', 'improves', 'F1', 'from', '0.60', 'to', '0.68', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'MD', 'VB', 'IN', 'VBG', 'IN', 'NN', 'NNS', 'VBZ', 'JJ', 'IN', 'PRP', 'VBZ', 'NNP', 'IN', 'CD', 'TO', 'CD', '.']",21
question-answering,0,207,Fine - tuning the embedding model is very beneficial to optimize the top of the list and grants a bump of 5 points of F1 : carefully tuning the similarity makes a clear difference .,"['Fine', '-', 'tuning', 'the', 'embedding', 'model', 'is', 'very', 'beneficial', 'to', 'optimize', 'the', 'top', 'of', 'the', 'list', 'and', 'grants', 'a', 'bump', 'of', '5', 'points', 'of', 'F1', ':', 'carefully', 'tuning', 'the', 'similarity', 'makes', 'a', 'clear', 'difference', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'VBG', 'DT', 'VBG', 'NN', 'VBZ', 'RB', 'JJ', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'CC', 'NNS', 'DT', 'NN', 'IN', 'CD', 'NNS', 'IN', 'NNP', ':', 'RB', 'VBG', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', '.']",35
question-answering,0,208,"All versions of our system greatly outperform paralex : the fine - tuned model improves the F1 - score by almost 20 points and , according to , is better in precision for all levels of recall .","['All', 'versions', 'of', 'our', 'system', 'greatly', 'outperform', 'paralex', ':', 'the', 'fine', '-', 'tuned', 'model', 'improves', 'the', 'F1', '-', 'score', 'by', 'almost', '20', 'points', 'and', ',', 'according', 'to', ',', 'is', 'better', 'in', 'precision', 'for', 'all', 'levels', 'of', 'recall', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'PRP$', 'NN', 'RB', 'JJ', 'NN', ':', 'DT', 'JJ', ':', 'JJ', 'NN', 'VBZ', 'DT', 'NNP', ':', 'NN', 'IN', 'RB', 'CD', 'NNS', 'CC', ',', 'VBG', 'TO', ',', 'VBZ', 'RBR', 'IN', 'NN', 'IN', 'DT', 'NNS', 'IN', 'NN', '.']",38
relation-classification,8,2,Extracting Multiple - Relations in One - Pass with Pre-Trained Transformers,"['Extracting', 'Multiple', '-', 'Relations', 'in', 'One', '-', 'Pass', 'with', 'Pre-Trained', 'Transformers']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['VBG', 'NNP', ':', 'NNPS', 'IN', 'CD', ':', 'NN', 'IN', 'JJ', 'NNS']",11
relation-classification,8,4,The state - of - the - art solutions for extracting multiple entity - relations from an input paragraph always require a multiple - pass encoding on the input .,"['The', 'state', '-', 'of', '-', 'the', '-', 'art', 'solutions', 'for', 'extracting', 'multiple', 'entity', '-', 'relations', 'from', 'an', 'input', 'paragraph', 'always', 'require', 'a', 'multiple', '-', 'pass', 'encoding', 'on', 'the', 'input', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'VBG', 'JJ', 'NN', ':', 'NNS', 'IN', 'DT', 'NN', 'NN', 'RB', 'VB', 'DT', 'JJ', ':', 'NN', 'VBG', 'IN', 'DT', 'NN', '.']",30
relation-classification,8,5,"This paper proposes a new solution that can complete the multiple entityrelations extraction task with only one - pass encoding on the input corpus , and achieve a new state - of - the - art accuracy performance , as demonstrated in the ACE 2005 benchmark .","['This', 'paper', 'proposes', 'a', 'new', 'solution', 'that', 'can', 'complete', 'the', 'multiple', 'entityrelations', 'extraction', 'task', 'with', 'only', 'one', '-', 'pass', 'encoding', 'on', 'the', 'input', 'corpus', ',', 'and', 'achieve', 'a', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'accuracy', 'performance', ',', 'as', 'demonstrated', 'in', 'the', 'ACE', '2005', 'benchmark', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'WDT', 'MD', 'VB', 'DT', 'JJ', 'NNS', 'NN', 'NN', 'IN', 'RB', 'CD', ':', 'NN', 'VBG', 'IN', 'DT', 'NN', 'NN', ',', 'CC', 'VB', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'NN', ',', 'IN', 'VBN', 'IN', 'DT', 'NNP', 'CD', 'NN', '.']",47
relation-classification,8,10,Relation extraction ( RE ) aims to find the semantic relation between a pair of entity mentions from an input paragraph .,"['Relation', 'extraction', '(', 'RE', ')', 'aims', 'to', 'find', 'the', 'semantic', 'relation', 'between', 'a', 'pair', 'of', 'entity', 'mentions', 'from', 'an', 'input', 'paragraph', '.']","['B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', '(', 'NNP', ')', 'VBZ', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",22
relation-classification,8,12,One particular type of the RE task is multiplerelations extraction ( MRE ) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph .,"['One', 'particular', 'type', 'of', 'the', 'RE', 'task', 'is', 'multiplerelations', 'extraction', '(', 'MRE', ')', 'that', 'aims', 'to', 'recognize', 'relations', 'of', 'multiple', 'pairs', 'of', 'entity', 'mentions', 'from', 'an', 'input', 'paragraph', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', 'VBZ', 'NNS', 'NN', '(', 'NNP', ')', 'WDT', 'VBZ', 'TO', 'VB', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",29
relation-classification,8,29,This section describes the proposed one - pass encoding MRE solution .,"['This', 'section', 'describes', 'the', 'proposed', 'one', '-', 'pass', 'encoding', 'MRE', 'solution', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'VBN', 'CD', ':', 'NN', 'VBG', 'NNP', 'NN', '.']",12
relation-classification,8,30,"The solution is built upon BERT with a structured prediction layer to enable BERT to predict multiple relations with onepass encoding , and an entity - aware self - attention mechanism to infuse the relational information with regard to multiple entities at each layer of hidden states .","['The', 'solution', 'is', 'built', 'upon', 'BERT', 'with', 'a', 'structured', 'prediction', 'layer', 'to', 'enable', 'BERT', 'to', 'predict', 'multiple', 'relations', 'with', 'onepass', 'encoding', ',', 'and', 'an', 'entity', '-', 'aware', 'self', '-', 'attention', 'mechanism', 'to', 'infuse', 'the', 'relational', 'information', 'with', 'regard', 'to', 'multiple', 'entities', 'at', 'each', 'layer', 'of', 'hidden', 'states', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'IN', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'NNP', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'NN', 'NN', ',', 'CC', 'DT', 'NN', ':', 'JJ', 'SYM', ':', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'TO', 'VB', 'NNS', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', '.']",48
relation-classification,8,76,"We compare our solution with previous works that predict a single relation per pass , our model that predicts single relation per pass for MRE , and with the following naive modifications of BERT that could achieve MRE in one - pass .","['We', 'compare', 'our', 'solution', 'with', 'previous', 'works', 'that', 'predict', 'a', 'single', 'relation', 'per', 'pass', ',', 'our', 'model', 'that', 'predicts', 'single', 'relation', 'per', 'pass', 'for', 'MRE', ',', 'and', 'with', 'the', 'following', 'naive', 'modifications', 'of', 'BERT', 'that', 'could', 'achieve', 'MRE', 'in', 'one', '-', 'pass', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'IN', 'JJ', 'NNS', 'WDT', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NN', ',', 'PRP$', 'NN', 'IN', 'VBZ', 'JJ', 'NN', 'IN', 'NN', 'IN', 'NNP', ',', 'CC', 'IN', 'DT', 'VBG', 'JJ', 'NNS', 'IN', 'NNP', 'WDT', 'MD', 'VB', 'NNP', 'IN', 'CD', ':', 'NN', '.']",43
relation-classification,8,77,"BERT SP : BERT with structured prediction only , which includes proposed improvement in 3.1 .","['BERT', 'SP', ':', 'BERT', 'with', 'structured', 'prediction', 'only', ',', 'which', 'includes', 'proposed', 'improvement', 'in', '3.1', '.']","['B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', ':', 'NN', 'IN', 'JJ', 'NN', 'RB', ',', 'WDT', 'VBZ', 'VBN', 'NN', 'IN', 'CD', '.']",16
relation-classification,8,78,"Entity - Aware BERT SP : our full model , which includes both improvements in 3.1 and 3.2 .","['Entity', '-', 'Aware', 'BERT', 'SP', ':', 'our', 'full', 'model', ',', 'which', 'includes', 'both', 'improvements', 'in', '3.1', 'and', '3.2', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NNP', 'NNP', 'NNP', ':', 'PRP$', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NNS', 'IN', 'CD', 'CC', 'CD', '.']",19
relation-classification,8,79,BERT SP with position embedding on the final attention layer .,"['BERT', 'SP', 'with', 'position', 'embedding', 'on', 'the', 'final', 'attention', 'layer', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'IN', 'NN', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",11
relation-classification,8,81,"In this method , the BERT model encode the paragraph to the last attention - layer .","['In', 'this', 'method', ',', 'the', 'BERT', 'model', 'encode', 'the', 'paragraph', 'to', 'the', 'last', 'attention', '-', 'layer', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'DT', 'NNP', 'NN', 'VBD', 'DT', 'NN', 'TO', 'DT', 'JJ', 'NN', ':', 'NN', '.']",17
relation-classification,8,82,"Then , for each entity pair , it takes the hidden states , adds the relative position embeddings corresponding to the target entities , and finally makes the relation prediction for this pair .","['Then', ',', 'for', 'each', 'entity', 'pair', ',', 'it', 'takes', 'the', 'hidden', 'states', ',', 'adds', 'the', 'relative', 'position', 'embeddings', 'corresponding', 'to', 'the', 'target', 'entities', ',', 'and', 'finally', 'makes', 'the', 'relation', 'prediction', 'for', 'this', 'pair', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['RB', ',', 'IN', 'DT', 'NN', 'NN', ',', 'PRP', 'VBZ', 'DT', 'JJ', 'NNS', ',', 'VBZ', 'DT', 'JJ', 'NN', 'NNS', 'VBG', 'TO', 'DT', 'NN', 'NNS', ',', 'CC', 'RB', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', '.']",34
relation-classification,8,84,Results on ACE 2005,"['Results', 'on', 'ACE', '2005']","['O', 'B-p', 'B-n', 'I-n']","['NNS', 'IN', 'NNP', 'CD']",4
relation-classification,8,86,The first observation is that our model architecture achieves much better results compared to the previous state - of - the - art methods .,"['The', 'first', 'observation', 'is', 'that', 'our', 'model', 'architecture', 'achieves', 'much', 'better', 'results', 'compared', 'to', 'the', 'previous', 'state', '-', 'of', '-', 'the', '-', 'art', 'methods', '.']","['O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'IN', 'PRP$', 'NN', 'NN', 'VBZ', 'RB', 'JJR', 'NNS', 'VBN', 'TO', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', '.']",25
relation-classification,8,94,"Our full model , with the structured fine - tuning of attention layers , brings further improvement of about 5.5 % , in the MRE one - pass setting , and achieves a new state - of - the - art performance when compared to the methods with domain adaptation .","['Our', 'full', 'model', ',', 'with', 'the', 'structured', 'fine', '-', 'tuning', 'of', 'attention', 'layers', ',', 'brings', 'further', 'improvement', 'of', 'about', '5.5', '%', ',', 'in', 'the', 'MRE', 'one', '-', 'pass', 'setting', ',', 'and', 'achieves', 'a', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'when', 'compared', 'to', 'the', 'methods', 'with', 'domain', 'adaptation', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'JJ', 'NN', ',', 'IN', 'DT', 'JJ', 'JJ', ':', 'NN', 'IN', 'NN', 'NNS', ',', 'VBZ', 'JJ', 'NN', 'IN', 'RB', 'CD', 'NN', ',', 'IN', 'DT', 'NNP', 'CD', ':', 'NN', 'NN', ',', 'CC', 'VBZ', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'WRB', 'VBN', 'TO', 'DT', 'NNS', 'IN', 'NN', 'NN', '.']",51
relation-classification,8,116,The results on SemEval 2018 Task 7 are shown in .,"['The', 'results', 'on', 'SemEval', '2018', 'Task', '7', 'are', 'shown', 'in', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'JJ', 'CD', 'NN', 'CD', 'VBP', 'VBN', 'IN', '.']",11
relation-classification,8,117,"Our Entity - Aware BERT SP gives comparable results to the top - ranked system in the shared task , with slightly lower Macro - F1 , which is the official metric of the task , and slightly higher Micro - F1 .","['Our', 'Entity', '-', 'Aware', 'BERT', 'SP', 'gives', 'comparable', 'results', 'to', 'the', 'top', '-', 'ranked', 'system', 'in', 'the', 'shared', 'task', ',', 'with', 'slightly', 'lower', 'Macro', '-', 'F1', ',', 'which', 'is', 'the', 'official', 'metric', 'of', 'the', 'task', ',', 'and', 'slightly', 'higher', 'Micro', '-', 'F1', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'VBZ', 'JJ', 'NNS', 'TO', 'DT', 'JJ', ':', 'JJ', 'NN', 'IN', 'DT', 'VBN', 'NN', ',', 'IN', 'RB', 'JJR', 'NNP', ':', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'JJ', 'IN', 'DT', 'NN', ',', 'CC', 'RB', 'JJR', 'NNP', ':', 'NN', '.']",43
relation-classification,8,118,"When predicting multiple relations in one - pass , we have 0.9 % drop on Macro - F1 , but a further 0.8 % improvement on Micro - F1 .","['When', 'predicting', 'multiple', 'relations', 'in', 'one', '-', 'pass', ',', 'we', 'have', '0.9', '%', 'drop', 'on', 'Macro', '-', 'F1', ',', 'but', 'a', 'further', '0.8', '%', 'improvement', 'on', 'Micro', '-', 'F1', '.']","['B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['WRB', 'VBG', 'JJ', 'NNS', 'IN', 'CD', ':', 'NN', ',', 'PRP', 'VBP', 'CD', 'NN', 'NN', 'IN', 'NNP', ':', 'NN', ',', 'CC', 'DT', 'JJ', 'CD', 'NN', 'NN', 'IN', 'NNP', ':', 'NN', '.']",30
relation-classification,8,120,"On the other hand , compared to the top singlemodel result , which makes use of additional word and entity embeddings pretrained on in - domain data , our methods demonstrate clear advantage as a single model .","['On', 'the', 'other', 'hand', ',', 'compared', 'to', 'the', 'top', 'singlemodel', 'result', ',', 'which', 'makes', 'use', 'of', 'additional', 'word', 'and', 'entity', 'embeddings', 'pretrained', 'on', 'in', '-', 'domain', 'data', ',', 'our', 'methods', 'demonstrate', 'clear', 'advantage', 'as', 'a', 'single', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'VBN', 'TO', 'DT', 'JJ', 'NN', 'NN', ',', 'WDT', 'VBZ', 'NN', 'IN', 'JJ', 'NN', 'CC', 'NN', 'NNS', 'VBN', 'IN', 'IN', ':', 'NN', 'NNS', ',', 'PRP$', 'NNS', 'VBP', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",38
relation-classification,9,2,SCIBERT : A Pretrained Language Model for Scientific Text,"['SCIBERT', ':', 'A', 'Pretrained', 'Language', 'Model', 'for', 'Scientific', 'Text']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['NN', ':', 'DT', 'JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",9
relation-classification,9,4,Obtaining large - scale annotated data for NLP tasks in the scientific domain is challenging and expensive .,"['Obtaining', 'large', '-', 'scale', 'annotated', 'data', 'for', 'NLP', 'tasks', 'in', 'the', 'scientific', 'domain', 'is', 'challenging', 'and', 'expensive', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', ':', 'NN', 'VBD', 'NNS', 'IN', 'NNP', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'VBG', 'CC', 'JJ', '.']",18
relation-classification,9,5,"We release SCIBERT , a pretrained language model based on BERT ( Devlin et al. , 2019 ) to address the lack of high - quality , large - scale labeled scientific data .","['We', 'release', 'SCIBERT', ',', 'a', 'pretrained', 'language', 'model', 'based', 'on', 'BERT', '(', 'Devlin', 'et', 'al.', ',', '2019', ')', 'to', 'address', 'the', 'lack', 'of', 'high', '-', 'quality', ',', 'large', '-', 'scale', 'labeled', 'scientific', 'data', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', ',', 'DT', 'JJ', 'NN', 'NN', 'VBN', 'IN', 'NNP', '(', 'NNP', 'RB', 'RB', ',', 'CD', ')', 'TO', 'VB', 'DT', 'NN', 'IN', 'JJ', ':', 'NN', ',', 'JJ', ':', 'NN', 'VBD', 'JJ', 'NN', '.']",34
relation-classification,9,9,The code and pretrained models are available at https://github.com/allenai/scibert/.,"['The', 'code', 'and', 'pretrained', 'models', 'are', 'available', 'at', 'https://github.com/allenai/scibert/.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']","['DT', 'NN', 'CC', 'JJ', 'NNS', 'VBP', 'JJ', 'IN', 'NN']",9
relation-classification,9,13,"In general domains , large - scale training data is often possible to obtain through crowdsourcing , but in scientific domains , annotated data is difficult and expensive to collect due to the expertise required for quality annotation .","['In', 'general', 'domains', ',', 'large', '-', 'scale', 'training', 'data', 'is', 'often', 'possible', 'to', 'obtain', 'through', 'crowdsourcing', ',', 'but', 'in', 'scientific', 'domains', ',', 'annotated', 'data', 'is', 'difficult', 'and', 'expensive', 'to', 'collect', 'due', 'to', 'the', 'expertise', 'required', 'for', 'quality', 'annotation', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', 'NNS', ',', 'JJ', ':', 'NN', 'NN', 'NNS', 'VBZ', 'RB', 'JJ', 'TO', 'VB', 'IN', 'NN', ',', 'CC', 'IN', 'JJ', 'NNS', ',', 'VBN', 'NN', 'VBZ', 'JJ', 'CC', 'JJ', 'TO', 'VB', 'JJ', 'TO', 'DT', 'NN', 'VBN', 'IN', 'NN', 'NN', '.']",39
relation-classification,9,27,SCIB - ERT follows the same architecture as BERT but is instead pretrained on scientific text .,"['SCIB', '-', 'ERT', 'follows', 'the', 'same', 'architecture', 'as', 'BERT', 'but', 'is', 'instead', 'pretrained', 'on', 'scientific', 'text', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['NNP', ':', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'CC', 'VBZ', 'RB', 'VBN', 'IN', 'JJ', 'NN', '.']",17
relation-classification,9,31,"We construct SCIVOCAB , a new WordPiece vocabulary on our scientific corpus using the Sen - tencePiece 1 library .","['We', 'construct', 'SCIVOCAB', ',', 'a', 'new', 'WordPiece', 'vocabulary', 'on', 'our', 'scientific', 'corpus', 'using', 'the', 'Sen', '-', 'tencePiece', '1', 'library', '.']","['O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', ',', 'DT', 'JJ', 'NNP', 'NN', 'IN', 'PRP$', 'JJ', 'NN', 'VBG', 'DT', 'NNP', ':', 'NN', 'CD', 'NN', '.']",20
relation-classification,9,34,Corpus,['Corpus'],['B-n'],['NN'],1
relation-classification,9,35,We train SCIBERT on a random sample of 1.14 M papers from Semantic Scholar .,"['We', 'train', 'SCIBERT', 'on', 'a', 'random', 'sample', 'of', '1.14', 'M', 'papers', 'from', 'Semantic', 'Scholar', '.']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NNP', 'NNS', 'IN', 'JJ', 'NNP', '.']",15
relation-classification,9,36,This corpus consists of 18 % papers from the computer science domain and 82 % from the broad biomedical domain .,"['This', 'corpus', 'consists', 'of', '18', '%', 'papers', 'from', 'the', 'computer', 'science', 'domain', 'and', '82', '%', 'from', 'the', 'broad', 'biomedical', 'domain', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'IN', 'CD', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'NN', 'CC', 'CD', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NN', '.']",21
relation-classification,9,44,Named Entity Recognition ( NER ),"['Named', 'Entity', 'Recognition', '(', 'NER', ')']","['B-n', 'I-n', 'I-n', 'O', 'B-n', 'O']","['VBN', 'NNP', 'NNP', '(', 'NNP', ')']",6
relation-classification,9,45,2 . PICO Extraction ( PICO ),"['2', '.', 'PICO', 'Extraction', '(', 'PICO', ')']","['O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['CD', '.', 'NNP', 'NNP', '(', 'NNP', ')']",7
relation-classification,9,46,3 . Text Classification ( CLS ),"['3', '.', 'Text', 'Classification', '(', 'CLS', ')']","['O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['LS', '.', 'JJ', 'NNP', '(', 'NNP', ')']",7
relation-classification,9,47,4 . Relation Classification ( REL ),"['4', '.', 'Relation', 'Classification', '(', 'REL', ')']","['O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['CD', '.', 'NN', 'NNP', '(', 'NNP', ')']",7
relation-classification,9,48,5 . Dependency Parsing ( DEP ),"['5', '.', 'Dependency', 'Parsing', '(', 'DEP', ')']","['O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['CD', '.', 'NN', 'NNP', '(', 'NNP', ')']",7
relation-classification,9,103,We observe that SCIBERT outperforms BERT - Base on scientific tasks ( + 2.11 F1 with finetuning and + 2.43 F1 without ),"['We', 'observe', 'that', 'SCIBERT', 'outperforms', 'BERT', '-', 'Base', 'on', 'scientific', 'tasks', '(', '+', '2.11', 'F1', 'with', 'finetuning', 'and', '+', '2.43', 'F1', 'without', ')']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['PRP', 'VBP', 'DT', 'NNP', 'VBZ', 'NNP', ':', 'NN', 'IN', 'JJ', 'NNS', '(', '$', 'CD', 'NNP', 'IN', 'NN', 'CC', '$', 'CD', 'NNP', 'IN', ')']",23
relation-classification,9,106,Biomedical Domain,"['Biomedical', 'Domain']","['B-n', 'I-n']","['JJ', 'NN']",2
relation-classification,9,107,We observe that SCIBERT outperforms BERT - Base on biomedical tasks ( + 1.92 F1 with finetuning and + 3.59 F1 without ) .,"['We', 'observe', 'that', 'SCIBERT', 'outperforms', 'BERT', '-', 'Base', 'on', 'biomedical', 'tasks', '(', '+', '1.92', 'F1', 'with', 'finetuning', 'and', '+', '3.59', 'F1', 'without', ')', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'VBZ', 'NNP', ':', 'NN', 'IN', 'JJ', 'NNS', '(', '$', 'CD', 'NNP', 'IN', 'NN', 'CC', '$', 'CD', 'NNP', 'IN', ')', '.']",24
relation-classification,9,108,"In addition , SCIB - ERT achieves new SOTA results on BC5 CDR and ChemProt , and EBM - NLP .","['In', 'addition', ',', 'SCIB', '-', 'ERT', 'achieves', 'new', 'SOTA', 'results', 'on', 'BC5', 'CDR', 'and', 'ChemProt', ',', 'and', 'EBM', '-', 'NLP', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'NNP', ':', 'NNP', 'VBZ', 'JJ', 'NNP', 'NNS', 'IN', 'NNP', 'NNP', 'CC', 'NNP', ',', 'CC', 'NNP', ':', 'NN', '.']",21
relation-classification,9,118,Computer Science Domain,"['Computer', 'Science', 'Domain']","['B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP']",3
relation-classification,9,119,We observe that SCIBERT outperforms BERT - Base on computer science tasks ( + 3.55 F1 with finetuning and + 1.13 F1 without ) .,"['We', 'observe', 'that', 'SCIBERT', 'outperforms', 'BERT', '-', 'Base', 'on', 'computer', 'science', 'tasks', '(', '+', '3.55', 'F1', 'with', 'finetuning', 'and', '+', '1.13', 'F1', 'without', ')', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'VBZ', 'NNP', ':', 'NN', 'IN', 'NN', 'NN', 'NNS', '(', '$', 'CD', 'NNP', 'IN', 'NN', 'CC', '$', 'CD', 'NNP', 'IN', ')', '.']",25
relation-classification,9,120,"In addition , SCIBERT achieves new SOTA results on ACL - ARC , and the NER part of SciERC .","['In', 'addition', ',', 'SCIBERT', 'achieves', 'new', 'SOTA', 'results', 'on', 'ACL', '-', 'ARC', ',', 'and', 'the', 'NER', 'part', 'of', 'SciERC', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'NNP', 'VBZ', 'JJ', 'NNP', 'NNS', 'IN', 'NNP', ':', 'NNP', ',', 'CC', 'DT', 'NNP', 'NN', 'IN', 'NNP', '.']",20
relation-classification,9,122,Multiple Domains,"['Multiple', 'Domains']","['B-n', 'I-n']","['JJ', 'NNS']",2
relation-classification,9,123,We observe that SCIBERT outperforms BERT - Base on the multidomain tasks ( + 0.49 F1 with finetuning and + 0.93 F1 without ) .,"['We', 'observe', 'that', 'SCIBERT', 'outperforms', 'BERT', '-', 'Base', 'on', 'the', 'multidomain', 'tasks', '(', '+', '0.49', 'F1', 'with', 'finetuning', 'and', '+', '0.93', 'F1', 'without', ')', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'VBZ', 'NNP', ':', 'NN', 'IN', 'DT', 'NN', 'NNS', '(', '$', 'CD', 'NNP', 'IN', 'NN', 'CC', '$', 'CD', 'NNP', 'IN', ')', '.']",25
relation-classification,9,124,"In addition , SCIBERT outperforms the SOTA on Sci - Cite .","['In', 'addition', ',', 'SCIBERT', 'outperforms', 'the', 'SOTA', 'on', 'Sci', '-', 'Cite', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'NNP', 'VBZ', 'DT', 'NNP', 'IN', 'NNP', ':', 'NNP', '.']",12
relation-classification,1,2,Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme,"['Joint', 'Extraction', 'of', 'Entities', 'and', 'Relations', 'Based', 'on', 'a', 'Novel', 'Tagging', 'Scheme']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'IN', 'NNPS', 'CC', 'NNPS', 'VBN', 'IN', 'DT', 'NNP', 'NNP', 'NNP']",12
relation-classification,1,10,"Joint extraction of entities and relations is to detect entity mentions and recognize their semantic relations simultaneously from unstructured text , as shows .","['Joint', 'extraction', 'of', 'entities', 'and', 'relations', 'is', 'to', 'detect', 'entity', 'mentions', 'and', 'recognize', 'their', 'semantic', 'relations', 'simultaneously', 'from', 'unstructured', 'text', ',', 'as', 'shows', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'IN', 'NNS', 'CC', 'NNS', 'VBZ', 'TO', 'VB', 'NN', 'NNS', 'CC', 'VB', 'PRP$', 'JJ', 'NNS', 'RB', 'IN', 'JJ', 'NN', ',', 'IN', 'NNS', '.']",24
relation-classification,1,17,"Different from the pipelined methods , joint learning framework is to extract entities together with relations using a single model .","['Different', 'from', 'the', 'pipelined', 'methods', ',', 'joint', 'learning', 'framework', 'is', 'to', 'extract', 'entities', 'together', 'with', 'relations', 'using', 'a', 'single', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'IN', 'DT', 'VBN', 'NNS', ',', 'JJ', 'NN', 'NN', 'VBZ', 'TO', 'VB', 'NNS', 'RB', 'IN', 'NNS', 'VBG', 'DT', 'JJ', 'NN', '.']",21
relation-classification,1,67,We propose a novel tagging scheme and an end -toend model with biased objective function to jointly extract entities and their relations .,"['We', 'propose', 'a', 'novel', 'tagging', 'scheme', 'and', 'an', 'end', '-toend', 'model', 'with', 'biased', 'objective', 'function', 'to', 'jointly', 'extract', 'entities', 'and', 'their', 'relations', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'CC', 'DT', 'NN', 'NN', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'TO', 'RB', 'VB', 'NNS', 'CC', 'PRP$', 'NNS', '.']",23
relation-classification,1,72,"Tag "" O "" represents the "" Other "" tag , which means that the corresponding word is independent of the extracted results .","['Tag', '""', 'O', '""', 'represents', 'the', '""', 'Other', '""', 'tag', ',', 'which', 'means', 'that', 'the', 'corresponding', 'word', 'is', 'independent', 'of', 'the', 'extracted', 'results', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'NN', 'JJ', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'JJ', 'IN', 'DT', 'JJ', 'NNS', '.']",24
relation-classification,1,74,"We use the "" BIES "" ( Begin , Inside , End , Single ) signs to represent the position information of a word in the entity .","['We', 'use', 'the', '""', 'BIES', '""', '(', 'Begin', ',', 'Inside', ',', 'End', ',', 'Single', ')', 'signs', 'to', 'represent', 'the', 'position', 'information', 'of', 'a', 'word', 'in', 'the', 'entity', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NNP', 'NNP', '(', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', ')', 'NNS', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",28
relation-classification,1,97,The End - to - end Model,"['The', 'End', '-', 'to', '-', 'end', 'Model']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['DT', 'NN', ':', 'TO', ':', 'NN', 'NN']",7
relation-classification,1,99,"In this paper , we investigate an end - toend model to produce the tags sequence as shows .","['In', 'this', 'paper', ',', 'we', 'investigate', 'an', 'end', '-', 'toend', 'model', 'to', 'produce', 'the', 'tags', 'sequence', 'as', 'shows', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', ':', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NNS', '.']",19
relation-classification,1,100,It contains a bi-directional Long Short Term Memory ( Bi - LSTM ) layer to encode the input sentence and a LSTM - based decoding layer with biased loss .,"['It', 'contains', 'a', 'bi-directional', 'Long', 'Short', 'Term', 'Memory', '(', 'Bi', '-', 'LSTM', ')', 'layer', 'to', 'encode', 'the', 'input', 'sentence', 'and', 'a', 'LSTM', '-', 'based', 'decoding', 'layer', 'with', 'biased', 'loss', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBZ', 'DT', 'JJ', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ':', 'NNP', ')', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'CC', 'DT', 'NNP', ':', 'VBN', 'VBG', 'NN', 'IN', 'JJ', 'NN', '.']",30
relation-classification,1,101,The biased loss can enhance the relevance of entity tags .,"['The', 'biased', 'loss', 'can', 'enhance', 'the', 'relevance', 'of', 'entity', 'tags', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'MD', 'VB', 'DT', 'NN', 'IN', 'NN', 'NNS', '.']",11
relation-classification,1,154,The word embeddings used in the encoding part are initialed by running word2vec 3 on NYT training corpus .,"['The', 'word', 'embeddings', 'used', 'in', 'the', 'encoding', 'part', 'are', 'initialed', 'by', 'running', 'word2vec', '3', 'on', 'NYT', 'training', 'corpus', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NNS', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'VBP', 'VBN', 'IN', 'VBG', 'RB', 'CD', 'IN', 'NNP', 'VBG', 'NN', '.']",19
relation-classification,1,155,The dimension of the word embeddings is d = 300 .,"['The', 'dimension', 'of', 'the', 'word', 'embeddings', 'is', 'd', '=', '300', '.']","['O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'VBZ', 'JJ', 'RB', 'CD', '.']",11
relation-classification,1,156,We regularize our network using dropout on embedding layer and the dropout ratio is 0.5 .,"['We', 'regularize', 'our', 'network', 'using', 'dropout', 'on', 'embedding', 'layer', 'and', 'the', 'dropout', 'ratio', 'is', '0.5', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'VBG', 'NN', 'IN', 'VBG', 'NN', 'CC', 'DT', 'NN', 'NN', 'VBZ', 'CD', '.']",16
relation-classification,1,157,The number of lstm units in encoding layer is 300 and the number in decoding layer is 600 .,"['The', 'number', 'of', 'lstm', 'units', 'in', 'encoding', 'layer', 'is', '300', 'and', 'the', 'number', 'in', 'decoding', 'layer', 'is', '600', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'VBG', 'NN', 'VBZ', 'CD', 'CC', 'DT', 'NN', 'IN', 'VBG', 'NN', 'VBZ', 'CD', '.']",19
relation-classification,1,158,The bias parameter ?,"['The', 'bias', 'parameter', '?']","['O', 'B-p', 'I-p', 'I-p']","['DT', 'NN', 'NN', '.']",4
relation-classification,1,159,corresponding to the results in is 10 .,"['corresponding', 'to', 'the', 'results', 'in', 'is', '10', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']","['VBG', 'TO', 'DT', 'NNS', 'IN', 'VBZ', 'CD', '.']",8
relation-classification,1,169,"We compare our method with several classical triplet extraction methods , which can be divided into the following categories : the pipelined methods , the jointly extracting methods and the end - to - end methods based our tagging scheme .","['We', 'compare', 'our', 'method', 'with', 'several', 'classical', 'triplet', 'extraction', 'methods', ',', 'which', 'can', 'be', 'divided', 'into', 'the', 'following', 'categories', ':', 'the', 'pipelined', 'methods', ',', 'the', 'jointly', 'extracting', 'methods', 'and', 'the', 'end', '-', 'to', '-', 'end', 'methods', 'based', 'our', 'tagging', 'scheme', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'NN', 'NNS', ',', 'WDT', 'MD', 'VB', 'VBN', 'IN', 'DT', 'JJ', 'NNS', ':', 'DT', 'JJ', 'NNS', ',', 'DT', 'RB', 'VBG', 'NNS', 'CC', 'DT', 'NN', ':', 'TO', ':', 'NN', 'NNS', 'VBN', 'PRP$', 'VBG', 'NN', '.']",41
relation-classification,1,170,"For the pipelined methods , we follow ) 's settings :","['For', 'the', 'pipelined', 'methods', ',', 'we', 'follow', ')', ""'s"", 'settings', ':']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NNS', ',', 'PRP', 'VBP', ')', 'POS', 'NNS', ':']",11
relation-classification,1,173,"( 1 ) DS-logistic ) is a distant supervised and feature based method , which combines the advantages of supervised IE and unsupervised IE features ; ( 2 ) LINE is a network embedding method , which is suitable for arbitrary types of information networks ;","['(', '1', ')', 'DS-logistic', ')', 'is', 'a', 'distant', 'supervised', 'and', 'feature', 'based', 'method', ',', 'which', 'combines', 'the', 'advantages', 'of', 'supervised', 'IE', 'and', 'unsupervised', 'IE', 'features', ';', '(', '2', ')', 'LINE', 'is', 'a', 'network', 'embedding', 'method', ',', 'which', 'is', 'suitable', 'for', 'arbitrary', 'types', 'of', 'information', 'networks', ';']","['O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'JJ', ')', 'VBZ', 'DT', 'JJ', 'VBN', 'CC', 'NN', 'VBN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NNS', 'IN', 'JJ', 'NNP', 'CC', 'JJ', 'NNP', 'NNS', ':', '(', 'CD', ')', 'NNP', 'VBZ', 'DT', 'NN', 'VBG', 'NN', ',', 'WDT', 'VBZ', 'JJ', 'IN', 'JJ', 'NNS', 'IN', 'NN', 'NNS', ':']",46
relation-classification,1,174,( 3 ) FCM ) is a compositional model that combines lexicalized linguistic context and word embeddings for relation extraction .,"['(', '3', ')', 'FCM', ')', 'is', 'a', 'compositional', 'model', 'that', 'combines', 'lexicalized', 'linguistic', 'context', 'and', 'word', 'embeddings', 'for', 'relation', 'extraction', '.']","['O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['(', 'CD', ')', 'NNP', ')', 'VBZ', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'VBN', 'JJ', 'NN', 'CC', 'NN', 'NNS', 'IN', 'NN', 'NN', '.']",21
relation-classification,1,175,"The jointly extracting methods used in this paper are listed as follows : ( 4 ) DS - Joint ) is a supervised method , which jointly extracts entities and relations using structured perceptron on human - annotated dataset ; ( 5 ) MultiR is a typical distant supervised method based on multi-instance learning algorithms to combat the noisy training data ; ( 6 ) CoType ) is a domain independent framework by jointly embedding entity mentions , relation mentions , text features and type labels into meaningful representations .","['The', 'jointly', 'extracting', 'methods', 'used', 'in', 'this', 'paper', 'are', 'listed', 'as', 'follows', ':', '(', '4', ')', 'DS', '-', 'Joint', ')', 'is', 'a', 'supervised', 'method', ',', 'which', 'jointly', 'extracts', 'entities', 'and', 'relations', 'using', 'structured', 'perceptron', 'on', 'human', '-', 'annotated', 'dataset', ';', '(', '5', ')', 'MultiR', 'is', 'a', 'typical', 'distant', 'supervised', 'method', 'based', 'on', 'multi-instance', 'learning', 'algorithms', 'to', 'combat', 'the', 'noisy', 'training', 'data', ';', '(', '6', ')', 'CoType', ')', 'is', 'a', 'domain', 'independent', 'framework', 'by', 'jointly', 'embedding', 'entity', 'mentions', ',', 'relation', 'mentions', ',', 'text', 'features', 'and', 'type', 'labels', 'into', 'meaningful', 'representations', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'RB', 'VBG', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'VBP', 'VBN', 'IN', 'VBZ', ':', '(', 'CD', ')', 'NNP', ':', 'NN', ')', 'VBZ', 'DT', 'JJ', 'NN', ',', 'WDT', 'RB', 'VBZ', 'NNS', 'CC', 'NNS', 'VBG', 'JJ', 'NN', 'IN', 'JJ', ':', 'JJ', 'NN', ':', '(', 'CD', ')', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'VBD', 'NNS', 'VBN', 'IN', 'NN', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NNS', ':', '(', 'CD', ')', 'NN', ')', 'VBZ', 'DT', 'NN', 'JJ', 'NN', 'IN', 'RB', 'VBG', 'NN', 'NNS', ',', 'NN', 'NNS', ',', 'NN', 'NNS', 'CC', 'NN', 'NNS', 'IN', 'JJ', 'NNS', '.']",90
relation-classification,1,176,"In addition , we also compare our method with two classical end - to - end tagging models : LSTM- CRF and LSTM - LSTM .","['In', 'addition', ',', 'we', 'also', 'compare', 'our', 'method', 'with', 'two', 'classical', 'end', '-', 'to', '-', 'end', 'tagging', 'models', ':', 'LSTM-', 'CRF', 'and', 'LSTM', '-', 'LSTM', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'RB', 'VBP', 'PRP$', 'NN', 'IN', 'CD', 'JJ', 'NN', ':', 'TO', ':', 'NN', 'NN', 'NNS', ':', 'JJ', 'NNP', 'CC', 'NNP', ':', 'NN', '.']",26
relation-classification,1,177,LSTM - CRF is proposed for entity recognition by using a bidirectional LSTM to encode input sentence and a conditional random fields to predict the entity tag sequence .,"['LSTM', '-', 'CRF', 'is', 'proposed', 'for', 'entity', 'recognition', 'by', 'using', 'a', 'bidirectional', 'LSTM', 'to', 'encode', 'input', 'sentence', 'and', 'a', 'conditional', 'random', 'fields', 'to', 'predict', 'the', 'entity', 'tag', 'sequence', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'VBZ', 'VBN', 'IN', 'NN', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NNP', 'TO', 'VB', 'JJ', 'NN', 'CC', 'DT', 'JJ', 'NN', 'NNS', 'TO', 'VB', 'DT', 'NN', 'JJ', 'NN', '.']",29
relation-classification,1,178,"Different from LSTM - CRF , LSTM - LSTM uses a LSTM layer to decode the tag sequence instead of CRF .","['Different', 'from', 'LSTM', '-', 'CRF', ',', 'LSTM', '-', 'LSTM', 'uses', 'a', 'LSTM', 'layer', 'to', 'decode', 'the', 'tag', 'sequence', 'instead', 'of', 'CRF', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'IN', 'NNP', ':', 'NNP', ',', 'NNP', ':', 'NNP', 'VBZ', 'DT', 'NNP', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'RB', 'IN', 'NNP', '.']",22
relation-classification,1,182,"It can be seen that our method , LSTM - LSTM - Bias , outperforms all other methods in F 1 score and achieves a 3 % improvement in F 1 over the best method CoType .","['It', 'can', 'be', 'seen', 'that', 'our', 'method', ',', 'LSTM', '-', 'LSTM', '-', 'Bias', ',', 'outperforms', 'all', 'other', 'methods', 'in', 'F', '1', 'score', 'and', 'achieves', 'a', '3', '%', 'improvement', 'in', 'F', '1', 'over', 'the', 'best', 'method', 'CoType', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'VBN', 'IN', 'PRP$', 'NN', ',', 'NNP', ':', 'NNP', ':', 'NNP', ',', 'VBZ', 'DT', 'JJ', 'NNS', 'IN', 'NNP', 'CD', 'NN', 'CC', 'VBZ', 'DT', 'CD', 'NN', 'NN', 'IN', 'NNP', 'CD', 'IN', 'DT', 'JJS', 'NN', 'NNP', '.']",37
relation-classification,1,186,"When compared with the traditional methods , the precisions of the end - to - end models are significantly improved .","['When', 'compared', 'with', 'the', 'traditional', 'methods', ',', 'the', 'precisions', 'of', 'the', 'end', '-', 'to', '-', 'end', 'models', 'are', 'significantly', 'improved', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O']","['WRB', 'VBN', 'IN', 'DT', 'JJ', 'NNS', ',', 'DT', 'NNS', 'IN', 'DT', 'NN', ':', 'TO', ':', 'NN', 'NNS', 'VBP', 'RB', 'VBN', '.']",21
relation-classification,1,191,We also find that the LSTM - LSTM model is better than LSTM - CRF model based on our tagging scheme .,"['We', 'also', 'find', 'that', 'the', 'LSTM', '-', 'LSTM', 'model', 'is', 'better', 'than', 'LSTM', '-', 'CRF', 'model', 'based', 'on', 'our', 'tagging', 'scheme', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'IN', 'DT', 'NNP', ':', 'NNP', 'NN', 'VBZ', 'JJR', 'IN', 'NNP', ':', 'NNP', 'NN', 'VBN', 'IN', 'PRP$', 'VBG', 'NN', '.']",22
relation-classification,5,2,End - to - end neural relation extraction using deep biaffine attention,"['End', '-', 'to', '-', 'end', 'neural', 'relation', 'extraction', 'using', 'deep', 'biaffine', 'attention']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NN', ':', 'TO', ':', 'VB', 'JJ', 'NN', 'NN', 'VBG', 'JJ', 'NN', 'NN']",12
relation-classification,5,4,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features .","['We', 'propose', 'a', 'neural', 'network', 'model', 'for', 'joint', 'extraction', 'of', 'named', 'entities', 'and', 'relations', 'between', 'them', ',', 'without', 'any', 'hand', '-', 'crafted', 'features', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'NN', 'IN', 'VBN', 'NNS', 'CC', 'NNS', 'IN', 'PRP', ',', 'IN', 'DT', 'NN', ':', 'VBN', 'NNS', '.']",24
relation-classification,5,8,Extracting entities and their semantic relations from raw text is a key information extraction task .,"['Extracting', 'entities', 'and', 'their', 'semantic', 'relations', 'from', 'raw', 'text', 'is', 'a', 'key', 'information', 'extraction', 'task', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NNS', 'CC', 'PRP$', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'NN', '.']",16
relation-classification,5,13,"More recently , end - to - end systems which jointly learn to extract entities and relations have been proposed with strong potential to obtain high performance .","['More', 'recently', ',', 'end', '-', 'to', '-', 'end', 'systems', 'which', 'jointly', 'learn', 'to', 'extract', 'entities', 'and', 'relations', 'have', 'been', 'proposed', 'with', 'strong', 'potential', 'to', 'obtain', 'high', 'performance', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RBR', 'RB', ',', 'VB', ':', 'TO', ':', 'NN', 'NNS', 'WDT', 'RB', 'VBP', 'TO', 'VB', 'NNS', 'CC', 'NNS', 'VBP', 'VBN', 'VBN', 'IN', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'NN', '.']",28
relation-classification,5,25,"In this paper , we present a novel end - to - end neural model for joint entity and relation extraction .","['In', 'this', 'paper', ',', 'we', 'present', 'a', 'novel', 'end', '-', 'to', '-', 'end', 'neural', 'model', 'for', 'joint', 'entity', 'and', 'relation', 'extraction', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', ':', 'TO', ':', 'VB', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'CC', 'NN', 'NN', '.']",22
relation-classification,5,26,"As illustrated in , our model architecture can be viewed as a mixture of a named entity recognition ( NER ) component and a relation classification ( RC ) component .","['As', 'illustrated', 'in', ',', 'our', 'model', 'architecture', 'can', 'be', 'viewed', 'as', 'a', 'mixture', 'of', 'a', 'named', 'entity', 'recognition', '(', 'NER', ')', 'component', 'and', 'a', 'relation', 'classification', '(', 'RC', ')', 'component', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'IN', ',', 'PRP$', 'NN', 'NN', 'MD', 'VB', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'VBN', 'NN', 'NN', '(', 'NNP', ')', 'NN', 'CC', 'DT', 'NN', 'NN', '(', 'NNP', ')', 'NN', '.']",31
relation-classification,5,27,Our NER component employs a BiLSTM - CRF architecture to predict entities from input word tokens .,"['Our', 'NER', 'component', 'employs', 'a', 'BiLSTM', '-', 'CRF', 'architecture', 'to', 'predict', 'entities', 'from', 'input', 'word', 'tokens', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'NNP', 'NN', 'VBZ', 'DT', 'NNP', ':', 'NNP', 'NN', 'TO', 'VB', 'NNS', 'IN', 'NN', 'NN', 'VBZ', '.']",17
relation-classification,5,28,"Based on both the input words and the predicted NER labels , the RC component uses another BiLSTM to learn latent features relevant for relation classification .","['Based', 'on', 'both', 'the', 'input', 'words', 'and', 'the', 'predicted', 'NER', 'labels', ',', 'the', 'RC', 'component', 'uses', 'another', 'BiLSTM', 'to', 'learn', 'latent', 'features', 'relevant', 'for', 'relation', 'classification', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'DT', 'NN', 'NNS', 'CC', 'DT', 'JJ', 'NNP', 'NNS', ',', 'DT', 'NNP', 'NN', 'VBZ', 'DT', 'NNP', 'TO', 'VB', 'JJ', 'NNS', 'VBP', 'IN', 'NN', 'NN', '.']",27
relation-classification,5,30,"In contrast , our RC component takes into account second - order interactions over the latent features via a tensor .","['In', 'contrast', ',', 'our', 'RC', 'component', 'takes', 'into', 'account', 'second', '-', 'order', 'interactions', 'over', 'the', 'latent', 'features', 'via', 'a', 'tensor', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['IN', 'NN', ',', 'PRP$', 'NNP', 'NN', 'VBZ', 'IN', 'NN', 'JJ', ':', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']",21
relation-classification,5,31,"In particular , for relation classification we propose a novel use of the deep biaffine attention mechanism which was first introduced in dependency parsing .","['In', 'particular', ',', 'for', 'relation', 'classification', 'we', 'propose', 'a', 'novel', 'use', 'of', 'the', 'deep', 'biaffine', 'attention', 'mechanism', 'which', 'was', 'first', 'introduced', 'in', 'dependency', 'parsing', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', ',', 'IN', 'NN', 'NN', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', 'WDT', 'VBD', 'RB', 'VBN', 'IN', 'NN', 'NN', '.']",25
relation-classification,5,72,Our model is implemented using DYNET v 2.0 .,"['Our', 'model', 'is', 'implemented', 'using', 'DYNET', 'v', '2.0', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'VBN', 'VBG', 'NNP', 'NN', 'CD', '.']",9
relation-classification,5,73,"We optimize the objective loss using Adam , no mini-batches and run for 100 epochs .","['We', 'optimize', 'the', 'objective', 'loss', 'using', 'Adam', ',', 'no', 'mini-batches', 'and', 'run', 'for', '100', 'epochs', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBG', 'NNP', ',', 'DT', 'NNS', 'CC', 'VB', 'IN', 'CD', 'NNS', '.']",16
relation-classification,5,77,Our code is available at : https : //github.com/datquocnguyen/jointRE,"['Our', 'code', 'is', 'available', 'at', ':', 'https', ':', '//github.com/datquocnguyen/jointRE']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['PRP$', 'NN', 'VBZ', 'JJ', 'IN', ':', 'NN', ':', 'NN']",9
relation-classification,5,94,"We provide in the results of a pipeline approach where we treat our two NER and RC components as independent networks , and train them separately .","['We', 'provide', 'in', 'the', 'results', 'of', 'a', 'pipeline', 'approach', 'where', 'we', 'treat', 'our', 'two', 'NER', 'and', 'RC', 'components', 'as', 'independent', 'networks', ',', 'and', 'train', 'them', 'separately', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', 'WRB', 'PRP', 'VB', 'PRP$', 'CD', 'NNP', 'CC', 'NNP', 'NNS', 'IN', 'JJ', 'NNS', ',', 'CC', 'VB', 'PRP', 'RB', '.']",27
relation-classification,5,96,"We find that the joint approach does slightly better than the pipeline approach in relation classification , although the .","['We', 'find', 'that', 'the', 'joint', 'approach', 'does', 'slightly', 'better', 'than', 'the', 'pipeline', 'approach', 'in', 'relation', 'classification', ',', 'although', 'the', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'RB', 'JJR', 'IN', 'DT', 'NN', 'NN', 'IN', 'NN', 'NN', ',', 'IN', 'DT', '.']",20
relation-classification,7,2,Data and text mining BioBERT : a pre-trained biomedical language representation model for biomedical text mining,"['Data', 'and', 'text', 'mining', 'BioBERT', ':', 'a', 'pre-trained', 'biomedical', 'language', 'representation', 'model', 'for', 'biomedical', 'text', 'mining']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NNP', 'CC', 'NN', 'NN', 'NNP', ':', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NN']",16
relation-classification,7,6,"With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models .","['With', 'the', 'progress', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', ',', 'extracting', 'valuable', 'information', 'from', 'biomedical', 'literature', 'has', 'gained', 'popularity', 'among', 'researchers', ',', 'and', 'deep', 'learning', 'has', 'boosted', 'the', 'development', 'of', 'effective', 'biomedical', 'text', 'mining', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', ',', 'VBG', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'VBZ', 'VBN', 'NN', 'IN', 'NNS', ',', 'CC', 'JJ', 'NN', 'VBZ', 'VBN', 'DT', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'NN', 'NNS', '.']",37
relation-classification,7,7,"However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora .","['However', ',', 'directly', 'applying', 'the', 'advancements', 'in', 'NLP', 'to', 'biomedical', 'text', 'mining', 'often', 'yields', 'unsatisfactory', 'results', 'due', 'to', 'a', 'word', 'distribution', 'shift', 'from', 'general', 'domain', 'corpora', 'to', 'biomedical', 'corpora', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'RB', 'VBG', 'DT', 'NNS', 'IN', 'NNP', 'TO', 'JJ', 'NN', 'NN', 'RB', 'VBZ', 'JJ', 'NNS', 'JJ', 'TO', 'DT', 'NN', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NN', 'TO', 'JJ', 'NN', '.']",30
relation-classification,7,8,"In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .","['In', 'this', 'article', ',', 'we', 'investigate', 'how', 'the', 'recently', 'introduced', 'pre-trained', 'language', 'model', 'BERT', 'can', 'be', 'adapted', 'for', 'biomedical', 'corpora', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'WRB', 'DT', 'RB', 'VBN', 'JJ', 'NN', 'NN', 'NNP', 'MD', 'VB', 'VBN', 'IN', 'JJ', 'NN', '.']",21
relation-classification,7,15,"We make the pre-trained weights of BioBERT freely available at https://github. com/naver/biobert-pretrained , and the source code for fine - tuning","['We', 'make', 'the', 'pre-trained', 'weights', 'of', 'BioBERT', 'freely', 'available', 'at', 'https://github.', 'com/naver/biobert-pretrained', ',', 'and', 'the', 'source', 'code', 'for', 'fine', '-', 'tuning']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NNS', 'IN', 'NNP', 'RB', 'JJ', 'IN', 'JJ', 'JJ', ',', 'CC', 'DT', 'NN', 'NN', 'IN', 'JJ', ':', 'NN']",21
relation-classification,7,27,"Also , the word distributions of general and biomedical corpora are quite different , which can often be a problem for biomedical text mining models .","['Also', ',', 'the', 'word', 'distributions', 'of', 'general', 'and', 'biomedical', 'corpora', 'are', 'quite', 'different', ',', 'which', 'can', 'often', 'be', 'a', 'problem', 'for', 'biomedical', 'text', 'mining', 'models', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'DT', 'NN', 'NNS', 'IN', 'JJ', 'CC', 'JJ', 'NNS', 'VBP', 'RB', 'JJ', ',', 'WDT', 'MD', 'RB', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', 'NNS', '.']",26
relation-classification,7,34,"In this article , we introduce BioBERT , which is a pre-trained language representation model for the biomedical domain .","['In', 'this', 'article', ',', 'we', 'introduce', 'BioBERT', ',', 'which', 'is', 'a', 'pre-trained', 'language', 'representation', 'model', 'for', 'the', 'biomedical', 'domain', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",20
relation-classification,7,36,"First , we initialize BioBERT with weights from BERT , which was pretrained on general domain corpora ( English Wikipedia and Books Corpus ) .","['First', ',', 'we', 'initialize', 'BioBERT', 'with', 'weights', 'from', 'BERT', ',', 'which', 'was', 'pretrained', 'on', 'general', 'domain', 'corpora', '(', 'English', 'Wikipedia', 'and', 'Books', 'Corpus', ')', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'NNP', 'IN', 'NNS', 'IN', 'NNP', ',', 'WDT', 'VBD', 'VBN', 'IN', 'JJ', 'NN', 'NN', '(', 'JJ', 'NNP', 'CC', 'NNP', 'NNP', ')', '.']",25
relation-classification,7,37,"Then , BioBERT is pre-trained on biomedical domain corpora ( PubMed abstracts and PMC full - text articles ) .","['Then', ',', 'BioBERT', 'is', 'pre-trained', 'on', 'biomedical', 'domain', 'corpora', '(', 'PubMed', 'abstracts', 'and', 'PMC', 'full', '-', 'text', 'articles', ')', '.']","['O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'NNP', 'VBZ', 'JJ', 'IN', 'JJ', 'NN', 'NN', '(', 'NNP', 'NNS', 'CC', 'NNP', 'JJ', ':', 'NN', 'NNS', ')', '.']",20
relation-classification,7,38,"To show the effectiveness of our approach in biomedical text mining , BioBERT is fine - tuned and evaluated on three popular biomedical text mining tasks .","['To', 'show', 'the', 'effectiveness', 'of', 'our', 'approach', 'in', 'biomedical', 'text', 'mining', ',', 'BioBERT', 'is', 'fine', '-', 'tuned', 'and', 'evaluated', 'on', 'three', 'popular', 'biomedical', 'text', 'mining', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'IN', 'JJ', 'NN', 'NN', ',', 'NNP', 'VBZ', 'JJ', ':', 'VBN', 'CC', 'VBN', 'IN', 'CD', 'JJ', 'JJ', 'NN', 'NN', 'NNS', '.']",27
relation-classification,7,119,We used the BERT BASE model pre-trained on English Wikipedia and Books Corpus for 1 M steps .,"['We', 'used', 'the', 'BERT', 'BASE', 'model', 'pre-trained', 'on', 'English', 'Wikipedia', 'and', 'Books', 'Corpus', 'for', '1', 'M', 'steps', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'DT', 'NNP', 'NNP', 'NN', 'JJ', 'IN', 'JJ', 'NNP', 'CC', 'NNP', 'NNP', 'IN', 'CD', 'NNP', 'NNS', '.']",18
relation-classification,7,120,BioBERT v1.0 ( PubMed PMC ) is the version of BioBERT ( PubMed PMC ) trained for 470 K steps .,"['BioBERT', 'v1.0', '(', 'PubMed', 'PMC', ')', 'is', 'the', 'version', 'of', 'BioBERT', '(', 'PubMed', 'PMC', ')', 'trained', 'for', '470', 'K', 'steps', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NN', '(', 'NNP', 'NNP', ')', 'VBZ', 'DT', 'NN', 'IN', 'NNP', '(', 'NNP', 'NNP', ')', 'VBD', 'IN', 'CD', 'NNP', 'NNS', '.']",21
relation-classification,7,126,We used eight NVIDIA V100 ( 32GB ) GPUs for the pre-training .,"['We', 'used', 'eight', 'NVIDIA', 'V100', '(', '32GB', ')', 'GPUs', 'for', 'the', 'pre-training', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['PRP', 'VBD', 'CD', 'NNP', 'NNP', '(', 'CD', ')', 'NNP', 'IN', 'DT', 'NN', '.']",13
relation-classification,7,127,"The maximum sequence length was fixed to 512 and the mini-batch size was set to 192 , resulting in 98 304 words per iteration .","['The', 'maximum', 'sequence', 'length', 'was', 'fixed', 'to', '512', 'and', 'the', 'mini-batch', 'size', 'was', 'set', 'to', '192', ',', 'resulting', 'in', '98', '304', 'words', 'per', 'iteration', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBD', 'VBN', 'TO', 'CD', 'CC', 'DT', 'JJ', 'NN', 'VBD', 'VBN', 'TO', 'CD', ',', 'VBG', 'IN', 'CD', 'CD', 'NNS', 'IN', 'NN', '.']",25
relation-classification,7,130,We used a single NVIDIA Titan Xp ( 12GB ) GPU to fine - tune BioBERT on each task .,"['We', 'used', 'a', 'single', 'NVIDIA', 'Titan', 'Xp', '(', '12GB', ')', 'GPU', 'to', 'fine', '-', 'tune', 'BioBERT', 'on', 'each', 'task', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBD', 'DT', 'JJ', 'NNP', 'NNP', 'NNP', '(', 'CD', ')', 'NNP', 'TO', 'VB', ':', 'NN', 'NNP', 'IN', 'DT', 'NN', '.']",20
relation-classification,7,136,The results of NER are shown in .,"['The', 'results', 'of', 'NER', 'are', 'shown', 'in', '.']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'NNP', 'VBP', 'VBN', 'IN', '.']",8
relation-classification,7,138,"On the other hand , BioBERT achieves higher scores than BERT on all the datasets .","['On', 'the', 'other', 'hand', ',', 'BioBERT', 'achieves', 'higher', 'scores', 'than', 'BERT', 'on', 'all', 'the', 'datasets', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'NNP', 'VBZ', 'JJR', 'NNS', 'IN', 'NNP', 'IN', 'PDT', 'DT', 'NNS', '.']",16
relation-classification,7,139,"BioBERT outperformed the state - of - the - art models on six out of nine datasets , and BioBERT v 1.1 ( PubMed ) outperformed the state - of - the - art models by 0.62 in terms of micro averaged F1 score .","['BioBERT', 'outperformed', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', 'on', 'six', 'out', 'of', 'nine', 'datasets', ',', 'and', 'BioBERT', 'v', '1.1', '(', 'PubMed', ')', 'outperformed', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', 'by', '0.62', 'in', 'terms', 'of', 'micro', 'averaged', 'F1', 'score', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBD', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'CD', 'IN', 'IN', 'CD', 'NNS', ',', 'CC', 'NNP', 'VBP', 'CD', '(', 'NNP', ')', 'VBD', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'CD', 'IN', 'NNS', 'IN', 'NN', 'VBD', 'NNP', 'NN', '.']",45
relation-classification,7,141,The RE results of each model are shown in .,"['The', 'RE', 'results', 'of', 'each', 'model', 'are', 'shown', 'in', '.']","['O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'VBN', 'IN', '.']",10
relation-classification,7,143,"On average ( micro ) , BioBERT v1.0 ( PubMed ) obtained a higher F1 score ( 2.80 higher ) than the state - of - the - art models .","['On', 'average', '(', 'micro', ')', ',', 'BioBERT', 'v1.0', '(', 'PubMed', ')', 'obtained', 'a', 'higher', 'F1', 'score', '(', '2.80', 'higher', ')', 'than', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', '(', 'NN', ')', ',', 'NNP', 'NNP', '(', 'NNP', ')', 'VBD', 'DT', 'JJR', 'NNP', 'NN', '(', 'CD', 'JJR', ')', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', '.']",31
relation-classification,7,144,"Also , BioBERT achieved the highest F 1 scores on 2 out of 3 biomedical datasets .","['Also', ',', 'BioBERT', 'achieved', 'the', 'highest', 'F', '1', 'scores', 'on', '2', 'out', 'of', '3', 'biomedical', 'datasets', '.']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'NNP', 'VBD', 'DT', 'JJS', 'JJ', 'CD', 'NNS', 'IN', 'CD', 'IN', 'IN', 'CD', 'JJ', 'NNS', '.']",17
relation-classification,7,145,The QA results are shown in .,"['The', 'QA', 'results', 'are', 'shown', 'in', '.']","['O', 'B-n', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NNS', 'VBP', 'VBN', 'IN', '.']",7
relation-classification,7,148,"All versions of BioBERT significantly outperformed BERT and the state - of - the - art models , and in particular , BioBERT v1.1 ( PubMed ) obtained a strict accuracy of 38.77 , a lenient accuracy of 53.81 and a mean reciprocal rank score of 44.77 , all of which were micro averaged .","['All', 'versions', 'of', 'BioBERT', 'significantly', 'outperformed', 'BERT', 'and', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', ',', 'and', 'in', 'particular', ',', 'BioBERT', 'v1.1', '(', 'PubMed', ')', 'obtained', 'a', 'strict', 'accuracy', 'of', '38.77', ',', 'a', 'lenient', 'accuracy', 'of', '53.81', 'and', 'a', 'mean', 'reciprocal', 'rank', 'score', 'of', '44.77', ',', 'all', 'of', 'which', 'were', 'micro', 'averaged', '.']","['O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'NNP', 'RB', 'VBD', 'NNP', 'CC', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', ',', 'CC', 'IN', 'JJ', ',', 'NNP', 'NNP', '(', 'NNP', ')', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'CD', ',', 'DT', 'JJ', 'NN', 'IN', 'CD', 'CC', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'CD', ',', 'DT', 'IN', 'WDT', 'VBD', 'NNS', 'VBN', '.']",55
relation-classification,7,149,"On all the biomedical QA datasets , BioBERT achieved new state - of - the - art performance in terms of MRR .","['On', 'all', 'the', 'biomedical', 'QA', 'datasets', ',', 'BioBERT', 'achieved', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'in', 'terms', 'of', 'MRR', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['IN', 'PDT', 'DT', 'JJ', 'NNP', 'NNS', ',', 'NNP', 'VBD', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'NNS', 'IN', 'NNP', '.']",23
relation-classification,4,2,Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,"['Graph', 'Convolution', 'over', 'Pruned', 'Dependency', 'Trees', 'Improves', 'Relation', 'Extraction']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'VBZ', 'NNP', 'NNP']",9
relation-classification,4,4,Dependency trees help relation extraction models capture long - range relations between words .,"['Dependency', 'trees', 'help', 'relation', 'extraction', 'models', 'capture', 'long', '-', 'range', 'relations', 'between', 'words', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'NNS', 'VBP', 'NN', 'NN', 'NNS', 'VBP', 'JJ', ':', 'NN', 'NNS', 'IN', 'NNS', '.']",14
relation-classification,4,18,"have proven to be very effective in relation extraction , because they capture long - range syntactic relations that are obscure from the surface form alone ( e.g. , when long clauses or complex scoping are present ) .","['have', 'proven', 'to', 'be', 'very', 'effective', 'in', 'relation', 'extraction', ',', 'because', 'they', 'capture', 'long', '-', 'range', 'syntactic', 'relations', 'that', 'are', 'obscure', 'from', 'the', 'surface', 'form', 'alone', '(', 'e.g.', ',', 'when', 'long', 'clauses', 'or', 'complex', 'scoping', 'are', 'present', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VB', 'VBN', 'TO', 'VB', 'RB', 'JJ', 'IN', 'NN', 'NN', ',', 'IN', 'PRP', 'VBP', 'JJ', ':', 'NN', 'JJ', 'NNS', 'WDT', 'VBP', 'JJ', 'IN', 'DT', 'NN', 'NN', 'RB', '(', 'UH', ',', 'WRB', 'JJ', 'NNS', 'CC', 'JJ', 'NN', 'VBP', 'JJ', ')', '.']",39
relation-classification,4,29,"Our model encodes the dependency structure over the input sentence with efficient graph convolution operations , then extracts entity - centric representations to make robust relation predictions .","['Our', 'model', 'encodes', 'the', 'dependency', 'structure', 'over', 'the', 'input', 'sentence', 'with', 'efficient', 'graph', 'convolution', 'operations', ',', 'then', 'extracts', 'entity', '-', 'centric', 'representations', 'to', 'make', 'robust', 'relation', 'predictions', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', 'NNS', ',', 'RB', 'VBZ', 'NN', ':', 'JJ', 'NNS', 'TO', 'VB', 'JJ', 'NN', 'NNS', '.']",28
relation-classification,4,30,"We also apply a novel path - centric pruning technique to remove irrelevant information from the tree while maximally keeping relevant content , which further improves the performance of several dependencybased models including ours .","['We', 'also', 'apply', 'a', 'novel', 'path', '-', 'centric', 'pruning', 'technique', 'to', 'remove', 'irrelevant', 'information', 'from', 'the', 'tree', 'while', 'maximally', 'keeping', 'relevant', 'content', ',', 'which', 'further', 'improves', 'the', 'performance', 'of', 'several', 'dependencybased', 'models', 'including', 'ours', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'JJ', 'NN', ':', 'JJ', 'NN', 'NN', 'TO', 'VB', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'RB', 'VBG', 'JJ', 'NN', ',', 'WDT', 'RBR', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'JJ', 'NNS', 'VBG', 'NNS', '.']",35
relation-classification,4,124,Dependency - based models .,"['Dependency', '-', 'based', 'models', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'VBN', 'NNS', '.']",5
relation-classification,4,126,( 1 ) A logistic regression ( LR ) classifier which combines dependencybased features with other lexical features .,"['(', '1', ')', 'A', 'logistic', 'regression', '(', 'LR', ')', 'classifier', 'which', 'combines', 'dependencybased', 'features', 'with', 'other', 'lexical', 'features', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'NNP', 'JJ', 'NN', '(', 'NNP', ')', 'NN', 'WDT', 'NNS', 'VBD', 'NNS', 'IN', 'JJ', 'JJ', 'NNS', '.']",19
relation-classification,4,127,"( 2 ) Shortest Dependency Path LSTM ( SDP - LSTM ) , which applies a neural sequence model on the shortest path between the subject and object entities in the dependency tree .","['(', '2', ')', 'Shortest', 'Dependency', 'Path', 'LSTM', '(', 'SDP', '-', 'LSTM', ')', ',', 'which', 'applies', 'a', 'neural', 'sequence', 'model', 'on', 'the', 'shortest', 'path', 'between', 'the', 'subject', 'and', 'object', 'entities', 'in', 'the', 'dependency', 'tree', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['(', 'CD', ')', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ':', 'NNP', ')', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NN', 'CC', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",34
relation-classification,4,128,"Tree - LSTM , which is a recursive model that generalizes the LSTM to arbitrary tree structures .","['Tree', '-', 'LSTM', ',', 'which', 'is', 'a', 'recursive', 'model', 'that', 'generalizes', 'the', 'LSTM', 'to', 'arbitrary', 'tree', 'structures', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'DT', 'NNP', 'TO', 'VB', 'JJ', 'NNS', '.']",18
relation-classification,4,132,Neural sequence model .,"['Neural', 'sequence', 'model', '.']","['B-n', 'I-n', 'I-n', 'O']","['JJ', 'NN', 'NN', '.']",4
relation-classification,4,133,"Our group presented a competitive sequence model that employs a position - aware attention mechanism over LSTM outputs ( PA - LSTM ) , and showed that it outperforms several CNN and dependency - based models by a substantial margin .","['Our', 'group', 'presented', 'a', 'competitive', 'sequence', 'model', 'that', 'employs', 'a', 'position', '-', 'aware', 'attention', 'mechanism', 'over', 'LSTM', 'outputs', '(', 'PA', '-', 'LSTM', ')', ',', 'and', 'showed', 'that', 'it', 'outperforms', 'several', 'CNN', 'and', 'dependency', '-', 'based', 'models', 'by', 'a', 'substantial', 'margin', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBD', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'NN', ':', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'NNS', '(', 'NNP', ':', 'NNP', ')', ',', 'CC', 'VBD', 'IN', 'PRP', 'VBZ', 'JJ', 'NNP', 'CC', 'NN', ':', 'VBN', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",41
relation-classification,4,149,Results on the TACRED Dataset,"['Results', 'on', 'the', 'TACRED', 'Dataset']","['O', 'B-p', 'O', 'B-n', 'I-n']","['NNS', 'IN', 'DT', 'NNP', 'NNP']",5
relation-classification,4,151,We observe that our GCN model Our Model ( C - GCN ) 84.8 * 76.5 * outperforms all dependency - based models by at least 1.6 F 1 .,"['We', 'observe', 'that', 'our', 'GCN', 'model', 'Our', 'Model', '(', 'C', '-', 'GCN', ')', '84.8', '*', '76.5', '*', 'outperforms', 'all', 'dependency', '-', 'based', 'models', 'by', 'at', 'least', '1.6', 'F', '1', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'PRP$', 'NNP', 'NN', 'PRP$', 'NNP', '(', 'NNP', ':', 'NNP', ')', 'CD', '$', 'CD', 'NN', 'NNS', 'DT', 'NN', ':', 'VBN', 'NNS', 'IN', 'IN', 'JJS', 'CD', 'NNP', 'CD', '.']",30
relation-classification,4,152,"By using contextualized word representations , the C - GCN model further outperforms the strong PA - LSTM model by 1.3 F 1 , and achieves a new state of the art .","['By', 'using', 'contextualized', 'word', 'representations', ',', 'the', 'C', '-', 'GCN', 'model', 'further', 'outperforms', 'the', 'strong', 'PA', '-', 'LSTM', 'model', 'by', '1.3', 'F', '1', ',', 'and', 'achieves', 'a', 'new', 'state', 'of', 'the', 'art', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBG', 'VBN', 'NN', 'NNS', ',', 'DT', 'NNP', ':', 'NNP', 'NN', 'RB', 'VBZ', 'DT', 'JJ', 'NNP', ':', 'NNP', 'NN', 'IN', 'CD', 'NNP', 'CD', ',', 'CC', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",33
relation-classification,4,153,"In addition , we find our model improves upon other dependencybased models in both precision and recall .","['In', 'addition', ',', 'we', 'find', 'our', 'model', 'improves', 'upon', 'other', 'dependencybased', 'models', 'in', 'both', 'precision', 'and', 'recall', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'PRP$', 'NN', 'VBZ', 'IN', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NN', '.']",18
relation-classification,4,154,"Comparing the C - GCN model with the GCN model , we find that the gain mainly comes from improved recall .","['Comparing', 'the', 'C', '-', 'GCN', 'model', 'with', 'the', 'GCN', 'model', ',', 'we', 'find', 'that', 'the', 'gain', 'mainly', 'comes', 'from', 'improved', 'recall', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['VBG', 'DT', 'NNP', ':', 'NNP', 'NN', 'IN', 'DT', 'NNP', 'NN', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'RB', 'VBZ', 'IN', 'VBN', 'NN', '.']",22
relation-classification,4,156,"As we will show in Section 6.2 , we find that our GCN models have complementary strengths when compared to the PA - LSTM .","['As', 'we', 'will', 'show', 'in', 'Section', '6.2', ',', 'we', 'find', 'that', 'our', 'GCN', 'models', 'have', 'complementary', 'strengths', 'when', 'compared', 'to', 'the', 'PA', '-', 'LSTM', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'PRP', 'MD', 'VB', 'IN', 'NNP', 'CD', ',', 'PRP', 'VBP', 'IN', 'PRP$', 'NNP', 'NNS', 'VBP', 'JJ', 'NNS', 'WRB', 'VBN', 'TO', 'DT', 'NNP', ':', 'NN', '.']",25
relation-classification,4,163,Results on the SemEval Dataset,"['Results', 'on', 'the', 'SemEval', 'Dataset']","['O', 'O', 'O', 'B-n', 'I-n']","['NNS', 'IN', 'DT', 'NNP', 'NNP']",5
relation-classification,4,165,"We find that under the conventional with- entity evaluation , our C - GCN model outperforms all existing dependency - based neural models on this sep - arate dataset .","['We', 'find', 'that', 'under', 'the', 'conventional', 'with-', 'entity', 'evaluation', ',', 'our', 'C', '-', 'GCN', 'model', 'outperforms', 'all', 'existing', 'dependency', '-', 'based', 'neural', 'models', 'on', 'this', 'sep', '-', 'arate', 'dataset', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'NN', ',', 'PRP$', 'NNP', ':', 'NNP', 'NN', 'VBZ', 'DT', 'VBG', 'NN', ':', 'VBN', 'JJ', 'NNS', 'IN', 'DT', 'JJ', ':', 'NN', 'NN', '.']",30
relation-classification,4,166,"Notably , by properly incorporating off - path information , our model outperforms the previous shortest dependency path - based model ( SDP - LSTM ) .","['Notably', ',', 'by', 'properly', 'incorporating', 'off', '-', 'path', 'information', ',', 'our', 'model', 'outperforms', 'the', 'previous', 'shortest', 'dependency', 'path', '-', 'based', 'model', '(', 'SDP', '-', 'LSTM', ')', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'IN', 'RB', 'VBG', 'RP', ':', 'NN', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'NN', ':', 'VBN', 'NN', '(', 'NNP', ':', 'NN', ')', '.']",27
relation-classification,4,167,"Under the mask - entity evaluation , our C - GCN model also outperforms PA - LSTM by a substantial margin , suggesting its generalizability even when entities are not seen .","['Under', 'the', 'mask', '-', 'entity', 'evaluation', ',', 'our', 'C', '-', 'GCN', 'model', 'also', 'outperforms', 'PA', '-', 'LSTM', 'by', 'a', 'substantial', 'margin', ',', 'suggesting', 'its', 'generalizability', 'even', 'when', 'entities', 'are', 'not', 'seen', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ':', 'NN', 'NN', ',', 'PRP$', 'NNP', ':', 'NNP', 'NN', 'RB', 'VBZ', 'NNP', ':', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'VBG', 'PRP$', 'NN', 'RB', 'WRB', 'NNS', 'VBP', 'RB', 'VBN', '.']",32
relation-classification,4,168,Effect of Path - centric Pruning,"['Effect', 'of', 'Path', '-', 'centric', 'Pruning']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NN', 'IN', 'NNP', ':', 'NN', 'VBG']",6
relation-classification,4,169,"To show the effectiveness of path - centric pruning , we compare the two GCN models and the Tree - LSTM when the pruning distance K is varied .","['To', 'show', 'the', 'effectiveness', 'of', 'path', '-', 'centric', 'pruning', ',', 'we', 'compare', 'the', 'two', 'GCN', 'models', 'and', 'the', 'Tree', '-', 'LSTM', 'when', 'the', 'pruning', 'distance', 'K', 'is', 'varied', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['TO', 'VB', 'DT', 'NN', 'IN', 'NN', ':', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'CD', 'NNP', 'NNS', 'CC', 'DT', 'NNP', ':', 'NN', 'WRB', 'DT', 'NN', 'NN', 'NNP', 'VBZ', 'VBN', '.']",29
relation-classification,4,171,"As shown in , the performance of all three models peaks when K = 1 , outperforming their respective dependency path - based counterpart ( K = 0 ) .","['As', 'shown', 'in', ',', 'the', 'performance', 'of', 'all', 'three', 'models', 'peaks', 'when', 'K', '=', '1', ',', 'outperforming', 'their', 'respective', 'dependency', 'path', '-', 'based', 'counterpart', '(', 'K', '=', '0', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'IN', ',', 'DT', 'NN', 'IN', 'DT', 'CD', 'NNS', 'VBP', 'WRB', 'NNP', 'NNP', 'CD', ',', 'VBG', 'PRP$', 'JJ', 'NN', 'NN', ':', 'VBN', 'NN', '(', 'NNP', 'NNP', 'CD', ')', '.']",30
relation-classification,4,175,"We find that all three models are less effective when the entire dependency tree is present , indicating that including extra information hurts performance .","['We', 'find', 'that', 'all', 'three', 'models', 'are', 'less', 'effective', 'when', 'the', 'entire', 'dependency', 'tree', 'is', 'present', ',', 'indicating', 'that', 'including', 'extra', 'information', 'hurts', 'performance', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'DT', 'CD', 'NNS', 'VBP', 'RBR', 'JJ', 'WRB', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'JJ', ',', 'VBG', 'IN', 'VBG', 'JJ', 'NN', 'VBZ', 'NN', '.']",25
relation-classification,4,176,"Finally , we note that contextualizing the GCN makes it less sensitive to changes in the tree structures provided , presumably because the model can use word sequence information in the LSTM layer to recover any off - path information that it needs for correct relation extraction .","['Finally', ',', 'we', 'note', 'that', 'contextualizing', 'the', 'GCN', 'makes', 'it', 'less', 'sensitive', 'to', 'changes', 'in', 'the', 'tree', 'structures', 'provided', ',', 'presumably', 'because', 'the', 'model', 'can', 'use', 'word', 'sequence', 'information', 'in', 'the', 'LSTM', 'layer', 'to', 'recover', 'any', 'off', '-', 'path', 'information', 'that', 'it', 'needs', 'for', 'correct', 'relation', 'extraction', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'IN', 'VBG', 'DT', 'NNP', 'VBZ', 'PRP', 'RBR', 'JJ', 'TO', 'NNS', 'IN', 'DT', 'JJ', 'NNS', 'VBN', ',', 'RB', 'IN', 'DT', 'NN', 'MD', 'VB', 'NN', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', 'TO', 'VB', 'DT', 'RP', ':', 'NN', 'NN', 'IN', 'PRP', 'VBZ', 'IN', 'JJ', 'NN', 'NN', '.']",48
relation-classification,4,179,"To study the contribution of each component in the C - GCN model , we ran an ablation study on the TACRED dev set ) .","['To', 'study', 'the', 'contribution', 'of', 'each', 'component', 'in', 'the', 'C', '-', 'GCN', 'model', ',', 'we', 'ran', 'an', 'ablation', 'study', 'on', 'the', 'TACRED', 'dev', 'set', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNP', ':', 'NNP', 'NN', ',', 'PRP', 'VBD', 'DT', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NN', ')', '.']",26
relation-classification,4,180,We find that : The entity representations and feedforward layers contribute 1.0 F 1 .,"['We', 'find', 'that', ':', 'The', 'entity', 'representations', 'and', 'feedforward', 'layers', 'contribute', '1.0', 'F', '1', '.']","['O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', ':', 'DT', 'NN', 'NNS', 'CC', 'JJ', 'NNS', 'VBP', 'CD', 'NNP', 'CD', '.']",15
relation-classification,4,181,"( 2 ) When we remove the dependency structure ( i.e. , setting to I ) , the score drops by 3.2 F 1 .","['(', '2', ')', 'When', 'we', 'remove', 'the', 'dependency', 'structure', '(', 'i.e.', ',', 'setting', 'to', 'I', ')', ',', 'the', 'score', 'drops', 'by', '3.2', 'F', '1', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'WRB', 'PRP', 'VBP', 'DT', 'NN', 'NN', '(', 'JJ', ',', 'VBG', 'TO', 'PRP', ')', ',', 'DT', 'NN', 'NNS', 'IN', 'CD', 'NNP', 'CD', '.']",25
relation-classification,4,182,"( 3 ) F 1 drops by 10.3 when we remove the feedforward layers , the LSTM component and the dependency structure altogether .","['(', '3', ')', 'F', '1', 'drops', 'by', '10.3', 'when', 'we', 'remove', 'the', 'feedforward', 'layers', ',', 'the', 'LSTM', 'component', 'and', 'the', 'dependency', 'structure', 'altogether', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'NNP', 'CD', 'NNS', 'IN', 'CD', 'WRB', 'PRP', 'VBP', 'DT', 'NN', 'NNS', ',', 'DT', 'NNP', 'NN', 'CC', 'DT', 'NN', 'NN', 'RB', '.']",24
relation-classification,4,183,"( 4 ) Removing the pruning ( i.e. , using full trees as input ) further hurts the result by another 9.7 F 1 .","['(', '4', ')', 'Removing', 'the', 'pruning', '(', 'i.e.', ',', 'using', 'full', 'trees', 'as', 'input', ')', 'further', 'hurts', 'the', 'result', 'by', 'another', '9.7', 'F', '1', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'VBG', 'DT', 'NN', '(', 'FW', ',', 'VBG', 'JJ', 'NNS', 'IN', 'NN', ')', 'RBR', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'CD', 'NNP', 'CD', '.']",25
relation-classification,6,2,Semantic Relation Classification via Bidirectional LSTM Networks with Entity - aware Attention using Latent Entity Typing,"['Semantic', 'Relation', 'Classification', 'via', 'Bidirectional', 'LSTM', 'Networks', 'with', 'Entity', '-', 'aware', 'Attention', 'using', 'Latent', 'Entity', 'Typing']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', ':', 'JJ', 'NNP', 'VBG', 'NNP', 'NNP', 'NNP']",16
relation-classification,6,4,Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) .,"['Classifying', 'semantic', 'relations', 'between', 'entity', 'pairs', 'in', 'sentences', 'is', 'an', 'important', 'task', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', 'NNS', 'IN', 'NN', 'NNS', 'IN', 'NNS', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', '.']",20
relation-classification,6,12,A task of relation classification is defined as predicting a semantic relationship between two tagged entities in a sentence .,"['A', 'task', 'of', 'relation', 'classification', 'is', 'defined', 'as', 'predicting', 'a', 'semantic', 'relationship', 'between', 'two', 'tagged', 'entities', 'in', 'a', 'sentence', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'CD', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']",20
relation-classification,6,48,"In this section , we introduce a novel recurrent neural model that incorporate an entity - aware attention mechanism with a LET method in detail .","['In', 'this', 'section', ',', 'we', 'introduce', 'a', 'novel', 'recurrent', 'neural', 'model', 'that', 'incorporate', 'an', 'entity', '-', 'aware', 'attention', 'mechanism', 'with', 'a', 'LET', 'method', 'in', 'detail', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'JJ', 'NN', 'WDT', 'VBP', 'DT', 'NN', ':', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'NN', '.']",26
relation-classification,6,49,"As shown inure 2 , our model consists of four main components : Word Representation that maps each word in a sentence into vector representations ; ( 2 ) Self Attention that captures the meaning of the correlation between words based on multi-head attention ; ( 3 ) BLSTM which sequentially encodes the representations of self attention layer ; ( 4 ) Entity - aware Attention that calculates attention weights with respect to the entity pairs , word positions relative to these pairs , and their latent types obtained by LET .","['As', 'shown', 'inure', '2', ',', 'our', 'model', 'consists', 'of', 'four', 'main', 'components', ':', 'Word', 'Representation', 'that', 'maps', 'each', 'word', 'in', 'a', 'sentence', 'into', 'vector', 'representations', ';', '(', '2', ')', 'Self', 'Attention', 'that', 'captures', 'the', 'meaning', 'of', 'the', 'correlation', 'between', 'words', 'based', 'on', 'multi-head', 'attention', ';', '(', '3', ')', 'BLSTM', 'which', 'sequentially', 'encodes', 'the', 'representations', 'of', 'self', 'attention', 'layer', ';', '(', '4', ')', 'Entity', '-', 'aware', 'Attention', 'that', 'calculates', 'attention', 'weights', 'with', 'respect', 'to', 'the', 'entity', 'pairs', ',', 'word', 'positions', 'relative', 'to', 'these', 'pairs', ',', 'and', 'their', 'latent', 'types', 'obtained', 'by', 'LET', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'NN', 'CD', ',', 'PRP$', 'NN', 'VBZ', 'IN', 'CD', 'JJ', 'NNS', ':', 'NNP', 'NNP', 'IN', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', ':', '(', 'CD', ')', 'NNP', 'NNP', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNS', 'VBN', 'IN', 'JJ', 'NN', ':', '(', 'CD', ')', 'NNP', 'WDT', 'RB', 'VBZ', 'DT', 'NNS', 'IN', 'JJ', 'NN', 'NN', ':', '(', 'CD', ')', 'NNP', ':', 'JJ', 'NNP', 'IN', 'VBZ', 'NN', 'NNS', 'IN', 'NN', 'TO', 'DT', 'NN', 'NNS', ',', 'NN', 'NNS', 'VBP', 'TO', 'DT', 'NNS', ',', 'CC', 'PRP$', 'JJ', 'NNS', 'VBN', 'IN', 'NNP', '.']",92
relation-classification,6,160,"Non Our proposed model achieves an F1-score of 85.2 % which outperforms all competing state - of - theart approaches except depLCNN + NS , DRNNs , and Attention - CNN .","['Non', 'Our', 'proposed', 'model', 'achieves', 'an', 'F1-score', 'of', '85.2', '%', 'which', 'outperforms', 'all', 'competing', 'state', '-', 'of', '-', 'theart', 'approaches', 'except', 'depLCNN', '+', 'NS', ',', 'DRNNs', ',', 'and', 'Attention', '-', 'CNN', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'PRP$', 'VBN', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', 'WDT', 'VBZ', 'DT', 'VBG', 'NN', ':', 'IN', ':', 'NN', 'VBZ', 'IN', 'NN', 'NNP', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', ':', 'NN', '.']",32
relation-classification,2,2,Joint entity recognition and relation extraction as a multi-head selection problem,"['Joint', 'entity', 'recognition', 'and', 'relation', 'extraction', 'as', 'a', 'multi-head', 'selection', 'problem']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', 'CC', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN']",11
relation-classification,2,7,"In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'joint', 'neural', 'model', 'which', 'performs', 'entity', 'recognition', 'and', 'relation', 'extraction', 'simultaneously', ',', 'without', 'the', 'need', 'of', 'any', 'manually', 'extracted', 'features', 'or', 'the', 'use', 'of', 'any', 'external', 'tool', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'WDT', 'VBZ', 'NN', 'NN', 'CC', 'NN', 'NN', 'RB', ',', 'IN', 'DT', 'NN', 'IN', 'DT', 'RB', 'JJ', 'NNS', 'CC', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",35
relation-classification,2,16,"On the other hand , more recent studies propose to use joint models to detect entities and their relations overcoming the aforementioned issues and achieving state - of - the - art performance .","['On', 'the', 'other', 'hand', ',', 'more', 'recent', 'studies', 'propose', 'to', 'use', 'joint', 'models', 'to', 'detect', 'entities', 'and', 'their', 'relations', 'overcoming', 'the', 'aforementioned', 'issues', 'and', 'achieving', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'RBR', 'JJ', 'NNS', 'VBP', 'TO', 'VB', 'JJ', 'NNS', 'TO', 'VB', 'NNS', 'CC', 'PRP$', 'NNS', 'VBG', 'DT', 'JJ', 'NNS', 'CC', 'VBG', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', '.']",34
relation-classification,2,28,"In this work , we focus on a new general purpose joint model that performs the two tasks of entity recognition and relation extraction simultaneously , and that can handle multiple relations together .","['In', 'this', 'work', ',', 'we', 'focus', 'on', 'a', 'new', 'general', 'purpose', 'joint', 'model', 'that', 'performs', 'the', 'two', 'tasks', 'of', 'entity', 'recognition', 'and', 'relation', 'extraction', 'simultaneously', ',', 'and', 'that', 'can', 'handle', 'multiple', 'relations', 'together', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'CD', 'NNS', 'IN', 'NN', 'NN', 'CC', 'NN', 'NN', 'RB', ',', 'CC', 'IN', 'MD', 'VB', 'JJ', 'NNS', 'RB', '.']",34
relation-classification,2,107,"In this section , we present our multi-head joint model illustrated in .","['In', 'this', 'section', ',', 'we', 'present', 'our', 'multi-head', 'joint', 'model', 'illustrated', 'in', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'PRP$', 'JJ', 'JJ', 'NN', 'VBN', 'IN', '.']",13
relation-classification,2,111,"The input of our model is a sequence of tokens ( i.e. , words of the sentence ) which are then represented as word vectors ( i.e. , word embeddings ) .","['The', 'input', 'of', 'our', 'model', 'is', 'a', 'sequence', 'of', 'tokens', '(', 'i.e.', ',', 'words', 'of', 'the', 'sentence', ')', 'which', 'are', 'then', 'represented', 'as', 'word', 'vectors', '(', 'i.e.', ',', 'word', 'embeddings', ')', '.']","['O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'PRP$', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'NNS', '(', 'FW', ',', 'NNS', 'IN', 'DT', 'NN', ')', 'WDT', 'VBP', 'RB', 'VBN', 'IN', 'NN', 'NNS', '(', 'FW', ',', 'NN', 'NNS', ')', '.']",32
relation-classification,2,114,"The outputs for each token ( e.g. , Smith ) are twofold : ( i ) an entity recognition label ( e.g. , I - PER , denoting the token is inside a named entity of type PER ) and ( ii ) a set of tuples comprising the head tokens of the entity and the types of relations between them ( e.g. , {( Center , Works for ) , ( Atlanta , Lives in ) } ) .","['The', 'outputs', 'for', 'each', 'token', '(', 'e.g.', ',', 'Smith', ')', 'are', 'twofold', ':', '(', 'i', ')', 'an', 'entity', 'recognition', 'label', '(', 'e.g.', ',', 'I', '-', 'PER', ',', 'denoting', 'the', 'token', 'is', 'inside', 'a', 'named', 'entity', 'of', 'type', 'PER', ')', 'and', '(', 'ii', ')', 'a', 'set', 'of', 'tuples', 'comprising', 'the', 'head', 'tokens', 'of', 'the', 'entity', 'and', 'the', 'types', 'of', 'relations', 'between', 'them', '(', 'e.g.', ',', '{(', 'Center', ',', 'Works', 'for', ')', ',', '(', 'Atlanta', ',', 'Lives', 'in', ')', '}', ')', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'DT', 'NN', '(', 'JJ', ',', 'NNP', ')', 'VBP', 'VBN', ':', '(', 'NN', ')', 'DT', 'NN', 'NN', 'NN', '(', 'JJ', ',', 'PRP', ':', 'NN', ',', 'VBG', 'DT', 'NN', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNP', ')', 'CC', '(', 'NN', ')', 'DT', 'NN', 'IN', 'NNS', 'VBG', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'DT', 'NNS', 'IN', 'NNS', 'IN', 'PRP', '(', 'NN', ',', 'NNP', 'NNP', ',', 'NNP', 'IN', ')', ',', '(', 'NNP', ',', 'NNP', 'IN', ')', ')', ')', '.']",80
relation-classification,2,208,We have developed our joint model by using Python with the TensorFlow machine learning library .,"['We', 'have', 'developed', 'our', 'joint', 'model', 'by', 'using', 'Python', 'with', 'the', 'TensorFlow', 'machine', 'learning', 'library', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'VBN', 'PRP$', 'JJ', 'NN', 'IN', 'VBG', 'NNP', 'IN', 'DT', 'NNP', 'NN', 'VBG', 'JJ', '.']",16
relation-classification,2,209,"Training is performed using the Adam optimizer ( Kingma & Ba , 2015 ) with a learning rate of 10 ?3 .","['Training', 'is', 'performed', 'using', 'the', 'Adam', 'optimizer', '(', 'Kingma', '&', 'Ba', ',', '2015', ')', 'with', 'a', 'learning', 'rate', 'of', '10', '?3', '.']","['B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NN', 'VBZ', 'VBN', 'VBG', 'DT', 'NNP', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', 'NNS', '.']",22
relation-classification,2,210,We fix the size of the LSTM to d = 64 and the layer width of the neural network to l = 64 ( both for the entity and the relation scoring layers ) .,"['We', 'fix', 'the', 'size', 'of', 'the', 'LSTM', 'to', 'd', '=', '64', 'and', 'the', 'layer', 'width', 'of', 'the', 'neural', 'network', 'to', 'l', '=', '64', '(', 'both', 'for', 'the', 'entity', 'and', 'the', 'relation', 'scoring', 'layers', ')', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NNP', 'TO', 'VB', 'JJ', 'CD', 'CC', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'CD', '(', 'DT', 'IN', 'DT', 'NN', 'CC', 'DT', 'NN', 'VBG', 'NNS', ')', '.']",35
relation-classification,2,211,We use dropout to regularize our network .,"['We', 'use', 'dropout', 'to', 'regularize', 'our', 'network', '.']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'TO', 'VB', 'PRP$', 'NN', '.']",8
relation-classification,2,214,The hidden dimension for the characterbased LSTMs is 25 ( for each direction ) .,"['The', 'hidden', 'dimension', 'for', 'the', 'characterbased', 'LSTMs', 'is', '25', '(', 'for', 'each', 'direction', ')', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O']","['DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'VBZ', 'CD', '(', 'IN', 'DT', 'NN', ')', '.']",15
relation-classification,2,217,We employ the technique of early stopping based on the validation set .,"['We', 'employ', 'the', 'technique', 'of', 'early', 'stopping', 'based', 'on', 'the', 'validation', 'set', '.']","['O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'NN', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",13
relation-classification,2,218,"In all the datasets examined in this study , we obtain the best hyperparameters after 60 to 200 epochs depending on the size of the dataset .","['In', 'all', 'the', 'datasets', 'examined', 'in', 'this', 'study', ',', 'we', 'obtain', 'the', 'best', 'hyperparameters', 'after', '60', 'to', '200', 'epochs', 'depending', 'on', 'the', 'size', 'of', 'the', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'PDT', 'DT', 'NNS', 'VBN', 'IN', 'DT', 'NN', ',', 'PRP', 'VB', 'DT', 'JJS', 'NNS', 'IN', 'CD', 'TO', 'CD', 'NNS', 'VBG', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",27
relation-classification,2,239,We observe that our model outperforms all previous models that do not rely on complex hand - crafted features by a large margin ( > 4 % for both tasks ) .,"['We', 'observe', 'that', 'our', 'model', 'outperforms', 'all', 'previous', 'models', 'that', 'do', 'not', 'rely', 'on', 'complex', 'hand', '-', 'crafted', 'features', 'by', 'a', 'large', 'margin', '(', '>', '4', '%', 'for', 'both', 'tasks', ')', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBP', 'IN', 'PRP$', 'NN', 'VBZ', 'DT', 'JJ', 'NNS', 'WDT', 'VBP', 'RB', 'VB', 'IN', 'JJ', 'NN', ':', 'VBN', 'NNS', 'IN', 'DT', 'JJ', 'NN', '(', 'JJ', 'CD', 'NN', 'IN', 'DT', 'NNS', ')', '.']",32
relation-classification,2,249,"We also report results for the DREC dataset , with two different evaluation settings .","['We', 'also', 'report', 'results', 'for', 'the', 'DREC', 'dataset', ',', 'with', 'two', 'different', 'evaluation', 'settings', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'NNS', 'IN', 'DT', 'NNP', 'NN', ',', 'IN', 'CD', 'JJ', 'NN', 'NNS', '.']",15
relation-classification,2,250,"Specifically , we use the boundaries and the strict settings .","['Specifically', ',', 'we', 'use', 'the', 'boundaries', 'and', 'the', 'strict', 'settings', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NNS', 'CC', 'DT', 'JJ', 'NNS', '.']",11
relation-classification,2,253,"In the boundaries evaluation , we achieve ? 3 % improvement for both tasks .","['In', 'the', 'boundaries', 'evaluation', ',', 'we', 'achieve', '?', '3', '%', 'improvement', 'for', 'both', 'tasks', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNS', 'NN', ',', 'PRP', 'VBP', '.', 'CD', 'NN', 'NN', 'IN', 'DT', 'NNS', '.']",15
relation-classification,2,261,We conduct ablation tests on the ACE04 dataset reported in to analyze the effectiveness of the various parts of our joint model .,"['We', 'conduct', 'ablation', 'tests', 'on', 'the', 'ACE04', 'dataset', 'reported', 'in', 'to', 'analyze', 'the', 'effectiveness', 'of', 'the', 'various', 'parts', 'of', 'our', 'joint', 'model', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'VBD', 'IN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'PRP$', 'JJ', 'NN', '.']",23
relation-classification,2,262,The performance of the RE task decreases ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and only use the LSTM hidden states as inputs for the RE task .,"['The', 'performance', 'of', 'the', 'RE', 'task', 'decreases', '(', '?', '1', '%', 'in', 'terms', 'of', 'F', '1', 'score', ')', 'when', 'we', 'remove', 'the', 'label', 'embeddings', 'layer', 'and', 'only', 'use', 'the', 'LSTM', 'hidden', 'states', 'as', 'inputs', 'for', 'the', 'RE', 'task', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NNS', '(', '.', 'CD', 'NN', 'IN', 'NNS', 'IN', 'NNP', 'CD', 'NN', ')', 'WRB', 'PRP', 'VBP', 'DT', 'NN', 'NNS', 'NN', 'CC', 'RB', 'VB', 'DT', 'NNP', 'JJ', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'NNP', 'NN', '.']",39
relation-classification,2,264,Removing character embeddings also degrades the performance of both NER ( ? 1 % ) and RE ( ? 2 % ) tasks by a relatively large margin .,"['Removing', 'character', 'embeddings', 'also', 'degrades', 'the', 'performance', 'of', 'both', 'NER', '(', '?', '1', '%', ')', 'and', 'RE', '(', '?', '2', '%', ')', 'tasks', 'by', 'a', 'relatively', 'large', 'margin', '.']","['B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['VBG', 'NN', 'NNS', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NNP', '(', '.', 'CD', 'NN', ')', 'CC', 'NNP', '(', '.', 'CD', 'NN', ')', 'NNS', 'IN', 'DT', 'RB', 'JJ', 'NN', '.']",29
relation-classification,2,266,"Finally , we conduct experiments for the NER task by removing the CRF loss layer and substituting it with a softmax .","['Finally', ',', 'we', 'conduct', 'experiments', 'for', 'the', 'NER', 'task', 'by', 'removing', 'the', 'CRF', 'loss', 'layer', 'and', 'substituting', 'it', 'with', 'a', 'softmax', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['RB', ',', 'PRP', 'VBP', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'IN', 'VBG', 'DT', 'NNP', 'NN', 'NN', 'CC', 'VBG', 'PRP', 'IN', 'DT', 'NN', '.']",22
relation-classification,2,267,"Assuming independent distribution of labels ( i.e. , softmax ) leads to a slight decrease in the F 1 performance of the NER module and a ? 2 % decrease in the performance of the RE task .","['Assuming', 'independent', 'distribution', 'of', 'labels', '(', 'i.e.', ',', 'softmax', ')', 'leads', 'to', 'a', 'slight', 'decrease', 'in', 'the', 'F', '1', 'performance', 'of', 'the', 'NER', 'module', 'and', 'a', '?', '2', '%', 'decrease', 'in', 'the', 'performance', 'of', 'the', 'RE', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['VBG', 'JJ', 'NN', 'IN', 'NNS', '(', 'FW', ',', 'NN', ')', 'VBZ', 'TO', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'CD', 'NN', 'IN', 'DT', 'NNP', 'NN', 'CC', 'DT', '.', 'CD', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",38
relation-classification,3,2,Adversarial training for multi-context joint entity and relation extraction,"['Adversarial', 'training', 'for', 'multi-context', 'joint', 'entity', 'and', 'relation', 'extraction']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['JJ', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'CC', 'NN', 'NN']",9
relation-classification,3,5,We show how to use AT for the tasks of entity recognition and relation extraction .,"['We', 'show', 'how', 'to', 'use', 'AT', 'for', 'the', 'tasks', 'of', 'entity', 'recognition', 'and', 'relation', 'extraction', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'WRB', 'TO', 'VB', 'NNP', 'IN', 'DT', 'NNS', 'IN', 'NN', 'NN', 'CC', 'NN', 'NN', '.']",16
relation-classification,3,6,"In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .","['In', 'particular', ',', 'we', 'demonstrate', 'that', 'applying', 'AT', 'to', 'a', 'general', 'purpose', 'baseline', 'model', 'for', 'jointly', 'extracting', 'entities', 'and', 'relations', ',', 'allows', 'improving', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'effectiveness', 'on', 'several', 'datasets', 'in', 'different', 'contexts', '(', 'i.e.', ',', 'news', ',', 'biomedical', ',', 'and', 'real', 'estate', 'data', ')', 'and', 'for', 'different', 'languages', '(', 'English', 'and', 'Dutch', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', ',', 'PRP', 'VBP', 'IN', 'VBG', 'NNP', 'TO', 'DT', 'JJ', 'NN', 'NN', 'NN', 'IN', 'RB', 'VBG', 'NNS', 'CC', 'NNS', ',', 'VBZ', 'VBG', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NN', '(', 'FW', ',', 'NN', ',', 'JJ', ',', 'CC', 'JJ', 'NN', 'NNS', ')', 'CC', 'IN', 'JJ', 'NNS', '(', 'JJ', 'CC', 'NNP', ')', '.']",60
relation-classification,3,36,"The baseline model , described in detail in , is illustrated in .","['The', 'baseline', 'model', ',', 'described', 'in', 'detail', 'in', ',', 'is', 'illustrated', 'in', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', ',', 'VBN', 'IN', 'NN', 'IN', ',', 'VBZ', 'VBN', 'IN', '.']",13
relation-classification,3,37,It aims to detect ( i ) the type and the boundaries of the entities and ( ii ) the relations between them .,"['It', 'aims', 'to', 'detect', '(', 'i', ')', 'the', 'type', 'and', 'the', 'boundaries', 'of', 'the', 'entities', 'and', '(', 'ii', ')', 'the', 'relations', 'between', 'them', '.']","['O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'TO', 'VB', '(', 'NN', ')', 'DT', 'NN', 'CC', 'DT', 'NNS', 'IN', 'DT', 'NNS', 'CC', '(', 'NN', ')', 'DT', 'NNS', 'IN', 'PRP', '.']",24
relation-classification,3,38,"The input is a sequence of tokens ( i.e. , sentence ) w = w 1 , ... , w n .","['The', 'input', 'is', 'a', 'sequence', 'of', 'tokens', '(', 'i.e.', ',', 'sentence', ')', 'w', '=', 'w', '1', ',', '...', ',', 'w', 'n', '.']","['O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'NNS', '(', 'FW', ',', 'NN', ')', 'NN', 'NNP', 'VBZ', 'CD', ',', ':', ',', 'JJ', 'NN', '.']",22
relation-classification,3,39,"We use character level embeddings to implicitly capture morphological features ( e.g. , prefixes and suffixes ) , representing each character by a vector ( embedding ) .","['We', 'use', 'character', 'level', 'embeddings', 'to', 'implicitly', 'capture', 'morphological', 'features', '(', 'e.g.', ',', 'prefixes', 'and', 'suffixes', ')', ',', 'representing', 'each', 'character', 'by', 'a', 'vector', '(', 'embedding', ')', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'NNS', 'TO', 'VB', 'JJ', 'JJ', 'NNS', '(', 'NN', ',', 'NNS', 'CC', 'NNS', ')', ',', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '(', 'VBG', ')', '.']",28
relation-classification,3,40,The character embeddings are fed to a bidirectional LSTM ( BiLSTM ) to obtain the character - based representation of the word .,"['The', 'character', 'embeddings', 'are', 'fed', 'to', 'a', 'bidirectional', 'LSTM', '(', 'BiLSTM', ')', 'to', 'obtain', 'the', 'character', '-', 'based', 'representation', 'of', 'the', 'word', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'DT', 'JJ', 'NNP', '(', 'NNP', ')', 'TO', 'VB', 'DT', 'NN', ':', 'VBN', 'NN', 'IN', 'DT', 'NN', '.']",23
relation-classification,3,41,We also use pre-trained word embeddings .,"['We', 'also', 'use', 'pre-trained', 'word', 'embeddings', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'JJ', 'NN', 'NNS', '.']",7
relation-classification,3,42,"Word and character embeddings are concatenated to form the final token representation , which is then fed to a BiLSTM layer to extract sequential information .","['Word', 'and', 'character', 'embeddings', 'are', 'concatenated', 'to', 'form', 'the', 'final', 'token', 'representation', ',', 'which', 'is', 'then', 'fed', 'to', 'a', 'BiLSTM', 'layer', 'to', 'extract', 'sequential', 'information', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['NNP', 'CC', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', ',', 'WDT', 'VBZ', 'RB', 'VBN', 'TO', 'DT', 'NNP', 'NN', 'TO', 'VB', 'JJ', 'NN', '.']",26
relation-classification,3,43,"For the NER task , we adopt the BIO ( Beginning , Inside , Outside ) encoding scheme .","['For', 'the', 'NER', 'task', ',', 'we', 'adopt', 'the', 'BIO', '(', 'Beginning', ',', 'Inside', ',', 'Outside', ')', 'encoding', 'scheme', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNP', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', '(', 'NNP', ',', 'NNP', ',', 'NNP', ')', 'VBG', 'NN', '.']",19
relation-classification,3,53,We model the relation extraction task as a multi-label head selection problem .,"['We', 'model', 'the', 'relation', 'extraction', 'task', 'as', 'a', 'multi-label', 'head', 'selection', 'problem', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', '.']",13
relation-classification,3,67,Adversarial training ( AT ),"['Adversarial', 'training', '(', 'AT', ')']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['JJ', 'NN', '(', 'NNP', ')']",5
relation-classification,3,68,We exploit the idea of AT as a regularization method to make our model robust to input perturbations .,"['We', 'exploit', 'the', 'idea', 'of', 'AT', 'as', 'a', 'regularization', 'method', 'to', 'make', 'our', 'model', 'robust', 'to', 'input', 'perturbations', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'NNP', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', 'PRP$', 'NN', 'NN', 'TO', 'VB', 'NNS', '.']",19
relation-classification,3,81,"We evaluate our models on four datasets , using the code as available from our github codebase .","['We', 'evaluate', 'our', 'models', 'on', 'four', 'datasets', ',', 'using', 'the', 'code', 'as', 'available', 'from', 'our', 'github', 'codebase', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'PRP$', 'NNS', 'IN', 'CD', 'NNS', ',', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'IN', 'PRP$', 'NN', 'NN', '.']",18
relation-classification,3,85,We also evaluate our models on the NER task similar to in the same dataset using 10 - fold cross validation .,"['We', 'also', 'evaluate', 'our', 'models', 'on', 'the', 'NER', 'task', 'similar', 'to', 'in', 'the', 'same', 'dataset', 'using', '10', '-', 'fold', 'cross', 'validation', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'PRP$', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'JJ', 'TO', 'IN', 'DT', 'JJ', 'NN', 'VBG', 'CD', ':', 'NN', 'NN', 'NN', '.']",22
relation-classification,3,89,We employ early stopping in all of the experiments .,"['We', 'employ', 'early', 'stopping', 'in', 'all', 'of', 'the', 'experiments', '.']","['O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'DT', 'IN', 'DT', 'NNS', '.']",10
relation-classification,3,90,"We use the Adam optimizer and we fix the hyperparameters ( i.e. , ? , dropout values , best epoch , learning rate ) on the validation sets .","['We', 'use', 'the', 'Adam', 'optimizer', 'and', 'we', 'fix', 'the', 'hyperparameters', '(', 'i.e.', ',', '?', ',', 'dropout', 'values', ',', 'best', 'epoch', ',', 'learning', 'rate', ')', 'on', 'the', 'validation', 'sets', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', 'CC', 'PRP', 'VBP', 'DT', 'NNS', '(', 'NN', ',', '.', ',', 'NN', 'NNS', ',', 'JJS', 'NN', ',', 'VBG', 'NN', ')', 'IN', 'DT', 'NN', 'NNS', '.']",29
relation-classification,3,95,"We use three types of evaluation , namely : ( i ) S( trict ) : we score an entity as correct if both the entity boundaries and the entity type are correct ( ACE04 , ADE , CoNLL04 , DREC ) , ( ii ) B ( oundaries ) : we score an entity as correct if only the entity boundaries are correct while the entity type is not taken into account ( DREC ) and ( iii ) R( elaxed ) : a multi-token entity is considered correct if at least one correct type is assigned to the tokens comprising the entity , assuming that the : Comparison of our method with the stateof - the - art in terms of F 1 score .","['We', 'use', 'three', 'types', 'of', 'evaluation', ',', 'namely', ':', '(', 'i', ')', 'S(', 'trict', ')', ':', 'we', 'score', 'an', 'entity', 'as', 'correct', 'if', 'both', 'the', 'entity', 'boundaries', 'and', 'the', 'entity', 'type', 'are', 'correct', '(', 'ACE04', ',', 'ADE', ',', 'CoNLL04', ',', 'DREC', ')', ',', '(', 'ii', ')', 'B', '(', 'oundaries', ')', ':', 'we', 'score', 'an', 'entity', 'as', 'correct', 'if', 'only', 'the', 'entity', 'boundaries', 'are', 'correct', 'while', 'the', 'entity', 'type', 'is', 'not', 'taken', 'into', 'account', '(', 'DREC', ')', 'and', '(', 'iii', ')', 'R(', 'elaxed', ')', ':', 'a', 'multi-token', 'entity', 'is', 'considered', 'correct', 'if', 'at', 'least', 'one', 'correct', 'type', 'is', 'assigned', 'to', 'the', 'tokens', 'comprising', 'the', 'entity', ',', 'assuming', 'that', 'the', ':', 'Comparison', 'of', 'our', 'method', 'with', 'the', 'stateof', '-', 'the', '-', 'art', 'in', 'terms', 'of', 'F', '1', 'score', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'CD', 'NNS', 'IN', 'NN', ',', 'RB', ':', '(', 'NN', ')', 'NNP', 'NN', ')', ':', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'IN', 'DT', 'DT', 'NN', 'NNS', 'CC', 'DT', 'NN', 'NN', 'VBP', 'JJ', '(', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', ')', ',', '(', 'NN', ')', 'NNP', '(', 'NNS', ')', ':', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'IN', 'RB', 'DT', 'NN', 'NNS', 'VBP', 'JJ', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'NN', '(', 'NNP', ')', 'CC', '(', 'NN', ')', 'NNP', 'VBN', ')', ':', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'JJ', 'IN', 'IN', 'JJS', 'CD', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'DT', 'NNS', 'VBG', 'DT', 'NN', ',', 'VBG', 'IN', 'DT', ':', 'NN', 'IN', 'PRP$', 'NN', 'IN', 'DT', 'NN', ':', 'DT', ':', 'NN', 'IN', 'NNS', 'IN', 'NNP', 'CD', 'NN', '.']",127
relation-classification,3,106,"For ACE04 , the baseline outperforms by ? 2 % in both tasks .","['For', 'ACE04', ',', 'the', 'baseline', 'outperforms', 'by', '?', '2', '%', 'in', 'both', 'tasks', '.']","['B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'NNP', ',', 'DT', 'NN', 'NNS', 'IN', '.', 'CD', 'NN', 'IN', 'DT', 'NNS', '.']",14
relation-classification,3,111,"For the CoNLL04 dataset , we use two evaluation settings .","['For', 'the', 'CoNLL04', 'dataset', ',', 'we', 'use', 'two', 'evaluation', 'settings', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNP', 'NN', ',', 'PRP', 'VBP', 'CD', 'NN', 'NNS', '.']",11
relation-classification,3,113,"The baseline model outperforms the state - of - the - art models that do not rely on manually extracted features ( > 4 % improvement for both tasks ) , since we directly model the whole sentence , instead of just considering pairs of entities .","['The', 'baseline', 'model', 'outperforms', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', 'that', 'do', 'not', 'rely', 'on', 'manually', 'extracted', 'features', '(', '>', '4', '%', 'improvement', 'for', 'both', 'tasks', ')', ',', 'since', 'we', 'directly', 'model', 'the', 'whole', 'sentence', ',', 'instead', 'of', 'just', 'considering', 'pairs', 'of', 'entities', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'WDT', 'VBP', 'RB', 'VB', 'IN', 'RB', 'VBN', 'NNS', '(', 'VB', 'CD', 'NN', 'NN', 'IN', 'DT', 'NNS', ')', ',', 'IN', 'PRP', 'RB', 'VBP', 'DT', 'JJ', 'NN', ',', 'RB', 'IN', 'RB', 'VBG', 'NNS', 'IN', 'NNS', '.']",47
relation-classification,3,117,"For the DREC dataset , we use two evaluation methods .","['For', 'the', 'DREC', 'dataset', ',', 'we', 'use', 'two', 'evaluation', 'methods', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNP', 'NN', ',', 'PRP', 'VBP', 'CD', 'NN', 'NNS', '.']",11
relation-classification,3,118,"In the boundaries evaluation , the baseline has an improvement of ? 3 % on both tasks compared to , whose quadratic scoring layer complicates NER .","['In', 'the', 'boundaries', 'evaluation', ',', 'the', 'baseline', 'has', 'an', 'improvement', 'of', '?', '3', '%', 'on', 'both', 'tasks', 'compared', 'to', ',', 'whose', 'quadratic', 'scoring', 'layer', 'complicates', 'NER', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNS', 'NN', ',', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', '.', 'CD', 'NN', 'IN', 'DT', 'NNS', 'VBN', 'TO', ',', 'WP$', 'JJ', 'VBG', 'NN', 'NNS', 'NNP', '.']",27
relation-classification,3,119,and show the effectiveness of the adversarial training on top of the baseline model .,"['and', 'show', 'the', 'effectiveness', 'of', 'the', 'adversarial', 'training', 'on', 'top', 'of', 'the', 'baseline', 'model', '.']","['O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['CC', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",15
relation-classification,3,120,"In all of the experiments , AT improves the predictive performance of the baseline model in the joint setting .","['In', 'all', 'of', 'the', 'experiments', ',', 'AT', 'improves', 'the', 'predictive', 'performance', 'of', 'the', 'baseline', 'model', 'in', 'the', 'joint', 'setting', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'IN', 'DT', 'NNS', ',', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",20
relation-classification,3,122,"Specifically , for ACE04 , there is an improvement in both tasks as well as in the over all F 1 performance ( 0.4 % ) .","['Specifically', ',', 'for', 'ACE04', ',', 'there', 'is', 'an', 'improvement', 'in', 'both', 'tasks', 'as', 'well', 'as', 'in', 'the', 'over', 'all', 'F', '1', 'performance', '(', '0.4', '%', ')', '.']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'IN', 'NNP', ',', 'EX', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NNS', 'RB', 'RB', 'IN', 'IN', 'DT', 'IN', 'DT', 'NNP', 'CD', 'NN', '(', 'CD', 'NN', ')', '.']",27
relation-classification,3,123,"For CoNLL04 , we note an improvement in the over all F 1 of 0.4 % for the EC and 0.8 % for the NER tasks , respectively .","['For', 'CoNLL04', ',', 'we', 'note', 'an', 'improvement', 'in', 'the', 'over', 'all', 'F', '1', 'of', '0.4', '%', 'for', 'the', 'EC', 'and', '0.8', '%', 'for', 'the', 'NER', 'tasks', ',', 'respectively', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['IN', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'IN', 'DT', 'NNP', 'CD', 'IN', 'CD', 'NN', 'IN', 'DT', 'NNP', 'CC', 'CD', 'NN', 'IN', 'DT', 'NNP', 'NNS', ',', 'RB', '.']",29
relation-classification,3,124,"For the DREC dataset , in both settings , there is an over all improvement of ? 1 % .","['For', 'the', 'DREC', 'dataset', ',', 'in', 'both', 'settings', ',', 'there', 'is', 'an', 'over', 'all', 'improvement', 'of', '?', '1', '%', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNP', 'NN', ',', 'IN', 'DT', 'NNS', ',', 'EX', 'VBZ', 'DT', 'IN', 'DT', 'NN', 'IN', '.', 'CD', 'NN', '.']",20
relation-classification,3,126,"Finally , for ADE , our AT model beats the baseline F 1 by 0.7 % .","['Finally', ',', 'for', 'ADE', ',', 'our', 'AT', 'model', 'beats', 'the', 'baseline', 'F', '1', 'by', '0.7', '%', '.']","['O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'IN', 'NNP', ',', 'PRP$', 'NNP', 'NN', 'VBZ', 'DT', 'NN', 'NNP', 'CD', 'IN', 'CD', 'NN', '.']",17
relation-classification,0,2,End - to - End Relation Extraction using LSTMs on Sequences and Tree Structures,"['End', '-', 'to', '-', 'End', 'Relation', 'Extraction', 'using', 'LSTMs', 'on', 'Sequences', 'and', 'Tree', 'Structures']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', ':', 'TO', ':', 'NN', 'NNP', 'NNP', 'VBG', 'NNP', 'IN', 'NNPS', 'CC', 'NNP', 'NNP']",14
relation-classification,0,4,We present a novel end - to - end neural model to extract entities and relations between them .,"['We', 'present', 'a', 'novel', 'end', '-', 'to', '-', 'end', 'neural', 'model', 'to', 'extract', 'entities', 'and', 'relations', 'between', 'them', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', ':', 'TO', ':', 'VB', 'JJ', 'NN', 'TO', 'VB', 'NNS', 'CC', 'NNS', 'IN', 'PRP', '.']",19
relation-classification,0,6,This allows our model to jointly represent both entities and relations with shared parameters in a single model .,"['This', 'allows', 'our', 'model', 'to', 'jointly', 'represent', 'both', 'entities', 'and', 'relations', 'with', 'shared', 'parameters', 'in', 'a', 'single', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'PRP$', 'NN', 'TO', 'RB', 'VB', 'DT', 'NNS', 'CC', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",19
relation-classification,0,12,Extracting semantic relations between entities in text is an important and well - studied task in information extraction and natural language processing ( NLP ) .,"['Extracting', 'semantic', 'relations', 'between', 'entities', 'in', 'text', 'is', 'an', 'important', 'and', 'well', '-', 'studied', 'task', 'in', 'information', 'extraction', 'and', 'natural', 'language', 'processing', '(', 'NLP', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', 'NNS', 'IN', 'NNS', 'IN', 'NN', 'VBZ', 'DT', 'JJ', 'CC', 'RB', ':', 'VBN', 'NN', 'IN', 'NN', 'NN', 'CC', 'JJ', 'NN', 'NN', '(', 'NNP', ')', '.']",26
relation-classification,0,13,"Traditional systems treat this task as a pipeline of two separated tasks , i.e. , named entity recognition ( NER ) ) and relation extraction , but recent studies show that end - to - end ( joint ) modeling of entity and relation is important for high performance since relations interact closely with entity information .","['Traditional', 'systems', 'treat', 'this', 'task', 'as', 'a', 'pipeline', 'of', 'two', 'separated', 'tasks', ',', 'i.e.', ',', 'named', 'entity', 'recognition', '(', 'NER', ')', ')', 'and', 'relation', 'extraction', ',', 'but', 'recent', 'studies', 'show', 'that', 'end', '-', 'to', '-', 'end', '(', 'joint', ')', 'modeling', 'of', 'entity', 'and', 'relation', 'is', 'important', 'for', 'high', 'performance', 'since', 'relations', 'interact', 'closely', 'with', 'entity', 'information', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNS', 'NN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'CD', 'JJ', 'NNS', ',', 'FW', ',', 'VBN', 'NN', 'NN', '(', 'NNP', ')', ')', 'CC', 'NN', 'NN', ',', 'CC', 'JJ', 'NNS', 'VBP', 'IN', 'VBP', ':', 'TO', ':', 'NN', '(', 'JJ', ')', 'NN', 'IN', 'NN', 'CC', 'NN', 'VBZ', 'JJ', 'IN', 'JJ', 'NN', 'IN', 'NNS', 'VBP', 'RB', 'IN', 'NN', 'NN', '.']",57
relation-classification,0,27,We present a novel end - to - end model to extract relations between entities on both word sequence and dependency tree structures .,"['We', 'present', 'a', 'novel', 'end', '-', 'to', '-', 'end', 'model', 'to', 'extract', 'relations', 'between', 'entities', 'on', 'both', 'word', 'sequence', 'and', 'dependency', 'tree', 'structures', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', ':', 'TO', ':', 'VB', 'NN', 'TO', 'VB', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'CC', 'NN', 'NN', 'NNS', '.']",24
relation-classification,0,28,Our model allows joint modeling of entities and relations in a single model by using both bidirectional sequential ( left - to - right and right - to - left ) and bidirectional tree - structured ( bottom - up and top - down ) LSTM - RNNs .,"['Our', 'model', 'allows', 'joint', 'modeling', 'of', 'entities', 'and', 'relations', 'in', 'a', 'single', 'model', 'by', 'using', 'both', 'bidirectional', 'sequential', '(', 'left', '-', 'to', '-', 'right', 'and', 'right', '-', 'to', '-', 'left', ')', 'and', 'bidirectional', 'tree', '-', 'structured', '(', 'bottom', '-', 'up', 'and', 'top', '-', 'down', ')', 'LSTM', '-', 'RNNs', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'JJ', 'NN', 'IN', 'NNS', 'CC', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', '(', 'VBN', ':', 'TO', ':', 'NN', 'CC', 'JJ', ':', 'TO', ':', 'NN', ')', 'CC', 'JJ', 'NN', ':', 'VBN', '(', 'SYM', ':', 'RB', 'CC', 'JJ', ':', 'RB', ')', 'NNP', ':', 'NN', '.']",49
relation-classification,0,29,"Our model first detects entities and then extracts relations between the detected entities using a single incrementally - decoded NN structure , and the NN parameters are jointly updated using both entity and relation labels .","['Our', 'model', 'first', 'detects', 'entities', 'and', 'then', 'extracts', 'relations', 'between', 'the', 'detected', 'entities', 'using', 'a', 'single', 'incrementally', '-', 'decoded', 'NN', 'structure', ',', 'and', 'the', 'NN', 'parameters', 'are', 'jointly', 'updated', 'using', 'both', 'entity', 'and', 'relation', 'labels', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'JJ', 'VBZ', 'NNS', 'CC', 'RB', 'VBZ', 'NNS', 'IN', 'DT', 'JJ', 'NNS', 'VBG', 'DT', 'JJ', 'RB', ':', 'VBD', 'NNP', 'NN', ',', 'CC', 'DT', 'NNP', 'NNS', 'VBP', 'RB', 'VBN', 'VBG', 'DT', 'NN', 'CC', 'NN', 'NNS', '.']",36
relation-classification,0,30,"Unlike traditional incremental end - to - end relation extraction models , our model further incorporates two enhancements into training : entity pretraining , which pretrains the entity model , and scheduled sampling , which replaces ( unreliable ) predicted labels with gold labels in a certain probability .","['Unlike', 'traditional', 'incremental', 'end', '-', 'to', '-', 'end', 'relation', 'extraction', 'models', ',', 'our', 'model', 'further', 'incorporates', 'two', 'enhancements', 'into', 'training', ':', 'entity', 'pretraining', ',', 'which', 'pretrains', 'the', 'entity', 'model', ',', 'and', 'scheduled', 'sampling', ',', 'which', 'replaces', '(', 'unreliable', ')', 'predicted', 'labels', 'with', 'gold', 'labels', 'in', 'a', 'certain', 'probability', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', 'JJ', 'NN', ':', 'TO', ':', 'VB', 'NN', 'NN', 'NNS', ',', 'PRP$', 'NN', 'RB', 'VBZ', 'CD', 'NNS', 'IN', 'NN', ':', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'NN', ',', 'CC', 'VBD', 'NN', ',', 'WDT', 'VBZ', '(', 'JJ', ')', 'VBD', 'NNS', 'IN', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",49
relation-classification,0,31,"These enhancements alleviate the problem of low - performance entity detection in early stages of training , as well as allow entity information to further help downstream relation classification .","['These', 'enhancements', 'alleviate', 'the', 'problem', 'of', 'low', '-', 'performance', 'entity', 'detection', 'in', 'early', 'stages', 'of', 'training', ',', 'as', 'well', 'as', 'allow', 'entity', 'information', 'to', 'further', 'help', 'downstream', 'relation', 'classification', '.']","['O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'DT', 'NN', 'IN', 'JJ', ':', 'NN', 'NN', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NN', ',', 'RB', 'RB', 'IN', 'JJ', 'NN', 'NN', 'TO', 'VB', 'NN', 'VB', 'NN', 'NN', '.']",30
relation-classification,0,154,We implemented our model using the cnn library .,"['We', 'implemented', 'our', 'model', 'using', 'the', 'cnn', 'library', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBD', 'PRP$', 'NN', 'VBG', 'DT', 'NN', 'NN', '.']",9
relation-classification,0,155,"We parsed the texts using the Stanford neural dependency parser 7 ( Chen and Manning , 2014 ) with the original Stanford Dependencies .","['We', 'parsed', 'the', 'texts', 'using', 'the', 'Stanford', 'neural', 'dependency', 'parser', '7', '(', 'Chen', 'and', 'Manning', ',', '2014', ')', 'with', 'the', 'original', 'Stanford', 'Dependencies', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'DT', 'NN', 'VBG', 'DT', 'NNP', 'JJ', 'NN', 'NN', 'CD', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'IN', 'DT', 'JJ', 'NNP', 'NNP', '.']",24
relation-classification,0,156,"Based on preliminary tuning , we fixed embedding dimensions n w to 200 , n p , n d , n e to 25 , and dimensions of intermediate layers ( n ls , n lt of LSTM - RNNs and n he , n hr of hidden layers ) to 100 .","['Based', 'on', 'preliminary', 'tuning', ',', 'we', 'fixed', 'embedding', 'dimensions', 'n', 'w', 'to', '200', ',', 'n', 'p', ',', 'n', 'd', ',', 'n', 'e', 'to', '25', ',', 'and', 'dimensions', 'of', 'intermediate', 'layers', '(', 'n', 'ls', ',', 'n', 'lt', 'of', 'LSTM', '-', 'RNNs', 'and', 'n', 'he', ',', 'n', 'hr', 'of', 'hidden', 'layers', ')', 'to', '100', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'O']","['VBN', 'IN', 'JJ', 'NN', ',', 'PRP', 'VBD', 'VBG', 'NNS', 'RB', 'VBP', 'TO', 'CD', ',', 'RB', 'NN', ',', 'JJ', 'NN', ',', 'JJ', 'NN', 'TO', 'CD', ',', 'CC', 'NNS', 'IN', 'JJ', 'NNS', '(', 'JJ', 'NN', ',', 'JJ', 'NN', 'IN', 'NNP', ':', 'NN', 'CC', 'NN', 'PRP', ',', 'JJ', 'NN', 'IN', 'JJ', 'NNS', ')', 'TO', 'CD', '.']",53
relation-classification,0,157,We initialized word vectors via word2 vec trained on Wikipedia 8 and randomly initialized all other parameters .,"['We', 'initialized', 'word', 'vectors', 'via', 'word2', 'vec', 'trained', 'on', 'Wikipedia', '8', 'and', 'randomly', 'initialized', 'all', 'other', 'parameters', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'NN', 'NNS', 'IN', 'NN', 'NN', 'VBN', 'IN', 'NNP', 'CD', 'CC', 'VB', 'VBN', 'DT', 'JJ', 'NNS', '.']",18
relation-classification,0,165,"Table 1 compares our model with the state - of - theart feature - based model of on final test sets , and shows that our model performs better than the state - of - the - art model .","['Table', '1', 'compares', 'our', 'model', 'with', 'the', 'state', '-', 'of', '-', 'theart', 'feature', '-', 'based', 'model', 'of', 'on', 'final', 'test', 'sets', ',', 'and', 'shows', 'that', 'our', 'model', 'performs', 'better', 'than', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'model', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'CD', 'VBZ', 'PRP$', 'NN', 'IN', 'DT', 'NN', ':', 'IN', ':', 'NN', 'NN', ':', 'VBN', 'NN', 'IN', 'IN', 'JJ', 'NN', 'NNS', ',', 'CC', 'VBZ', 'IN', 'PRP$', 'NN', 'VBZ', 'JJR', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', '.']",40
relation-classification,0,166,"To analyze the contributions and effects of the various components of our end - to - end relation extraction model , we perform ablation tests on the ACE05 development set ( ) .","['To', 'analyze', 'the', 'contributions', 'and', 'effects', 'of', 'the', 'various', 'components', 'of', 'our', 'end', '-', 'to', '-', 'end', 'relation', 'extraction', 'model', ',', 'we', 'perform', 'ablation', 'tests', 'on', 'the', 'ACE05', 'development', 'set', '(', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'NNS', 'CC', 'NNS', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'PRP$', 'NN', ':', 'TO', ':', 'VB', 'NN', 'NN', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'NN', '(', ')', '.']",33
relation-classification,0,167,"The performance slightly degraded without scheduled sampling , and the performance significantly degraded when we removed entity pretraining or removed both ( p < 0.05 ) .","['The', 'performance', 'slightly', 'degraded', 'without', 'scheduled', 'sampling', ',', 'and', 'the', 'performance', 'significantly', 'degraded', 'when', 'we', 'removed', 'entity', 'pretraining', 'or', 'removed', 'both', '(', 'p', '<', '0.05', ')', '.']","['O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'RB', 'VBD', 'IN', 'VBN', 'NN', ',', 'CC', 'DT', 'NN', 'RB', 'VBD', 'WRB', 'PRP', 'VBD', 'NN', 'NN', 'CC', 'VBN', 'DT', '(', 'NN', 'RB', 'CD', ')', '.']",27
relation-classification,0,180,"When we removed all the enhancements , i.e. , scheduled sampling , entity pretraining , label embedding , and shared parameters , the performance is significantly worse than SP - Tree ( p < 0.01 ) , showing that these enhancements provide complementary benefits to end - to - end relation extraction .","['When', 'we', 'removed', 'all', 'the', 'enhancements', ',', 'i.e.', ',', 'scheduled', 'sampling', ',', 'entity', 'pretraining', ',', 'label', 'embedding', ',', 'and', 'shared', 'parameters', ',', 'the', 'performance', 'is', 'significantly', 'worse', 'than', 'SP', '-', 'Tree', '(', 'p', '<', '0.01', ')', ',', 'showing', 'that', 'these', 'enhancements', 'provide', 'complementary', 'benefits', 'to', 'end', '-', 'to', '-', 'end', 'relation', 'extraction', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['WRB', 'PRP', 'VBD', 'PDT', 'DT', 'NNS', ',', 'FW', ',', 'VBN', 'NN', ',', 'NN', 'NN', ',', 'NN', 'NN', ',', 'CC', 'VBD', 'NNS', ',', 'DT', 'NN', 'VBZ', 'RB', 'JJR', 'IN', 'NNP', ':', 'NNP', '(', 'JJ', 'NNP', 'CD', ')', ',', 'VBG', 'IN', 'DT', 'NNS', 'VBP', 'JJ', 'NNS', 'TO', 'VB', ':', 'TO', ':', 'VB', 'NN', 'NN', '.']",53
named-entity-recognition,8,2,BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding,"['BERT', ':', 'Pre-training', 'of', 'Deep', 'Bidirectional', 'Transformers', 'for', 'Language', 'Understanding']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNS', ':', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",10
named-entity-recognition,8,4,"We introduce a new language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers .","['We', 'introduce', 'a', 'new', 'language', 'representation', 'model', 'called', 'BERT', ',', 'which', 'stands', 'for', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'NN', 'VBN', 'NNP', ',', 'WDT', 'VBZ', 'IN', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', '.']",19
named-entity-recognition,8,14,Language model pre-training has been shown to be effective for improving many natural language processing tasks .,"['Language', 'model', 'pre-training', 'has', 'been', 'shown', 'to', 'be', 'effective', 'for', 'improving', 'many', 'natural', 'language', 'processing', 'tasks', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', 'VBZ', 'VBN', 'VBN', 'TO', 'VB', 'JJ', 'IN', 'VBG', 'JJ', 'JJ', 'NN', 'NN', 'NNS', '.']",17
named-entity-recognition,8,24,"In this paper , we improve the fine - tuning based approaches by proposing BERT : Bidirectional Encoder Representations from Transformers .","['In', 'this', 'paper', ',', 'we', 'improve', 'the', 'fine', '-', 'tuning', 'based', 'approaches', 'by', 'proposing', 'BERT', ':', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', ':', 'NN', 'VBN', 'NNS', 'IN', 'VBG', 'NNP', ':', 'JJ', 'NNP', 'NNP', 'IN', 'NNP', '.']",22
named-entity-recognition,8,25,"BERT alleviates the previously mentioned unidirectionality constraint by using a "" masked language model "" ( MLM ) pre-training objective , inspired by the Cloze task .","['BERT', 'alleviates', 'the', 'previously', 'mentioned', 'unidirectionality', 'constraint', 'by', 'using', 'a', '""', 'masked', 'language', 'model', '""', '(', 'MLM', ')', 'pre-training', 'objective', ',', 'inspired', 'by', 'the', 'Cloze', 'task', '.']","['B-n', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'DT', 'RB', 'VBN', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'VBN', 'NN', 'NN', 'NNP', '(', 'NNP', ')', 'NN', 'NN', ',', 'VBN', 'IN', 'DT', 'NNP', 'NN', '.']",27
named-entity-recognition,8,26,"The masked language model randomly masks some of the tokens from the input , and the objective is to predict the original vocabulary id of the masked word based only on its context .","['The', 'masked', 'language', 'model', 'randomly', 'masks', 'some', 'of', 'the', 'tokens', 'from', 'the', 'input', ',', 'and', 'the', 'objective', 'is', 'to', 'predict', 'the', 'original', 'vocabulary', 'id', 'of', 'the', 'masked', 'word', 'based', 'only', 'on', 'its', 'context', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'RB', 'VBZ', 'DT', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', ',', 'CC', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'DT', 'VBN', 'NN', 'VBN', 'RB', 'IN', 'PRP$', 'NN', '.']",34
named-entity-recognition,8,27,"Unlike left - toright language model pre-training , the MLM objective enables the representation to fuse the left and the right context , which allows us to pretrain a deep bidirectional Transformer .","['Unlike', 'left', '-', 'toright', 'language', 'model', 'pre-training', ',', 'the', 'MLM', 'objective', 'enables', 'the', 'representation', 'to', 'fuse', 'the', 'left', 'and', 'the', 'right', 'context', ',', 'which', 'allows', 'us', 'to', 'pretrain', 'a', 'deep', 'bidirectional', 'Transformer', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', ':', 'JJ', 'NN', 'NN', 'NN', ',', 'DT', 'NNP', 'JJ', 'VBZ', 'DT', 'NN', 'TO', 'VB', 'DT', 'NN', 'CC', 'DT', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'PRP', 'TO', 'VB', 'DT', 'JJ', 'JJ', 'NNP', '.']",33
named-entity-recognition,8,28,"In addition to the masked language model , we also use a "" next sentence prediction "" task that jointly pretrains text - pair representations .","['In', 'addition', 'to', 'the', 'masked', 'language', 'model', ',', 'we', 'also', 'use', 'a', '""', 'next', 'sentence', 'prediction', '""', 'task', 'that', 'jointly', 'pretrains', 'text', '-', 'pair', 'representations', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'DT', 'JJ', 'NN', 'NN', ',', 'PRP', 'RB', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'NNP', 'NN', 'WDT', 'RB', 'VBZ', 'JJ', ':', 'NN', 'NNS', '.']",26
named-entity-recognition,8,155,GLUE,['GLUE'],['B-n'],['NN'],1
named-entity-recognition,8,156,"The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al. , 2018 a ) is a collection of diverse natural language understanding tasks .","['The', 'General', 'Language', 'Understanding', 'Evaluation', '(', 'GLUE', ')', 'benchmark', '(', 'Wang', 'et', 'al.', ',', '2018', 'a', ')', 'is', 'a', 'collection', 'of', 'diverse', 'natural', 'language', 'understanding', 'tasks', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', '(', 'NNP', 'RB', 'RB', ',', 'CD', 'DT', ')', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'JJ', 'NNS', '.']",27
named-entity-recognition,8,170,We use a batch size of 32 and fine - tune for 3 epochs over the data for all GLUE tasks .,"['We', 'use', 'a', 'batch', 'size', 'of', '32', 'and', 'fine', '-', 'tune', 'for', '3', 'epochs', 'over', 'the', 'data', 'for', 'all', 'GLUE', 'tasks', '.']","['O', 'B-p', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CD', 'CC', 'JJ', ':', 'NN', 'IN', 'CD', 'NN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'NNS', '.']",22
named-entity-recognition,8,171,"For each task , we selected the best fine - tuning learning rate ( among 5 e - 5 , 4 e - 5 , 3 e - 5 , and 2 e - 5 ) on the Dev set .","['For', 'each', 'task', ',', 'we', 'selected', 'the', 'best', 'fine', '-', 'tuning', 'learning', 'rate', '(', 'among', '5', 'e', '-', '5', ',', '4', 'e', '-', '5', ',', '3', 'e', '-', '5', ',', 'and', '2', 'e', '-', '5', ')', 'on', 'the', 'Dev', 'set', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBD', 'DT', 'JJS', 'JJ', ':', 'VBG', 'VBG', 'NN', '(', 'IN', 'CD', 'SYM', ':', 'CD', ',', 'CD', 'SYM', ':', 'CD', ',', 'CD', 'SYM', ':', 'CD', ',', 'CC', 'CD', 'SYM', ':', 'CD', ')', 'IN', 'DT', 'NNP', 'NN', '.']",41
named-entity-recognition,8,172,"Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and selected the best model on the Dev set .","['Additionally', ',', 'for', 'BERT', 'LARGE', 'we', 'found', 'that', 'finetuning', 'was', 'sometimes', 'unstable', 'on', 'small', 'datasets', ',', 'so', 'we', 'ran', 'several', 'random', 'restarts', 'and', 'selected', 'the', 'best', 'model', 'on', 'the', 'Dev', 'set', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'IN', 'NNP', 'NNP', 'PRP', 'VBD', 'IN', 'NN', 'VBD', 'RB', 'JJ', 'IN', 'JJ', 'NNS', ',', 'IN', 'PRP', 'VBD', 'JJ', 'JJ', 'NNS', 'CC', 'VBD', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",32
named-entity-recognition,8,175,"Both BERT BASE and BERT LARGE outperform all systems on all tasks by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .","['Both', 'BERT', 'BASE', 'and', 'BERT', 'LARGE', 'outperform', 'all', 'systems', 'on', 'all', 'tasks', 'by', 'a', 'substantial', 'margin', ',', 'obtaining', '4.5', '%', 'and', '7.0', '%', 'respective', 'average', 'accuracy', 'improvement', 'over', 'the', 'prior', 'state', 'of', 'the', 'art', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'NNP', 'CC', 'NNP', 'NNP', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', ',', 'VBG', 'CD', 'NN', 'CC', 'CD', 'NN', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",35
named-entity-recognition,8,179,"We find that BERT LARGE significantly outperforms BERT BASE across all tasks , especially those with very little training data .","['We', 'find', 'that', 'BERT', 'LARGE', 'significantly', 'outperforms', 'BERT', 'BASE', 'across', 'all', 'tasks', ',', 'especially', 'those', 'with', 'very', 'little', 'training', 'data', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'NNP', 'NNP', 'RB', 'VBZ', 'NNP', 'NNP', 'IN', 'DT', 'NNS', ',', 'RB', 'DT', 'IN', 'RB', 'JJ', 'NN', 'NNS', '.']",21
named-entity-recognition,8,181,SQuAD v 1.1,"['SQuAD', 'v', '1.1']","['B-n', 'I-n', 'I-n']","['NNP', 'VBD', 'CD']",3
named-entity-recognition,8,182,The Stanford Question Answering Dataset ( SQuAD v1.1 ) is a collection of 100 k crowdsourced question / answer pairs .,"['The', 'Stanford', 'Question', 'Answering', 'Dataset', '(', 'SQuAD', 'v1.1', ')', 'is', 'a', 'collection', 'of', '100', 'k', 'crowdsourced', 'question', '/', 'answer', 'pairs', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', 'NN', ')', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NNS', 'VBN', 'NN', 'NNP', 'NN', 'NNS', '.']",21
named-entity-recognition,8,195,We fine - tune for 3 epochs with a learning rate of 5 e - 5 and a batch size of 32 .,"['We', 'fine', '-', 'tune', 'for', '3', 'epochs', 'with', 'a', 'learning', 'rate', 'of', '5', 'e', '-', '5', 'and', 'a', 'batch', 'size', 'of', '32', '.']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBP', ':', 'NN', 'IN', 'CD', 'NNS', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', 'SYM', ':', 'CD', 'CC', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",23
named-entity-recognition,8,199,Our best performing system outperforms the top leaderboard system by + 1.5 F1 in ensembling and + 1.3 F1 as a single system .,"['Our', 'best', 'performing', 'system', 'outperforms', 'the', 'top', 'leaderboard', 'system', 'by', '+', '1.5', 'F1', 'in', 'ensembling', 'and', '+', '1.3', 'F1', 'as', 'a', 'single', 'system', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'JJS', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', '$', 'CD', 'NNP', 'IN', 'VBG', 'CC', '$', 'CD', 'NNP', 'IN', 'DT', 'JJ', 'NN', '.']",24
named-entity-recognition,8,203,SQuAD v 2.0,"['SQuAD', 'v', '2.0']","['B-n', 'I-n', 'I-n']","['NNP', 'VBD', 'CD']",3
named-entity-recognition,8,213,We fine - tuned for 2 epochs with a learning rate of 5 e - 5 and a batch size of 48 .,"['We', 'fine', '-', 'tuned', 'for', '2', 'epochs', 'with', 'a', 'learning', 'rate', 'of', '5', 'e', '-', '5', 'and', 'a', 'batch', 'size', 'of', '48', '.']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBP', ':', 'VBN', 'IN', 'CD', 'NNS', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', 'SYM', ':', 'CD', 'CC', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",23
named-entity-recognition,8,215,We observe a + 5.1 F1 improvement over the previous best system .,"['We', 'observe', 'a', '+', '5.1', 'F1', 'improvement', 'over', 'the', 'previous', 'best', 'system', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'CD', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'JJS', 'NN', '.']",13
named-entity-recognition,8,216,SWAG,['SWAG'],['B-n'],['NN'],1
named-entity-recognition,8,217,The Situations With Adversarial Generations ( SWAG ) dataset contains 113 k sentence - pair completion examples that evaluate grounded commonsense inference .,"['The', 'Situations', 'With', 'Adversarial', 'Generations', '(', 'SWAG', ')', 'dataset', 'contains', '113', 'k', 'sentence', '-', 'pair', 'completion', 'examples', 'that', 'evaluate', 'grounded', 'commonsense', 'inference', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNPS', 'IN', 'JJ', 'NNP', '(', 'NNP', ')', 'NN', 'VBZ', 'CD', 'NN', 'NN', ':', 'NN', 'NN', 'VBZ', 'IN', 'NN', 'VBD', 'JJ', 'NN', '.']",23
named-entity-recognition,8,221,We fine - tune the model for 3 epochs with a learning rate of 2 e - 5 and a batch size of 16 .,"['We', 'fine', '-', 'tune', 'the', 'model', 'for', '3', 'epochs', 'with', 'a', 'learning', 'rate', 'of', '2', 'e', '-', '5', 'and', 'a', 'batch', 'size', 'of', '16', '.']","['O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBP', ':', 'NN', 'DT', 'NN', 'IN', 'CD', 'NNS', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', 'SYM', ':', 'CD', 'CC', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",25
named-entity-recognition,8,223,BERT LARGE outperforms the authors ' baseline ESIM + ELMo system by + 27.1 % and OpenAI GPT by 8.3 % .,"['BERT', 'LARGE', 'outperforms', 'the', 'authors', ""'"", 'baseline', 'ESIM', '+', 'ELMo', 'system', 'by', '+', '27.1', '%', 'and', 'OpenAI', 'GPT', 'by', '8.3', '%', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'NNP', 'VBZ', 'DT', 'NNS', 'POS', 'NN', 'NNP', 'NNP', 'NNP', 'NN', 'IN', '$', 'CD', 'NN', 'CC', 'NNP', 'NNP', 'IN', 'CD', 'NN', '.']",22
named-entity-recognition,8,250,Effect of Model Size,"['Effect', 'of', 'Model', 'Size']","['B-n', 'I-n', 'I-n', 'I-n']","['NN', 'IN', 'NNP', 'NNP']",4
named-entity-recognition,8,260,"However , we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks , provided that the model has been sufficiently pre-trained .","['However', ',', 'we', 'believe', 'that', 'this', 'is', 'the', 'first', 'work', 'to', 'demonstrate', 'convincingly', 'that', 'scaling', 'to', 'extreme', 'model', 'sizes', 'also', 'leads', 'to', 'large', 'improvements', 'on', 'very', 'small', 'scale', 'tasks', ',', 'provided', 'that', 'the', 'model', 'has', 'been', 'sufficiently', 'pre-trained', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'IN', 'DT', 'VBZ', 'DT', 'JJ', 'NN', 'TO', 'VB', 'RB', 'IN', 'VBG', 'TO', 'VB', 'NN', 'NNS', 'RB', 'VBZ', 'TO', 'JJ', 'NNS', 'IN', 'RB', 'JJ', 'NN', 'NNS', ',', 'VBD', 'IN', 'DT', 'NN', 'VBZ', 'VBN', 'RB', 'JJ', '.']",39
named-entity-recognition,8,263,Feature - based Approach with BERT,"['Feature', '-', 'based', 'Approach', 'with', 'BERT']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'VBN', 'NN', 'IN', 'NNP']",6
named-entity-recognition,8,275,BERT LARGE performs competitively with state - of - the - art methods .,"['BERT', 'LARGE', 'performs', 'competitively', 'with', 'state', '-', 'of', '-', 'the', '-', 'art', 'methods', '.']","['B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NNS', 'RB', 'IN', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', '.']",14
named-entity-recognition,8,277,This demonstrates that BERT is effective for both finetuning and feature - based approaches .,"['This', 'demonstrates', 'that', 'BERT', 'is', 'effective', 'for', 'both', 'finetuning', 'and', 'feature', '-', 'based', 'approaches', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'IN', 'NNP', 'VBZ', 'JJ', 'IN', 'DT', 'NN', 'CC', 'NN', ':', 'VBN', 'NNS', '.']",15
named-entity-recognition,9,2,Data and text mining BioBERT : a pre-trained biomedical language representation model for biomedical text mining,"['Data', 'and', 'text', 'mining', 'BioBERT', ':', 'a', 'pre-trained', 'biomedical', 'language', 'representation', 'model', 'for', 'biomedical', 'text', 'mining']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NNP', 'CC', 'NN', 'NN', 'NNP', ':', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NN']",16
named-entity-recognition,9,6,"With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models .","['With', 'the', 'progress', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', ',', 'extracting', 'valuable', 'information', 'from', 'biomedical', 'literature', 'has', 'gained', 'popularity', 'among', 'researchers', ',', 'and', 'deep', 'learning', 'has', 'boosted', 'the', 'development', 'of', 'effective', 'biomedical', 'text', 'mining', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', ',', 'VBG', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'VBZ', 'VBN', 'NN', 'IN', 'NNS', ',', 'CC', 'JJ', 'NN', 'VBZ', 'VBN', 'DT', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'NN', 'NNS', '.']",37
named-entity-recognition,9,7,"However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora .","['However', ',', 'directly', 'applying', 'the', 'advancements', 'in', 'NLP', 'to', 'biomedical', 'text', 'mining', 'often', 'yields', 'unsatisfactory', 'results', 'due', 'to', 'a', 'word', 'distribution', 'shift', 'from', 'general', 'domain', 'corpora', 'to', 'biomedical', 'corpora', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'RB', 'VBG', 'DT', 'NNS', 'IN', 'NNP', 'TO', 'JJ', 'NN', 'NN', 'RB', 'VBZ', 'JJ', 'NNS', 'JJ', 'TO', 'DT', 'NN', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NN', 'TO', 'JJ', 'NN', '.']",30
named-entity-recognition,9,8,"In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .","['In', 'this', 'article', ',', 'we', 'investigate', 'how', 'the', 'recently', 'introduced', 'pre-trained', 'language', 'model', 'BERT', 'can', 'be', 'adapted', 'for', 'biomedical', 'corpora', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'WRB', 'DT', 'RB', 'VBN', 'JJ', 'NN', 'NN', 'NNP', 'MD', 'VB', 'VBN', 'IN', 'JJ', 'NN', '.']",21
named-entity-recognition,9,15,"We make the pre-trained weights of BioBERT freely available at https://github. com/naver/biobert-pretrained , and the source code for fine - tuning","['We', 'make', 'the', 'pre-trained', 'weights', 'of', 'BioBERT', 'freely', 'available', 'at', 'https://github.', 'com/naver/biobert-pretrained', ',', 'and', 'the', 'source', 'code', 'for', 'fine', '-', 'tuning']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NNS', 'IN', 'NNP', 'RB', 'JJ', 'IN', 'JJ', 'JJ', ',', 'CC', 'DT', 'NN', 'NN', 'IN', 'JJ', ':', 'NN']",21
named-entity-recognition,9,27,"Also , the word distributions of general and biomedical corpora are quite different , which can often be a problem for biomedical text mining models .","['Also', ',', 'the', 'word', 'distributions', 'of', 'general', 'and', 'biomedical', 'corpora', 'are', 'quite', 'different', ',', 'which', 'can', 'often', 'be', 'a', 'problem', 'for', 'biomedical', 'text', 'mining', 'models', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'DT', 'NN', 'NNS', 'IN', 'JJ', 'CC', 'JJ', 'NNS', 'VBP', 'RB', 'JJ', ',', 'WDT', 'MD', 'RB', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', 'NNS', '.']",26
named-entity-recognition,9,34,"In this article , we introduce BioBERT , which is a pre-trained language representation model for the biomedical domain .","['In', 'this', 'article', ',', 'we', 'introduce', 'BioBERT', ',', 'which', 'is', 'a', 'pre-trained', 'language', 'representation', 'model', 'for', 'the', 'biomedical', 'domain', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",20
named-entity-recognition,9,36,"First , we initialize BioBERT with weights from BERT , which was pretrained on general domain corpora ( English Wikipedia and Books Corpus ) .","['First', ',', 'we', 'initialize', 'BioBERT', 'with', 'weights', 'from', 'BERT', ',', 'which', 'was', 'pretrained', 'on', 'general', 'domain', 'corpora', '(', 'English', 'Wikipedia', 'and', 'Books', 'Corpus', ')', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'NNP', 'IN', 'NNS', 'IN', 'NNP', ',', 'WDT', 'VBD', 'VBN', 'IN', 'JJ', 'NN', 'NN', '(', 'JJ', 'NNP', 'CC', 'NNP', 'NNP', ')', '.']",25
named-entity-recognition,9,37,"Then , BioBERT is pre-trained on biomedical domain corpora ( PubMed abstracts and PMC full - text articles ) .","['Then', ',', 'BioBERT', 'is', 'pre-trained', 'on', 'biomedical', 'domain', 'corpora', '(', 'PubMed', 'abstracts', 'and', 'PMC', 'full', '-', 'text', 'articles', ')', '.']","['O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'NNP', 'VBZ', 'JJ', 'IN', 'JJ', 'NN', 'NN', '(', 'NNP', 'NNS', 'CC', 'NNP', 'JJ', ':', 'NN', 'NNS', ')', '.']",20
named-entity-recognition,9,38,"To show the effectiveness of our approach in biomedical text mining , BioBERT is fine - tuned and evaluated on three popular biomedical text mining tasks .","['To', 'show', 'the', 'effectiveness', 'of', 'our', 'approach', 'in', 'biomedical', 'text', 'mining', ',', 'BioBERT', 'is', 'fine', '-', 'tuned', 'and', 'evaluated', 'on', 'three', 'popular', 'biomedical', 'text', 'mining', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'IN', 'JJ', 'NN', 'NN', ',', 'NNP', 'VBZ', 'JJ', ':', 'VBN', 'CC', 'VBN', 'IN', 'CD', 'JJ', 'JJ', 'NN', 'NN', 'NNS', '.']",27
named-entity-recognition,9,119,We used the BERT BASE model pre-trained on English Wikipedia and Books Corpus for 1 M steps .,"['We', 'used', 'the', 'BERT', 'BASE', 'model', 'pre-trained', 'on', 'English', 'Wikipedia', 'and', 'Books', 'Corpus', 'for', '1', 'M', 'steps', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'DT', 'NNP', 'NNP', 'NN', 'JJ', 'IN', 'JJ', 'NNP', 'CC', 'NNP', 'NNP', 'IN', 'CD', 'NNP', 'NNS', '.']",18
named-entity-recognition,9,120,BioBERT v1.0 ( PubMed PMC ) is the version of BioBERT ( PubMed PMC ) trained for 470 K steps .,"['BioBERT', 'v1.0', '(', 'PubMed', 'PMC', ')', 'is', 'the', 'version', 'of', 'BioBERT', '(', 'PubMed', 'PMC', ')', 'trained', 'for', '470', 'K', 'steps', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NN', '(', 'NNP', 'NNP', ')', 'VBZ', 'DT', 'NN', 'IN', 'NNP', '(', 'NNP', 'NNP', ')', 'VBD', 'IN', 'CD', 'NNP', 'NNS', '.']",21
named-entity-recognition,9,126,We used eight NVIDIA V100 ( 32GB ) GPUs for the pre-training .,"['We', 'used', 'eight', 'NVIDIA', 'V100', '(', '32GB', ')', 'GPUs', 'for', 'the', 'pre-training', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['PRP', 'VBD', 'CD', 'NNP', 'NNP', '(', 'CD', ')', 'NNP', 'IN', 'DT', 'NN', '.']",13
named-entity-recognition,9,127,"The maximum sequence length was fixed to 512 and the mini-batch size was set to 192 , resulting in 98 304 words per iteration .","['The', 'maximum', 'sequence', 'length', 'was', 'fixed', 'to', '512', 'and', 'the', 'mini-batch', 'size', 'was', 'set', 'to', '192', ',', 'resulting', 'in', '98', '304', 'words', 'per', 'iteration', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBD', 'VBN', 'TO', 'CD', 'CC', 'DT', 'JJ', 'NN', 'VBD', 'VBN', 'TO', 'CD', ',', 'VBG', 'IN', 'CD', 'CD', 'NNS', 'IN', 'NN', '.']",25
named-entity-recognition,9,130,We used a single NVIDIA Titan Xp ( 12GB ) GPU to fine - tune BioBERT on each task .,"['We', 'used', 'a', 'single', 'NVIDIA', 'Titan', 'Xp', '(', '12GB', ')', 'GPU', 'to', 'fine', '-', 'tune', 'BioBERT', 'on', 'each', 'task', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBD', 'DT', 'JJ', 'NNP', 'NNP', 'NNP', '(', 'CD', ')', 'NNP', 'TO', 'VB', ':', 'NN', 'NNP', 'IN', 'DT', 'NN', '.']",20
named-entity-recognition,9,136,The results of NER are shown in .,"['The', 'results', 'of', 'NER', 'are', 'shown', 'in', '.']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'NNP', 'VBP', 'VBN', 'IN', '.']",8
named-entity-recognition,9,138,"On the other hand , BioBERT achieves higher scores than BERT on all the datasets .","['On', 'the', 'other', 'hand', ',', 'BioBERT', 'achieves', 'higher', 'scores', 'than', 'BERT', 'on', 'all', 'the', 'datasets', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'NNP', 'VBZ', 'JJR', 'NNS', 'IN', 'NNP', 'IN', 'PDT', 'DT', 'NNS', '.']",16
named-entity-recognition,9,139,"BioBERT outperformed the state - of - the - art models on six out of nine datasets , and BioBERT v 1.1 ( PubMed ) outperformed the state - of - the - art models by 0.62 in terms of micro averaged F1 score .","['BioBERT', 'outperformed', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', 'on', 'six', 'out', 'of', 'nine', 'datasets', ',', 'and', 'BioBERT', 'v', '1.1', '(', 'PubMed', ')', 'outperformed', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', 'by', '0.62', 'in', 'terms', 'of', 'micro', 'averaged', 'F1', 'score', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBD', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'CD', 'IN', 'IN', 'CD', 'NNS', ',', 'CC', 'NNP', 'VBP', 'CD', '(', 'NNP', ')', 'VBD', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'CD', 'IN', 'NNS', 'IN', 'NN', 'VBD', 'NNP', 'NN', '.']",45
named-entity-recognition,9,141,The RE results of each model are shown in .,"['The', 'RE', 'results', 'of', 'each', 'model', 'are', 'shown', 'in', '.']","['O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'VBN', 'IN', '.']",10
named-entity-recognition,9,143,"On average ( micro ) , BioBERT v1.0 ( PubMed ) obtained a higher F1 score ( 2.80 higher ) than the state - of - the - art models .","['On', 'average', '(', 'micro', ')', ',', 'BioBERT', 'v1.0', '(', 'PubMed', ')', 'obtained', 'a', 'higher', 'F1', 'score', '(', '2.80', 'higher', ')', 'than', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', '(', 'NN', ')', ',', 'NNP', 'NNP', '(', 'NNP', ')', 'VBD', 'DT', 'JJR', 'NNP', 'NN', '(', 'CD', 'JJR', ')', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', '.']",31
named-entity-recognition,9,144,"Also , BioBERT achieved the highest F 1 scores on 2 out of 3 biomedical datasets .","['Also', ',', 'BioBERT', 'achieved', 'the', 'highest', 'F', '1', 'scores', 'on', '2', 'out', 'of', '3', 'biomedical', 'datasets', '.']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'NNP', 'VBD', 'DT', 'JJS', 'JJ', 'CD', 'NNS', 'IN', 'CD', 'IN', 'IN', 'CD', 'JJ', 'NNS', '.']",17
named-entity-recognition,9,145,The QA results are shown in .,"['The', 'QA', 'results', 'are', 'shown', 'in', '.']","['O', 'B-n', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NNS', 'VBP', 'VBN', 'IN', '.']",7
named-entity-recognition,9,148,"All versions of BioBERT significantly outperformed BERT and the state - of - the - art models , and in particular , BioBERT v1.1 ( PubMed ) obtained a strict accuracy of 38.77 , a lenient accuracy of 53.81 and a mean reciprocal rank score of 44.77 , all of which were micro averaged .","['All', 'versions', 'of', 'BioBERT', 'significantly', 'outperformed', 'BERT', 'and', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', ',', 'and', 'in', 'particular', ',', 'BioBERT', 'v1.1', '(', 'PubMed', ')', 'obtained', 'a', 'strict', 'accuracy', 'of', '38.77', ',', 'a', 'lenient', 'accuracy', 'of', '53.81', 'and', 'a', 'mean', 'reciprocal', 'rank', 'score', 'of', '44.77', ',', 'all', 'of', 'which', 'were', 'micro', 'averaged', '.']","['O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'NNP', 'RB', 'VBD', 'NNP', 'CC', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', ',', 'CC', 'IN', 'JJ', ',', 'NNP', 'NNP', '(', 'NNP', ')', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'CD', ',', 'DT', 'JJ', 'NN', 'IN', 'CD', 'CC', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'CD', ',', 'DT', 'IN', 'WDT', 'VBD', 'NNS', 'VBN', '.']",55
named-entity-recognition,9,149,"On all the biomedical QA datasets , BioBERT achieved new state - of - the - art performance in terms of MRR .","['On', 'all', 'the', 'biomedical', 'QA', 'datasets', ',', 'BioBERT', 'achieved', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'in', 'terms', 'of', 'MRR', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['IN', 'PDT', 'DT', 'JJ', 'NNP', 'NNS', ',', 'NNP', 'VBD', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'NNS', 'IN', 'NNP', '.']",23
named-entity-recognition,1,2,Neural Architectures for Named Entity Recognition,"['Neural', 'Architectures', 'for', 'Named', 'Entity', 'Recognition']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['JJ', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",6
named-entity-recognition,1,7,Our models obtain state - of - the - art performance in NER in four languages without resorting to any language - specific knowledge or resources such as gazetteers .,"['Our', 'models', 'obtain', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'in', 'NER', 'in', 'four', 'languages', 'without', 'resorting', 'to', 'any', 'language', '-', 'specific', 'knowledge', 'or', 'resources', 'such', 'as', 'gazetteers', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NNS', 'VB', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'NNP', 'IN', 'CD', 'NNS', 'IN', 'VBG', 'TO', 'DT', 'NN', ':', 'JJ', 'NN', 'CC', 'NNS', 'JJ', 'IN', 'NNS', '.']",30
named-entity-recognition,1,20,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .","['We', 'compare', 'two', 'models', 'here', ',', '(', 'i', ')', 'a', 'bidirectional', 'LSTM', 'with', 'a', 'sequential', 'conditional', 'random', 'layer', 'above', 'it', '(', 'LSTM', '-', 'CRF', ';', '2', ')', ',', 'and', '(', 'ii', ')', 'a', 'new', 'model', 'that', 'constructs', 'and', 'labels', 'chunks', 'of', 'input', 'sentences', 'using', 'an', 'algorithm', 'inspired', 'by', 'transition', '-', 'based', 'parsing', 'with', 'states', 'represented', 'by', 'stack', 'LSTMs', '(', 'S', '-', 'LSTM', ';', '3', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'CD', 'NNS', 'RB', ',', '(', 'NN', ')', 'DT', 'JJ', 'NNP', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'PRP', '(', 'NNP', ':', 'NNP', ':', 'CD', ')', ',', 'CC', '(', 'NN', ')', 'DT', 'JJ', 'NN', 'WDT', 'NNS', 'CC', 'NNS', 'NNS', 'IN', 'NN', 'NNS', 'VBG', 'DT', 'NN', 'VBN', 'IN', 'NN', ':', 'VBN', 'VBG', 'IN', 'NNS', 'VBN', 'IN', 'NN', 'NNP', '(', 'NNP', ':', 'NNP', ':', 'CD', ')', '.']",66
named-entity-recognition,1,22,"To capture orthographic sensitivity , we use character - based word representation model to capture distributional sensitivity , we combine these representations with distributional representations .","['To', 'capture', 'orthographic', 'sensitivity', ',', 'we', 'use', 'character', '-', 'based', 'word', 'representation', 'model', 'to', 'capture', 'distributional', 'sensitivity', ',', 'we', 'combine', 'these', 'representations', 'with', 'distributional', 'representations', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['TO', 'VB', 'JJ', 'NN', ',', 'PRP', 'VBP', 'JJR', ':', 'VBN', 'NN', 'NN', 'NN', 'TO', 'VB', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNS', 'IN', 'JJ', 'NNS', '.']",26
named-entity-recognition,1,157,"For both models presented , we train our networks using the back - propagation algorithm updating our parameters on every training example , one at a time , using stochastic gradient descent ( SGD ) with a learning rate of 0.01 and a gradient clipping of 5.0 .","['For', 'both', 'models', 'presented', ',', 'we', 'train', 'our', 'networks', 'using', 'the', 'back', '-', 'propagation', 'algorithm', 'updating', 'our', 'parameters', 'on', 'every', 'training', 'example', ',', 'one', 'at', 'a', 'time', ',', 'using', 'stochastic', 'gradient', 'descent', '(', 'SGD', ')', 'with', 'a', 'learning', 'rate', 'of', '0.01', 'and', 'a', 'gradient', 'clipping', 'of', '5.0', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['IN', 'DT', 'NNS', 'VBN', ',', 'PRP', 'VBP', 'PRP$', 'NNS', 'VBG', 'DT', 'JJ', ':', 'NN', 'IN', 'VBG', 'PRP$', 'NNS', 'IN', 'DT', 'NN', 'NN', ',', 'CD', 'IN', 'DT', 'NN', ',', 'VBG', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', 'CC', 'DT', 'JJ', 'NN', 'IN', 'CD', '.']",48
named-entity-recognition,1,160,Our LSTM - CRF model uses a single layer for the forward and backward LSTMs whose dimensions are set to 100 .,"['Our', 'LSTM', '-', 'CRF', 'model', 'uses', 'a', 'single', 'layer', 'for', 'the', 'forward', 'and', 'backward', 'LSTMs', 'whose', 'dimensions', 'are', 'set', 'to', '100', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'O']","['PRP$', 'NNP', ':', 'NNP', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'CC', 'NN', 'NNP', 'WP$', 'NNS', 'VBP', 'VBN', 'TO', 'CD', '.']",22
named-entity-recognition,1,162,We set the dropout rate to 0.5 .,"['We', 'set', 'the', 'dropout', 'rate', 'to', '0.5', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'CD', '.']",8
named-entity-recognition,1,164,The stack - LSTM model uses two layers each of dimension 100 for each stack .,"['The', 'stack', '-', 'LSTM', 'model', 'uses', 'two', 'layers', 'each', 'of', 'dimension', '100', 'for', 'each', 'stack', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', ':', 'NNP', 'NN', 'VBZ', 'CD', 'NNS', 'DT', 'IN', 'NN', 'CD', 'IN', 'DT', 'NN', '.']",16
named-entity-recognition,1,165,"The embeddings of the actions used in the composition functions have 16 dimensions each , and the output embedding is of dimension 20 .","['The', 'embeddings', 'of', 'the', 'actions', 'used', 'in', 'the', 'composition', 'functions', 'have', '16', 'dimensions', 'each', ',', 'and', 'the', 'output', 'embedding', 'is', 'of', 'dimension', '20', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O']","['DT', 'NNS', 'IN', 'DT', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'NNS', 'VBP', 'CD', 'NNS', 'DT', ',', 'CC', 'DT', 'NN', 'VBG', 'VBZ', 'IN', 'NN', 'CD', '.']",24
named-entity-recognition,1,181,"Our LSTM - CRF model outperforms all other systems , including the ones using external labeled data like gazetteers .","['Our', 'LSTM', '-', 'CRF', 'model', 'outperforms', 'all', 'other', 'systems', ',', 'including', 'the', 'ones', 'using', 'external', 'labeled', 'data', 'like', 'gazetteers', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NNP', ':', 'NNP', 'NN', 'NNS', 'DT', 'JJ', 'NNS', ',', 'VBG', 'DT', 'NNS', 'VBG', 'JJ', 'VBN', 'NNS', 'IN', 'NNS', '.']",20
named-entity-recognition,1,182,"Our Stack - LSTM model also outperforms all previous models that do not incorporate external features , apart from the one presented by .","['Our', 'Stack', '-', 'LSTM', 'model', 'also', 'outperforms', 'all', 'previous', 'models', 'that', 'do', 'not', 'incorporate', 'external', 'features', ',', 'apart', 'from', 'the', 'one', 'presented', 'by', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NNP', ':', 'NNP', 'NN', 'RB', 'VBZ', 'DT', 'JJ', 'NNS', 'WDT', 'VBP', 'RB', 'VB', 'JJ', 'NNS', ',', 'RB', 'IN', 'DT', 'CD', 'VBN', 'IN', '.']",24
named-entity-recognition,1,183,"and 4 present our results on NER for German , Dutch and Spanish respectively in comparison to other models .","['and', '4', 'present', 'our', 'results', 'on', 'NER', 'for', 'German', ',', 'Dutch', 'and', 'Spanish', 'respectively', 'in', 'comparison', 'to', 'other', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['CC', 'CD', 'JJ', 'PRP$', 'NNS', 'IN', 'NNP', 'IN', 'NNP', ',', 'NNP', 'CC', 'NNP', 'RB', 'IN', 'NN', 'TO', 'JJ', 'NNS', '.']",20
named-entity-recognition,1,184,"On these three languages , the LSTM - CRF model significantly outperforms all previous methods , including the ones using external labeled data .","['On', 'these', 'three', 'languages', ',', 'the', 'LSTM', '-', 'CRF', 'model', 'significantly', 'outperforms', 'all', 'previous', 'methods', ',', 'including', 'the', 'ones', 'using', 'external', 'labeled', 'data', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'CD', 'NNS', ',', 'DT', 'NNP', ':', 'NNP', 'NN', 'RB', 'VBZ', 'DT', 'JJ', 'NNS', ',', 'VBG', 'DT', 'NNS', 'VBG', 'JJ', 'VBN', 'NNS', '.']",24
named-entity-recognition,1,185,"The only exception is Dutch , where the model of can perform better by leveraging the information from other NER datasets .","['The', 'only', 'exception', 'is', 'Dutch', ',', 'where', 'the', 'model', 'of', 'can', 'perform', 'better', 'by', 'leveraging', 'the', 'information', 'from', 'other', 'NER', 'datasets', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'NNP', ',', 'WRB', 'DT', 'NN', 'IN', 'MD', 'VB', 'JJR', 'IN', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'NNP', 'NNS', '.']",22
named-entity-recognition,1,186,The Stack - LSTM also consistently presents statethe - art ( or close to ) results compared to systems that do not use external data .,"['The', 'Stack', '-', 'LSTM', 'also', 'consistently', 'presents', 'statethe', '-', 'art', '(', 'or', 'close', 'to', ')', 'results', 'compared', 'to', 'systems', 'that', 'do', 'not', 'use', 'external', 'data', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', ':', 'NNP', 'RB', 'RB', 'VBZ', 'JJ', ':', 'NN', '(', 'CC', 'RB', 'TO', ')', 'NNS', 'VBN', 'TO', 'NNS', 'WDT', 'VBP', 'RB', 'VB', 'JJ', 'NNS', '.']",26
named-entity-recognition,5,2,Sentence - State LSTM for Text Representation,"['Sentence', '-', 'State', 'LSTM', 'for', 'Text', 'Representation']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NN', 'NNP', 'IN', 'NNP', 'NNP']",7
named-entity-recognition,5,7,"We investigate an alternative LSTM structure for encoding text , which consists of a parallel state for each word .","['We', 'investigate', 'an', 'alternative', 'LSTM', 'structure', 'for', 'encoding', 'text', ',', 'which', 'consists', 'of', 'a', 'parallel', 'state', 'for', 'each', 'word', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NNP', 'NN', 'IN', 'VBG', 'NN', ',', 'WDT', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",20
named-entity-recognition,5,18,We investigate an alternative recurrent neural network structure for addressing these issues .,"['We', 'investigate', 'an', 'alternative', 'recurrent', 'neural', 'network', 'structure', 'for', 'addressing', 'these', 'issues', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'JJ', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NNS', '.']",13
named-entity-recognition,5,19,"As shown in , the main idea is to model the hidden states of all words simultaneously at each recurrent step , rather than one word at a time .","['As', 'shown', 'in', ',', 'the', 'main', 'idea', 'is', 'to', 'model', 'the', 'hidden', 'states', 'of', 'all', 'words', 'simultaneously', 'at', 'each', 'recurrent', 'step', ',', 'rather', 'than', 'one', 'word', 'at', 'a', 'time', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBN', 'IN', ',', 'DT', 'JJ', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NNS', 'RB', 'IN', 'DT', 'JJ', 'NN', ',', 'RB', 'IN', 'CD', 'NN', 'IN', 'DT', 'NN', '.']",30
named-entity-recognition,5,20,"In particular , we view the whole sentence as a single state , which consists of sub-states for individual words and an over all sentence - level state .","['In', 'particular', ',', 'we', 'view', 'the', 'whole', 'sentence', 'as', 'a', 'single', 'state', ',', 'which', 'consists', 'of', 'sub-states', 'for', 'individual', 'words', 'and', 'an', 'over', 'all', 'sentence', '-', 'level', 'state', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'IN', 'NNS', 'IN', 'JJ', 'NNS', 'CC', 'DT', 'IN', 'DT', 'NN', ':', 'NN', 'NN', '.']",29
named-entity-recognition,5,21,"To capture local and non-local contexts , states are updated recurrently by exchanging information between each other .","['To', 'capture', 'local', 'and', 'non-local', 'contexts', ',', 'states', 'are', 'updated', 'recurrently', 'by', 'exchanging', 'information', 'between', 'each', 'other', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'JJ', 'CC', 'JJ', 'NN', ',', 'NNS', 'VBP', 'VBN', 'RB', 'IN', 'VBG', 'NN', 'IN', 'DT', 'JJ', '.']",18
named-entity-recognition,5,25,"At each recurrent step , information exchange is conducted between consecutive words in the sentence , and between the sentence - level state and each word .","['At', 'each', 'recurrent', 'step', ',', 'information', 'exchange', 'is', 'conducted', 'between', 'consecutive', 'words', 'in', 'the', 'sentence', ',', 'and', 'between', 'the', 'sentence', '-', 'level', 'state', 'and', 'each', 'word', '.']","['B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', ',', 'CC', 'IN', 'DT', 'NN', ':', 'NN', 'NN', 'CC', 'DT', 'NN', '.']",27
named-entity-recognition,5,26,"In particular , each word receives information from its predecessor and successor simultaneously .","['In', 'particular', ',', 'each', 'word', 'receives', 'information', 'from', 'its', 'predecessor', 'and', 'successor', 'simultaneously', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', ',', 'DT', 'NN', 'VBZ', 'NN', 'IN', 'PRP$', 'NN', 'CC', 'NN', 'RB', '.']",14
named-entity-recognition,5,31,"We release our code and models at https://github.com/ leuchine /S - LSTM , which include all baselines and the final model .","['We', 'release', 'our', 'code', 'and', 'models', 'at', 'https://github.com/', 'leuchine', '/S', '-', 'LSTM', ',', 'which', 'include', 'all', 'baselines', 'and', 'the', 'final', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'CC', 'NNS', 'IN', 'JJ', 'NN', 'NNP', ':', 'NNP', ',', 'WDT', 'VBP', 'DT', 'NNS', 'CC', 'DT', 'JJ', 'NN', '.']",22
named-entity-recognition,5,148,All experiments are conducted using a GeForce GTX 1080 GPU with 8 GB memory .,"['All', 'experiments', 'are', 'conducted', 'using', 'a', 'GeForce', 'GTX', '1080', 'GPU', 'with', '8', 'GB', 'memory', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'VBG', 'DT', 'NNP', 'NNP', 'CD', 'NNP', 'IN', 'CD', 'NNP', 'NN', '.']",15
named-entity-recognition,5,165,Final Results for Classification,"['Final', 'Results', 'for', 'Classification']","['O', 'O', 'B-p', 'B-n']","['JJ', 'NNS', 'IN', 'NN']",4
named-entity-recognition,5,170,"As shown in , the final results on the movie review dataset are consistent with the development results , where S - LSTM outperforms BiL - STM significantly , with a faster speed .","['As', 'shown', 'in', ',', 'the', 'final', 'results', 'on', 'the', 'movie', 'review', 'dataset', 'are', 'consistent', 'with', 'the', 'development', 'results', ',', 'where', 'S', '-', 'LSTM', 'outperforms', 'BiL', '-', 'STM', 'significantly', ',', 'with', 'a', 'faster', 'speed', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['IN', 'VBN', 'IN', ',', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', 'NN', 'VBP', 'JJ', 'IN', 'DT', 'NN', 'NNS', ',', 'WRB', 'NNP', ':', 'NN', 'NNS', 'NNP', ':', 'NNP', 'RB', ',', 'IN', 'DT', 'JJR', 'NN', '.']",34
named-entity-recognition,5,173,"As shown in , among the 16 datasets of , S - LSTM gives the best results on 12 , compared with BiLSTM and 2 layered BiL - STM models .","['As', 'shown', 'in', ',', 'among', 'the', '16', 'datasets', 'of', ',', 'S', '-', 'LSTM', 'gives', 'the', 'best', 'results', 'on', '12', ',', 'compared', 'with', 'BiLSTM', 'and', '2', 'layered', 'BiL', '-', 'STM', 'models', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBN', 'IN', ',', 'IN', 'DT', 'CD', 'NNS', 'IN', ',', 'NNP', ':', 'NNP', 'VBZ', 'DT', 'JJS', 'NNS', 'IN', 'CD', ',', 'VBN', 'IN', 'NNP', 'CC', 'CD', 'VBD', 'NNP', ':', 'NNP', 'NNS', '.']",31
named-entity-recognition,5,174,"The average accuracy of S - LSTM is 85.6 % , significantly higher compared with 84.9 % by 2 - layer stacked BiLSTM .","['The', 'average', 'accuracy', 'of', 'S', '-', 'LSTM', 'is', '85.6', '%', ',', 'significantly', 'higher', 'compared', 'with', '84.9', '%', 'by', '2', '-', 'layer', 'stacked', 'BiLSTM', '.']","['O', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'IN', 'NNP', ':', 'NNP', 'VBZ', 'CD', 'NN', ',', 'RB', 'JJR', 'VBN', 'IN', 'CD', 'NN', 'IN', 'CD', ':', 'NN', 'VBD', 'NNP', '.']",24
named-entity-recognition,5,178,Final Results for Sequence Labelling,"['Final', 'Results', 'for', 'Sequence', 'Labelling']","['O', 'O', 'O', 'B-n', 'I-n']","['JJ', 'NNS', 'IN', 'NNP', 'NNP']",5
named-entity-recognition,5,187,"For NER , S - LSTM gives an F1 - score of 91.57 % on the CoNLL test set , which is significantly better compared with BiLSTMs .","['For', 'NER', ',', 'S', '-', 'LSTM', 'gives', 'an', 'F1', '-', 'score', 'of', '91.57', '%', 'on', 'the', 'CoNLL', 'test', 'set', ',', 'which', 'is', 'significantly', 'better', 'compared', 'with', 'BiLSTMs', '.']","['B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ',', 'NNP', ':', 'NNP', 'VBZ', 'DT', 'NNP', ':', 'NN', 'IN', 'CD', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NN', ',', 'WDT', 'VBZ', 'RB', 'RBR', 'VBN', 'IN', 'NNP', '.']",28
named-entity-recognition,7,2,A Neural Transition - based Model for Nested Mention Recognition,"['A', 'Neural', 'Transition', '-', 'based', 'Model', 'for', 'Nested', 'Mention', 'Recognition']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['DT', 'JJ', 'NNP', ':', 'VBN', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",10
named-entity-recognition,7,5,This paper introduces a scalable transition - based method to model the nested structure of mentions .,"['This', 'paper', 'introduces', 'a', 'scalable', 'transition', '-', 'based', 'method', 'to', 'model', 'the', 'nested', 'structure', 'of', 'mentions', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', ':', 'VBN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NNS', '.']",17
named-entity-recognition,7,12,"There has been an increasing interest in named entity recognition or more generally recognizing entity mentions 2 ) that the nested hierarchical structure of entity mentions should betaken into account to better facilitate downstream tasks like question answering , relation extraction , event extraction , and coreference resolution .","['There', 'has', 'been', 'an', 'increasing', 'interest', 'in', 'named', 'entity', 'recognition', 'or', 'more', 'generally', 'recognizing', 'entity', 'mentions', '2', ')', 'that', 'the', 'nested', 'hierarchical', 'structure', 'of', 'entity', 'mentions', 'should', 'betaken', 'into', 'account', 'to', 'better', 'facilitate', 'downstream', 'tasks', 'like', 'question', 'answering', ',', 'relation', 'extraction', ',', 'event', 'extraction', ',', 'and', 'coreference', 'resolution', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['EX', 'VBZ', 'VBN', 'DT', 'VBG', 'NN', 'IN', 'VBN', 'NN', 'NN', 'CC', 'JJR', 'RB', 'VBG', 'NN', 'NNS', 'CD', ')', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'NN', 'NNS', 'MD', 'VB', 'IN', 'NN', 'TO', 'JJR', 'VB', 'NN', 'NNS', 'IN', 'NN', 'NN', ',', 'NN', 'NN', ',', 'NN', 'NN', ',', 'CC', 'NN', 'NN', '.']",49
named-entity-recognition,7,17,"To achieve a scalable and effective solution for recognizing nested mentions , we design a transition - based system which is inspired by the recent success of employing transition - based methods for constituent parsing ) and named entity recognition , especially when they are paired with neural networks .","['To', 'achieve', 'a', 'scalable', 'and', 'effective', 'solution', 'for', 'recognizing', 'nested', 'mentions', ',', 'we', 'design', 'a', 'transition', '-', 'based', 'system', 'which', 'is', 'inspired', 'by', 'the', 'recent', 'success', 'of', 'employing', 'transition', '-', 'based', 'methods', 'for', 'constituent', 'parsing', ')', 'and', 'named', 'entity', 'recognition', ',', 'especially', 'when', 'they', 'are', 'paired', 'with', 'neural', 'networks', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'JJ', 'CC', 'JJ', 'NN', 'IN', 'VBG', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', ':', 'VBN', 'NN', 'WDT', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'NN', ':', 'VBN', 'NNS', 'IN', 'JJ', 'NN', ')', 'CC', 'VBN', 'NN', 'NN', ',', 'RB', 'WRB', 'PRP', 'VBP', 'VBN', 'IN', 'JJ', 'NNS', '.']",50
named-entity-recognition,7,18,"Generally , each sentence with nested mentions is mapped to a forest where each outermost mention forms a tree consisting of its inner mentions .","['Generally', ',', 'each', 'sentence', 'with', 'nested', 'mentions', 'is', 'mapped', 'to', 'a', 'forest', 'where', 'each', 'outermost', 'mention', 'forms', 'a', 'tree', 'consisting', 'of', 'its', 'inner', 'mentions', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'VBZ', 'VBN', 'TO', 'DT', 'NN', 'WRB', 'DT', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'PRP$', 'JJ', 'NNS', '.']",25
named-entity-recognition,7,19,Then our transition - based system learns to construct this forest through a sequence of shift - reduce actions .,"['Then', 'our', 'transition', '-', 'based', 'system', 'learns', 'to', 'construct', 'this', 'forest', 'through', 'a', 'sequence', 'of', 'shift', '-', 'reduce', 'actions', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', 'PRP$', 'NN', ':', 'VBN', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', ':', 'VB', 'NNS', '.']",20
named-entity-recognition,7,23,"Following , we employ Stack - LSTM to represent the system 's state , which consists of the states of input , stack and action history , in a continuous space incrementally .","['Following', ',', 'we', 'employ', 'Stack', '-', 'LSTM', 'to', 'represent', 'the', 'system', ""'s"", 'state', ',', 'which', 'consists', 'of', 'the', 'states', 'of', 'input', ',', 'stack', 'and', 'action', 'history', ',', 'in', 'a', 'continuous', 'space', 'incrementally', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['VBG', ',', 'PRP', 'VBP', 'NNP', ':', 'NN', 'TO', 'VB', 'DT', 'NN', 'POS', 'NN', ',', 'WDT', 'VBZ', 'IN', 'DT', 'NNS', 'IN', 'NN', ',', 'NN', 'CC', 'NN', 'NN', ',', 'IN', 'DT', 'JJ', 'NN', 'RB', '.']",33
named-entity-recognition,7,24,The ( partially ) processed nested mentions in the stack are encoded with recursive neural networks where composition functions are used to capture dependencies between nested mentions .,"['The', '(', 'partially', ')', 'processed', 'nested', 'mentions', 'in', 'the', 'stack', 'are', 'encoded', 'with', 'recursive', 'neural', 'networks', 'where', 'composition', 'functions', 'are', 'used', 'to', 'capture', 'dependencies', 'between', 'nested', 'mentions', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', '(', 'RB', ')', 'VBD', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'VBN', 'IN', 'JJ', 'JJ', 'NNS', 'WRB', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'VB', 'NNS', 'IN', 'JJ', 'NNS', '.']",28
named-entity-recognition,7,25,"Based on the observation that letter - level patterns such as capitalization and prefix can be beneficial in detecting mentions , we incorporate a characterlevel LSTM to capture such morphological information .","['Based', 'on', 'the', 'observation', 'that', 'letter', '-', 'level', 'patterns', 'such', 'as', 'capitalization', 'and', 'prefix', 'can', 'be', 'beneficial', 'in', 'detecting', 'mentions', ',', 'we', 'incorporate', 'a', 'characterlevel', 'LSTM', 'to', 'capture', 'such', 'morphological', 'information', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'NN', 'IN', 'NN', ':', 'NN', 'NNS', 'JJ', 'IN', 'NN', 'CC', 'NN', 'MD', 'VB', 'JJ', 'IN', 'VBG', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'NNP', 'TO', 'VB', 'JJ', 'JJ', 'NN', '.']",32
named-entity-recognition,7,26,"Meanwhile , this character - level component can also help deal with the out - of - vocabulary problem of neural models .","['Meanwhile', ',', 'this', 'character', '-', 'level', 'component', 'can', 'also', 'help', 'deal', 'with', 'the', 'out', '-', 'of', '-', 'vocabulary', 'problem', 'of', 'neural', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', ':', 'NN', 'NN', 'MD', 'RB', 'VB', 'NN', 'IN', 'DT', 'RP', ':', 'IN', ':', 'JJ', 'NN', 'IN', 'JJ', 'NNS', '.']",23
named-entity-recognition,7,123,Pre-trained embeddings,"['Pre-trained', 'embeddings']","['B-p', 'I-p']","['JJ', 'NNS']",2
named-entity-recognition,7,124,Glo Ve of dimension 100 are used to initialize the word vectors for all three datasets .,"['Glo', 'Ve', 'of', 'dimension', '100', 'are', 'used', 'to', 'initialize', 'the', 'word', 'vectors', 'for', 'all', 'three', 'datasets', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'IN', 'NN', 'CD', 'VBP', 'VBN', 'TO', 'VB', 'DT', 'NN', 'NNS', 'IN', 'DT', 'CD', 'NNS', '.']",17
named-entity-recognition,7,125,9 The embeddings of POS tags are initialized randomly with dimension 32 .,"['9', 'The', 'embeddings', 'of', 'POS', 'tags', 'are', 'initialized', 'randomly', 'with', 'dimension', '32', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['CD', 'DT', 'NNS', 'IN', 'NNP', 'NNS', 'VBP', 'VBN', 'RB', 'IN', 'NN', 'CD', '.']",13
named-entity-recognition,7,126,The model is trained using Adam and a gradient clipping of 3.0 .,"['The', 'model', 'is', 'trained', 'using', 'Adam', 'and', 'a', 'gradient', 'clipping', 'of', '3.0', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'VBG', 'NNP', 'CC', 'DT', 'JJ', 'NN', 'IN', 'CD', '.']",13
named-entity-recognition,7,133,Our neural transition - based model achieves the best results in ACE datasets and comparable results in GENIA dataset in terms of F 1 measure .,"['Our', 'neural', 'transition', '-', 'based', 'model', 'achieves', 'the', 'best', 'results', 'in', 'ACE', 'datasets', 'and', 'comparable', 'results', 'in', 'GENIA', 'dataset', 'in', 'terms', 'of', 'F', '1', 'measure', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'JJ', 'NN', ':', 'VBN', 'NN', 'VBZ', 'DT', 'JJS', 'NNS', 'IN', 'NNP', 'NNS', 'CC', 'JJ', 'NNS', 'IN', 'NNP', 'NN', 'IN', 'NNS', 'IN', 'NNP', 'CD', 'NN', '.']",26
named-entity-recognition,7,139,"We can observe that the margin of improvement is more significant in the portion of nested mentions , revealing our model 's effectiveness in handling nested mentions .","['We', 'can', 'observe', 'that', 'the', 'margin', 'of', 'improvement', 'is', 'more', 'significant', 'in', 'the', 'portion', 'of', 'nested', 'mentions', ',', 'revealing', 'our', 'model', ""'s"", 'effectiveness', 'in', 'handling', 'nested', 'mentions', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'IN', 'DT', 'NN', 'IN', 'NN', 'VBZ', 'RBR', 'JJ', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', ',', 'VBG', 'PRP$', 'NN', 'POS', 'NN', 'IN', 'VBG', 'JJ', 'NNS', '.']",28
named-entity-recognition,7,148,"To evaluate the contribution of neural components including pre-trained embeddings , the characterlevel LSTM and dropout layers , we test the performances of ablated models .","['To', 'evaluate', 'the', 'contribution', 'of', 'neural', 'components', 'including', 'pre-trained', 'embeddings', ',', 'the', 'characterlevel', 'LSTM', 'and', 'dropout', 'layers', ',', 'we', 'test', 'the', 'performances', 'of', 'ablated', 'models', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'VBG', 'JJ', 'NNS', ',', 'DT', 'NN', 'NNP', 'CC', 'NN', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NNS', 'IN', 'JJ', 'NNS', '.']",26
named-entity-recognition,7,150,"From the performance gap , we can conclude that these components contribute significantly to the effectiveness of our model in all three datasets .","['From', 'the', 'performance', 'gap', ',', 'we', 'can', 'conclude', 'that', 'these', 'components', 'contribute', 'significantly', 'to', 'the', 'effectiveness', 'of', 'our', 'model', 'in', 'all', 'three', 'datasets', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'NN', ',', 'PRP', 'MD', 'VB', 'IN', 'DT', 'NNS', 'VBP', 'RB', 'TO', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'IN', 'DT', 'CD', 'NNS', '.']",24
named-entity-recognition,4,2,Deep contextualized word representations,"['Deep', 'contextualized', 'word', 'representations']","['B-n', 'I-n', 'I-n', 'I-n']","['NNP', 'VBD', 'NN', 'NNS']",4
named-entity-recognition,4,13,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,"['Our', 'representations', 'differ', 'from', 'traditional', 'word', 'type', 'embeddings', 'in', 'that', 'each', 'token', 'is', 'assigned', 'a', 'representation', 'that', 'is', 'a', 'function', 'of', 'the', 'entire', 'input', 'sentence', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NNS', 'VBP', 'IN', 'JJ', 'NN', 'NN', 'NNS', 'IN', 'DT', 'DT', 'NN', 'VBZ', 'VBN', 'DT', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",26
named-entity-recognition,4,14,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,"['We', 'use', 'vectors', 'derived', 'from', 'a', 'bidirectional', 'LSTM', 'that', 'is', 'trained', 'with', 'a', 'coupled', 'lan', '-', 'guage', 'model', '(', 'LM', ')', 'objective', 'on', 'a', 'large', 'text', 'corpus', '.']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNS', 'VBN', 'IN', 'DT', 'JJ', 'NNP', 'WDT', 'VBZ', 'VBN', 'IN', 'DT', 'VBN', 'NN', ':', 'NN', 'NN', '(', 'NNP', ')', 'VBP', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",28
named-entity-recognition,4,15,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .","['For', 'this', 'reason', ',', 'we', 'call', 'them', 'ELMo', '(', 'Embeddings', 'from', 'Language', 'Models', ')', 'representations', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'PRP', 'NNP', '(', 'NNS', 'IN', 'NNP', 'NNP', ')', 'NNS', '.']",16
named-entity-recognition,4,17,"More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .","['More', 'specifically', ',', 'we', 'learn', 'a', 'linear', 'combination', 'of', 'the', 'vectors', 'stacked', 'above', 'each', 'input', 'word', 'for', 'each', 'end', 'task', ',', 'which', 'markedly', 'improves', 'performance', 'over', 'just', 'using', 'the', 'top', 'LSTM', 'layer', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RBR', 'RB', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNS', 'VBD', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'WDT', 'RB', 'VBZ', 'NN', 'IN', 'RB', 'VBG', 'DT', 'JJ', 'NNP', 'NN', '.']",33
named-entity-recognition,4,108,"Textual entailment is the task of determining whether a "" hypothesis "" is true , given a "" premise "" .","['Textual', 'entailment', 'is', 'the', 'task', 'of', 'determining', 'whether', 'a', '""', 'hypothesis', '""', 'is', 'true', ',', 'given', 'a', '""', 'premise', '""', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'JJ', ',', 'VBN', 'DT', 'JJ', 'NN', 'NN', '.']",21
named-entity-recognition,4,109,The Stanford Natural Language Inference ( SNLI ) corpus provides approximately 550K hypothesis / premise pairs .,"['The', 'Stanford', 'Natural', 'Language', 'Inference', '(', 'SNLI', ')', 'corpus', 'provides', 'approximately', '550K', 'hypothesis', '/', 'premise', 'pairs', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', 'VBZ', 'RB', 'CD', 'NN', 'NNP', 'NN', 'NN', '.']",17
named-entity-recognition,4,111,"Overall , adding ELMo to the ESIM model improves accuracy by an average of 0.7 % across five random seeds .","['Overall', ',', 'adding', 'ELMo', 'to', 'the', 'ESIM', 'model', 'improves', 'accuracy', 'by', 'an', 'average', 'of', '0.7', '%', 'across', 'five', 'random', 'seeds', '.']","['O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['JJ', ',', 'VBG', 'NNP', 'TO', 'DT', 'NNP', 'NN', 'VBZ', 'RP', 'IN', 'DT', 'NN', 'IN', 'CD', 'NN', 'IN', 'CD', 'NN', 'NNS', '.']",21
named-entity-recognition,4,117,As shown in Coreference resolution Coreference resolution is the task of clustering mentions in text that refer to the same underlying real world entities .,"['As', 'shown', 'in', 'Coreference', 'resolution', 'Coreference', 'resolution', 'is', 'the', 'task', 'of', 'clustering', 'mentions', 'in', 'text', 'that', 'refer', 'to', 'the', 'same', 'underlying', 'real', 'world', 'entities', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBN', 'IN', 'NNP', 'NN', 'NNP', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'NNS', 'IN', 'NN', 'WDT', 'VBP', 'TO', 'DT', 'JJ', 'JJ', 'JJ', 'NN', 'NNS', '.']",25
named-entity-recognition,4,120,"In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task , adding ELMo improved the average F 1 by 3.2 % from 67.2 to 70.4 , establishing a new state of the art , again improving over the previous best ensemble result by 1.6 % F 1 .","['In', 'our', 'experiments', 'with', 'the', 'OntoNotes', 'coreference', 'annotations', 'from', 'the', 'CoNLL', '2012', 'shared', 'task', ',', 'adding', 'ELMo', 'improved', 'the', 'average', 'F', '1', 'by', '3.2', '%', 'from', '67.2', 'to', '70.4', ',', 'establishing', 'a', 'new', 'state', 'of', 'the', 'art', ',', 'again', 'improving', 'over', 'the', 'previous', 'best', 'ensemble', 'result', 'by', '1.6', '%', 'F', '1', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PRP$', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'CD', 'VBD', 'NN', ',', 'VBG', 'NNP', 'VBD', 'DT', 'JJ', 'NNP', 'CD', 'IN', 'CD', 'NN', 'IN', 'CD', 'TO', 'CD', ',', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', ',', 'RB', 'VBG', 'IN', 'DT', 'JJ', 'RBS', 'JJ', 'NN', 'IN', 'CD', 'NN', 'NNP', 'CD', '.']",52
named-entity-recognition,6,2,Robust Lexical Features for Improved Neural Network Named - Entity Recognition,"['Robust', 'Lexical', 'Features', 'for', 'Improved', 'Neural', 'Network', 'Named', '-', 'Entity', 'Recognition']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', ':', 'NN', 'NN']",11
named-entity-recognition,6,4,Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features .,"['Neural', 'network', 'approaches', 'to', 'Named', '-', 'Entity', 'Recognition', 'reduce', 'the', 'need', 'for', 'carefully', 'handcrafted', 'features', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NNS', 'TO', 'NNP', ':', 'NN', 'NNP', 'VB', 'DT', 'NN', 'IN', 'RB', 'VBN', 'NNS', '.']",16
named-entity-recognition,6,14,Named - Entity Recognition ( NER ) is the task of identifying textual mentions and classifying them into a predefined set of types .,"['Named', '-', 'Entity', 'Recognition', '(', 'NER', ')', 'is', 'the', 'task', 'of', 'identifying', 'textual', 'mentions', 'and', 'classifying', 'them', 'into', 'a', 'predefined', 'set', 'of', 'types', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', ':', 'NN', 'NNP', '(', 'NNP', ')', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'JJ', 'NNS', 'CC', 'VBG', 'PRP', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', '.']",24
named-entity-recognition,6,24,"In this work , we discuss some of the limitations of gazetteer features and propose an alternative lexical representation which is trained offline and that can be added to any neural NER system .","['In', 'this', 'work', ',', 'we', 'discuss', 'some', 'of', 'the', 'limitations', 'of', 'gazetteer', 'features', 'and', 'propose', 'an', 'alternative', 'lexical', 'representation', 'which', 'is', 'trained', 'offline', 'and', 'that', 'can', 'be', 'added', 'to', 'any', 'neural', 'NER', 'system', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'IN', 'DT', 'NNS', 'IN', 'NN', 'NNS', 'CC', 'VB', 'DT', 'JJ', 'JJ', 'NN', 'WDT', 'VBZ', 'JJ', 'NN', 'CC', 'DT', 'MD', 'VB', 'VBN', 'TO', 'DT', 'JJ', 'NNP', 'NN', '.']",34
named-entity-recognition,6,25,"In a nutshell , we embed words and entity types into a joint vector space by leveraging WiFiNE , a ressource which automatically annotates mentions in Wikipedia with 120 entity types .","['In', 'a', 'nutshell', ',', 'we', 'embed', 'words', 'and', 'entity', 'types', 'into', 'a', 'joint', 'vector', 'space', 'by', 'leveraging', 'WiFiNE', ',', 'a', 'ressource', 'which', 'automatically', 'annotates', 'mentions', 'in', 'Wikipedia', 'with', '120', 'entity', 'types', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBD', 'NNS', 'CC', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'VBG', 'NNP', ',', 'DT', 'NN', 'WDT', 'RB', 'VBZ', 'NNS', 'IN', 'NNP', 'IN', 'CD', 'NN', 'NNS', '.']",32
named-entity-recognition,6,139,Training is carried out by mini-batch stochastic gradient descent ( SGD ) with a momentum of 0.9 and a gradient clipping of 5.0 .,"['Training', 'is', 'carried', 'out', 'by', 'mini-batch', 'stochastic', 'gradient', 'descent', '(', 'SGD', ')', 'with', 'a', 'momentum', 'of', '0.9', 'and', 'a', 'gradient', 'clipping', 'of', '5.0', '.']","['B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'VBZ', 'VBN', 'RP', 'IN', 'JJ', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'IN', 'DT', 'NN', 'IN', 'CD', 'CC', 'DT', 'JJ', 'NN', 'IN', 'CD', '.']",24
named-entity-recognition,6,140,"The mini-batch is 10 for both datasets , and learning rates are 0.009 and 0.013 for CONLL and ONTONOTES respectively .","['The', 'mini-batch', 'is', '10', 'for', 'both', 'datasets', ',', 'and', 'learning', 'rates', 'are', '0.009', 'and', '0.013', 'for', 'CONLL', 'and', 'ONTONOTES', 'respectively', '.']","['O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O']","['DT', 'NN', 'VBZ', 'CD', 'IN', 'DT', 'NNS', ',', 'CC', 'VBG', 'NNS', 'VBP', 'CD', 'CC', 'CD', 'IN', 'NNP', 'CC', 'NNP', 'RB', '.']",21
named-entity-recognition,6,147,"We varied dropout . 65 ] ) , hidden units ) , capitalization ) and char ) embedding dimensions , learning rate ( [ 0.001 , 0.015 ] by step 0.002 ) , and optimization algorithms and fixed the other hyper - parameters .","['We', 'varied', 'dropout', '.', '65', ']', ')', ',', 'hidden', 'units', ')', ',', 'capitalization', ')', 'and', 'char', ')', 'embedding', 'dimensions', ',', 'learning', 'rate', '(', '[', '0.001', ',', '0.015', ']', 'by', 'step', '0.002', ')', ',', 'and', 'optimization', 'algorithms', 'and', 'fixed', 'the', 'other', 'hyper', '-', 'parameters', '.']","['O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'NN', '.', 'CD', 'NN', ')', ',', 'JJ', 'NNS', ')', ',', 'NN', ')', 'CC', 'NN', ')', 'VBG', 'NNS', ',', 'VBG', 'NN', '(', '$', 'CD', ',', 'CD', 'NN', 'IN', 'NN', 'CD', ')', ',', 'CC', 'NN', 'NNS', 'CC', 'VBN', 'DT', 'JJ', 'JJ', ':', 'NNS', '.']",44
named-entity-recognition,6,148,"We implemented our system using the Tensorflow library , and ran our models on a GeForce GTX TITAN Xp GPU .","['We', 'implemented', 'our', 'system', 'using', 'the', 'Tensorflow', 'library', ',', 'and', 'ran', 'our', 'models', 'on', 'a', 'GeForce', 'GTX', 'TITAN', 'Xp', 'GPU', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'PRP$', 'NN', 'VBG', 'DT', 'NNP', 'NN', ',', 'CC', 'VBD', 'PRP$', 'NNS', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', '.']",21
named-entity-recognition,6,155,"First , we observe that our model significantly outperforms models that use extensive sets of handcrafted features ) as well as the system of Standard deviation on the test set is reported in 2015 ) that uses NE and Entity Linking annotations to jointly optimize the performance on both tasks .","['First', ',', 'we', 'observe', 'that', 'our', 'model', 'significantly', 'outperforms', 'models', 'that', 'use', 'extensive', 'sets', 'of', 'handcrafted', 'features', ')', 'as', 'well', 'as', 'the', 'system', 'of', 'Standard', 'deviation', 'on', 'the', 'test', 'set', 'is', 'reported', 'in', '2015', ')', 'that', 'uses', 'NE', 'and', 'Entity', 'Linking', 'annotations', 'to', 'jointly', 'optimize', 'the', 'performance', 'on', 'both', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'IN', 'PRP$', 'NN', 'RB', 'VBZ', 'NNS', 'IN', 'VBP', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', ')', 'RB', 'RB', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', ')', 'WDT', 'VBZ', 'NNP', 'CC', 'NNP', 'NNP', 'NNS', 'TO', 'RB', 'VB', 'DT', 'NN', 'IN', 'DT', 'NNS', '.']",51
named-entity-recognition,6,156,"Second , our model outperforms as well other NN models that only use standard word embeddings , which indicates that our lexical feature vector is complementary to standard word embeddings .","['Second', ',', 'our', 'model', 'outperforms', 'as', 'well', 'other', 'NN', 'models', 'that', 'only', 'use', 'standard', 'word', 'embeddings', ',', 'which', 'indicates', 'that', 'our', 'lexical', 'feature', 'vector', 'is', 'complementary', 'to', 'standard', 'word', 'embeddings', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', ',', 'PRP$', 'NN', 'VBZ', 'RB', 'RB', 'JJ', 'NNP', 'NNS', 'IN', 'RB', 'VB', 'JJ', 'NN', 'NNS', ',', 'WDT', 'VBZ', 'IN', 'PRP$', 'JJ', 'NN', 'NN', 'VBZ', 'JJ', 'TO', 'VB', 'NN', 'NNS', '.']",31
named-entity-recognition,6,157,"Third , our system matches state - of - the - art performances of models that use either more complex architectures or more elaborate features .","['Third', ',', 'our', 'system', 'matches', 'state', '-', 'of', '-', 'the', '-', 'art', 'performances', 'of', 'models', 'that', 'use', 'either', 'more', 'complex', 'architectures', 'or', 'more', 'elaborate', 'features', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ',', 'PRP$', 'NN', 'NNS', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'NNS', 'WDT', 'VBP', 'DT', 'RBR', 'JJ', 'NNS', 'CC', 'JJR', 'JJ', 'NNS', '.']",26
named-entity-recognition,6,167,"In particular , our system significantly outperforms the Bi - LSTM - CNN - CRF models of ( Chiu and Nichols , 2016 ) and by an absolute gain of 1.68 and 0.96 points respectively .","['In', 'particular', ',', 'our', 'system', 'significantly', 'outperforms', 'the', 'Bi', '-', 'LSTM', '-', 'CNN', '-', 'CRF', 'models', 'of', '(', 'Chiu', 'and', 'Nichols', ',', '2016', ')', 'and', 'by', 'an', 'absolute', 'gain', 'of', '1.68', 'and', '0.96', 'points', 'respectively', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', ',', 'PRP$', 'NN', 'RB', 'VBZ', 'DT', 'NNP', ':', 'NNP', ':', 'NNP', ':', 'NNP', 'NNS', 'IN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'CC', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', 'CC', 'CD', 'NNS', 'RB', '.']",36
named-entity-recognition,6,168,"Less surprisingly , it surpasses systems with hand - crafted features , including that use gazetteers , and the system of which uses coreference annotation in ONTONOTES to jointly model NER , entity linking , and coreference resolution tasks .","['Less', 'surprisingly', ',', 'it', 'surpasses', 'systems', 'with', 'hand', '-', 'crafted', 'features', ',', 'including', 'that', 'use', 'gazetteers', ',', 'and', 'the', 'system', 'of', 'which', 'uses', 'coreference', 'annotation', 'in', 'ONTONOTES', 'to', 'jointly', 'model', 'NER', ',', 'entity', 'linking', ',', 'and', 'coreference', 'resolution', 'tasks', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'RB', ',', 'PRP', 'VBZ', 'NNS', 'IN', 'NN', ':', 'VBN', 'NNS', ',', 'VBG', 'IN', 'NN', 'NNS', ',', 'CC', 'DT', 'NN', 'IN', 'WDT', 'VBZ', 'NN', 'NN', 'IN', 'NNP', 'TO', 'RB', 'VB', 'NNP', ',', 'NN', 'NN', ',', 'CC', 'NN', 'NN', 'NNS', '.']",40
named-entity-recognition,6,171,Results on CONLL,"['Results', 'on', 'CONLL']","['O', 'B-p', 'B-n']","['NNS', 'IN', 'NNP']",3
named-entity-recognition,6,172,Results on ONTONOTES,"['Results', 'on', 'ONTONOTES']","['O', 'O', 'B-n']","['NNS', 'IN', 'NNS']",3
named-entity-recognition,6,174,We also observe that models that use both feature sets significantly outperform other configurations .,"['We', 'also', 'observe', 'that', 'models', 'that', 'use', 'both', 'feature', 'sets', 'significantly', 'outperform', 'other', 'configurations', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NNS', 'WDT', 'VBP', 'DT', 'NN', 'VBZ', 'RB', 'VB', 'JJ', 'NNS', '.']",15
named-entity-recognition,6,183,"We observe that on both CONLL and ONTONOTES , the SSKIP model outperforms our feature vector approach by 0.65 F1 points on average .","['We', 'observe', 'that', 'on', 'both', 'CONLL', 'and', 'ONTONOTES', ',', 'the', 'SSKIP', 'model', 'outperforms', 'our', 'feature', 'vector', 'approach', 'by', '0.65', 'F1', 'points', 'on', 'average', '.']","['O', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'IN', 'DT', 'NNP', 'CC', 'NNP', ',', 'DT', 'NNP', 'NN', 'VBZ', 'PRP$', 'NN', 'NN', 'NN', 'IN', 'CD', 'NNP', 'NNS', 'IN', 'NN', '.']",24
named-entity-recognition,2,2,Fast and Accurate Entity Recognition with Iterated Dilated Convolutions,"['Fast', 'and', 'Accurate', 'Entity', 'Recognition', 'with', 'Iterated', 'Dilated', 'Convolutions']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NNP', 'CC', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",9
named-entity-recognition,2,7,"This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction .","['This', 'paper', 'proposes', 'a', 'faster', 'alternative', 'to', 'Bi', '-', 'LSTMs', 'for', 'NER', ':', 'Iterated', 'Dilated', 'Convolutional', 'Neural', 'Networks', '(', 'ID', '-', 'CNNs', ')', ',', 'which', 'have', 'better', 'capacity', 'than', 'traditional', 'CNNs', 'for', 'large', 'context', 'and', 'structured', 'prediction', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'DT', 'RBR', 'JJ', 'TO', 'NNP', ':', 'NN', 'IN', 'NNP', ':', 'VBN', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ':', 'NN', ')', ',', 'WDT', 'VBP', 'JJR', 'NN', 'IN', 'JJ', 'NNP', 'IN', 'JJ', 'NN', 'CC', 'JJ', 'NN', '.']",38
named-entity-recognition,2,12,"In order to democratize large - scale NLP and information extraction while minimizing our environmental footprint , we require fast , resource - efficient methods for sequence tagging tasks such as part - of - speech tagging and named entity recognition ( NER ) .","['In', 'order', 'to', 'democratize', 'large', '-', 'scale', 'NLP', 'and', 'information', 'extraction', 'while', 'minimizing', 'our', 'environmental', 'footprint', ',', 'we', 'require', 'fast', ',', 'resource', '-', 'efficient', 'methods', 'for', 'sequence', 'tagging', 'tasks', 'such', 'as', 'part', '-', 'of', '-', 'speech', 'tagging', 'and', 'named', 'entity', 'recognition', '(', 'NER', ')', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', 'TO', 'VB', 'JJ', ':', 'NN', 'NNP', 'CC', 'NN', 'NN', 'IN', 'VBG', 'PRP$', 'JJ', 'NN', ',', 'PRP', 'VBP', 'JJ', ',', 'VB', ':', 'JJ', 'NNS', 'IN', 'NN', 'VBG', 'NNS', 'JJ', 'IN', 'NN', ':', 'IN', ':', 'NN', 'NN', 'CC', 'VBN', 'NN', 'NN', '(', 'NNP', ')', '.']",45
named-entity-recognition,2,27,"In response , this paper presents an application of dilated convolutions for sequence labeling ) .","['In', 'response', ',', 'this', 'paper', 'presents', 'an', 'application', 'of', 'dilated', 'convolutions', 'for', 'sequence', 'labeling', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O']","['IN', 'NN', ',', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NN', 'NN', ')', '.']",16
named-entity-recognition,2,29,"Like typical CNN layers , dilated convolutions operate on a sliding window of context over the sequence , but unlike conventional convolutions , the context need not be consecutive ; the dilated window skips over every dilation width d inputs .","['Like', 'typical', 'CNN', 'layers', ',', 'dilated', 'convolutions', 'operate', 'on', 'a', 'sliding', 'window', 'of', 'context', 'over', 'the', 'sequence', ',', 'but', 'unlike', 'conventional', 'convolutions', ',', 'the', 'context', 'need', 'not', 'be', 'consecutive', ';', 'the', 'dilated', 'window', 'skips', 'over', 'every', 'dilation', 'width', 'd', 'inputs', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', 'NNP', 'NNS', ',', 'VBD', 'NNS', 'VBP', 'IN', 'DT', 'VBG', 'NN', 'IN', 'NN', 'IN', 'DT', 'NN', ',', 'CC', 'IN', 'JJ', 'NNS', ',', 'DT', 'NN', 'MD', 'RB', 'VB', 'JJ', ':', 'DT', 'VBN', 'NN', 'NNS', 'IN', 'DT', 'NN', 'WRB', 'NN', 'NNS', '.']",41
named-entity-recognition,2,30,"By stacking layers of dilated convolutions of exponentially increasing dilation width , we can expand the size of the effective input width to cover the entire length of most sequences using only a few layers :","['By', 'stacking', 'layers', 'of', 'dilated', 'convolutions', 'of', 'exponentially', 'increasing', 'dilation', 'width', ',', 'we', 'can', 'expand', 'the', 'size', 'of', 'the', 'effective', 'input', 'width', 'to', 'cover', 'the', 'entire', 'length', 'of', 'most', 'sequences', 'using', 'only', 'a', 'few', 'layers', ':']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBG', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'RB', 'VBG', 'NN', 'NN', ',', 'PRP', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'JJS', 'NNS', 'VBG', 'RB', 'DT', 'JJ', 'NNS', ':']",36
named-entity-recognition,2,33,Our over all iterated dilated CNN architecture ( ID - CNN ) repeatedly applies the same block of dilated convolutions to token - wise representations .,"['Our', 'over', 'all', 'iterated', 'dilated', 'CNN', 'architecture', '(', 'ID', '-', 'CNN', ')', 'repeatedly', 'applies', 'the', 'same', 'block', 'of', 'dilated', 'convolutions', 'to', 'token', '-', 'wise', 'representations', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'IN', 'DT', 'VBN', 'VBD', 'NNP', 'NN', '(', 'NNP', ':', 'NNP', ')', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'TO', 'VB', ':', 'NN', 'NNS', '.']",26
named-entity-recognition,2,34,This parameter sharing prevents overfitting and also provides opportunities to inject supervision on intermediate activations of the network .,"['This', 'parameter', 'sharing', 'prevents', 'overfitting', 'and', 'also', 'provides', 'opportunities', 'to', 'inject', 'supervision', 'on', 'intermediate', 'activations', 'of', 'the', 'network', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBG', 'NNS', 'VBG', 'CC', 'RB', 'VBZ', 'NNS', 'TO', 'VB', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']",19
named-entity-recognition,2,35,"Similar to models that use logits produced by an RNN , the ID - CNN provides two methods for performing prediction : we can predict each token 's label independently , or by running Viterbi inference in a chain structured graphical model .","['Similar', 'to', 'models', 'that', 'use', 'logits', 'produced', 'by', 'an', 'RNN', ',', 'the', 'ID', '-', 'CNN', 'provides', 'two', 'methods', 'for', 'performing', 'prediction', ':', 'we', 'can', 'predict', 'each', 'token', ""'s"", 'label', 'independently', ',', 'or', 'by', 'running', 'Viterbi', 'inference', 'in', 'a', 'chain', 'structured', 'graphical', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'TO', 'NNS', 'IN', 'VBP', 'NNS', 'VBN', 'IN', 'DT', 'NNP', ',', 'DT', 'NNP', ':', 'NNP', 'VBZ', 'CD', 'NNS', 'IN', 'VBG', 'NN', ':', 'PRP', 'MD', 'VB', 'DT', 'NN', 'POS', 'NN', 'RB', ',', 'CC', 'IN', 'VBG', 'NNP', 'NN', 'IN', 'DT', 'NN', 'JJ', 'JJ', 'NN', '.']",43
named-entity-recognition,2,162,"We compare our ID - CNN against strong LSTM and CNN baselines : a Bi - LSTM with local decoding , and one with CRF decoding ( Bi - LSTM - CRF ) .","['We', 'compare', 'our', 'ID', '-', 'CNN', 'against', 'strong', 'LSTM', 'and', 'CNN', 'baselines', ':', 'a', 'Bi', '-', 'LSTM', 'with', 'local', 'decoding', ',', 'and', 'one', 'with', 'CRF', 'decoding', '(', 'Bi', '-', 'LSTM', '-', 'CRF', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'PRP$', 'NNP', ':', 'NN', 'IN', 'JJ', 'NNP', 'CC', 'NNP', 'NNS', ':', 'DT', 'NNP', ':', 'NN', 'IN', 'JJ', 'NN', ',', 'CC', 'CD', 'IN', 'NNP', 'VBG', '(', 'NNP', ':', 'NNP', ':', 'NN', ')', '.']",34
named-entity-recognition,2,163,We also compare against a non-dilated CNN architecture with the same number of convolutional layers as our dilated network ( 4 - layer CNN ) and one with enough layers to incorporate an effective input width of the same size as that of the dilated network ( 5 - layer CNN ) to demonstrate that the dilated convolutions more effectively aggregate contextual information than simple convolutions ( i.e. using fewer parameters ) .,"['We', 'also', 'compare', 'against', 'a', 'non-dilated', 'CNN', 'architecture', 'with', 'the', 'same', 'number', 'of', 'convolutional', 'layers', 'as', 'our', 'dilated', 'network', '(', '4', '-', 'layer', 'CNN', ')', 'and', 'one', 'with', 'enough', 'layers', 'to', 'incorporate', 'an', 'effective', 'input', 'width', 'of', 'the', 'same', 'size', 'as', 'that', 'of', 'the', 'dilated', 'network', '(', '5', '-', 'layer', 'CNN', ')', 'to', 'demonstrate', 'that', 'the', 'dilated', 'convolutions', 'more', 'effectively', 'aggregate', 'contextual', 'information', 'than', 'simple', 'convolutions', '(', 'i.e.', 'using', 'fewer', 'parameters', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'IN', 'DT', 'JJ', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'PRP$', 'JJ', 'NN', '(', 'CD', ':', 'NN', 'NNP', ')', 'CC', 'CD', 'IN', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'IN', 'DT', 'JJ', 'NN', '(', 'CD', ':', 'NN', 'NNP', ')', 'TO', 'VB', 'IN', 'DT', 'JJ', 'NNS', 'RBR', 'RB', 'JJ', 'JJ', 'NN', 'IN', 'JJ', 'NNS', '(', 'FW', 'VBG', 'JJR', 'NNS', ')', '.']",73
named-entity-recognition,2,169,6.3 CoNLL - 2003 English NER 6.3.1 Sentence - level prediction lists F 1 scores of models predicting with sentence - level context on CoNLL - 2003 .,"['6.3', 'CoNLL', '-', '2003', 'English', 'NER', '6.3.1', 'Sentence', '-', 'level', 'prediction', 'lists', 'F', '1', 'scores', 'of', 'models', 'predicting', 'with', 'sentence', '-', 'level', 'context', 'on', 'CoNLL', '-', '2003', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['CD', 'NNP', ':', 'CD', 'NNP', 'NNP', 'CD', 'NNP', ':', 'NN', 'NN', 'VBZ', 'NNP', 'CD', 'NNS', 'IN', 'NNS', 'VBG', 'IN', 'NN', ':', 'NN', 'NN', 'IN', 'NNP', ':', 'CD', '.']",28
named-entity-recognition,2,171,"The Viterbi - decoding Bi - LSTM - CRF and ID - CNN - CRF and greedy ID - CNN obtain the highest average scores , with the ID - CNN - CRF outperforming the Bi - LSTM - CRF by 0.11 points of F1 on average , and the Bi - LSTM - CRF out - performing the greedy ID - CNN by 0.11 as well .","['The', 'Viterbi', '-', 'decoding', 'Bi', '-', 'LSTM', '-', 'CRF', 'and', 'ID', '-', 'CNN', '-', 'CRF', 'and', 'greedy', 'ID', '-', 'CNN', 'obtain', 'the', 'highest', 'average', 'scores', ',', 'with', 'the', 'ID', '-', 'CNN', '-', 'CRF', 'outperforming', 'the', 'Bi', '-', 'LSTM', '-', 'CRF', 'by', '0.11', 'points', 'of', 'F1', 'on', 'average', ',', 'and', 'the', 'Bi', '-', 'LSTM', '-', 'CRF', 'out', '-', 'performing', 'the', 'greedy', 'ID', '-', 'CNN', 'by', '0.11', 'as', 'well', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', ':', 'VBG', 'NNP', ':', 'NNP', ':', 'NN', 'CC', 'NNP', ':', 'NNP', ':', 'NN', 'CC', 'NN', 'NNP', ':', 'NNP', 'VB', 'DT', 'JJS', 'JJ', 'NNS', ',', 'IN', 'DT', 'NNP', ':', 'NNP', ':', 'NN', 'VBG', 'DT', 'NNP', ':', 'NNP', ':', 'NN', 'IN', 'CD', 'NNS', 'IN', 'NNP', 'IN', 'NN', ',', 'CC', 'DT', 'NNP', ':', 'NNP', ':', 'NN', 'IN', ':', 'VBG', 'DT', 'NN', 'NNP', ':', 'NN', 'IN', 'CD', 'RB', 'RB', '.']",68
named-entity-recognition,2,172,"Our greedy ID - CNN outperforms the Bi - LSTM and the 4 - layer CNN , which uses the same number of parameters as the ID - CNN , and performs similarly to the 5 - layer CNN which uses more parameters but covers the same effective input width .","['Our', 'greedy', 'ID', '-', 'CNN', 'outperforms', 'the', 'Bi', '-', 'LSTM', 'and', 'the', '4', '-', 'layer', 'CNN', ',', 'which', 'uses', 'the', 'same', 'number', 'of', 'parameters', 'as', 'the', 'ID', '-', 'CNN', ',', 'and', 'performs', 'similarly', 'to', 'the', '5', '-', 'layer', 'CNN', 'which', 'uses', 'more', 'parameters', 'but', 'covers', 'the', 'same', 'effective', 'input', 'width', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'NNP', ':', 'NN', 'VBZ', 'DT', 'NNP', ':', 'NN', 'CC', 'DT', 'CD', ':', 'NN', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NNP', ':', 'NNP', ',', 'CC', 'NNS', 'RB', 'TO', 'DT', 'CD', ':', 'NN', 'NNP', 'WDT', 'VBZ', 'JJR', 'NNS', 'CC', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'NN', '.']",51
named-entity-recognition,2,174,"When paired with Viterbi decoding , our ID - CNN performs on par with the Bi - LSTM , showing that the ID - CNN is also an effective token encoder for structured inference .","['When', 'paired', 'with', 'Viterbi', 'decoding', ',', 'our', 'ID', '-', 'CNN', 'performs', 'on', 'par', 'with', 'the', 'Bi', '-', 'LSTM', ',', 'showing', 'that', 'the', 'ID', '-', 'CNN', 'is', 'also', 'an', 'effective', 'token', 'encoder', 'for', 'structured', 'inference', '.']","['B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['WRB', 'VBN', 'IN', 'NNP', 'NN', ',', 'PRP$', 'NNP', ':', 'NN', 'NNS', 'IN', 'NN', 'IN', 'DT', 'NNP', ':', 'NNP', ',', 'VBG', 'IN', 'DT', 'NNP', ':', 'NN', 'VBZ', 'RB', 'DT', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'NN', '.']",35
named-entity-recognition,2,186,Document - level prediction,"['Document', '-', 'level', 'prediction']","['B-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NN', 'NN']",4
named-entity-recognition,2,187,In we show that adding document - level context improves every model on CoNLL - 2003 .,"['In', 'we', 'show', 'that', 'adding', 'document', '-', 'level', 'context', 'improves', 'every', 'model', 'on', 'CoNLL', '-', '2003', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'PRP', 'VBP', 'IN', 'VBG', 'JJ', ':', 'NN', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'NNP', ':', 'CD', '.']",17
named-entity-recognition,2,194,OntoNotes 5.0 English NER,"['OntoNotes', '5.0', 'English', 'NER']","['B-n', 'I-n', 'I-n', 'I-n']","['VBZ', 'CD', 'JJ', 'NNP']",4
named-entity-recognition,2,200,"Our greedy model is out - performed by the Bi - LSTM - CRF reported in Chiu and Nichols ( 2016 ) as well as our own re-implementation , which appears to be the new state - of - the - art on this dataset .","['Our', 'greedy', 'model', 'is', 'out', '-', 'performed', 'by', 'the', 'Bi', '-', 'LSTM', '-', 'CRF', 'reported', 'in', 'Chiu', 'and', 'Nichols', '(', '2016', ')', 'as', 'well', 'as', 'our', 'own', 're-implementation', ',', 'which', 'appears', 'to', 'be', 'the', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'on', 'this', 'dataset', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'NN', 'VBZ', 'RP', ':', 'VBN', 'IN', 'DT', 'NNP', ':', 'NNP', ':', 'NN', 'VBD', 'IN', 'NNP', 'CC', 'NNP', '(', 'CD', ')', 'RB', 'RB', 'IN', 'PRP$', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'TO', 'VB', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'DT', 'NN', '.']",46
named-entity-recognition,3,2,Semi-supervised sequence tagging with bidirectional language models,"['Semi-supervised', 'sequence', 'tagging', 'with', 'bidirectional', 'language', 'models']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'VBG', 'IN', 'JJ', 'NN', 'NNS']",7
named-entity-recognition,3,6,"In this paper , we demonstrate a general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks .","['In', 'this', 'paper', ',', 'we', 'demonstrate', 'a', 'general', 'semi-supervised', 'approach', 'for', 'adding', 'pretrained', 'context', 'embeddings', 'from', 'bidirectional', 'language', 'models', 'to', 'NLP', 'systems', 'and', 'apply', 'it', 'to', 'sequence', 'labeling', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'VBG', 'JJ', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NNS', 'TO', 'NNP', 'NNS', 'CC', 'VB', 'PRP', 'TO', 'VB', 'VBG', 'NNS', '.']",30
named-entity-recognition,3,16,"In this paper , we explore an alternate semisupervised approach which does not require additional labeled data .","['In', 'this', 'paper', ',', 'we', 'explore', 'an', 'alternate', 'semisupervised', 'approach', 'which', 'does', 'not', 'require', 'additional', 'labeled', 'data', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBD', 'NN', 'WDT', 'VBZ', 'RB', 'VB', 'JJ', 'VBN', 'NNS', '.']",18
named-entity-recognition,3,17,"We use a neural language model ( LM ) , pre-trained on a large , unlabeled corpus to compute an encoding of the context at each position in the sequence ( hereafter an LM embedding ) and use it in the supervised sequence tagging model .","['We', 'use', 'a', 'neural', 'language', 'model', '(', 'LM', ')', ',', 'pre-trained', 'on', 'a', 'large', ',', 'unlabeled', 'corpus', 'to', 'compute', 'an', 'encoding', 'of', 'the', 'context', 'at', 'each', 'position', 'in', 'the', 'sequence', '(', 'hereafter', 'an', 'LM', 'embedding', ')', 'and', 'use', 'it', 'in', 'the', 'supervised', 'sequence', 'tagging', 'model', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', '(', 'NNP', ')', ',', 'JJ', 'IN', 'DT', 'JJ', ',', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', '(', 'PDT', 'DT', 'NNP', 'NN', ')', 'CC', 'VB', 'PRP', 'IN', 'DT', 'JJ', 'NN', 'VBG', 'NN', '.']",46
named-entity-recognition,3,75,CoNLL 2003 NER .,"['CoNLL', '2003', 'NER', '.']","['B-n', 'I-n', 'I-n', 'O']","['NN', 'CD', 'NNP', '.']",4
named-entity-recognition,3,80,We use two bidirectional GRUs with 80 hidden units and 25 dimensional character embeddings for the token character encoder .,"['We', 'use', 'two', 'bidirectional', 'GRUs', 'with', '80', 'hidden', 'units', 'and', '25', 'dimensional', 'character', 'embeddings', 'for', 'the', 'token', 'character', 'encoder', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'JJ', 'NNP', 'IN', 'CD', 'JJ', 'NNS', 'CC', 'CD', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",20
named-entity-recognition,3,81,The sequence layer uses two bidirectional GRUs with 300 hidden units each .,"['The', 'sequence', 'layer', 'uses', 'two', 'bidirectional', 'GRUs', 'with', '300', 'hidden', 'units', 'each', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'CD', 'JJ', 'NNP', 'IN', 'CD', 'JJ', 'NNS', 'DT', '.']",13
named-entity-recognition,3,82,"For regularization , we add 25 % dropout to the input of each GRU , but not to the recurrent connections .","['For', 'regularization', ',', 'we', 'add', '25', '%', 'dropout', 'to', 'the', 'input', 'of', 'each', 'GRU', ',', 'but', 'not', 'to', 'the', 'recurrent', 'connections', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'CD', 'NN', 'NN', 'TO', 'DT', 'NN', 'IN', 'DT', 'NNP', ',', 'CC', 'RB', 'TO', 'DT', 'NN', 'NNS', '.']",22
named-entity-recognition,3,83,CoNLL 2000 chunking .,"['CoNLL', '2000', 'chunking', '.']","['B-n', 'I-n', 'I-n', 'O']","['NN', 'CD', 'NN', '.']",4
named-entity-recognition,3,87,The baseline sequence tagger uses 30 dimensional character embeddings and a CNN with 30 filters of width 3 characters followed by a tanh non-linearity for the token character encoder .,"['The', 'baseline', 'sequence', 'tagger', 'uses', '30', 'dimensional', 'character', 'embeddings', 'and', 'a', 'CNN', 'with', '30', 'filters', 'of', 'width', '3', 'characters', 'followed', 'by', 'a', 'tanh', 'non-linearity', 'for', 'the', 'token', 'character', 'encoder', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'NN', 'VBZ', 'CD', 'JJ', 'NN', 'NNS', 'CC', 'DT', 'NNP', 'IN', 'CD', 'NNS', 'IN', 'JJ', 'CD', 'NNS', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",30
named-entity-recognition,3,88,The sequence layer uses two bidirectional LSTMs with 200 hidden units .,"['The', 'sequence', 'layer', 'uses', 'two', 'bidirectional', 'LSTMs', 'with', '200', 'hidden', 'units', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'O']","['DT', 'NN', 'NN', 'VBZ', 'CD', 'JJ', 'NNP', 'IN', 'CD', 'JJ', 'NNS', '.']",12
named-entity-recognition,3,89,"Following we added 50 % dropout to the character embeddings , the input to each LSTM layer ( but not recurrent connections ) and to the output of the final LSTM layer .","['Following', 'we', 'added', '50', '%', 'dropout', 'to', 'the', 'character', 'embeddings', ',', 'the', 'input', 'to', 'each', 'LSTM', 'layer', '(', 'but', 'not', 'recurrent', 'connections', ')', 'and', 'to', 'the', 'output', 'of', 'the', 'final', 'LSTM', 'layer', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'PRP', 'VBD', 'CD', 'NN', 'NN', 'TO', 'DT', 'NN', 'NNS', ',', 'DT', 'NN', 'TO', 'DT', 'NNP', 'NN', '(', 'CC', 'RB', 'JJ', 'NNS', ')', 'CC', 'TO', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'NN', '.']",33
named-entity-recognition,3,104,"All experiments use the Adam optimizer ( Kingma and Ba , 2015 ) with gradient norms clipped at 5.0 .","['All', 'experiments', 'use', 'the', 'Adam', 'optimizer', '(', 'Kingma', 'and', 'Ba', ',', '2015', ')', 'with', 'gradient', 'norms', 'clipped', 'at', '5.0', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NNS', 'VBP', 'DT', 'NNP', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'IN', 'NN', 'NNS', 'VBD', 'IN', 'CD', '.']",20
named-entity-recognition,3,105,"In all experiments , we fine tune the pre-trained Senna word embeddings but fix all weights in the pre-trained language models .","['In', 'all', 'experiments', ',', 'we', 'fine', 'tune', 'the', 'pre-trained', 'Senna', 'word', 'embeddings', 'but', 'fix', 'all', 'weights', 'in', 'the', 'pre-trained', 'language', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'PRP', 'VBP', 'IN', 'DT', 'JJ', 'NNP', 'NN', 'NNS', 'CC', 'VBP', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NNS', '.']",22
named-entity-recognition,3,106,"In addition to explicit dropout regularization , we also use early stopping to prevent over-fitting and use the following process to determine when to stop training .","['In', 'addition', 'to', 'explicit', 'dropout', 'regularization', ',', 'we', 'also', 'use', 'early', 'stopping', 'to', 'prevent', 'over-fitting', 'and', 'use', 'the', 'following', 'process', 'to', 'determine', 'when', 'to', 'stop', 'training', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', 'TO', 'VB', 'NN', 'NN', ',', 'PRP', 'RB', 'VBP', 'JJ', 'NN', 'TO', 'VB', 'NN', 'CC', 'VB', 'DT', 'JJ', 'NN', 'TO', 'VB', 'WRB', 'TO', 'VB', 'NN', '.']",27
named-entity-recognition,3,107,We first train with a constant learning rate ? = 0.001 on the training data and monitor the development set performance at each epoch .,"['We', 'first', 'train', 'with', 'a', 'constant', 'learning', 'rate', '?', '=', '0.001', 'on', 'the', 'training', 'data', 'and', 'monitor', 'the', 'development', 'set', 'performance', 'at', 'each', 'epoch', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'IN', 'DT', 'JJ', 'NN', 'NN', '.', '$', 'CD', 'IN', 'DT', 'NN', 'NNS', 'CC', 'VB', 'DT', 'NN', 'VBN', 'NN', 'IN', 'DT', 'NN', '.']",25
named-entity-recognition,3,118,"In the CoNLL 2003 NER task , our model scores 91.93 mean F 1 , which is a statistically significant increase over the previous best result of 91.62 0.33 from that used gazetteers ( at 95 % , two - sided Welch t- test , p = 0.021 ) .","['In', 'the', 'CoNLL', '2003', 'NER', 'task', ',', 'our', 'model', 'scores', '91.93', 'mean', 'F', '1', ',', 'which', 'is', 'a', 'statistically', 'significant', 'increase', 'over', 'the', 'previous', 'best', 'result', 'of', '91.62', '0.33', 'from', 'that', 'used', 'gazetteers', '(', 'at', '95', '%', ',', 'two', '-', 'sided', 'Welch', 't-', 'test', ',', 'p', '=', '0.021', ')', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNP', 'CD', 'NNP', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'CD', 'NN', 'NNP', 'CD', ',', 'WDT', 'VBZ', 'DT', 'RB', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'JJS', 'NN', 'IN', 'CD', 'CD', 'IN', 'DT', 'JJ', 'NNS', '(', 'IN', 'CD', 'NN', ',', 'CD', ':', 'VBD', 'NNP', 'JJ', 'NN', ',', 'NN', 'NNP', 'CD', ')', '.']",50
named-entity-recognition,3,119,"In the CoNLL 2000 Chunking task , Tag LM achieves 96.37 mean F 1 , exceeding all previously published results without additional labeled data by more then 1 % absolute F 1 .","['In', 'the', 'CoNLL', '2000', 'Chunking', 'task', ',', 'Tag', 'LM', 'achieves', '96.37', 'mean', 'F', '1', ',', 'exceeding', 'all', 'previously', 'published', 'results', 'without', 'additional', 'labeled', 'data', 'by', 'more', 'then', '1', '%', 'absolute', 'F', '1', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNP', 'CD', 'NNP', 'NN', ',', 'NNP', 'NNP', 'VBZ', 'CD', 'NN', 'NNP', 'CD', ',', 'VBG', 'DT', 'RB', 'VBN', 'NNS', 'IN', 'JJ', 'VBN', 'NNS', 'IN', 'JJR', 'RB', 'CD', 'NN', 'JJ', 'NNP', 'CD', '.']",33
named-entity-recognition,0,2,"0,000 + Times Accelerated Robust Subset Selection ( ARSS )","['0,000', '+', 'Times', 'Accelerated', 'Robust', 'Subset', 'Selection', '(', 'ARSS', ')']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['CD', 'JJ', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')']",10
named-entity-recognition,0,4,Subset selection from massive data with noised information is increasingly popular for various applications .,"['Subset', 'selection', 'from', 'massive', 'data', 'with', 'noised', 'information', 'is', 'increasingly', 'popular', 'for', 'various', 'applications', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'VBZ', 'RB', 'JJ', 'IN', 'JJ', 'NNS', '.']",15
named-entity-recognition,0,7,"Specifically in the subset selection area , this is the first attempt to employ the p ( 0 < p ? 1 ) - norm based measure for the representation loss , preventing large errors from dominating our objective .","['Specifically', 'in', 'the', 'subset', 'selection', 'area', ',', 'this', 'is', 'the', 'first', 'attempt', 'to', 'employ', 'the', 'p', '(', '0', '<', 'p', '?', '1', ')', '-', 'norm', 'based', 'measure', 'for', 'the', 'representation', 'loss', ',', 'preventing', 'large', 'errors', 'from', 'dominating', 'our', 'objective', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'IN', 'DT', 'NN', 'NN', 'NN', ',', 'DT', 'VBZ', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', '(', 'CD', 'NN', 'NN', '.', 'CD', ')', ':', 'NN', 'VBN', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'VBG', 'JJ', 'NNS', 'IN', 'VBG', 'PRP$', 'NN', '.']",40
named-entity-recognition,0,48,"In this paper , we propose an accelerated robust subset selection method to highly raise the speed on the one hand , and to boost the robustness on the other .","['In', 'this', 'paper', ',', 'we', 'propose', 'an', 'accelerated', 'robust', 'subset', 'selection', 'method', 'to', 'highly', 'raise', 'the', 'speed', 'on', 'the', 'one', 'hand', ',', 'and', 'to', 'boost', 'the', 'robustness', 'on', 'the', 'other', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'VBN', 'NN', 'NN', 'TO', 'RB', 'VB', 'DT', 'NN', 'IN', 'DT', 'CD', 'NN', ',', 'CC', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', '.']",31
named-entity-recognition,0,49,"To this end , we use the p ( 0 < p ? 1 ) - norm based robust measure for the representation loss , preventing large errors from dominating our objective .","['To', 'this', 'end', ',', 'we', 'use', 'the', 'p', '(', '0', '<', 'p', '?', '1', ')', '-', 'norm', 'based', 'robust', 'measure', 'for', 'the', 'representation', 'loss', ',', 'preventing', 'large', 'errors', 'from', 'dominating', 'our', 'objective', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', '(', 'CD', 'NN', 'NN', '.', 'CD', ')', ':', 'NN', 'VBN', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'VBG', 'JJ', 'NNS', 'IN', 'VBG', 'PRP$', 'NN', '.']",33
named-entity-recognition,0,51,"Then , based on the observation that data size is generally much larger than feature length , i.e. N L , we propose a speedup solver .","['Then', ',', 'based', 'on', 'the', 'observation', 'that', 'data', 'size', 'is', 'generally', 'much', 'larger', 'than', 'feature', 'length', ',', 'i.e.', 'N', 'L', ',', 'we', 'propose', 'a', 'speedup', 'solver', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NNS', 'NN', 'VBZ', 'RB', 'JJ', 'JJR', 'IN', 'NN', 'NN', ',', 'FW', 'NNP', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', '.']",27
named-entity-recognition,0,52,The main acceleration is owing to the Augmented Lagrange Multiplier ( ALM ) and an equivalent derivation .,"['The', 'main', 'acceleration', 'is', 'owing', 'to', 'the', 'Augmented', 'Lagrange', 'Multiplier', '(', 'ALM', ')', 'and', 'an', 'equivalent', 'derivation', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'VBG', 'TO', 'DT', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'CC', 'DT', 'JJ', 'NN', '.']",18
named-entity-recognition,0,203,"All experiments are conducted on a server with 64 - core Intel Xeon E7-4820 @ 2.00 GHz , 18 Mb Cache and 0.986 TB RAM , using Matlab 2012 .","['All', 'experiments', 'are', 'conducted', 'on', 'a', 'server', 'with', '64', '-', 'core', 'Intel', 'Xeon', 'E7-4820', '@', '2.00', 'GHz', ',', '18', 'Mb', 'Cache', 'and', '0.986', 'TB', 'RAM', ',', 'using', 'Matlab', '2012', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'NN', 'IN', 'CD', ':', 'NN', 'NNP', 'NNP', 'NNP', 'NNP', 'CD', 'NNP', ',', 'CD', 'NNP', 'NNP', 'CC', 'CD', 'NNP', 'NNP', ',', 'VBG', 'NNP', 'CD', '.']",30
named-entity-recognition,0,210,Speed Comparisons,"['Speed', 'Comparisons']","['B-p', 'I-p']","['NN', 'NNS']",2
named-entity-recognition,0,215,"Speed vs. increasing N To verify the great superiority of our method over the state - of - the - art methods in speed , three experiments are conducted .","['Speed', 'vs.', 'increasing', 'N', 'To', 'verify', 'the', 'great', 'superiority', 'of', 'our', 'method', 'over', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'methods', 'in', 'speed', ',', 'three', 'experiments', 'are', 'conducted', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'FW', 'VBG', 'NNP', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'PRP$', 'NN', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'NN', ',', 'CD', 'NNS', 'VBP', 'VBN', '.']",30
named-entity-recognition,0,221,"Compared with TED and RRSS Nie , the curve of ARSS is surprisingly lower and highly stable against increasing N ; there is almost no rise of selection time over growing N .","['Compared', 'with', 'TED', 'and', 'RRSS', 'Nie', ',', 'the', 'curve', 'of', 'ARSS', 'is', 'surprisingly', 'lower', 'and', 'highly', 'stable', 'against', 'increasing', 'N', ';', 'there', 'is', 'almost', 'no', 'rise', 'of', 'selection', 'time', 'over', 'growing', 'N', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O']","['VBN', 'IN', 'NNP', 'CC', 'NNP', 'NNP', ',', 'DT', 'NN', 'IN', 'NNP', 'VBZ', 'RB', 'JJR', 'CC', 'RB', 'JJ', 'IN', 'VBG', 'NNP', ':', 'EX', 'VBZ', 'RB', 'DT', 'NN', 'IN', 'NN', 'NN', 'IN', 'VBG', 'NNP', '.']",33
named-entity-recognition,0,226,"Speed with fixed N The speed of four algorithms is summarized in , where each row shows the results on one dataset and the last row displays the average results .","['Speed', 'with', 'fixed', 'N', 'The', 'speed', 'of', 'four', 'algorithms', 'is', 'summarized', 'in', ',', 'where', 'each', 'row', 'shows', 'the', 'results', 'on', 'one', 'dataset', 'and', 'the', 'last', 'row', 'displays', 'the', 'average', 'results', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'IN', 'JJ', 'NNP', 'DT', 'NN', 'IN', 'CD', 'NN', 'VBZ', 'VBN', 'IN', ',', 'WRB', 'DT', 'NN', 'VBZ', 'DT', 'NNS', 'IN', 'CD', 'NN', 'CC', 'DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NNS', '.']",31
named-entity-recognition,0,228,"First , ARSS is the fastest algorithm , and RRSS our is the second fastest algorithm .","['First', ',', 'ARSS', 'is', 'the', 'fastest', 'algorithm', ',', 'and', 'RRSS', 'our', 'is', 'the', 'second', 'fastest', 'algorithm', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'NNP', 'VBZ', 'DT', 'JJS', 'NN', ',', 'CC', 'NNP', 'PRP$', 'VBZ', 'DT', 'JJ', 'JJS', 'NN', '.']",17
named-entity-recognition,0,241,Prediction Accuracy,"['Prediction', 'Accuracy']","['B-p', 'I-p']","['NN', 'NN']",2
named-entity-recognition,0,242,Accuracy comparison,"['Accuracy', 'comparison']","['B-n', 'I-n']","['NNP', 'NN']",2
named-entity-recognition,0,249,"Third , compared with TED , both RRSS and ARSS achieve an appreciable advantage .","['Third', ',', 'compared', 'with', 'TED', ',', 'both', 'RRSS', 'and', 'ARSS', 'achieve', 'an', 'appreciable', 'advantage', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', ',', 'VBN', 'IN', 'NNP', ',', 'DT', 'NNP', 'CC', 'NNP', 'VBP', 'DT', 'JJ', 'NN', '.']",15
named-entity-recognition,0,252,"Prediction accuracies vs. increasing K To give a more detailed comparison , shows the prediction accuracies versus growing K ( the number of selected samples ) .","['Prediction', 'accuracies', 'vs.', 'increasing', 'K', 'To', 'give', 'a', 'more', 'detailed', 'comparison', ',', 'shows', 'the', 'prediction', 'accuracies', 'versus', 'growing', 'K', '(', 'the', 'number', 'of', 'selected', 'samples', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NNS', 'IN', 'VBG', 'NNP', 'TO', 'VB', 'DT', 'RBR', 'JJ', 'NN', ',', 'VBZ', 'DT', 'NN', 'NNS', 'IN', 'VBG', 'NNP', '(', 'DT', 'NN', 'IN', 'VBN', 'NNS', ')', '.']",27
named-entity-recognition,0,256,"As we shall see , the prediction accuracies generally increase as K increases .","['As', 'we', 'shall', 'see', ',', 'the', 'prediction', 'accuracies', 'generally', 'increase', 'as', 'K', 'increases', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O']","['IN', 'PRP', 'MD', 'VB', ',', 'DT', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'NNP', 'VBZ', '.']",14
named-entity-recognition,0,258,"For each sub-figure , ARSS is generally among the best .","['For', 'each', 'sub-figure', ',', 'ARSS', 'is', 'generally', 'among', 'the', 'best', '.']","['O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O']","['IN', 'DT', 'NN', ',', 'NNP', 'VBZ', 'RB', 'IN', 'DT', 'JJS', '.']",11
