topic,paper_ID,sentence_ID,sentence,words,BIO,POS,length
question_generation,1,2,Multimodal Differential Network for Visual Question Generation,"['Multimodal', 'Differential', 'Network', 'for', 'Visual', 'Question', 'Generation']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",7
question_generation,1,4,Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations .,"['Generating', 'natural', 'questions', 'from', 'an', 'image', 'is', 'a', 'semantic', 'task', 'that', 'requires', 'using', 'visual', 'and', 'language', 'modality', 'to', 'learn', 'multimodal', 'representations', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'VBG', 'JJ', 'CC', 'NN', 'NN', 'TO', 'VB', 'JJ', 'NNS', '.']",22
question_generation,1,18,Here the au-thors have proposed the challenging task of generating natural questions for an image .,"['Here', 'the', 'au-thors', 'have', 'proposed', 'the', 'challenging', 'task', 'of', 'generating', 'natural', 'questions', 'for', 'an', 'image', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', 'DT', 'NNS', 'VBP', 'VBN', 'DT', 'VBG', 'NN', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']",16
question_generation,1,24,"To solve this problem , we use the context obtained by considering exemplars , specifically we use the difference between relevant and irrelevant exemplars .","['To', 'solve', 'this', 'problem', ',', 'we', 'use', 'the', 'context', 'obtained', 'by', 'considering', 'exemplars', ',', 'specifically', 'we', 'use', 'the', 'difference', 'between', 'relevant', 'and', 'irrelevant', 'exemplars', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBN', 'IN', 'VBG', 'NNS', ',', 'RB', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'CC', 'JJ', 'NNS', '.']",25
question_generation,1,25,"We consider different contexts in the form of Location , Caption , and Part of Speech tags .","['We', 'consider', 'different', 'contexts', 'in', 'the', 'form', 'of', 'Location', ',', 'Caption', ',', 'and', 'Part', 'of', 'Speech', 'tags', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', 'IN', 'NNP', 'NNS', '.']",18
question_generation,1,26,Our method implicitly uses a differential context obtained through supporting and contrasting exemplars to obtain a differentiable embedding .,"['Our', 'method', 'implicitly', 'uses', 'a', 'differential', 'context', 'obtained', 'through', 'supporting', 'and', 'contrasting', 'exemplars', 'to', 'obtain', 'a', 'differentiable', 'embedding', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'VBN', 'IN', 'VBG', 'CC', 'VBG', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NN', '.']",19
question_generation,1,27,This embedding is used by a question decoder to decode the appropriate question .,"['This', 'embedding', 'is', 'used', 'by', 'a', 'question', 'decoder', 'to', 'decode', 'the', 'appropriate', 'question', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', '.']",14
question_generation,1,36,"To summarize , we propose a multimodal differential network to solve the task of visual question generation .","['To', 'summarize', ',', 'we', 'propose', 'a', 'multimodal', 'differential', 'network', 'to', 'solve', 'the', 'task', 'of', 'visual', 'question', 'generation', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'VB', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",18
question_generation,0,2,Neural Question Generation from Text : A Preliminary Study,"['Neural', 'Question', 'Generation', 'from', 'Text', ':', 'A', 'Preliminary', 'Study']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', ':', 'DT', 'NNP', 'NN']",9
question_generation,0,4,Automatic question generation aims to generate questions from a text passage where the generated questions can be answered by certain sub- spans of the given passage .,"['Automatic', 'question', 'generation', 'aims', 'to', 'generate', 'questions', 'from', 'a', 'text', 'passage', 'where', 'the', 'generated', 'questions', 'can', 'be', 'answered', 'by', 'certain', 'sub-', 'spans', 'of', 'the', 'given', 'passage', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', 'VBZ', 'TO', 'VB', 'NNS', 'IN', 'DT', 'NN', 'NN', 'WRB', 'DT', 'JJ', 'NNS', 'MD', 'VB', 'VBN', 'IN', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'VBN', 'NN', '.']",27
question_generation,0,10,"Automatic question generation from natural language text aims to generate questions taking text as input , which has the potential value of education purpose ) .","['Automatic', 'question', 'generation', 'from', 'natural', 'language', 'text', 'aims', 'to', 'generate', 'questions', 'taking', 'text', 'as', 'input', ',', 'which', 'has', 'the', 'potential', 'value', 'of', 'education', 'purpose', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NN', 'VBZ', 'TO', 'VB', 'NNS', 'VBG', 'NN', 'IN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NN', ')', '.']",26
question_generation,0,11,"As the reverse task of question answering , question generation also has the potential for providing a large scale corpus of question - answer pairs .","['As', 'the', 'reverse', 'task', 'of', 'question', 'answering', ',', 'question', 'generation', 'also', 'has', 'the', 'potential', 'for', 'providing', 'a', 'large', 'scale', 'corpus', 'of', 'question', '-', 'answer', 'pairs', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'NN', 'IN', 'NN', 'NN', ',', 'NN', 'NN', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NN', ':', 'NN', 'NNS', '.']",26
question_generation,0,16,"In this work we conduct a preliminary study on question generation from text with neural networks , which is denoted as the Neural Question Generation ( NQG ) framework , to generate natural language questions from text without pre-defined rules .","['In', 'this', 'work', 'we', 'conduct', 'a', 'preliminary', 'study', 'on', 'question', 'generation', 'from', 'text', 'with', 'neural', 'networks', ',', 'which', 'is', 'denoted', 'as', 'the', 'Neural', 'Question', 'Generation', '(', 'NQG', ')', 'framework', ',', 'to', 'generate', 'natural', 'language', 'questions', 'from', 'text', 'without', 'pre-defined', 'rules', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NN', 'IN', 'NN', 'IN', 'JJ', 'NNS', ',', 'WDT', 'VBZ', 'VBN', 'IN', 'DT', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', ',', 'TO', 'VB', 'JJ', 'NN', 'NNS', 'IN', 'NN', 'IN', 'JJ', 'NNS', '.']",41
question_generation,0,17,The Neural Question Generation framework extends the sequence - to - sequence models by enriching the encoder with answer and lexical features to generate answer focused questions .,"['The', 'Neural', 'Question', 'Generation', 'framework', 'extends', 'the', 'sequence', '-', 'to', '-', 'sequence', 'models', 'by', 'enriching', 'the', 'encoder', 'with', 'answer', 'and', 'lexical', 'features', 'to', 'generate', 'answer', 'focused', 'questions', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'NNP', 'NNP', 'NN', 'VBZ', 'DT', 'NN', ':', 'TO', ':', 'NN', 'NNS', 'IN', 'VBG', 'DT', 'NN', 'IN', 'NN', 'CC', 'JJ', 'NNS', 'TO', 'VB', 'NN', 'JJ', 'NNS', '.']",28
question_generation,0,18,"Concretely , the encoder reads not only the input sentence , but also the answer position indicator and lexical features .","['Concretely', ',', 'the', 'encoder', 'reads', 'not', 'only', 'the', 'input', 'sentence', ',', 'but', 'also', 'the', 'answer', 'position', 'indicator', 'and', 'lexical', 'features', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'VBZ', 'RB', 'RB', 'DT', 'NN', 'NN', ',', 'CC', 'RB', 'DT', 'JJR', 'NN', 'NN', 'CC', 'JJ', 'NNS', '.']",21
question_generation,0,19,"The answer position feature denotes the answer span in the input sentence , which is essential to generate answer relevant questions .","['The', 'answer', 'position', 'feature', 'denotes', 'the', 'answer', 'span', 'in', 'the', 'input', 'sentence', ',', 'which', 'is', 'essential', 'to', 'generate', 'answer', 'relevant', 'questions', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJR', 'NN', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'WDT', 'VBZ', 'JJ', 'TO', 'VB', 'NN', 'JJ', 'NNS', '.']",22
question_generation,0,20,The lexical features include part - of - speech ( POS ) and named entity ( NER ) tags to help produce better sentence encoding .,"['The', 'lexical', 'features', 'include', 'part', '-', 'of', '-', 'speech', '(', 'POS', ')', 'and', 'named', 'entity', '(', 'NER', ')', 'tags', 'to', 'help', 'produce', 'better', 'sentence', 'encoding', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NNS', 'VBP', 'NN', ':', 'IN', ':', 'NN', '(', 'NNP', ')', 'CC', 'VBN', 'NN', '(', 'NNP', ')', 'VBZ', 'TO', 'VB', 'VB', 'JJR', 'NN', 'NN', '.']",26
question_generation,0,21,"Lastly , the decoder with attention mechanism generates an answer specific question of the sentence .","['Lastly', ',', 'the', 'decoder', 'with', 'attention', 'mechanism', 'generates', 'an', 'answer', 'specific', 'question', 'of', 'the', 'sentence', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['RB', ',', 'DT', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'DT', 'NN', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",16
question_generation,0,71,PCFG - Trans,"['PCFG', '-', 'Trans']","['B-n', 'I-n', 'I-n']","['NNP', ':', 'NNS']",3
question_generation,0,72,The rule - based system 1 modified on the code released by .,"['The', 'rule', '-', 'based', 'system', '1', 'modified', 'on', 'the', 'code', 'released', 'by', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', ':', 'VBN', 'NN', 'CD', 'VBN', 'IN', 'DT', 'NN', 'VBN', 'IN', '.']",13
question_generation,0,74,s 2 s+ att,"['s', '2', 's+', 'att']","['B-n', 'I-n', 'I-n', 'I-n']","['RB', 'CD', 'NNS', 'NN']",4
question_generation,0,75,We implement a seq2seq with attention as the baseline method .,"['We', 'implement', 'a', 'seq2seq', 'with', 'attention', 'as', 'the', 'baseline', 'method', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",11
question_generation,0,76,NQG,['NQG'],['B-n'],['NN'],1
question_generation,0,77,We extend the s 2s+ att with our feature - rich encoder to build the NQG system .,"['We', 'extend', 'the', 's', '2s+', 'att', 'with', 'our', 'feature', '-', 'rich', 'encoder', 'to', 'build', 'the', 'NQG', 'system', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'CD', 'NN', 'IN', 'PRP$', 'NN', ':', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NNP', 'NN', '.']",18
question_generation,0,78,"NQG + Based on NQG , we incorporate copy mechanism to deal with rare words problem .","['NQG', '+', 'Based', 'on', 'NQG', ',', 'we', 'incorporate', 'copy', 'mechanism', 'to', 'deal', 'with', 'rare', 'words', 'problem', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'VBD', 'IN', 'NNP', ',', 'PRP', 'VBP', 'JJ', 'NN', 'TO', 'VB', 'IN', 'JJ', 'NNS', 'NN', '.']",17
question_generation,0,79,"NQG + Pretrain Based on NQG + , we initialize the word embedding matrix with pre-trained GloVe vectors .","['NQG', '+', 'Pretrain', 'Based', 'on', 'NQG', '+', ',', 'we', 'initialize', 'the', 'word', 'embedding', 'matrix', 'with', 'pre-trained', 'GloVe', 'vectors', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NNP', 'VBD', 'IN', 'NNP', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBG', 'NN', 'IN', 'JJ', 'NNP', 'NNS', '.']",19
question_generation,0,80,"NQG + STshare Based on NQG + , we make the encoder and decoder share the same embedding matrix .","['NQG', '+', 'STshare', 'Based', 'on', 'NQG', '+', ',', 'we', 'make', 'the', 'encoder', 'and', 'decoder', 'share', 'the', 'same', 'embedding', 'matrix', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NNP', 'VBD', 'IN', 'NNP', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NN', 'CC', 'NN', 'NN', 'DT', 'JJ', 'NN', 'NN', '.']",20
question_generation,0,81,NQG ++,"['NQG', '++']","['B-n', 'I-n']","['NNP', 'NN']",2
question_generation,0,82,"Based on NQG + , we use both pre-train word embedding and STshare methods , to further improve the performance .","['Based', 'on', 'NQG', '+', ',', 'we', 'use', 'both', 'pre-train', 'word', 'embedding', 'and', 'STshare', 'methods', ',', 'to', 'further', 'improve', 'the', 'performance', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['VBN', 'IN', 'NNP', 'NNP', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'CC', 'NNP', 'NNS', ',', 'TO', 'RBR', 'VB', 'DT', 'NN', '.']",21
question_generation,0,89,Our NQG framework outperforms the PCFG - Trans and s 2s + att baselines by a large margin .,"['Our', 'NQG', 'framework', 'outperforms', 'the', 'PCFG', '-', 'Trans', 'and', 's', '2s', '+', 'att', 'baselines', 'by', 'a', 'large', 'margin', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NNP', 'NN', 'VBZ', 'DT', 'NNP', ':', 'NNPS', 'CC', 'VB', 'CD', 'NNP', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",19
question_generation,0,91,"With the help of copy mechanism , NQG + has a 2.05 BLEU improvement since it solves the rare words problem .","['With', 'the', 'help', 'of', 'copy', 'mechanism', ',', 'NQG', '+', 'has', 'a', '2.05', 'BLEU', 'improvement', 'since', 'it', 'solves', 'the', 'rare', 'words', 'problem', '.']","['B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'IN', 'NN', 'NN', ',', 'NNP', 'NNP', 'VBZ', 'DT', 'CD', 'NNP', 'NN', 'IN', 'PRP', 'VBZ', 'DT', 'JJ', 'NNS', 'NN', '.']",22
question_generation,0,92,"The extended version , NQG ++ , has 1.11 BLEU score gain over NQG + , which shows that initializing with pre-trained word vectors and sharing them between encoder and decoder help learn better word representation .","['The', 'extended', 'version', ',', 'NQG', '++', ',', 'has', '1.11', 'BLEU', 'score', 'gain', 'over', 'NQG', '+', ',', 'which', 'shows', 'that', 'initializing', 'with', 'pre-trained', 'word', 'vectors', 'and', 'sharing', 'them', 'between', 'encoder', 'and', 'decoder', 'help', 'learn', 'better', 'word', 'representation', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', ',', 'NNP', 'NNP', ',', 'VBZ', 'CD', 'NNP', 'RB', 'NN', 'IN', 'NNP', 'NNP', ',', 'WDT', 'VBZ', 'IN', 'VBG', 'IN', 'JJ', 'NN', 'NNS', 'CC', 'VBG', 'PRP', 'IN', 'NN', 'CC', 'NN', 'NN', 'VB', 'JJR', 'NN', 'NN', '.']",37
question_generation,0,104,"The answer position indicator , as expected , plays a crucial role in answer focused question generation as shown in the NQG ?","['The', 'answer', 'position', 'indicator', ',', 'as', 'expected', ',', 'plays', 'a', 'crucial', 'role', 'in', 'answer', 'focused', 'question', 'generation', 'as', 'shown', 'in', 'the', 'NQG', '?']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJR', 'NN', 'NN', ',', 'IN', 'VBN', ',', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'VBN', 'NN', 'NN', 'IN', 'VBN', 'IN', 'DT', 'NNP', '.']",23
question_generation,0,109,"NER , show that word case , POS and NER tag features contributes to question generation .","['NER', ',', 'show', 'that', 'word', 'case', ',', 'POS', 'and', 'NER', 'tag', 'features', 'contributes', 'to', 'question', 'generation', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['NNP', ',', 'VBP', 'IN', 'NN', 'NN', ',', 'NNP', 'CC', 'NNP', 'NN', 'NNS', 'VBZ', 'TO', 'VB', 'NN', '.']",17
part-of-speech_tagging,1,2,Learning Better Internal Structure of Words for Sequence Labeling,"['Learning', 'Better', 'Internal', 'Structure', 'of', 'Words', 'for', 'Sequence', 'Labeling']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['VBG', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'IN', 'NNP', 'NNP']",9
part-of-speech_tagging,1,43,"Furthermore , we propose IntNet , a funnel - shaped wide convolutional neural network for learning the internal structure of words by composing their characters .","['Furthermore', ',', 'we', 'propose', 'IntNet', ',', 'a', 'funnel', '-', 'shaped', 'wide', 'convolutional', 'neural', 'network', 'for', 'learning', 'the', 'internal', 'structure', 'of', 'words', 'by', 'composing', 'their', 'characters', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['RB', ',', 'PRP', 'VBP', 'NNP', ',', 'DT', 'NN', ':', 'VBD', 'JJ', 'JJ', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'VBG', 'PRP$', 'NNS', '.']",26
part-of-speech_tagging,1,44,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores deeper and wider architecture with no down - sampling for learning character - to - word representations from limited supervised training corpora .","['Unlike', 'previous', 'CNN', '-', 'based', 'approaches', ',', 'our', 'funnel', '-', 'shaped', 'Int', '-', 'Net', 'explores', 'deeper', 'and', 'wider', 'architecture', 'with', 'no', 'down', '-', 'sampling', 'for', 'learning', 'character', '-', 'to', '-', 'word', 'representations', 'from', 'limited', 'supervised', 'training', 'corpora', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', 'NNP', ':', 'VBN', 'NNS', ',', 'PRP$', 'SYM', ':', 'VBN', 'NNP', ':', 'NN', 'VBZ', 'JJR', 'CC', 'JJR', 'NN', 'IN', 'DT', 'RP', ':', 'NN', 'IN', 'VBG', 'NN', ':', 'TO', ':', 'NN', 'NNS', 'IN', 'JJ', 'VBD', 'NN', 'NN', '.']",38
part-of-speech_tagging,1,45,"Lastly , we combine our IntNet model with LSTM - CRF , which captures both word shape and context information , and jointly decode tags for sequence labeling .","['Lastly', ',', 'we', 'combine', 'our', 'IntNet', 'model', 'with', 'LSTM', '-', 'CRF', ',', 'which', 'captures', 'both', 'word', 'shape', 'and', 'context', 'information', ',', 'and', 'jointly', 'decode', 'tags', 'for', 'sequence', 'labeling', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'PRP$', 'NNP', 'NN', 'IN', 'NNP', ':', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NN', 'NN', 'CC', 'NN', 'NN', ',', 'CC', 'RB', 'VB', 'NNS', 'IN', 'NN', 'NN', '.']",29
part-of-speech_tagging,1,177,The size of the dimensions of character embeddings is 32 which are randomly initialized using a uniform distribution .,"['The', 'size', 'of', 'the', 'dimensions', 'of', 'character', 'embeddings', 'is', '32', 'which', 'are', 'randomly', 'initialized', 'using', 'a', 'uniform', 'distribution', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'DT', 'NNS', 'IN', 'NN', 'NNS', 'VBZ', 'CD', 'WDT', 'VBP', 'RB', 'VBN', 'VBG', 'DT', 'JJ', 'NN', '.']",19
part-of-speech_tagging,1,178,We adopt the same initialization method for randomly initialized word embeddings that are updated during training .,"['We', 'adopt', 'the', 'same', 'initialization', 'method', 'for', 'randomly', 'initialized', 'word', 'embeddings', 'that', 'are', 'updated', 'during', 'training', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'RB', 'VBN', 'NN', 'NNS', 'WDT', 'VBP', 'VBN', 'IN', 'NN', '.']",17
part-of-speech_tagging,1,179,"For IntNet , the filter size of the initial convolution is 32 and that of other convolutions is 16 .","['For', 'IntNet', ',', 'the', 'filter', 'size', 'of', 'the', 'initial', 'convolution', 'is', '32', 'and', 'that', 'of', 'other', 'convolutions', 'is', '16', '.']","['B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'NNP', ',', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'CD', 'CC', 'DT', 'IN', 'JJ', 'NNS', 'VBZ', 'CD', '.']",20
part-of-speech_tagging,1,181,"The number of convolutional layers are 5 and 9 for IntNet - 5 and IntNet - 9 , respectively , and we have adopted the same weight initialization as that of ResNet .","['The', 'number', 'of', 'convolutional', 'layers', 'are', '5', 'and', '9', 'for', 'IntNet', '-', '5', 'and', 'IntNet', '-', '9', ',', 'respectively', ',', 'and', 'we', 'have', 'adopted', 'the', 'same', 'weight', 'initialization', 'as', 'that', 'of', 'ResNet', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'O']","['DT', 'NN', 'IN', 'JJ', 'NNS', 'VBP', 'CD', 'CC', 'CD', 'IN', 'NNP', ':', 'CD', 'CC', 'NNP', ':', 'CD', ',', 'RB', ',', 'CC', 'PRP', 'VBP', 'VBN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'IN', 'NNP', '.']",33
part-of-speech_tagging,1,182,"We use pre-trained word embeddings for initialization , GloVe 100 - dimension word embeddings for English , and fastText 300 dimension word embeddings for Spanish , Dutch , and German .","['We', 'use', 'pre-trained', 'word', 'embeddings', 'for', 'initialization', ',', 'GloVe', '100', '-', 'dimension', 'word', 'embeddings', 'for', 'English', ',', 'and', 'fastText', '300', 'dimension', 'word', 'embeddings', 'for', 'Spanish', ',', 'Dutch', ',', 'and', 'German', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'NNS', 'IN', 'NN', ',', 'NNP', 'CD', ':', 'NN', 'NN', 'NNS', 'IN', 'NNP', ',', 'CC', 'JJ', 'CD', 'NN', 'NN', 'NNS', 'IN', 'JJ', ',', 'NNP', ',', 'CC', 'NNP', '.']",31
part-of-speech_tagging,1,183,The state size of the bi-directional LSTMs is set to 256 .,"['The', 'state', 'size', 'of', 'the', 'bi-directional', 'LSTMs', 'is', 'set', 'to', '256', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'VBZ', 'VBN', 'TO', 'CD', '.']",12
part-of-speech_tagging,1,184,We adopt standard BIOES tagging scheme for NER and Chunking .,"['We', 'adopt', 'standard', 'BIOES', 'tagging', 'scheme', 'for', 'NER', 'and', 'Chunking', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NNP', 'VBG', 'NN', 'IN', 'NNP', 'CC', 'NNP', '.']",11
part-of-speech_tagging,1,186,We employ mini-batch stochastic gradient descent with momentum .,"['We', 'employ', 'mini-batch', 'stochastic', 'gradient', 'descent', 'with', 'momentum', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'NN', '.']",9
part-of-speech_tagging,1,190,"0.05 is the decay ratio , the value of gradient clipping is 5 .","['0.05', 'is', 'the', 'decay', 'ratio', ',', 'the', 'value', 'of', 'gradient', 'clipping', 'is', '5', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['CD', 'VBZ', 'DT', 'NN', 'NN', ',', 'DT', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'CD', '.']",14
part-of-speech_tagging,1,191,"Dropout is applied on the input of IntNet , LSTMs , and CRF , and its ratio 0.5 is fixed , but with no dropout inside of IntNet .","['Dropout', 'is', 'applied', 'on', 'the', 'input', 'of', 'IntNet', ',', 'LSTMs', ',', 'and', 'CRF', ',', 'and', 'its', 'ratio', '0.5', 'is', 'fixed', ',', 'but', 'with', 'no', 'dropout', 'inside', 'of', 'IntNet', '.']","['B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', ',', 'CC', 'PRP$', 'NN', 'CD', 'VBZ', 'VBN', ',', 'CC', 'IN', 'DT', 'NN', 'IN', 'IN', 'NNP', '.']",29
part-of-speech_tagging,1,194,"Firstly , we use LSTM - CRF with randomly initialized word embeddings as our initial baseline .","['Firstly', ',', 'we', 'use', 'LSTM', '-', 'CRF', 'with', 'randomly', 'initialized', 'word', 'embeddings', 'as', 'our', 'initial', 'baseline', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'JJ', ':', 'NN', 'IN', 'JJ', 'VBN', 'NN', 'NNS', 'IN', 'PRP$', 'JJ', 'NN', '.']",17
part-of-speech_tagging,1,195,"We adopt two state - of - the - art methods in sequence labeling , denoted as char - LSTM and char - CNN .","['We', 'adopt', 'two', 'state', '-', 'of', '-', 'the', '-', 'art', 'methods', 'in', 'sequence', 'labeling', ',', 'denoted', 'as', 'char', '-', 'LSTM', 'and', 'char', '-', 'CNN', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'NN', 'NN', ',', 'VBN', 'IN', 'NN', ':', 'NNP', 'CC', 'SYM', ':', 'NN', '.']",25
part-of-speech_tagging,1,196,"We add more layers to the char - CNN model and refer to that as char - CNN - 5 and char - CNN - 9 , respectively for 5 and 9 convolutional layers .","['We', 'add', 'more', 'layers', 'to', 'the', 'char', '-', 'CNN', 'model', 'and', 'refer', 'to', 'that', 'as', 'char', '-', 'CNN', '-', '5', 'and', 'char', '-', 'CNN', '-', '9', ',', 'respectively', 'for', '5', 'and', '9', 'convolutional', 'layers', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJR', 'NNS', 'TO', 'DT', 'NN', ':', 'NNP', 'NN', 'CC', 'NN', 'TO', 'DT', 'IN', 'NN', ':', 'NNP', ':', 'CD', 'CC', 'VB', ':', 'NNP', ':', 'CD', ',', 'RB', 'IN', 'CD', 'CC', 'CD', 'JJ', 'NNS', '.']",35
part-of-speech_tagging,1,197,"Furthermore , we add residual connections to the char - CNN - 9 and refer it as char - ResNet .","['Furthermore', ',', 'we', 'add', 'residual', 'connections', 'to', 'the', 'char', '-', 'CNN', '-', '9', 'and', 'refer', 'it', 'as', 'char', '-', 'ResNet', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'TO', 'DT', 'NN', ':', 'NNP', ':', 'CD', 'CC', 'VB', 'PRP', 'IN', 'NN', ':', 'NN', '.']",21
part-of-speech_tagging,1,198,"Also , we apply 3 dense blocks based on char - ResNet which we refer to as char - DenseNet , to compare the difference between residual connection and dense connection .","['Also', ',', 'we', 'apply', '3', 'dense', 'blocks', 'based', 'on', 'char', '-', 'ResNet', 'which', 'we', 'refer', 'to', 'as', 'char', '-', 'DenseNet', ',', 'to', 'compare', 'the', 'difference', 'between', 'residual', 'connection', 'and', 'dense', 'connection', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'CD', 'NN', 'NNS', 'VBN', 'IN', 'NN', ':', 'NN', 'WDT', 'PRP', 'VBP', 'TO', 'IN', 'JJ', ':', 'NN', ',', 'TO', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NN', 'CC', 'NN', 'NN', '.']",32
part-of-speech_tagging,1,200,5 Results and Analysis 5.1 Character - to - word Models presents the performance of different character - to - word models on six benchmark datasets .,"['5', 'Results', 'and', 'Analysis', '5.1', 'Character', '-', 'to', '-', 'word', 'Models', 'presents', 'the', 'performance', 'of', 'different', 'character', '-', 'to', '-', 'word', 'models', 'on', 'six', 'benchmark', 'datasets', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['CD', 'NNS', 'CC', 'NNP', 'CD', 'NNP', ':', 'TO', ':', 'NN', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NN', ':', 'TO', ':', 'NN', 'NNS', 'IN', 'CD', 'JJ', 'NNS', '.']",27
part-of-speech_tagging,1,204,"The result shows that for most of the datasets , the F1 score does not improve much when we directly add more layers .","['The', 'result', 'shows', 'that', 'for', 'most', 'of', 'the', 'datasets', ',', 'the', 'F1', 'score', 'does', 'not', 'improve', 'much', 'when', 'we', 'directly', 'add', 'more', 'layers', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'IN', 'IN', 'JJS', 'IN', 'DT', 'NNS', ',', 'DT', 'NNP', 'NN', 'VBZ', 'RB', 'VB', 'RB', 'WRB', 'PRP', 'RB', 'VBP', 'JJR', 'NNS', '.']",24
part-of-speech_tagging,1,205,We also observe some accuracy drop when we continuously increase the depth .,"['We', 'also', 'observe', 'some', 'accuracy', 'drop', 'when', 'we', 'continuously', 'increase', 'the', 'depth', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', 'NN', 'WRB', 'PRP', 'RB', 'VB', 'DT', 'NN', '.']",13
part-of-speech_tagging,1,207,"Furthermore , we add residual connections to char - CNN - 9 as char - ResNet - 9 , which confirms that residual connections can help train deep layers .","['Furthermore', ',', 'we', 'add', 'residual', 'connections', 'to', 'char', '-', 'CNN', '-', '9', 'as', 'char', '-', 'ResNet', '-', '9', ',', 'which', 'confirms', 'that', 'residual', 'connections', 'can', 'help', 'train', 'deep', 'layers', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'TO', 'VB', ':', 'NNP', ':', 'CD', 'IN', 'NN', ':', 'NNP', ':', 'CD', ',', 'WDT', 'VBZ', 'IN', 'JJ', 'NNS', 'MD', 'VB', 'VB', 'JJ', 'NNS', '.']",30
part-of-speech_tagging,1,208,"We further improve char - ResNet - 9 by changing residual connections into dense connection blocks as char - DenseNet - 9 , which shows that the dense connections are better than residual connections for learning word shape information .","['We', 'further', 'improve', 'char', '-', 'ResNet', '-', '9', 'by', 'changing', 'residual', 'connections', 'into', 'dense', 'connection', 'blocks', 'as', 'char', '-', 'DenseNet', '-', '9', ',', 'which', 'shows', 'that', 'the', 'dense', 'connections', 'are', 'better', 'than', 'residual', 'connections', 'for', 'learning', 'word', 'shape', 'information', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'VB', 'NN', ':', 'NNP', ':', 'CD', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NNS', 'IN', 'NN', ':', 'NNP', ':', 'CD', ',', 'WDT', 'VBZ', 'IN', 'DT', 'NN', 'NNS', 'VBP', 'JJR', 'IN', 'JJ', 'NNS', 'IN', 'VBG', 'NN', 'NN', 'NN', '.']",40
part-of-speech_tagging,1,209,"Our proposed character - to - word model , char - IntNet - 5 and char - IntNet - 9 generally improves the results across all datasets .","['Our', 'proposed', 'character', '-', 'to', '-', 'word', 'model', ',', 'char', '-', 'IntNet', '-', '5', 'and', 'char', '-', 'IntNet', '-', '9', 'generally', 'improves', 'the', 'results', 'across', 'all', 'datasets', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP$', 'VBN', 'NN', ':', 'TO', ':', 'NN', 'NN', ',', 'SYM', ':', 'NNP', ':', 'CD', 'CC', 'VB', ':', 'NNP', ':', 'CD', 'RB', 'VBZ', 'DT', 'NNS', 'IN', 'DT', 'NNS', '.']",28
part-of-speech_tagging,1,210,"Our IntNet significantly outperforms other character embedding models , for example , the improvement is more than 2 % in terms of F 1 score for German and Dutch .","['Our', 'IntNet', 'significantly', 'outperforms', 'other', 'character', 'embedding', 'models', ',', 'for', 'example', ',', 'the', 'improvement', 'is', 'more', 'than', '2', '%', 'in', 'terms', 'of', 'F', '1', 'score', 'for', 'German', 'and', 'Dutch', '.']","['O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NNP', 'RB', 'VBZ', 'JJ', 'NN', 'VBG', 'NNS', ',', 'IN', 'NN', ',', 'DT', 'NN', 'VBZ', 'JJR', 'IN', 'CD', 'NN', 'IN', 'NNS', 'IN', 'NNP', 'CD', 'NN', 'IN', 'JJ', 'CC', 'NNP', '.']",30
part-of-speech_tagging,1,211,"Also , we observe that char - IntNet - 5 is more effective for learning character - to - word representations than char - IntNet - 9 in most of the cases .","['Also', ',', 'we', 'observe', 'that', 'char', '-', 'IntNet', '-', '5', 'is', 'more', 'effective', 'for', 'learning', 'character', '-', 'to', '-', 'word', 'representations', 'than', 'char', '-', 'IntNet', '-', '9', 'in', 'most', 'of', 'the', 'cases', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'VBP', ':', 'NNP', ':', 'CD', 'VBZ', 'RBR', 'JJ', 'IN', 'VBG', 'NN', ':', 'TO', ':', 'NN', 'NNS', 'IN', 'SYM', ':', 'NNP', ':', 'CD', 'IN', 'JJS', 'IN', 'DT', 'NNS', '.']",33
part-of-speech_tagging,1,214,Table 2 presents our proposed model in comparison with state - of - the - art results .,"['Table', '2', 'presents', 'our', 'proposed', 'model', 'in', 'comparison', 'with', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'CD', 'NNS', 'PRP$', 'VBN', 'NN', 'IN', 'NN', 'IN', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', '.']",18
part-of-speech_tagging,1,220,These experiments show that our char - IntNet generally improves results across different models and datasets .,"['These', 'experiments', 'show', 'that', 'our', 'char', '-', 'IntNet', 'generally', 'improves', 'results', 'across', 'different', 'models', 'and', 'datasets', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'IN', 'PRP$', 'NN', ':', 'NNP', 'RB', 'VBZ', 'NNS', 'IN', 'JJ', 'NNS', 'CC', 'NNS', '.']",17
part-of-speech_tagging,1,221,"The improvement is more pronounced for non-English datasets , for example , IntNet improves the F - 1 score over the stateof - the - art results by more than 2 % for Dutch and Spanish .","['The', 'improvement', 'is', 'more', 'pronounced', 'for', 'non-English', 'datasets', ',', 'for', 'example', ',', 'IntNet', 'improves', 'the', 'F', '-', '1', 'score', 'over', 'the', 'stateof', '-', 'the', '-', 'art', 'results', 'by', 'more', 'than', '2', '%', 'for', 'Dutch', 'and', 'Spanish', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'RBR', 'JJ', 'IN', 'JJ', 'NNS', ',', 'IN', 'NN', ',', 'NNP', 'VBZ', 'DT', 'NNP', ':', 'CD', 'NN', 'IN', 'DT', 'JJ', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'JJR', 'IN', 'CD', 'NN', 'IN', 'NNP', 'CC', 'NNP', '.']",37
part-of-speech_tagging,1,222,"It also shows that the results of LSTM - CRF are significantly improved after adding character - to - word models , which confirms that word shape information is very important for sequence labeling .","['It', 'also', 'shows', 'that', 'the', 'results', 'of', 'LSTM', '-', 'CRF', 'are', 'significantly', 'improved', 'after', 'adding', 'character', '-', 'to', '-', 'word', 'models', ',', 'which', 'confirms', 'that', 'word', 'shape', 'information', 'is', 'very', 'important', 'for', 'sequence', 'labeling', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBZ', 'IN', 'DT', 'NNS', 'IN', 'NNP', ':', 'NN', 'VBP', 'RB', 'VBN', 'IN', 'VBG', 'NN', ':', 'TO', ':', 'NN', 'NNS', ',', 'WDT', 'VBZ', 'IN', 'NN', 'NN', 'NN', 'VBZ', 'RB', 'JJ', 'IN', 'NN', 'NN', '.']",35
part-of-speech_tagging,5,2,Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings,"['Morphosyntactic', 'Tagging', 'with', 'a', 'Meta-BiLSTM', 'Model', 'over', 'Context', 'Sensitive', 'Token', 'Encodings']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'IN', 'DT', 'JJ', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'NNS']",11
part-of-speech_tagging,5,24,We propose a novel model where we learn context sensitive initial character and word representations through two separate sentence - level recurrent models .,"['We', 'propose', 'a', 'novel', 'model', 'where', 'we', 'learn', 'context', 'sensitive', 'initial', 'character', 'and', 'word', 'representations', 'through', 'two', 'separate', 'sentence', '-', 'level', 'recurrent', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'WRB', 'PRP', 'VBP', 'JJ', 'JJ', 'JJ', 'NN', 'CC', 'NN', 'NNS', 'IN', 'CD', 'JJ', 'NN', ':', 'NN', 'NN', 'NNS', '.']",24
part-of-speech_tagging,5,25,These are then combined via a meta-BiLSTM model that builds a unified representation of each word that is then used for syntactic tagging .,"['These', 'are', 'then', 'combined', 'via', 'a', 'meta-BiLSTM', 'model', 'that', 'builds', 'a', 'unified', 'representation', 'of', 'each', 'word', 'that', 'is', 'then', 'used', 'for', 'syntactic', 'tagging', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'VBP', 'RB', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'WDT', 'VBZ', 'RB', 'VBN', 'IN', 'JJ', 'NN', '.']",24
part-of-speech_tagging,5,119,The word embeddings are initialized with zero values and the pre-trained embeddings are not updated during training .,"['The', 'word', 'embeddings', 'are', 'initialized', 'with', 'zero', 'values', 'and', 'the', 'pre-trained', 'embeddings', 'are', 'not', 'updated', 'during', 'training', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'NN', 'NNS', 'CC', 'DT', 'JJ', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'NN', '.']",18
part-of-speech_tagging,5,120,The dropout used on the embeddings is achieved by a RRIE is the relative reduction in error .,"['The', 'dropout', 'used', 'on', 'the', 'embeddings', 'is', 'achieved', 'by', 'a', 'RRIE', 'is', 'the', 'relative', 'reduction', 'in', 'error', '.']","['O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBN', 'IN', 'DT', 'NNS', 'VBZ', 'VBN', 'IN', 'DT', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', '.']",18
part-of-speech_tagging,5,135,Part - of - Speech Tagging Results,"['Part', '-', 'of', '-', 'Speech', 'Tagging', 'Results']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'IN', ':', 'NN', 'NNP', 'NNP']",7
part-of-speech_tagging,5,141,Our model outperforms in 32 of the 54 treebanks with 13 ties .,"['Our', 'model', 'outperforms', 'in', '32', 'of', 'the', '54', 'treebanks', 'with', '13', 'ties', '.']","['B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'NNS', 'IN', 'CD', 'IN', 'DT', 'CD', 'NNS', 'IN', 'CD', 'NNS', '.']",13
part-of-speech_tagging,5,143,"Our model tends to produce better results , especially for morphologically rich languages ( e.g. Slavic","['Our', 'model', 'tends', 'to', 'produce', 'better', 'results', ',', 'especially', 'for', 'morphologically', 'rich', 'languages', '(', 'e.g.', 'Slavic']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['PRP$', 'NN', 'VBZ', 'TO', 'VB', 'JJR', 'NNS', ',', 'RB', 'IN', 'RB', 'JJ', 'NNS', '(', 'JJ', 'NNP']",16
part-of-speech_tagging,5,150,Morphological Tagging Results,"['Morphological', 'Tagging', 'Results']","['B-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NNS']",3
part-of-speech_tagging,5,155,"Our models tend to produce significantly better results than the winners of the CoNLL 2017 Shared Task ( i.e. , 1.8 % absolute improvement on average , corresponding to a RRIE of 21.20 % ) .","['Our', 'models', 'tend', 'to', 'produce', 'significantly', 'better', 'results', 'than', 'the', 'winners', 'of', 'the', 'CoNLL', '2017', 'Shared', 'Task', '(', 'i.e.', ',', '1.8', '%', 'absolute', 'improvement', 'on', 'average', ',', 'corresponding', 'to', 'a', 'RRIE', 'of', '21.20', '%', ')', '.']","['B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NNS', 'VBP', 'TO', 'VB', 'RB', 'JJR', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'CD', 'NNP', 'NNP', '(', 'FW', ',', 'CD', 'NN', 'JJ', 'NN', 'IN', 'NN', ',', 'VBG', 'TO', 'DT', 'NNP', 'IN', 'CD', 'NN', ')', '.']",36
part-of-speech_tagging,5,167,shows that separately optimized models are significantly more accurate on average than jointly optimized models .,"['shows', 'that', 'separately', 'optimized', 'models', 'are', 'significantly', 'more', 'accurate', 'on', 'average', 'than', 'jointly', 'optimized', 'models', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNS', 'WDT', 'RB', 'VBN', 'NNS', 'VBP', 'RB', 'RBR', 'JJ', 'IN', 'NN', 'IN', 'RB', 'VBN', 'NNS', '.']",16
part-of-speech_tagging,5,168,Separate optimization leads to better accuracy for 34 out of 40 treebanks for the morphological features task and for 30 out of 39 treebanks for xpos tagging .,"['Separate', 'optimization', 'leads', 'to', 'better', 'accuracy', 'for', '34', 'out', 'of', '40', 'treebanks', 'for', 'the', 'morphological', 'features', 'task', 'and', 'for', '30', 'out', 'of', '39', 'treebanks', 'for', 'xpos', 'tagging', '.']","['B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['JJ', 'NN', 'VBZ', 'TO', 'RBR', 'NN', 'IN', 'CD', 'IN', 'IN', 'CD', 'NNS', 'IN', 'DT', 'JJ', 'NNS', 'NN', 'CC', 'IN', 'CD', 'IN', 'IN', 'CD', 'NNS', 'IN', 'JJ', 'NN', '.']",28
part-of-speech_tagging,5,169,"Separate optimization outperformed joint optimization by up to 2.1 percent absolute , while joint never out - performed separate by more than 0.5 % absolute .","['Separate', 'optimization', 'outperformed', 'joint', 'optimization', 'by', 'up', 'to', '2.1', 'percent', 'absolute', ',', 'while', 'joint', 'never', 'out', '-', 'performed', 'separate', 'by', 'more', 'than', '0.5', '%', 'absolute', '.']","['O', 'O', 'B-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'NN', 'VBD', 'JJ', 'NN', 'IN', 'IN', 'TO', 'CD', 'NN', 'NN', ',', 'IN', 'JJ', 'RB', 'RP', ':', 'VBN', 'JJ', 'IN', 'JJR', 'IN', 'CD', 'NN', 'NN', '.']",26
part-of-speech_tagging,5,179,The examples show that the combined model has significantly higher accuracy compared with either the character and word models individually .,"['The', 'examples', 'show', 'that', 'the', 'combined', 'model', 'has', 'significantly', 'higher', 'accuracy', 'compared', 'with', 'either', 'the', 'character', 'and', 'word', 'models', 'individually', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['DT', 'NNS', 'VBP', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'RB', 'JJR', 'NN', 'VBN', 'IN', 'CC', 'DT', 'NN', 'CC', 'NN', 'NNS', 'RB', '.']",21
part-of-speech_tagging,5,194,"For all of the network sizes in the grid search , we still observed during training that the accuracy reach a high value and degrades with more iterations for the character and word model .","['For', 'all', 'of', 'the', 'network', 'sizes', 'in', 'the', 'grid', 'search', ',', 'we', 'still', 'observed', 'during', 'training', 'that', 'the', 'accuracy', 'reach', 'a', 'high', 'value', 'and', 'degrades', 'with', 'more', 'iterations', 'for', 'the', 'character', 'and', 'word', 'model', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'IN', 'DT', 'NN', 'VBZ', 'IN', 'DT', 'JJ', 'NN', ',', 'PRP', 'RB', 'VBD', 'IN', 'NN', 'IN', 'DT', 'NN', 'VBP', 'DT', 'JJ', 'NN', 'CC', 'NNS', 'IN', 'JJR', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NN', 'NN', '.']",35
part-of-speech_tagging,7,2,Multilingual Part - of - Speech Tagging with Bidirectional Long Short - Term Memory Models and Auxiliary Loss,"['Multilingual', 'Part', '-', 'of', '-', 'Speech', 'Tagging', 'with', 'Bidirectional', 'Long', 'Short', '-', 'Term', 'Memory', 'Models', 'and', 'Auxiliary', 'Loss']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', ':', 'IN', ':', 'NN', 'VBG', 'IN', 'NNP', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', 'NNP']",18
part-of-speech_tagging,7,5,"We address these issues and evaluate bi - LSTMs with word , character , and unicode byte embeddings for POS tagging .","['We', 'address', 'these', 'issues', 'and', 'evaluate', 'bi', '-', 'LSTMs', 'with', 'word', ',', 'character', ',', 'and', 'unicode', 'byte', 'embeddings', 'for', 'POS', 'tagging', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNS', 'CC', 'VB', 'SYM', ':', 'NN', 'IN', 'NN', ',', 'NN', ',', 'CC', 'JJ', 'NN', 'NNS', 'IN', 'NNP', 'NN', '.']",22
part-of-speech_tagging,7,22,"Finally , we introduce a novel model , a bi - LSTM trained with auxiliary loss .","['Finally', ',', 'we', 'introduce', 'a', 'novel', 'model', ',', 'a', 'bi', '-', 'LSTM', 'trained', 'with', 'auxiliary', 'loss', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', ',', 'DT', 'NN', ':', 'NN', 'VBD', 'IN', 'JJ', 'NN', '.']",17
part-of-speech_tagging,7,23,The model jointly predicts the POS and the log frequency of the word .,"['The', 'model', 'jointly', 'predicts', 'the', 'POS', 'and', 'the', 'log', 'frequency', 'of', 'the', 'word', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'RB', 'VBZ', 'DT', 'NNP', 'CC', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",14
part-of-speech_tagging,7,52,"epochs , default learning rate ( 0.1 ) , 128 dimensions for word embeddings , 100 for character and byte embeddings , 100 hidden states and Gaussian noise with ?= 0.2 .","['epochs', ',', 'default', 'learning', 'rate', '(', '0.1', ')', ',', '128', 'dimensions', 'for', 'word', 'embeddings', ',', '100', 'for', 'character', 'and', 'byte', 'embeddings', ',', '100', 'hidden', 'states', 'and', 'Gaussian', 'noise', 'with', '?=', '0.2', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['NN', ',', 'NN', 'VBG', 'NN', '(', 'CD', ')', ',', 'CD', 'NNS', 'IN', 'NN', 'NNS', ',', 'CD', 'IN', 'NN', 'CC', 'JJ', 'NNS', ',', 'CD', 'JJ', 'NNS', 'CC', 'JJ', 'NN', 'IN', 'JJ', 'CD', '.']",32
part-of-speech_tagging,7,53,"As training is stochastic in nature , we use a fixed seed throughout .","['As', 'training', 'is', 'stochastic', 'in', 'nature', ',', 'we', 'use', 'a', 'fixed', 'seed', 'throughout', '.']","['O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O']","['IN', 'NN', 'VBZ', 'JJ', 'IN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', '.']",14
part-of-speech_tagging,7,55,In that case we use offthe - shelf polyglot embeddings .,"['In', 'that', 'case', 'we', 'use', 'offthe', '-', 'shelf', 'polyglot', 'embeddings', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'JJ', ':', 'NN', 'NN', 'NNS', '.']",11
part-of-speech_tagging,7,58,The code is released at : https : //github.com/bplank/bilstm-aux,"['The', 'code', 'is', 'released', 'at', ':', 'https', ':', '//github.com/bplank/bilstm-aux']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['DT', 'NN', 'VBZ', 'VBN', 'IN', ':', 'NN', ':', 'JJ']",9
part-of-speech_tagging,7,79,"In an initial investigation , we compared Tnt , HunPos and TreeTagger and found Tnt to be consistently better than Treetagger , Hunpos followed closely but crashed on some languages ( e.g. , Arabic ) .","['In', 'an', 'initial', 'investigation', ',', 'we', 'compared', 'Tnt', ',', 'HunPos', 'and', 'TreeTagger', 'and', 'found', 'Tnt', 'to', 'be', 'consistently', 'better', 'than', 'Treetagger', ',', 'Hunpos', 'followed', 'closely', 'but', 'crashed', 'on', 'some', 'languages', '(', 'e.g.', ',', 'Arabic', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBN', 'NNP', ',', 'NNP', 'CC', 'NNP', 'CC', 'VBD', 'NNP', 'TO', 'VB', 'RB', 'JJR', 'IN', 'JJR', ',', 'NNP', 'VBD', 'RB', 'CC', 'VBD', 'IN', 'DT', 'NNS', '(', 'NN', ',', 'NNP', ')', '.']",36
part-of-speech_tagging,7,80,"The combined word + character representation model is the best representation , outperforming the baseline on all except one language ( Indonesian ) , providing strong results already without pre-trained embeddings .","['The', 'combined', 'word', '+', 'character', 'representation', 'model', 'is', 'the', 'best', 'representation', ',', 'outperforming', 'the', 'baseline', 'on', 'all', 'except', 'one', 'language', '(', 'Indonesian', ')', ',', 'providing', 'strong', 'results', 'already', 'without', 'pre-trained', 'embeddings', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NNP', 'NN', 'NN', 'NN', 'VBZ', 'DT', 'JJS', 'NN', ',', 'VBG', 'DT', 'NN', 'IN', 'DT', 'IN', 'CD', 'NN', '(', 'JJ', ')', ',', 'VBG', 'JJ', 'NNS', 'RB', 'IN', 'JJ', 'NNS', '.']",32
part-of-speech_tagging,7,81,This model ( w + c ) reaches the biggest improvement ( more than + 2 % accuracy ) on Hebrew and Slovene .,"['This', 'model', '(', 'w', '+', 'c', ')', 'reaches', 'the', 'biggest', 'improvement', '(', 'more', 'than', '+', '2', '%', 'accuracy', ')', 'on', 'Hebrew', 'and', 'Slovene', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', '(', 'JJ', 'NNP', 'NN', ')', 'VBZ', 'DT', 'JJS', 'NN', '(', 'JJR', 'IN', 'RB', 'CD', 'NN', 'NN', ')', 'IN', 'NNP', 'CC', 'NNP', '.']",24
part-of-speech_tagging,7,82,Initializing the word embeddings ( + POLYGLOT ) with off - the - shelf languagespecific embeddings further improves accuracy .,"['Initializing', 'the', 'word', 'embeddings', '(', '+', 'POLYGLOT', ')', 'with', 'off', '-', 'the', '-', 'shelf', 'languagespecific', 'embeddings', 'further', 'improves', 'accuracy', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O']","['VBG', 'DT', 'NN', 'NNS', '(', 'NNP', 'NNP', ')', 'IN', 'RP', ':', 'DT', ':', 'NN', 'JJ', 'NNS', 'RBR', 'NNS', 'NN', '.']",20
part-of-speech_tagging,7,85,The over all best system is the multi-task bi - LSTM FREQBIN ( it uses w + c and POLYGLOT initialization for w ) .,"['The', 'over', 'all', 'best', 'system', 'is', 'the', 'multi-task', 'bi', '-', 'LSTM', 'FREQBIN', '(', 'it', 'uses', 'w', '+', 'c', 'and', 'POLYGLOT', 'initialization', 'for', 'w', ')', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'IN', 'DT', 'JJS', 'NN', 'VBZ', 'DT', 'JJ', 'NN', ':', 'NNP', 'NNP', '(', 'PRP', 'VBZ', 'JJ', 'NNP', 'NN', 'CC', 'NNP', 'NN', 'IN', 'NN', ')', '.']",25
part-of-speech_tagging,4,2,Hierarchically - Refined Label Attention Network for Sequence Labeling,"['Hierarchically', '-', 'Refined', 'Label', 'Attention', 'Network', 'for', 'Sequence', 'Labeling']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', ':', 'VBD', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",9
part-of-speech_tagging,4,4,CRF has been used as a powerful model for statistical sequence labeling .,"['CRF', 'has', 'been', 'used', 'as', 'a', 'powerful', 'model', 'for', 'statistical', 'sequence', 'labeling', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'VBN', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",13
part-of-speech_tagging,4,19,"To this question , we investigate a neural network model for output label sequences .","['To', 'this', 'question', ',', 'we', 'investigate', 'a', 'neural', 'network', 'model', 'for', 'output', 'label', 'sequences', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NN', 'NN', 'NNS', '.']",15
part-of-speech_tagging,4,20,"In particular , we represent each possible label using an embedding vector , and aim to encode sequences of label distributions using a recurrent neural network .","['In', 'particular', ',', 'we', 'represent', 'each', 'possible', 'label', 'using', 'an', 'embedding', 'vector', ',', 'and', 'aim', 'to', 'encode', 'sequences', 'of', 'label', 'distributions', 'using', 'a', 'recurrent', 'neural', 'network', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBG', 'DT', 'NN', 'NN', ',', 'CC', 'NN', 'TO', 'VB', 'NNS', 'IN', 'NN', 'NNS', 'VBG', 'DT', 'JJ', 'JJ', 'NN', '.']",27
part-of-speech_tagging,4,22,This makes our task essentially to represent a full - exponential search space without making Markov assumptions .,"['This', 'makes', 'our', 'task', 'essentially', 'to', 'represent', 'a', 'full', '-', 'exponential', 'search', 'space', 'without', 'making', 'Markov', 'assumptions', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'VBZ', 'PRP$', 'NN', 'RB', 'TO', 'VB', 'DT', 'JJ', ':', 'JJ', 'NN', 'NN', 'IN', 'VBG', 'NNP', 'NNS', '.']",18
part-of-speech_tagging,4,179,WSJ . shows the final POS tagging results on WSJ .,"['WSJ', '.', 'shows', 'the', 'final', 'POS', 'tagging', 'results', 'on', 'WSJ', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', '.', 'VBZ', 'DT', 'JJ', 'NNP', 'VBG', 'NNS', 'IN', 'NNP', '.']",11
part-of-speech_tagging,4,181,"BiLSTM - LAN gives significant accuracy improvements over both BiLSTM - CRF and BiLSTM- softmax ( p < 0.01 ) , which is consistent with observations on development experiments .","['BiLSTM', '-', 'LAN', 'gives', 'significant', 'accuracy', 'improvements', 'over', 'both', 'BiLSTM', '-', 'CRF', 'and', 'BiLSTM-', 'softmax', '(', 'p', '<', '0.01', ')', ',', 'which', 'is', 'consistent', 'with', 'observations', 'on', 'development', 'experiments', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NNP', 'VBZ', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'NNP', ':', 'NN', 'CC', 'NNP', 'NN', '(', 'JJ', 'NNP', 'CD', ')', ',', 'WDT', 'VBZ', 'JJ', 'IN', 'NNS', 'IN', 'NN', 'NNS', '.']",30
part-of-speech_tagging,4,186,"Universal Dependencies ( UD ) v 2.2 . We design a multilingual experiment to compare BiLSTMsoftmax , BiLSTM - CRF ( strictly following 1 , which is the state - of - theart on multi-lingual POS tagging ) and BiLSTM - LAN .","['Universal', 'Dependencies', '(', 'UD', ')', 'v', '2.2', '.', 'We', 'design', 'a', 'multilingual', 'experiment', 'to', 'compare', 'BiLSTMsoftmax', ',', 'BiLSTM', '-', 'CRF', '(', 'strictly', 'following', '1', ',', 'which', 'is', 'the', 'state', '-', 'of', '-', 'theart', 'on', 'multi-lingual', 'POS', 'tagging', ')', 'and', 'BiLSTM', '-', 'LAN', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', '(', 'NNP', ')', 'VBD', 'CD', '.', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'TO', 'VB', 'NNP', ',', 'NNP', ':', 'NNP', '(', 'RB', 'VBG', 'CD', ',', 'WDT', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'NN', 'IN', 'JJ', 'NNP', 'NN', ')', 'CC', 'NNP', ':', 'NN', '.']",43
part-of-speech_tagging,4,188,Our model outperforms all the baselines on all the languages .,"['Our', 'model', 'outperforms', 'all', 'the', 'baselines', 'on', 'all', 'the', 'languages', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'PDT', 'DT', 'NNS', 'IN', 'PDT', 'DT', 'NNS', '.']",11
part-of-speech_tagging,4,189,"The improvements are statistically significant for all the languages ( p < 0.01 ) , suggesting that BiLSTM - LAN is generally effective across languages .","['The', 'improvements', 'are', 'statistically', 'significant', 'for', 'all', 'the', 'languages', '(', 'p', '<', '0.01', ')', ',', 'suggesting', 'that', 'BiLSTM', '-', 'LAN', 'is', 'generally', 'effective', 'across', 'languages', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NNS', 'VBP', 'RB', 'JJ', 'IN', 'PDT', 'DT', 'NNS', '(', 'JJ', 'NNP', 'CD', ')', ',', 'VBG', 'IN', 'NNP', ':', 'NN', 'VBZ', 'RB', 'JJ', 'IN', 'NNS', '.']",26
part-of-speech_tagging,4,190,"OntoNotes 5.0 . In NER , BiLSTM - CRF is widely used , because local dependencies between neighboring labels relatively more important that POS tagging and CCG supertagging .","['OntoNotes', '5.0', '.', 'In', 'NER', ',', 'BiLSTM', '-', 'CRF', 'is', 'widely', 'used', ',', 'because', 'local', 'dependencies', 'between', 'neighboring', 'labels', 'relatively', 'more', 'important', 'that', 'POS', 'tagging', 'and', 'CCG', 'supertagging', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBZ', 'CD', '.', 'IN', 'NNP', ',', 'NNP', ':', 'NN', 'VBZ', 'RB', 'VBN', ',', 'IN', 'JJ', 'NNS', 'IN', 'VBG', 'NNS', 'RB', 'RBR', 'JJ', 'IN', 'NNP', 'VBG', 'CC', 'NNP', 'NN', '.']",29
part-of-speech_tagging,4,191,BiLSTM - LAN also significantly outperforms BiLSTM - CRF by 1.17 F1-score ( p < 0.01 ) . CCGBank .,"['BiLSTM', '-', 'LAN', 'also', 'significantly', 'outperforms', 'BiLSTM', '-', 'CRF', 'by', '1.17', 'F1-score', '(', 'p', '<', '0.01', ')', '.', 'CCGBank', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NNP', 'RB', 'RB', 'VBZ', 'NNP', ':', 'NN', 'IN', 'CD', 'NN', '(', 'JJ', 'NNP', 'CD', ')', '.', 'NNP', '.']",20
part-of-speech_tagging,4,193,"As shown in , BiLSTM - LAN significantly outperforms both BiLSTMsoftmax and BiLSTM - CRF ( p < 0.01 ) , showing the advantage of LAN . and explore BiRNN - softmax and BiLSTM - softmax , respectively .","['As', 'shown', 'in', ',', 'BiLSTM', '-', 'LAN', 'significantly', 'outperforms', 'both', 'BiLSTMsoftmax', 'and', 'BiLSTM', '-', 'CRF', '(', 'p', '<', '0.01', ')', ',', 'showing', 'the', 'advantage', 'of', 'LAN', '.', 'and', 'explore', 'BiRNN', '-', 'softmax', 'and', 'BiLSTM', '-', 'softmax', ',', 'respectively', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBN', 'IN', ',', 'NNP', ':', 'NNP', 'RB', 'VBZ', 'DT', 'NNP', 'CC', 'NNP', ':', 'NNP', '(', 'JJ', 'NNP', 'CD', ')', ',', 'VBG', 'DT', 'NN', 'IN', 'NNP', '.', 'CC', 'VB', 'NNP', ':', 'NN', 'CC', 'NNP', ':', 'NN', ',', 'RB', '.']",39
part-of-speech_tagging,4,198,"Compared with these methods , BiLSTM - LAN obtains new state - of - theart results on CCGBank , matching the tri-training performance of , without training on external data .","['Compared', 'with', 'these', 'methods', ',', 'BiLSTM', '-', 'LAN', 'obtains', 'new', 'state', '-', 'of', '-', 'theart', 'results', 'on', 'CCGBank', ',', 'matching', 'the', 'tri-training', 'performance', 'of', ',', 'without', 'training', 'on', 'external', 'data', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'IN', 'DT', 'NNS', ',', 'NNP', ':', 'NN', 'VBZ', 'JJ', 'NN', ':', 'IN', ':', 'NN', 'NNS', 'IN', 'NNP', ',', 'VBG', 'DT', 'JJ', 'NN', 'IN', ',', 'IN', 'VBG', 'IN', 'JJ', 'NNS', '.']",31
part-of-speech_tagging,6,2,A Novel Neural Network Model for Joint POS Tagging and Graph - based Dependency Parsing,"['A', 'Novel', 'Neural', 'Network', 'Model', 'for', 'Joint', 'POS', 'Tagging', 'and', 'Graph', '-', 'based', 'Dependency', 'Parsing']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['DT', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', ':', 'VBN', 'NNP', 'VBG']",15
part-of-speech_tagging,6,7,Our code is open - source and available together with pre-trained models at : https://github.com/ datquocnguyen/jPTDP .,"['Our', 'code', 'is', 'open', '-', 'source', 'and', 'available', 'together', 'with', 'pre-trained', 'models', 'at', ':', 'https://github.com/', 'datquocnguyen/jPTDP', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'JJ', ':', 'NN', 'CC', 'JJ', 'RB', 'IN', 'JJ', 'NNS', 'IN', ':', 'NN', 'NN', '.']",17
part-of-speech_tagging,6,19,"In this paper , we propose a novel neural architecture for joint POS tagging and graph - based dependency parsing .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', 'neural', 'architecture', 'for', 'joint', 'POS', 'tagging', 'and', 'graph', '-', 'based', 'dependency', 'parsing', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'JJ', 'NNP', 'NN', 'CC', 'NN', ':', 'VBN', 'NN', 'VBG', '.']",21
part-of-speech_tagging,6,20,Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTMthe bidirectional LSTM .,"['Our', 'model', 'learns', 'latent', 'feature', 'representations', 'shared', 'for', 'both', 'POS', 'tagging', 'and', 'dependency', 'parsing', 'tasks', 'by', 'using', 'BiLSTMthe', 'bidirectional', 'LSTM', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'JJ', 'NN', 'NNS', 'VBN', 'IN', 'DT', 'NNP', 'NN', 'CC', 'NN', 'VBG', 'NNS', 'IN', 'VBG', 'NNP', 'JJ', 'NNP', '.']",21
part-of-speech_tagging,6,75,Our jPTDP is implemented using DYNET v 2.0 .,"['Our', 'jPTDP', 'is', 'implemented', 'using', 'DYNET', 'v', '2.0', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'VBN', 'VBG', 'NNP', 'NN', 'CD', '.']",9
part-of-speech_tagging,6,76,"We optimize the objective function using Adam ( Kingma and Ba , 2014 ) with default DYNET parameter settings and no mini-batches .","['We', 'optimize', 'the', 'objective', 'function', 'using', 'Adam', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', 'with', 'default', 'DYNET', 'parameter', 'settings', 'and', 'no', 'mini-batches', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBG', 'NNP', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'IN', 'NN', 'NNP', 'NN', 'NNS', 'CC', 'DT', 'NNS', '.']",23
part-of-speech_tagging,6,78,"Following Kiperwasser and Goldberg ( 2016 b ) and , we apply a word dropout rate of 0.25 and Gaussian noise with ? = 0.2 .","['Following', 'Kiperwasser', 'and', 'Goldberg', '(', '2016', 'b', ')', 'and', ',', 'we', 'apply', 'a', 'word', 'dropout', 'rate', 'of', '0.25', 'and', 'Gaussian', 'noise', 'with', '?', '=', '0.2', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['VBG', 'NNP', 'CC', 'NNP', '(', 'CD', 'NN', ')', 'CC', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'NN', 'IN', 'CD', 'CC', 'JJ', 'NN', 'IN', '.', '$', 'CD', '.']",26
part-of-speech_tagging,6,79,"For training , we run for 30 epochs , and evaluate the mixed accuracy of correctly assigning POS tag together with dependency arc and relation type on the development set after each training epoch .","['For', 'training', ',', 'we', 'run', 'for', '30', 'epochs', ',', 'and', 'evaluate', 'the', 'mixed', 'accuracy', 'of', 'correctly', 'assigning', 'POS', 'tag', 'together', 'with', 'dependency', 'arc', 'and', 'relation', 'type', 'on', 'the', 'development', 'set', 'after', 'each', 'training', 'epoch', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'IN', 'CD', 'NNS', ',', 'CC', 'VB', 'DT', 'JJ', 'NN', 'IN', 'RB', 'VBG', 'NNP', 'NN', 'RB', 'IN', 'NN', 'NN', 'CC', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",35
part-of-speech_tagging,6,80,We perform a minimal grid search of hyper - parameters on English .,"['We', 'perform', 'a', 'minimal', 'grid', 'search', 'of', 'hyper', '-', 'parameters', 'on', 'English', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'JJ', ':', 'NNS', 'IN', 'NNP', '.']",13
part-of-speech_tagging,6,84,"compares the POS tagging and dependency parsing results of our model jPTDP with results reported in prior work , using the same experimental setup .","['compares', 'the', 'POS', 'tagging', 'and', 'dependency', 'parsing', 'results', 'of', 'our', 'model', 'jPTDP', 'with', 'results', 'reported', 'in', 'prior', 'work', ',', 'using', 'the', 'same', 'experimental', 'setup', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBZ', 'DT', 'NNP', 'NN', 'CC', 'NN', 'VBG', 'NNS', 'IN', 'PRP$', 'NN', 'NN', 'IN', 'NNS', 'VBN', 'IN', 'JJ', 'NN', ',', 'VBG', 'DT', 'JJ', 'JJ', 'NN', '.']",25
part-of-speech_tagging,6,90,"In terms of dependency parsing , in most cases , our model jPTDP outperforms Stack - propagation .","['In', 'terms', 'of', 'dependency', 'parsing', ',', 'in', 'most', 'cases', ',', 'our', 'model', 'jPTDP', 'outperforms', 'Stack', '-', 'propagation', '.']","['B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NNS', 'IN', 'NN', 'NN', ',', 'IN', 'JJS', 'NNS', ',', 'PRP$', 'NN', 'NN', 'VBZ', 'NNP', ':', 'NN', '.']",18
part-of-speech_tagging,6,91,It is somewhat unexpected that our model produces about 7 % absolute lower LAS score than Stack - propagation on Dutch ( nl ) .,"['It', 'is', 'somewhat', 'unexpected', 'that', 'our', 'model', 'produces', 'about', '7', '%', 'absolute', 'lower', 'LAS', 'score', 'than', 'Stack', '-', 'propagation', 'on', 'Dutch', '(', 'nl', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'RB', 'JJ', 'IN', 'PRP$', 'NN', 'VBZ', 'RB', 'CD', 'NN', 'NN', 'JJR', 'NNP', 'JJR', 'IN', 'NNP', ':', 'NN', 'IN', 'NNP', '(', 'NN', ')', '.']",25
part-of-speech_tagging,6,94,"Without taking "" nl "" into account , our averaged LAS score over all remaining languages is 1.1 % absolute higher than Stack - propagation 's .","['Without', 'taking', '""', 'nl', '""', 'into', 'account', ',', 'our', 'averaged', 'LAS', 'score', 'over', 'all', 'remaining', 'languages', 'is', '1.1', '%', 'absolute', 'higher', 'than', 'Stack', '-', 'propagation', ""'s"", '.']","['B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBG', 'JJ', 'JJ', 'NN', 'IN', 'NN', ',', 'PRP$', 'VBD', 'NNP', 'NN', 'IN', 'DT', 'VBG', 'NNS', 'VBZ', 'CD', 'NN', 'NN', 'JJR', 'IN', 'NNP', ':', 'NN', 'POS', '.']",27
part-of-speech_tagging,6,96,"The last row in shows an absolute LAS improvement of 4.4 % on average when comparing our jPTDP with its simplified version of not using characterbased representations : specifically , morphologically rich languages get an averaged improvement of 9.3 % , vice versa 2.6 % for others .","['The', 'last', 'row', 'in', 'shows', 'an', 'absolute', 'LAS', 'improvement', 'of', '4.4', '%', 'on', 'average', 'when', 'comparing', 'our', 'jPTDP', 'with', 'its', 'simplified', 'version', 'of', 'not', 'using', 'characterbased', 'representations', ':', 'specifically', ',', 'morphologically', 'rich', 'languages', 'get', 'an', 'averaged', 'improvement', 'of', '9.3', '%', ',', 'vice', 'versa', '2.6', '%', 'for', 'others', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'IN', 'NNS', 'DT', 'JJ', 'NNP', 'NN', 'IN', 'CD', 'NN', 'IN', 'NN', 'WRB', 'VBG', 'PRP$', 'NN', 'IN', 'PRP$', 'JJ', 'NN', 'IN', 'RB', 'VBG', 'VBN', 'NNS', ':', 'RB', ',', 'RB', 'JJ', 'NNS', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NN', ',', 'NN', 'RB', 'CD', 'NN', 'IN', 'NNS', '.']",48
part-of-speech_tagging,6,98,"So , our jPDTP is particularly good for morphologically rich languages , with 1.7 % higher averaged LAS than Stack - propagation over these languages .","['So', ',', 'our', 'jPDTP', 'is', 'particularly', 'good', 'for', 'morphologically', 'rich', 'languages', ',', 'with', '1.7', '%', 'higher', 'averaged', 'LAS', 'than', 'Stack', '-', 'propagation', 'over', 'these', 'languages', '.']","['O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP$', 'NN', 'VBZ', 'RB', 'JJ', 'IN', 'RB', 'JJ', 'NNS', ',', 'IN', 'CD', 'NN', 'JJR', 'VBD', 'NNP', 'IN', 'NNP', ':', 'NN', 'IN', 'DT', 'NNS', '.']",26
part-of-speech_tagging,2,2,TRANSFER LEARNING FOR SEQUENCE TAGGING WITH HIERARCHICAL RECURRENT NETWORKS,"['TRANSFER', 'LEARNING', 'FOR', 'SEQUENCE', 'TAGGING', 'WITH', 'HIERARCHICAL', 'RECURRENT', 'NETWORKS']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",9
part-of-speech_tagging,2,11,1 Code is available at https://github.com/kimiyoung/transfer 1 ar Xiv:1703.06345v1 [ cs.CL ],"['1', 'Code', 'is', 'available', 'at', 'https://github.com/kimiyoung/transfer', '1', 'ar', 'Xiv:1703.06345v1', '[', 'cs.CL', ']']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']","['CD', 'NNP', 'VBZ', 'JJ', 'IN', 'NN', 'CD', 'NN', 'NNP', 'NNP', 'NN', 'NN']",12
part-of-speech_tagging,2,25,"We present a transfer learning approach based on a deep hierarchical recurrent neural network , which shares the hidden feature repre-sentation and part of the model parameters between the source task and the target task .","['We', 'present', 'a', 'transfer', 'learning', 'approach', 'based', 'on', 'a', 'deep', 'hierarchical', 'recurrent', 'neural', 'network', ',', 'which', 'shares', 'the', 'hidden', 'feature', 'repre-sentation', 'and', 'part', 'of', 'the', 'model', 'parameters', 'between', 'the', 'source', 'task', 'and', 'the', 'target', 'task', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'VBG', 'NN', 'VBN', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'JJ', 'NN', ',', 'WDT', 'NNS', 'DT', 'JJ', 'NN', 'NN', 'CC', 'NN', 'IN', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'CC', 'DT', 'NN', 'NN', '.']",36
part-of-speech_tagging,2,26,Our approach combines the objectives of the two tasks and uses gradient - based methods for efficient training .,"['Our', 'approach', 'combines', 'the', 'objectives', 'of', 'the', 'two', 'tasks', 'and', 'uses', 'gradient', '-', 'based', 'methods', 'for', 'efficient', 'training', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'DT', 'NNS', 'IN', 'DT', 'CD', 'NNS', 'CC', 'VBZ', 'NN', ':', 'VBN', 'NNS', 'IN', 'JJ', 'NN', '.']",19
part-of-speech_tagging,2,132,TRANSFER LEARNING PERFORMANCE,"['TRANSFER', 'LEARNING', 'PERFORMANCE']","['B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP']",3
part-of-speech_tagging,2,134,"We fix the hyperparameters for all the results reported in this section : we set the character embedding dimension at 25 , the word embedding dimension at 50 for English and 64 for Spanish , the dimension of hidden states of the character - level GRUs at 80 , the dimension of hidden states of the word - level GRUs at 300 , and the initial learning rate at 0.01 .","['We', 'fix', 'the', 'hyperparameters', 'for', 'all', 'the', 'results', 'reported', 'in', 'this', 'section', ':', 'we', 'set', 'the', 'character', 'embedding', 'dimension', 'at', '25', ',', 'the', 'word', 'embedding', 'dimension', 'at', '50', 'for', 'English', 'and', '64', 'for', 'Spanish', ',', 'the', 'dimension', 'of', 'hidden', 'states', 'of', 'the', 'character', '-', 'level', 'GRUs', 'at', '80', ',', 'the', 'dimension', 'of', 'hidden', 'states', 'of', 'the', 'word', '-', 'level', 'GRUs', 'at', '300', ',', 'and', 'the', 'initial', 'learning', 'rate', 'at', '0.01', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNS', 'IN', 'PDT', 'DT', 'NNS', 'VBD', 'IN', 'DT', 'NN', ':', 'PRP', 'VBP', 'DT', 'NN', 'VBG', 'NN', 'IN', 'CD', ',', 'DT', 'NN', 'VBG', 'NN', 'IN', 'CD', 'IN', 'NNP', 'CC', 'CD', 'IN', 'JJ', ',', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', ':', 'NN', 'NNP', 'IN', 'CD', ',', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', ':', 'NN', 'NNP', 'IN', 'CD', ',', 'CC', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', '.']",71
part-of-speech_tagging,2,142,We can see that our transfer learning approach consistently improved over the non-transfer results .,"['We', 'can', 'see', 'that', 'our', 'transfer', 'learning', 'approach', 'consistently', 'improved', 'over', 'the', 'non-transfer', 'results', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'IN', 'PRP$', 'NN', 'VBG', 'NN', 'RB', 'VBN', 'IN', 'DT', 'JJ', 'NNS', '.']",15
part-of-speech_tagging,2,143,We also observe that the improvement by transfer learning is more substantial when the labeling rate is lower .,"['We', 'also', 'observe', 'that', 'the', 'improvement', 'by', 'transfer', 'learning', 'is', 'more', 'substantial', 'when', 'the', 'labeling', 'rate', 'is', 'lower', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'RB', 'VBP', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'RBR', 'JJ', 'WRB', 'DT', 'NN', 'NN', 'VBZ', 'JJR', '.']",19
part-of-speech_tagging,2,146,"As shown in and 2 ( e ) , our transfer learning approach can improve the performance on Twitter POS tagging and NER for all labeling rates , and the improvements with 0.1 labels are more than 8 % for both datasets .","['As', 'shown', 'in', 'and', '2', '(', 'e', ')', ',', 'our', 'transfer', 'learning', 'approach', 'can', 'improve', 'the', 'performance', 'on', 'Twitter', 'POS', 'tagging', 'and', 'NER', 'for', 'all', 'labeling', 'rates', ',', 'and', 'the', 'improvements', 'with', '0.1', 'labels', 'are', 'more', 'than', '8', '%', 'for', 'both', 'datasets', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'VBN', 'IN', 'CC', 'CD', '(', 'NN', ')', ',', 'PRP$', 'NN', 'VBG', 'NN', 'MD', 'VB', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'NN', 'CC', 'NNP', 'IN', 'DT', 'NN', 'NNS', ',', 'CC', 'DT', 'NNS', 'IN', 'CD', 'NNS', 'VBP', 'JJR', 'IN', 'CD', 'NN', 'IN', 'DT', 'NNS', '.']",43
part-of-speech_tagging,2,147,Cross - application transfer also leads to substantial improvement under low - resource conditions .,"['Cross', '-', 'application', 'transfer', 'also', 'leads', 'to', 'substantial', 'improvement', 'under', 'low', '-', 'resource', 'conditions', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', 'NN', 'RB', 'VBZ', 'TO', 'JJ', 'NN', 'IN', 'JJ', ':', 'NN', 'NNS', '.']",15
part-of-speech_tagging,2,157,COMPARISON WITH STATE - OF - THE - ART RESULTS,"['COMPARISON', 'WITH', 'STATE', '-', 'OF', '-', 'THE', '-', 'ART', 'RESULTS']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NN', 'NNP', 'NNP', ':', 'SYM', ':', 'DT', ':', 'NN', 'NN']",10
part-of-speech_tagging,2,160,We use publicly available pretrained word embeddings as initialization .,"['We', 'use', 'publicly', 'available', 'pretrained', 'word', 'embeddings', 'as', 'initialization', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'RB', 'JJ', 'VBN', 'NN', 'NNS', 'IN', 'NN', '.']",10
part-of-speech_tagging,2,161,"On the English datasets , following previous works that are based on neural networks , we experiment with both the 50 - dimensional SENNA embeddings and the 100 - dimensional GloVe embeddings and use the development set to choose the embeddings for different tasks and settings .","['On', 'the', 'English', 'datasets', ',', 'following', 'previous', 'works', 'that', 'are', 'based', 'on', 'neural', 'networks', ',', 'we', 'experiment', 'with', 'both', 'the', '50', '-', 'dimensional', 'SENNA', 'embeddings', 'and', 'the', '100', '-', 'dimensional', 'GloVe', 'embeddings', 'and', 'use', 'the', 'development', 'set', 'to', 'choose', 'the', 'embeddings', 'for', 'different', 'tasks', 'and', 'settings', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNP', 'NNS', ',', 'VBG', 'JJ', 'NNS', 'WDT', 'VBP', 'VBN', 'IN', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'IN', 'DT', 'DT', 'CD', ':', 'JJ', 'NNP', 'NNS', 'CC', 'DT', 'CD', ':', 'JJ', 'NNP', 'NNS', 'CC', 'VB', 'DT', 'NN', 'VBN', 'TO', 'VB', 'DT', 'NNS', 'IN', 'JJ', 'NNS', 'CC', 'NNS', '.']",47
part-of-speech_tagging,2,162,"For Spanish and Dutch , we use the 64 - dimensional Polyglot embeddings .","['For', 'Spanish', 'and', 'Dutch', ',', 'we', 'use', 'the', '64', '-', 'dimensional', 'Polyglot', 'embeddings', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', 'CC', 'NNP', ',', 'PRP', 'VBP', 'DT', 'CD', ':', 'JJ', 'NNP', 'NNS', '.']",14
part-of-speech_tagging,2,163,We set the hidden state dimensions to be 300 for the word - level GRU .,"['We', 'set', 'the', 'hidden', 'state', 'dimensions', 'to', 'be', '300', 'for', 'the', 'word', '-', 'level', 'GRU', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NNS', 'TO', 'VB', 'CD', 'IN', 'DT', 'NN', ':', 'NN', 'NNP', '.']",16
part-of-speech_tagging,2,164,The initial learning rate for AdaGrad is fixed at 0.01 .,"['The', 'initial', 'learning', 'rate', 'for', 'AdaGrad', 'is', 'fixed', 'at', '0.01', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'VBZ', 'VBN', 'IN', 'CD', '.']",11
part-of-speech_tagging,2,171,"First , our transfer learning approach achieves new state - of - the - art results on all the considered benchmark datasets except PTB POS tagging , which indicates that transfer learning can still improve the performance even on datasets with relatively abundant labels .","['First', ',', 'our', 'transfer', 'learning', 'approach', 'achieves', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'on', 'all', 'the', 'considered', 'benchmark', 'datasets', 'except', 'PTB', 'POS', 'tagging', ',', 'which', 'indicates', 'that', 'transfer', 'learning', 'can', 'still', 'improve', 'the', 'performance', 'even', 'on', 'datasets', 'with', 'relatively', 'abundant', 'labels', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP$', 'NN', 'VBG', 'NN', 'VBZ', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'PDT', 'DT', 'VBN', 'NN', 'NNS', 'IN', 'NNP', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'IN', 'NN', 'NN', 'MD', 'RB', 'VB', 'DT', 'NN', 'RB', 'IN', 'NNS', 'IN', 'RB', 'JJ', 'NNS', '.']",45
part-of-speech_tagging,2,172,"Second , our base model ( w/o transfer ) performs competitively compared to the state - of - the - art systems , which means that the improvements shown in Section 4.2 are obtained over a strong baseline .","['Second', ',', 'our', 'base', 'model', '(', 'w/o', 'transfer', ')', 'performs', 'competitively', 'compared', 'to', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'systems', ',', 'which', 'means', 'that', 'the', 'improvements', 'shown', 'in', 'Section', '4.2', 'are', 'obtained', 'over', 'a', 'strong', 'baseline', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', ',', 'PRP$', 'NN', 'NN', '(', 'JJ', 'NN', ')', 'VBZ', 'RB', 'VBN', 'TO', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', ',', 'WDT', 'VBZ', 'IN', 'DT', 'NNS', 'VBN', 'IN', 'NN', 'CD', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",39
part-of-speech_tagging,3,2,End - to - end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,"['End', '-', 'to', '-', 'end', 'Sequence', 'Labeling', 'via', 'Bi-directional', 'LSTM-CNNs-CRF']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['NN', ':', 'TO', ':', 'NN', 'NN', 'VBG', 'IN', 'NNP', 'NNP']",10
part-of-speech_tagging,3,4,State - of - the - art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing .,"['State', '-', 'of', '-', 'the', '-', 'art', 'sequence', 'labeling', 'systems', 'traditionally', 'require', 'large', 'amounts', 'of', 'taskspecific', 'knowledge', 'in', 'the', 'form', 'of', 'handcrafted', 'features', 'and', 'data', 'pre-processing', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'VBG', 'NNS', 'RB', 'VBP', 'JJ', 'NNS', 'IN', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'CC', 'NNS', 'NN', '.']",27
part-of-speech_tagging,3,10,"Linguistic sequence labeling , such as part - ofspeech ( POS ) tagging and named entity recognition ( NER ) , is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community .","['Linguistic', 'sequence', 'labeling', ',', 'such', 'as', 'part', '-', 'ofspeech', '(', 'POS', ')', 'tagging', 'and', 'named', 'entity', 'recognition', '(', 'NER', ')', ',', 'is', 'one', 'of', 'the', 'first', 'stages', 'in', 'deep', 'language', 'understanding', 'and', 'its', 'importance', 'has', 'been', 'well', 'recognized', 'in', 'the', 'natural', 'language', 'processing', 'community', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', ',', 'JJ', 'IN', 'NN', ':', 'NN', '(', 'NNP', ')', 'NN', 'CC', 'VBN', 'NN', 'NN', '(', 'NNP', ')', ',', 'VBZ', 'CD', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NN', 'CC', 'PRP$', 'NN', 'VBZ', 'VBN', 'RB', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', '.']",45
part-of-speech_tagging,3,21,"In this paper , we propose a neural network architecture for sequence labeling .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'neural', 'network', 'architecture', 'for', 'sequence', 'labeling', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",14
part-of-speech_tagging,3,22,"It is a truly endto - end model requiring no task - specific resources , feature engineering , or data pre-processing beyond pre-trained word embeddings on unlabeled corpora .","['It', 'is', 'a', 'truly', 'endto', '-', 'end', 'model', 'requiring', 'no', 'task', '-', 'specific', 'resources', ',', 'feature', 'engineering', ',', 'or', 'data', 'pre-processing', 'beyond', 'pre-trained', 'word', 'embeddings', 'on', 'unlabeled', 'corpora', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBZ', 'DT', 'RB', 'JJ', ':', 'NN', 'NN', 'VBG', 'DT', 'NN', ':', 'JJ', 'NNS', ',', 'NN', 'NN', ',', 'CC', 'NNS', 'JJ', 'IN', 'JJ', 'NN', 'NNS', 'IN', 'JJ', 'NN', '.']",29
part-of-speech_tagging,3,23,"Thus , our model can be easily applied to a wide range of sequence labeling tasks on different languages and domains .","['Thus', ',', 'our', 'model', 'can', 'be', 'easily', 'applied', 'to', 'a', 'wide', 'range', 'of', 'sequence', 'labeling', 'tasks', 'on', 'different', 'languages', 'and', 'domains', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'NN', 'MD', 'VB', 'RB', 'VBN', 'TO', 'DT', 'JJ', 'NN', 'IN', 'NN', 'VBG', 'NNS', 'IN', 'JJ', 'NNS', 'CC', 'NNS', '.']",22
part-of-speech_tagging,3,24,We first use convolutional neural networks ( CNNs ) to encode character - level information of a word into its character - level representation .,"['We', 'first', 'use', 'convolutional', 'neural', 'networks', '(', 'CNNs', ')', 'to', 'encode', 'character', '-', 'level', 'information', 'of', 'a', 'word', 'into', 'its', 'character', '-', 'level', 'representation', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'JJ', 'JJ', 'NNS', '(', 'NNP', ')', 'TO', 'VB', 'NN', ':', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'PRP$', 'NN', ':', 'NN', 'NN', '.']",25
part-of-speech_tagging,3,25,Then we combine character - and word - level representations and feed them into bi-directional LSTM ( BLSTM ) to model context information of each word .,"['Then', 'we', 'combine', 'character', '-', 'and', 'word', '-', 'level', 'representations', 'and', 'feed', 'them', 'into', 'bi-directional', 'LSTM', '(', 'BLSTM', ')', 'to', 'model', 'context', 'information', 'of', 'each', 'word', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', 'PRP', 'VBP', 'JJ', ':', 'CC', 'NN', ':', 'NN', 'NNS', 'CC', 'VB', 'PRP', 'IN', 'JJ', 'NNP', '(', 'NNP', ')', 'TO', 'VB', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",27
part-of-speech_tagging,3,26,"On top of BLSTM , we use a sequential CRF to jointly decode labels for the whole sentence .","['On', 'top', 'of', 'BLSTM', ',', 'we', 'use', 'a', 'sequential', 'CRF', 'to', 'jointly', 'decode', 'labels', 'for', 'the', 'whole', 'sentence', '.']","['B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', 'IN', 'NNP', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNP', 'TO', 'RB', 'VB', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",19
part-of-speech_tagging,3,97,Parameter optimization is performed with minibatch stochastic gradient descent ( SGD ) with batch size 10 and momentum 0.9 .,"['Parameter', 'optimization', 'is', 'performed', 'with', 'minibatch', 'stochastic', 'gradient', 'descent', '(', 'SGD', ')', 'with', 'batch', 'size', '10', 'and', 'momentum', '0.9', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'B-n', 'O']","['NNP', 'NN', 'VBZ', 'VBN', 'IN', 'NN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'IN', 'NN', 'NN', 'CD', 'CC', '$', 'CD', '.']",20
part-of-speech_tagging,3,98,"We choose an initial learning rate of ? 0 ( ? 0 = 0.01 for POS tagging , and 0.015 for NER , see Section 3.3 . ) , and the learning rate is updated on each epoch of training as ? t = ? 0 / ( 1 + ?t ) , with decay rate ? =","['We', 'choose', 'an', 'initial', 'learning', 'rate', 'of', '?', '0', '(', '?', '0', '=', '0.01', 'for', 'POS', 'tagging', ',', 'and', '0.015', 'for', 'NER', ',', 'see', 'Section', '3.3', '.', ')', ',', 'and', 'the', 'learning', 'rate', 'is', 'updated', 'on', 'each', 'epoch', 'of', 'training', 'as', '?', 't', '=', '?', '0', '/', '(', '1', '+', '?t', ')', ',', 'with', 'decay', 'rate', '?', '=']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', '.', 'CD', '(', '.', 'CD', '$', 'CD', 'IN', 'NNP', 'NN', ',', 'CC', 'CD', 'IN', 'NNP', ',', 'VBP', 'NN', 'CD', '.', ')', ',', 'CC', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NN', 'IN', '.', 'NN', 'NN', '.', 'CD', 'NN', '(', 'CD', 'NNP', 'NNP', ')', ',', 'IN', 'NN', 'NN', '.', 'NN']",58
part-of-speech_tagging,3,100,"To reduce the effects of "" gradient exploding "" , we use a gradient clipping of 5.0 .","['To', 'reduce', 'the', 'effects', 'of', '""', 'gradient', 'exploding', '""', ',', 'we', 'use', 'a', 'gradient', 'clipping', 'of', '5.0', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['TO', 'VB', 'DT', 'NNS', 'IN', 'JJ', 'NN', 'VBG', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",18
part-of-speech_tagging,3,103,We use early stopping based on performance on validation sets .,"['We', 'use', 'early', 'stopping', 'based', 'on', 'performance', 'on', 'validation', 'sets', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'VBN', 'IN', 'NN', 'IN', 'NN', 'NNS', '.']",11
part-of-speech_tagging,3,104,"The "" best "" parameters appear at around 50 epochs , according to our experiments .","['The', '""', 'best', '""', 'parameters', 'appear', 'at', 'around', '50', 'epochs', ',', 'according', 'to', 'our', 'experiments', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'JJS', 'NN', 'NNS', 'VBP', 'IN', 'IN', 'CD', 'NN', ',', 'VBG', 'TO', 'PRP$', 'NNS', '.']",16
part-of-speech_tagging,3,106,"For each of the embeddings , we fine - tune initial embeddings , modifying them during gradient updates of the neural network model by back - propagating gradients .","['For', 'each', 'of', 'the', 'embeddings', ',', 'we', 'fine', '-', 'tune', 'initial', 'embeddings', ',', 'modifying', 'them', 'during', 'gradient', 'updates', 'of', 'the', 'neural', 'network', 'model', 'by', 'back', '-', 'propagating', 'gradients', '.']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'IN', 'DT', 'NNS', ',', 'PRP', 'VBP', ':', 'NN', 'JJ', 'NNS', ',', 'VBG', 'PRP', 'IN', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'JJ', ':', 'NN', 'NNS', '.']",29
part-of-speech_tagging,3,109,"To mitigate overfitting , we apply the dropout method ( Srivastava et al. , 2014 ) to regularize our model .","['To', 'mitigate', 'overfitting', ',', 'we', 'apply', 'the', 'dropout', 'method', '(', 'Srivastava', 'et', 'al.', ',', '2014', ')', 'to', 'regularize', 'our', 'model', '.']","['B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['TO', 'VB', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', '(', 'NNP', 'RB', 'RB', ',', 'CD', ')', 'TO', 'VB', 'PRP$', 'NN', '.']",21
part-of-speech_tagging,3,110,"As shown in and 3 , we apply dropout on character embeddings before inputting to CNN , and on both the input and output vectors of BLSTM .","['As', 'shown', 'in', 'and', '3', ',', 'we', 'apply', 'dropout', 'on', 'character', 'embeddings', 'before', 'inputting', 'to', 'CNN', ',', 'and', 'on', 'both', 'the', 'input', 'and', 'output', 'vectors', 'of', 'BLSTM', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'VBN', 'IN', 'CC', 'CD', ',', 'PRP', 'VBP', 'RB', 'IN', 'NN', 'NNS', 'IN', 'VBG', 'TO', 'NNP', ',', 'CC', 'IN', 'DT', 'DT', 'NN', 'CC', 'NN', 'NNS', 'IN', 'NNP', '.']",28
part-of-speech_tagging,3,111,We fix dropout rate at 0.5 for all dropout layers through all the experiments .,"['We', 'fix', 'dropout', 'rate', 'at', '0.5', 'for', 'all', 'dropout', 'layers', 'through', 'all', 'the', 'experiments', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', 'NNS', 'IN', 'PDT', 'DT', 'NNS', '.']",15
part-of-speech_tagging,3,118,"We compare the performance with three baseline systems - BRNN , the bi-direction RNN ; BLSTM , the bidirection LSTM , and BLSTM - CNNs , the combination of BLSTM with CNN to model characterlevel information .","['We', 'compare', 'the', 'performance', 'with', 'three', 'baseline', 'systems', '-', 'BRNN', ',', 'the', 'bi-direction', 'RNN', ';', 'BLSTM', ',', 'the', 'bidirection', 'LSTM', ',', 'and', 'BLSTM', '-', 'CNNs', ',', 'the', 'combination', 'of', 'BLSTM', 'with', 'CNN', 'to', 'model', 'characterlevel', 'information', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'CD', 'NN', 'NNS', ':', 'NNP', ',', 'DT', 'NN', 'NNP', ':', 'NNP', ',', 'DT', 'NN', 'NNP', ',', 'CC', 'NNP', ':', 'NN', ',', 'DT', 'NN', 'IN', 'NNP', 'IN', 'NNP', 'TO', 'VB', 'JJ', 'NN', '.']",37
part-of-speech_tagging,3,123,"Finally , by adding CRF layer for joint decoding we achieve significant improvements over BLSTM - CNN models for both POS tagging and NER on all metrics .","['Finally', ',', 'by', 'adding', 'CRF', 'layer', 'for', 'joint', 'decoding', 'we', 'achieve', 'significant', 'improvements', 'over', 'BLSTM', '-', 'CNN', 'models', 'for', 'both', 'POS', 'tagging', 'and', 'NER', 'on', 'all', 'metrics', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['RB', ',', 'IN', 'VBG', 'NNP', 'NN', 'IN', 'JJ', 'NN', 'PRP', 'VBP', 'JJ', 'NNS', 'IN', 'NNP', ':', 'NNP', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'CC', 'NNP', 'IN', 'DT', 'NNS', '.']",28
part-of-speech_tagging,3,127,"Comparing with traditional statistical models , our system achieves state - of - the - art accuracy , obtaining 0.05 % improvement over the previously best reported results by .","['Comparing', 'with', 'traditional', 'statistical', 'models', ',', 'our', 'system', 'achieves', 'state', '-', 'of', '-', 'the', '-', 'art', 'accuracy', ',', 'obtaining', '0.05', '%', 'improvement', 'over', 'the', 'previously', 'best', 'reported', 'results', 'by', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['VBG', 'IN', 'JJ', 'JJ', 'NNS', ',', 'PRP$', 'NN', 'VBZ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', ',', 'VBG', 'CD', 'NN', 'NN', 'IN', 'DT', 'RB', 'JJS', 'VBN', 'NNS', 'IN', '.']",30
part-of-speech_tagging,3,133,"Similar to the observations of POS tagging , our model achieves significant improvements over Senna and the other three neural models , namely the LSTM - CRF proposed by , LSTM - CNNs pro- :","['Similar', 'to', 'the', 'observations', 'of', 'POS', 'tagging', ',', 'our', 'model', 'achieves', 'significant', 'improvements', 'over', 'Senna', 'and', 'the', 'other', 'three', 'neural', 'models', ',', 'namely', 'the', 'LSTM', '-', 'CRF', 'proposed', 'by', ',', 'LSTM', '-', 'CNNs', 'pro-', ':']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'TO', 'DT', 'NNS', 'IN', 'NNP', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'JJ', 'NNS', 'IN', 'NNP', 'CC', 'DT', 'JJ', 'CD', 'JJ', 'NNS', ',', 'RB', 'DT', 'NNP', ':', 'NN', 'VBN', 'IN', ',', 'NNP', ':', 'JJ', 'NN', ':']",35
part-of-speech_tagging,0,2,Robust Multilingual Part - of - Speech Tagging via Adversarial Training,"['Robust', 'Multilingual', 'Part', '-', 'of', '-', 'Speech', 'Tagging', 'via', 'Adversarial', 'Training']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', ':', 'IN', ':', 'NN', 'VBG', 'IN', 'NNP', 'NNP']",11
part-of-speech_tagging,0,6,"In this paper , we propose and analyze a neural POS tagging model that exploits AT .","['In', 'this', 'paper', ',', 'we', 'propose', 'and', 'analyze', 'a', 'neural', 'POS', 'tagging', 'model', 'that', 'exploits', 'AT', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'CC', 'VBP', 'DT', 'JJ', 'NNP', 'VBG', 'NN', 'IN', 'VBZ', 'NNP', '.']",17
part-of-speech_tagging,0,32,"In this paper , spotlighting a well - studied core problem of NLP , we propose and carefully analyze a neural part - of - speech ( POS ) tagging model that exploits adversarial training .","['In', 'this', 'paper', ',', 'spotlighting', 'a', 'well', '-', 'studied', 'core', 'problem', 'of', 'NLP', ',', 'we', 'propose', 'and', 'carefully', 'analyze', 'a', 'neural', 'part', '-', 'of', '-', 'speech', '(', 'POS', ')', 'tagging', 'model', 'that', 'exploits', 'adversarial', 'training', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'VBG', 'DT', 'NN', ':', 'VBN', 'NN', 'NN', 'IN', 'NNP', ',', 'PRP', 'VBP', 'CC', 'RB', 'VB', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'NN', '(', 'NNP', ')', 'VBG', 'NN', 'IN', 'VBZ', 'JJ', 'NN', '.']",36
part-of-speech_tagging,0,33,"With a BiLSTM - CRF model as our baseline POS tagger , we apply adversarial training by considering perturbations to input word / character embeddings .","['With', 'a', 'BiLSTM', '-', 'CRF', 'model', 'as', 'our', 'baseline', 'POS', 'tagger', ',', 'we', 'apply', 'adversarial', 'training', 'by', 'considering', 'perturbations', 'to', 'input', 'word', '/', 'character', 'embeddings', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNP', ':', 'NNP', 'NN', 'IN', 'PRP$', 'NN', 'NNP', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'IN', 'VBG', 'NNS', 'TO', 'VB', 'NN', 'NNP', 'NN', 'NNS', '.']",26
part-of-speech_tagging,0,132,"We train the model parameters and word / character embeddings by the mini-batch stochastic gradient descent ( SGD ) with batch size 10 , momentum 0.9 , initial learning rate 0.01 and decay rate 0.05 .","['We', 'train', 'the', 'model', 'parameters', 'and', 'word', '/', 'character', 'embeddings', 'by', 'the', 'mini-batch', 'stochastic', 'gradient', 'descent', '(', 'SGD', ')', 'with', 'batch', 'size', '10', ',', 'momentum', '0.9', ',', 'initial', 'learning', 'rate', '0.01', 'and', 'decay', 'rate', '0.05', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NNS', 'CC', 'NN', 'NNP', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'IN', 'NN', 'NN', 'CD', ',', 'NN', 'CD', ',', 'JJ', 'VBG', 'NN', 'CD', 'CC', 'VB', 'NN', 'CD', '.']",36
part-of-speech_tagging,0,133,We also use a gradient clipping of 5.0 .,"['We', 'also', 'use', 'a', 'gradient', 'clipping', 'of', '5.0', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",9
part-of-speech_tagging,0,134,The models are trained with early stopping ) based on the development performance .,"['The', 'models', 'are', 'trained', 'with', 'early', 'stopping', ')', 'based', 'on', 'the', 'development', 'performance', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'JJ', 'VBG', ')', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",14
part-of-speech_tagging,0,139,PTB - WSJ dataset .,"['PTB', '-', 'WSJ', 'dataset', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'NN', '.']",5
part-of-speech_tagging,0,141,"As expected , our baseline ( BiLSTM - CRF ) model ( accuracy 97.54 % ) performs on par with other state - of - the - art systems .","['As', 'expected', ',', 'our', 'baseline', '(', 'BiLSTM', '-', 'CRF', ')', 'model', '(', 'accuracy', '97.54', '%', ')', 'performs', 'on', 'par', 'with', 'other', 'state', '-', 'of', '-', 'the', '-', 'art', 'systems', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', ',', 'PRP$', 'NN', '(', 'NNP', ':', 'NN', ')', 'NN', '(', 'JJ', 'CD', 'NN', ')', 'VBZ', 'IN', 'NN', 'IN', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', '.']",30
part-of-speech_tagging,0,142,"Built upon this baseline , our adversarial training ( AT ) model reaches accuracy 97.58 % thanks to its regularization power , outperforming recent POS taggers except .","['Built', 'upon', 'this', 'baseline', ',', 'our', 'adversarial', 'training', '(', 'AT', ')', 'model', 'reaches', 'accuracy', '97.58', '%', 'thanks', 'to', 'its', 'regularization', 'power', ',', 'outperforming', 'recent', 'POS', 'taggers', 'except', '.']","['B-p', 'I-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O']","['VBN', 'IN', 'DT', 'NN', ',', 'PRP$', 'JJ', 'NN', '(', 'NNP', ')', 'NN', 'VBZ', 'RB', 'CD', 'NN', 'NNS', 'TO', 'PRP$', 'NN', 'NN', ',', 'VBG', 'JJ', 'NNP', 'NNS', 'IN', '.']",28
paraphrase_generation,1,2,A Deep Generative Framework for Paraphrase Generation,"['A', 'Deep', 'Generative', 'Framework', 'for', 'Paraphrase', 'Generation']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['DT', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",7
paraphrase_generation,1,5,"In this paper , we address the problem of generating paraphrases automatically .","['In', 'this', 'paper', ',', 'we', 'address', 'the', 'problem', 'of', 'generating', 'paraphrases', 'automatically', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'NNS', 'RB', '.']",13
paraphrase_generation,1,25,"In this paper , we present a deep generative framework for automatically generating paraphrases , given a sentence .","['In', 'this', 'paper', ',', 'we', 'present', 'a', 'deep', 'generative', 'framework', 'for', 'automatically', 'generating', 'paraphrases', ',', 'given', 'a', 'sentence', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'RB', 'VBG', 'NNS', ',', 'VBN', 'DT', 'NN', '.']",19
paraphrase_generation,1,26,"Our framework combines the power of sequenceto - sequence models , specifically the long short - term memory ( LSTM ) , and deep generative models , specifically the variational autoencoder ( VAE ) , to develop a novel , end - to - end deep learning architecture for the task of paraphrase generation .","['Our', 'framework', 'combines', 'the', 'power', 'of', 'sequenceto', '-', 'sequence', 'models', ',', 'specifically', 'the', 'long', 'short', '-', 'term', 'memory', '(', 'LSTM', ')', ',', 'and', 'deep', 'generative', 'models', ',', 'specifically', 'the', 'variational', 'autoencoder', '(', 'VAE', ')', ',', 'to', 'develop', 'a', 'novel', ',', 'end', '-', 'to', '-', 'end', 'deep', 'learning', 'architecture', 'for', 'the', 'task', 'of', 'paraphrase', 'generation', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'NN', ':', 'NN', 'NNS', ',', 'RB', 'DT', 'JJ', 'JJ', ':', 'NN', 'NN', '(', 'NNP', ')', ',', 'CC', 'JJ', 'NN', 'NNS', ',', 'RB', 'DT', 'JJ', 'NN', '(', 'NNP', ')', ',', 'TO', 'VB', 'DT', 'NN', ',', 'VB', ':', 'TO', ':', 'VB', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', '.']",55
paraphrase_generation,1,30,"To address this limitation , we present a mechanism to condition our VAE model on the original sentence for which we wish to generate the paraphrases .","['To', 'address', 'this', 'limitation', ',', 'we', 'present', 'a', 'mechanism', 'to', 'condition', 'our', 'VAE', 'model', 'on', 'the', 'original', 'sentence', 'for', 'which', 'we', 'wish', 'to', 'generate', 'the', 'paraphrases', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'TO', 'VB', 'PRP$', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'WDT', 'PRP', 'VBP', 'TO', 'VB', 'DT', 'NNS', '.']",27
paraphrase_generation,1,32,"Unlike these methods where number of classes are finite , and do not require any intermediate representation , our method conditions both the sides ( i.e. encoder and decoder ) of VAE on the intermediate representation of the input question obtained through LSTM .","['Unlike', 'these', 'methods', 'where', 'number', 'of', 'classes', 'are', 'finite', ',', 'and', 'do', 'not', 'require', 'any', 'intermediate', 'representation', ',', 'our', 'method', 'conditions', 'both', 'the', 'sides', '(', 'i.e.', 'encoder', 'and', 'decoder', ')', 'of', 'VAE', 'on', 'the', 'intermediate', 'representation', 'of', 'the', 'input', 'question', 'obtained', 'through', 'LSTM', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'DT', 'NNS', 'WRB', 'NN', 'IN', 'NNS', 'VBP', 'JJ', ',', 'CC', 'VBP', 'RB', 'VB', 'DT', 'JJ', 'NN', ',', 'PRP$', 'NN', 'NNS', 'VBP', 'DT', 'NNS', '(', 'JJ', 'NN', 'CC', 'NN', ')', 'IN', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBN', 'IN', 'NNP', '.']",44
paraphrase_generation,1,35,"In contrast , our deep generative model enjoys a simple , modular architecture , and can generate not just a single but multiple , semantically sensible , paraphrases for any given sentence .","['In', 'contrast', ',', 'our', 'deep', 'generative', 'model', 'enjoys', 'a', 'simple', ',', 'modular', 'architecture', ',', 'and', 'can', 'generate', 'not', 'just', 'a', 'single', 'but', 'multiple', ',', 'semantically', 'sensible', ',', 'paraphrases', 'for', 'any', 'given', 'sentence', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ',', 'PRP$', 'JJ', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', ',', 'JJ', 'NN', ',', 'CC', 'MD', 'VB', 'RB', 'RB', 'DT', 'JJ', 'CC', 'JJ', ',', 'RB', 'JJ', ',', 'NNS', 'IN', 'DT', 'VBN', 'NN', '.']",33
paraphrase_generation,1,38,"This is in contrast to the proposed method where all variations will be of relatively better quality since they are the top beam - search result , generated based on different z sampled from a latent space .","['This', 'is', 'in', 'contrast', 'to', 'the', 'proposed', 'method', 'where', 'all', 'variations', 'will', 'be', 'of', 'relatively', 'better', 'quality', 'since', 'they', 'are', 'the', 'top', 'beam', '-', 'search', 'result', ',', 'generated', 'based', 'on', 'different', 'z', 'sampled', 'from', 'a', 'latent', 'space', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'VBZ', 'IN', 'NN', 'TO', 'DT', 'VBN', 'NN', 'WRB', 'DT', 'NNS', 'MD', 'VB', 'IN', 'RB', 'JJ', 'NN', 'IN', 'PRP', 'VBP', 'DT', 'JJ', 'NN', ':', 'NN', 'NN', ',', 'VBD', 'VBN', 'IN', 'JJ', 'NN', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",38
paraphrase_generation,1,122,Residual LSTM is also the current state - of - the - art on the MSCOCO dataset .,"['Residual', 'LSTM', 'is', 'also', 'the', 'current', 'state', '-', 'of', '-', 'the', '-', 'art', 'on', 'the', 'MSCOCO', 'dataset', '.']","['B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['JJ', 'NNP', 'VBZ', 'RB', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",18
paraphrase_generation,1,123,"For the Quora dataset , there were no known baseline results , so we compare our model with ( 1 ) standard VAE model i.e. , the unsupervised version , and ( 2 ) a "" supervised "" variant VAE - S of the unsupervised model .","['For', 'the', 'Quora', 'dataset', ',', 'there', 'were', 'no', 'known', 'baseline', 'results', ',', 'so', 'we', 'compare', 'our', 'model', 'with', '(', '1', ')', 'standard', 'VAE', 'model', 'i.e.', ',', 'the', 'unsupervised', 'version', ',', 'and', '(', '2', ')', 'a', '""', 'supervised', '""', 'variant', 'VAE', '-', 'S', 'of', 'the', 'unsupervised', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNP', 'NN', ',', 'EX', 'VBD', 'DT', 'VBN', 'NN', 'NNS', ',', 'IN', 'PRP', 'VBP', 'PRP$', 'NN', 'IN', '(', 'CD', ')', 'NN', 'NNP', 'NN', 'NN', ',', 'DT', 'JJ', 'NN', ',', 'CC', '(', 'CD', ')', 'DT', 'NN', 'VBN', 'NNP', 'JJ', 'NNP', ':', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",47
paraphrase_generation,1,131,"The dimension of the embedding vector is set to 300 , the dimension of both encoder and decoder is 600 , and the latent space dimension is 1100 .","['The', 'dimension', 'of', 'the', 'embedding', 'vector', 'is', 'set', 'to', '300', ',', 'the', 'dimension', 'of', 'both', 'encoder', 'and', 'decoder', 'is', '600', ',', 'and', 'the', 'latent', 'space', 'dimension', 'is', '1100', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O']","['DT', 'NN', 'IN', 'DT', 'VBG', 'NN', 'VBZ', 'VBN', 'TO', 'CD', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'CC', 'NN', 'VBZ', 'CD', ',', 'CC', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'CD', '.']",29
paraphrase_generation,1,132,The number of layers in the encoder is 1 and in decoder,"['The', 'number', 'of', 'layers', 'in', 'the', 'encoder', 'is', '1', 'and', 'in', 'decoder']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n']","['DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'VBZ', 'CD', 'CC', 'IN', 'NN']",12
paraphrase_generation,1,133,2 . Models are trained with stochastic gradient descent with learning rate fixed at a value of 5 10 ? 5 with dropout rate of 30 % .,"['2', '.', 'Models', 'are', 'trained', 'with', 'stochastic', 'gradient', 'descent', 'with', 'learning', 'rate', 'fixed', 'at', 'a', 'value', 'of', '5', '10', '?', '5', 'with', 'dropout', 'rate', 'of', '30', '%', '.']","['B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['CD', '.', 'NNS', 'VBP', 'VBN', 'IN', 'JJ', 'NN', 'NN', 'IN', 'VBG', 'NN', 'VBN', 'IN', 'DT', 'NN', 'IN', 'CD', 'CD', '.', 'CD', 'IN', 'NN', 'NN', 'IN', 'CD', 'NN', '.']",28
paraphrase_generation,1,134,Batch size is kept at 32 .,"['Batch', 'size', 'is', 'kept', 'at', '32', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', '.']",7
paraphrase_generation,1,135,"Models are trained for a predefined number of iterations , rather than a fixed number of epochs .","['Models', 'are', 'trained', 'for', 'a', 'predefined', 'number', 'of', 'iterations', ',', 'rather', 'than', 'a', 'fixed', 'number', 'of', 'epochs', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', ',', 'RB', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', '.']",18
paraphrase_generation,1,139,Number of units in LSTM are set to be the maximum length of the sequence in the training data .,"['Number', 'of', 'units', 'in', 'LSTM', 'are', 'set', 'to', 'be', 'the', 'maximum', 'length', 'of', 'the', 'sequence', 'in', 'the', 'training', 'data', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', 'IN', 'NNS', 'IN', 'NNP', 'VBP', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NNS', '.']",20
paraphrase_generation,1,172,"Furthermore , the paraphrases generated by our system are well - formed , semantically sensible , and grammatically correct for the most part .","['Furthermore', ',', 'the', 'paraphrases', 'generated', 'by', 'our', 'system', 'are', 'well', '-', 'formed', ',', 'semantically', 'sensible', ',', 'and', 'grammatically', 'correct', 'for', 'the', 'most', 'part', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'DT', 'NNS', 'VBN', 'IN', 'PRP$', 'NN', 'VBP', 'RB', ':', 'VBN', ',', 'RB', 'JJ', ',', 'CC', 'RB', 'VB', 'IN', 'DT', 'JJS', 'NN', '.']",24
paraphrase_generation,1,187,"Those numbers are reported in the Measure column with row best - BLEU / best - METEOR . , we report the results for MSCOCO dataset .","['Those', 'numbers', 'are', 'reported', 'in', 'the', 'Measure', 'column', 'with', 'row', 'best', '-', 'BLEU', '/', 'best', '-', 'METEOR', '.', ',', 'we', 'report', 'the', 'results', 'for', 'MSCOCO', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'NN', 'NN', 'IN', 'NN', 'JJS', ':', 'NNP', 'NNP', 'JJS', ':', 'NN', '.', ',', 'PRP', 'VBP', 'DT', 'NNS', 'IN', 'NNP', 'NN', '.']",27
paraphrase_generation,1,189,"As we can see , we have a significant improvement w.r.t. the baselines .","['As', 'we', 'can', 'see', ',', 'we', 'have', 'a', 'significant', 'improvement', 'w.r.t.', 'the', 'baselines', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['IN', 'PRP', 'MD', 'VB', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBD', 'DT', 'NNS', '.']",14
paraphrase_generation,1,190,"Both variations of our supervised model i.e. , VAE - SVG and VAE - SVG - eq perform better than the state - of - the - art with VAE - SVG performing slightly better than VAE - SVG - eq .","['Both', 'variations', 'of', 'our', 'supervised', 'model', 'i.e.', ',', 'VAE', '-', 'SVG', 'and', 'VAE', '-', 'SVG', '-', 'eq', 'perform', 'better', 'than', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'with', 'VAE', '-', 'SVG', 'performing', 'slightly', 'better', 'than', 'VAE', '-', 'SVG', '-', 'eq', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'PRP$', 'JJ', 'NN', 'NN', ',', 'NNP', ':', 'NN', 'CC', 'NNP', ':', 'NNP', ':', 'NN', 'NN', 'RBR', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'NNP', ':', 'NNP', 'VBG', 'RB', 'JJR', 'IN', 'NNP', ':', 'NNP', ':', 'NN', '.']",42
paraphrase_generation,1,196,"When comparing our results with the state - of - the - art baseline , the average metric of the VAE - SVG model is able to give a 10 % absolute point performance improvement for the TER metric , a significant number with respect to the difference between the best and second best baseline which only stands at 2 % absolute point .","['When', 'comparing', 'our', 'results', 'with', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'baseline', ',', 'the', 'average', 'metric', 'of', 'the', 'VAE', '-', 'SVG', 'model', 'is', 'able', 'to', 'give', 'a', '10', '%', 'absolute', 'point', 'performance', 'improvement', 'for', 'the', 'TER', 'metric', ',', 'a', 'significant', 'number', 'with', 'respect', 'to', 'the', 'difference', 'between', 'the', 'best', 'and', 'second', 'best', 'baseline', 'which', 'only', 'stands', 'at', '2', '%', 'absolute', 'point', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['WRB', 'VBG', 'PRP$', 'NNS', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', ',', 'DT', 'JJ', 'JJ', 'IN', 'DT', 'NNP', ':', 'NNP', 'NN', 'VBZ', 'JJ', 'TO', 'VB', 'DT', 'CD', 'NN', 'JJ', 'NN', 'NN', 'NN', 'IN', 'DT', 'NNP', 'JJ', ',', 'DT', 'JJ', 'NN', 'IN', 'NN', 'TO', 'DT', 'NN', 'IN', 'DT', 'JJS', 'CC', 'JJ', 'RBS', 'NN', 'WDT', 'RB', 'VBZ', 'IN', 'CD', 'NN', 'JJ', 'NN', '.']",64
paraphrase_generation,1,197,"For the BLEU and METEOR , our best results are 4.7 % and 4 % absolute point improvement over the state - of - the - art .","['For', 'the', 'BLEU', 'and', 'METEOR', ',', 'our', 'best', 'results', 'are', '4.7', '%', 'and', '4', '%', 'absolute', 'point', 'improvement', 'over', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNP', 'CC', 'NNP', ',', 'PRP$', 'JJS', 'NNS', 'VBP', 'CD', 'NN', 'CC', 'CD', 'NN', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', '.']",28
paraphrase_generation,1,198,"In , we report results for the Quora dataset .","['In', ',', 'we', 'report', 'results', 'for', 'the', 'Quora', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['IN', ',', 'PRP', 'VBP', 'NNS', 'IN', 'DT', 'NNP', 'NN', '.']",10
paraphrase_generation,1,199,"As we can see , both variations of our model perform significantly better than unsupervised VAE and VAE - S , which is not surprising .","['As', 'we', 'can', 'see', ',', 'both', 'variations', 'of', 'our', 'model', 'perform', 'significantly', 'better', 'than', 'unsupervised', 'VAE', 'and', 'VAE', '-', 'S', ',', 'which', 'is', 'not', 'surprising', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PRP', 'MD', 'VB', ',', 'DT', 'NNS', 'IN', 'PRP$', 'NN', 'NN', 'RB', 'RBR', 'IN', 'JJ', 'NNP', 'CC', 'NNP', ':', 'NNP', ',', 'WDT', 'VBZ', 'RB', 'JJ', '.']",26
paraphrase_generation,1,200,"We also report the results on different training sizes , and as expected , as we increase the training data size , results improve .","['We', 'also', 'report', 'the', 'results', 'on', 'different', 'training', 'sizes', ',', 'and', 'as', 'expected', ',', 'as', 'we', 'increase', 'the', 'training', 'data', 'size', ',', 'results', 'improve', '.']","['O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NNS', 'IN', 'JJ', 'NN', 'NNS', ',', 'CC', 'IN', 'VBN', ',', 'IN', 'PRP', 'VBP', 'DT', 'NN', 'NNS', 'NN', ',', 'NNS', 'VBP', '.']",25
paraphrase_generation,1,201,"Comparing the results across different variants of supervised model , VAE - SVG - eq performs the best .","['Comparing', 'the', 'results', 'across', 'different', 'variants', 'of', 'supervised', 'model', ',', 'VAE', '-', 'SVG', '-', 'eq', 'performs', 'the', 'best', '.']","['B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['VBG', 'DT', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NN', ',', 'NNP', ':', 'NNP', ':', 'NN', 'VBZ', 'DT', 'JJS', '.']",19
paraphrase_generation,1,203,"We also experimented with generating paraphrases through beam - search , and , unlike MSCOCO , it turns out that beam search improves the results significantly .","['We', 'also', 'experimented', 'with', 'generating', 'paraphrases', 'through', 'beam', '-', 'search', ',', 'and', ',', 'unlike', 'MSCOCO', ',', 'it', 'turns', 'out', 'that', 'beam', 'search', 'improves', 'the', 'results', 'significantly', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-n', 'O']","['PRP', 'RB', 'VBD', 'IN', 'NN', 'NNS', 'IN', 'NN', ':', 'NN', ',', 'CC', ',', 'IN', 'NNP', ',', 'PRP', 'VBZ', 'RP', 'IN', 'NN', 'NN', 'VBZ', 'DT', 'NNS', 'RB', '.']",27
paraphrase_generation,1,205,"When comparing the best variant of our model with unsupervised model ( VAE ) , we are able to get more than 27 % absolute point ( more than 3 times ) boost in BLEU score , and more than 19 % absolute point ( more than 2 times ) boost in METEOR ; and when comparing with VAE - S , we are able to get a boost of almost 19 % absolute points in BLEU ( 2 times ) and more than 10 % absolute points in METEOR ( 1.5 times ) .","['When', 'comparing', 'the', 'best', 'variant', 'of', 'our', 'model', 'with', 'unsupervised', 'model', '(', 'VAE', ')', ',', 'we', 'are', 'able', 'to', 'get', 'more', 'than', '27', '%', 'absolute', 'point', '(', 'more', 'than', '3', 'times', ')', 'boost', 'in', 'BLEU', 'score', ',', 'and', 'more', 'than', '19', '%', 'absolute', 'point', '(', 'more', 'than', '2', 'times', ')', 'boost', 'in', 'METEOR', ';', 'and', 'when', 'comparing', 'with', 'VAE', '-', 'S', ',', 'we', 'are', 'able', 'to', 'get', 'a', 'boost', 'of', 'almost', '19', '%', 'absolute', 'points', 'in', 'BLEU', '(', '2', 'times', ')', 'and', 'more', 'than', '10', '%', 'absolute', 'points', 'in', 'METEOR', '(', '1.5', 'times', ')', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['WRB', 'VBG', 'DT', 'JJS', 'NN', 'IN', 'PRP$', 'NN', 'IN', 'JJ', 'NN', '(', 'NNP', ')', ',', 'PRP', 'VBP', 'JJ', 'TO', 'VB', 'JJR', 'IN', 'CD', 'NN', 'JJ', 'NN', '(', 'JJR', 'IN', 'CD', 'NNS', ')', 'NN', 'IN', 'NNP', 'NN', ',', 'CC', 'JJR', 'IN', 'CD', 'NN', 'JJ', 'NN', '(', 'JJR', 'IN', 'CD', 'NNS', ')', 'NN', 'IN', 'NNP', ':', 'CC', 'WRB', 'VBG', 'IN', 'NNP', ':', 'NN', ',', 'PRP', 'VBP', 'JJ', 'TO', 'VB', 'DT', 'NN', 'IN', 'RB', 'CD', 'NN', 'JJ', 'NNS', 'IN', 'NNP', '(', 'CD', 'NNS', ')', 'CC', 'JJR', 'IN', 'CD', 'NN', 'JJ', 'NNS', 'IN', 'NNP', '(', 'CD', 'NNS', ')', '.']",95
paraphrase_generation,0,2,Learning Semantic Sentence Embeddings using Pair- wise Discriminator,"['Learning', 'Semantic', 'Sentence', 'Embeddings', 'using', 'Pair-', 'wise', 'Discriminator']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['VBG', 'JJ', 'NNP', 'NNP', 'VBG', 'NNP', 'NN', 'NN']",8
paraphrase_generation,0,4,"In this paper , we propose a method for obtaining sentence - level embeddings .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'method', 'for', 'obtaining', 'sentence', '-', 'level', 'embeddings', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'NN', ':', 'NN', 'NNS', '.']",15
paraphrase_generation,0,22,Our model consists of a sequential encoder - decoder that is further trained using a pairwise discriminator .,"['Our', 'model', 'consists', 'of', 'a', 'sequential', 'encoder', '-', 'decoder', 'that', 'is', 'further', 'trained', 'using', 'a', 'pairwise', 'discriminator', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'IN', 'DT', 'JJ', 'NN', ':', 'NN', 'WDT', 'VBZ', 'RB', 'VBN', 'VBG', 'DT', 'NN', 'NN', '.']",18
paraphrase_generation,0,23,The encoder - decoder architecture has been widely used for machine translation and machine comprehension tasks .,"['The', 'encoder', '-', 'decoder', 'architecture', 'has', 'been', 'widely', 'used', 'for', 'machine', 'translation', 'and', 'machine', 'comprehension', 'tasks', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O']","['DT', 'NN', ':', 'NN', 'NN', 'VBZ', 'VBN', 'RB', 'VBN', 'IN', 'NN', 'NN', 'CC', 'NN', 'NN', 'NNS', '.']",17
paraphrase_generation,0,24,"In general , the model ensures a ' local ' loss that is incurred for each recurrent unit cell .","['In', 'general', ',', 'the', 'model', 'ensures', 'a', ""'"", 'local', ""'"", 'loss', 'that', 'is', 'incurred', 'for', 'each', 'recurrent', 'unit', 'cell', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', ',', 'DT', 'NN', 'VBZ', 'DT', ""''"", 'JJ', 'POS', 'NN', 'WDT', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'NN', 'NN', '.']",20
paraphrase_generation,0,27,"To ensure that the whole sentence is correctly encoded , we make further use of a pair - wise discriminator that encodes the whole sentence and obtains an embedding for it .","['To', 'ensure', 'that', 'the', 'whole', 'sentence', 'is', 'correctly', 'encoded', ',', 'we', 'make', 'further', 'use', 'of', 'a', 'pair', '-', 'wise', 'discriminator', 'that', 'encodes', 'the', 'whole', 'sentence', 'and', 'obtains', 'an', 'embedding', 'for', 'it', '.']","['B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O']","['TO', 'VB', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'RB', 'VBN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'IN', 'DT', 'NN', ':', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'CC', 'VBZ', 'DT', 'NN', 'IN', 'PRP', '.']",32
paraphrase_generation,0,28,We further ensure that this is close to the desired ground - truth embeddings while being far from other ( sentences in the corpus ) embeddings .,"['We', 'further', 'ensure', 'that', 'this', 'is', 'close', 'to', 'the', 'desired', 'ground', '-', 'truth', 'embeddings', 'while', 'being', 'far', 'from', 'other', '(', 'sentences', 'in', 'the', 'corpus', ')', 'embeddings', '.']","['O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'VB', 'IN', 'DT', 'VBZ', 'RB', 'TO', 'DT', 'VBN', 'NN', ':', 'NN', 'NNS', 'IN', 'VBG', 'RB', 'IN', 'JJ', '(', 'NNS', 'IN', 'DT', 'NN', ')', 'NNS', '.']",27
paraphrase_generation,0,29,This model thus provides a ' global ' loss that ensures the sentence embedding as a whole is close to other semantically related sentence embeddings .,"['This', 'model', 'thus', 'provides', 'a', ""'"", 'global', ""'"", 'loss', 'that', 'ensures', 'the', 'sentence', 'embedding', 'as', 'a', 'whole', 'is', 'close', 'to', 'other', 'semantically', 'related', 'sentence', 'embeddings', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'RB', 'VBZ', 'DT', ""''"", 'JJ', 'POS', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'TO', 'JJ', 'RB', 'JJ', 'NN', 'NNS', '.']",26
paraphrase_generation,0,148,We start with baseline model which we take as a simple encoder and decoder network with only the local loss ( ED - Local ) .,"['We', 'start', 'with', 'baseline', 'model', 'which', 'we', 'take', 'as', 'a', 'simple', 'encoder', 'and', 'decoder', 'network', 'with', 'only', 'the', 'local', 'loss', '(', 'ED', '-', 'Local', ')', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'NN', 'NN', 'WDT', 'PRP', 'VBP', 'IN', 'DT', 'JJ', 'NN', 'CC', 'NN', 'NN', 'IN', 'RB', 'DT', 'JJ', 'NN', '(', 'NNP', ':', 'JJ', ')', '.']",26
paraphrase_generation,0,149,Further we have experimented with encoder - decoder and a discriminator network with only global loss ( EDD - Global ) to distinguish the ground truth paraphrase with the predicted one .,"['Further', 'we', 'have', 'experimented', 'with', 'encoder', '-', 'decoder', 'and', 'a', 'discriminator', 'network', 'with', 'only', 'global', 'loss', '(', 'EDD', '-', 'Global', ')', 'to', 'distinguish', 'the', 'ground', 'truth', 'paraphrase', 'with', 'the', 'predicted', 'one', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', 'PRP', 'VBP', 'VBN', 'IN', 'NN', ':', 'NN', 'CC', 'DT', 'NN', 'NN', 'IN', 'JJ', 'JJ', 'NN', '(', 'NNP', ':', 'NNP', ')', 'TO', 'VB', 'DT', 'NN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'CD', '.']",32
paraphrase_generation,0,150,Another variation of our model is used both the global and local loss ( EDD - LG ) .,"['Another', 'variation', 'of', 'our', 'model', 'is', 'used', 'both', 'the', 'global', 'and', 'local', 'loss', '(', 'EDD', '-', 'LG', ')', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'PRP$', 'NN', 'VBZ', 'VBN', 'DT', 'DT', 'JJ', 'CC', 'JJ', 'NN', '(', 'NNP', ':', 'NN', ')', '.']",19
paraphrase_generation,0,152,"Finally , we make the discriminator share weights with the encoder and train this network with both the losses ( EDD - LG ( shared ) ) .","['Finally', ',', 'we', 'make', 'the', 'discriminator', 'share', 'weights', 'with', 'the', 'encoder', 'and', 'train', 'this', 'network', 'with', 'both', 'the', 'losses', '(', 'EDD', '-', 'LG', '(', 'shared', ')', ')', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NN', 'DT', 'NN', 'IN', 'DT', 'DT', 'NNS', '(', 'NNP', ':', 'NNP', '(', 'VBN', ')', ')', '.']",28
paraphrase_generation,0,155,"Among the ablations , the proposed EDD - LG ( shared ) method works way better than the other variants in terms of BLEU and METEOR metrics by achieving an improvement of 8 % and 6 % in the scores respectively over the baseline method for 50 K dataset and an improvement of 10 % and 7 % in the scores respectively for 100 K dataset .","['Among', 'the', 'ablations', ',', 'the', 'proposed', 'EDD', '-', 'LG', '(', 'shared', ')', 'method', 'works', 'way', 'better', 'than', 'the', 'other', 'variants', 'in', 'terms', 'of', 'BLEU', 'and', 'METEOR', 'metrics', 'by', 'achieving', 'an', 'improvement', 'of', '8', '%', 'and', '6', '%', 'in', 'the', 'scores', 'respectively', 'over', 'the', 'baseline', 'method', 'for', '50', 'K', 'dataset', 'and', 'an', 'improvement', 'of', '10', '%', 'and', '7', '%', 'in', 'the', 'scores', 'respectively', 'for', '100', 'K', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'DT', 'VBN', 'NNP', ':', 'NNP', '(', 'VBN', ')', 'NN', 'VBZ', 'NN', 'RBR', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NNS', 'IN', 'NNP', 'CC', 'NNP', 'NNS', 'IN', 'VBG', 'DT', 'NN', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'DT', 'NNS', 'RB', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', 'NNP', 'NN', 'CC', 'DT', 'NN', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'DT', 'NNS', 'RB', 'IN', 'CD', 'NNP', 'NN', '.']",67
relation_extraction,13,2,Matching the Blanks : Distributional Similarity for Relation Learning,"['Matching', 'the', 'Blanks', ':', 'Distributional', 'Similarity', 'for', 'Relation', 'Learning']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['VBG', 'DT', 'NNS', ':', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",9
relation_extraction,13,11,Reading text to identify and extract relations between entities has been along standing goal in natural language processing .,"['Reading', 'text', 'to', 'identify', 'and', 'extract', 'relations', 'between', 'entities', 'has', 'been', 'along', 'standing', 'goal', 'in', 'natural', 'language', 'processing', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NN', 'TO', 'VB', 'CC', 'VB', 'NNS', 'IN', 'NNS', 'VBZ', 'VBN', 'IN', 'VBG', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",19
relation_extraction,13,12,Typically efforts in relation extraction fall into one of three groups .,"['Typically', 'efforts', 'in', 'relation', 'extraction', 'fall', 'into', 'one', 'of', 'three', 'groups', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'NNS', 'IN', 'NN', 'NN', 'NN', 'IN', 'CD', 'IN', 'CD', 'NNS', '.']",12
relation_extraction,13,19,"First , we study the ability of the Transformer neural network architecture to encode relations between entity pairs , and we identify a method of representation that outperforms previous work in supervised relation extraction .","['First', ',', 'we', 'study', 'the', 'ability', 'of', 'the', 'Transformer', 'neural', 'network', 'architecture', 'to', 'encode', 'relations', 'between', 'entity', 'pairs', ',', 'and', 'we', 'identify', 'a', 'method', 'of', 'representation', 'that', 'outperforms', 'previous', 'work', 'in', 'supervised', 'relation', 'extraction', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NNP', 'JJ', 'NN', 'NN', 'TO', 'VB', 'NNS', 'IN', 'NN', 'NNS', ',', 'CC', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'IN', 'VBZ', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",35
relation_extraction,13,20,"Then , we present a method of training this relation representation without any supervision from a knowledge graph or human annotators by matching the blanks .","['Then', ',', 'we', 'present', 'a', 'method', 'of', 'training', 'this', 'relation', 'representation', 'without', 'any', 'supervision', 'from', 'a', 'knowledge', 'graph', 'or', 'human', 'annotators', 'by', 'matching', 'the', 'blanks', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'JJ', 'NNS', 'IN', 'VBG', 'DT', 'NNS', '.']",26
relation_extraction,13,187,shows that the task agnostic BERT EM and BERT EM + MTB models outperform the previous published state of the art on FewRel task even when they have not seen any FewRel training data .,"['shows', 'that', 'the', 'task', 'agnostic', 'BERT', 'EM', 'and', 'BERT', 'EM', '+', 'MTB', 'models', 'outperform', 'the', 'previous', 'published', 'state', 'of', 'the', 'art', 'on', 'FewRel', 'task', 'even', 'when', 'they', 'have', 'not', 'seen', 'any', 'FewRel', 'training', 'data', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'IN', 'DT', 'NN', 'JJ', 'NNP', 'NNP', 'CC', 'NNP', 'NNP', 'NNP', 'NNP', 'NNS', 'IN', 'DT', 'JJ', 'VBN', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NN', 'RB', 'WRB', 'PRP', 'VBP', 'RB', 'VBN', 'DT', 'NNP', 'NN', 'NNS', '.']",35
relation_extraction,13,188,"For BERT EM + MTB , the increase over 's supervised approach is very significant - 8.8 % on the 5 - way - 1 - shot task and 12.7 % on the 10 - way - 1 - shot task .","['For', 'BERT', 'EM', '+', 'MTB', ',', 'the', 'increase', 'over', ""'s"", 'supervised', 'approach', 'is', 'very', 'significant', '-', '8.8', '%', 'on', 'the', '5', '-', 'way', '-', '1', '-', 'shot', 'task', 'and', '12.7', '%', 'on', 'the', '10', '-', 'way', '-', '1', '-', 'shot', 'task', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', 'NNP', 'NNP', 'NNP', ',', 'DT', 'NN', 'IN', 'POS', 'JJ', 'NN', 'VBZ', 'RB', 'JJ', ':', 'CD', 'NN', 'IN', 'DT', 'CD', ':', 'NN', ':', 'CD', ':', 'NN', 'NN', 'CC', 'CD', 'NN', 'IN', 'DT', 'CD', ':', 'NN', ':', 'CD', ':', 'NN', 'NN', '.']",42
relation_extraction,13,189,"BERT EM + MTB also significantly outperforms BERT EM in this unsupervised setting , which is to be expected since there is no relation - specific loss during BERT EM 's training .","['BERT', 'EM', '+', 'MTB', 'also', 'significantly', 'outperforms', 'BERT', 'EM', 'in', 'this', 'unsupervised', 'setting', ',', 'which', 'is', 'to', 'be', 'expected', 'since', 'there', 'is', 'no', 'relation', '-', 'specific', 'loss', 'during', 'BERT', 'EM', ""'s"", 'training', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', 'NNP', 'RB', 'RB', 'VBZ', 'NNP', 'NNP', 'IN', 'DT', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'TO', 'VB', 'VBN', 'IN', 'EX', 'VBZ', 'DT', 'NN', ':', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'POS', 'NN', '.']",33
relation_extraction,13,192,"When given access to all of the training data , BERT EM approaches BERT EM + MTB 's performance .","['When', 'given', 'access', 'to', 'all', 'of', 'the', 'training', 'data', ',', 'BERT', 'EM', 'approaches', 'BERT', 'EM', '+', 'MTB', ""'s"", 'performance', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['WRB', 'VBN', 'NN', 'TO', 'DT', 'IN', 'DT', 'NN', 'NNS', ',', 'NNP', 'NNP', 'VBZ', 'NNP', 'NNP', 'NNP', 'NNP', 'POS', 'NN', '.']",20
relation_extraction,13,195,The results in show that MTB training could be used to significantly reduce effort in implementing an exemplar based relation extraction system .,"['The', 'results', 'in', 'show', 'that', 'MTB', 'training', 'could', 'be', 'used', 'to', 'significantly', 'reduce', 'effort', 'in', 'implementing', 'an', 'exemplar', 'based', 'relation', 'extraction', 'system', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'NN', 'IN', 'NNP', 'NN', 'MD', 'VB', 'VBN', 'TO', 'RB', 'VB', 'NN', 'IN', 'VBG', 'DT', 'NN', 'VBN', 'NN', 'NN', 'NN', '.']",23
relation_extraction,13,199,The additional MTB based training further increases F 1 scores for all tasks .,"['The', 'additional', 'MTB', 'based', 'training', 'further', 'increases', 'F', '1', 'scores', 'for', 'all', 'tasks', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NNP', 'VBN', 'VBG', 'JJ', 'NNS', 'NNP', 'CD', 'NNS', 'IN', 'DT', 'NNS', '.']",14
relation_extraction,13,202,"For all tasks , we see that MTB based training is even more effective for low - resource cases , where there is a larger gap in performance between our BERT EM and BERT EM + MTB based classifiers .","['For', 'all', 'tasks', ',', 'we', 'see', 'that', 'MTB', 'based', 'training', 'is', 'even', 'more', 'effective', 'for', 'low', '-', 'resource', 'cases', ',', 'where', 'there', 'is', 'a', 'larger', 'gap', 'in', 'performance', 'between', 'our', 'BERT', 'EM', 'and', 'BERT', 'EM', '+', 'MTB', 'based', 'classifiers', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNS', ',', 'PRP', 'VBP', 'IN', 'NNP', 'VBN', 'NN', 'VBZ', 'RB', 'RBR', 'JJ', 'IN', 'JJ', ':', 'NN', 'NNS', ',', 'WRB', 'EX', 'VBZ', 'DT', 'JJR', 'NN', 'IN', 'NN', 'IN', 'PRP$', 'NNP', 'NNP', 'CC', 'NNP', 'NNP', 'NNP', 'NNP', 'VBN', 'NNS', '.']",40
relation_extraction,13,203,"This further supports our argument that training by matching the blanks can significantly reduce the amount of human input required to create relation extractors , and populate a knowledge base .","['This', 'further', 'supports', 'our', 'argument', 'that', 'training', 'by', 'matching', 'the', 'blanks', 'can', 'significantly', 'reduce', 'the', 'amount', 'of', 'human', 'input', 'required', 'to', 'create', 'relation', 'extractors', ',', 'and', 'populate', 'a', 'knowledge', 'base', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'VBZ', 'PRP$', 'NN', 'IN', 'NN', 'IN', 'VBG', 'DT', 'NNS', 'MD', 'RB', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NN', 'VBN', 'TO', 'VB', 'NN', 'NNS', ',', 'CC', 'VB', 'DT', 'NN', 'NN', '.']",31
relation_extraction,8,2,Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks,"['Distant', 'Supervision', 'for', 'Relation', 'Extraction', 'via', 'Piecewise', 'Convolutional', 'Neural', 'Networks']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'NNP']",10
relation_extraction,8,15,"In relation extraction , one challenge that is faced when building a machine learning system is the generation of training examples .","['In', 'relation', 'extraction', ',', 'one', 'challenge', 'that', 'is', 'faced', 'when', 'building', 'a', 'machine', 'learning', 'system', 'is', 'the', 'generation', 'of', 'training', 'examples', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', 'NN', ',', 'CD', 'NN', 'WDT', 'VBZ', 'VBN', 'WRB', 'VBG', 'DT', 'NN', 'NN', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'NN', 'NNS', '.']",22
relation_extraction,8,36,"In this paper , we propose a novel model dubbed Piecewise Convolutional Neural Networks ( PC - NNs ) with multi-instance learning to address the two problems described above .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', 'model', 'dubbed', 'Piecewise', 'Convolutional', 'Neural', 'Networks', '(', 'PC', '-', 'NNs', ')', 'with', 'multi-instance', 'learning', 'to', 'address', 'the', 'two', 'problems', 'described', 'above', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBN', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ':', 'NN', ')', 'IN', 'NN', 'VBG', 'TO', 'VB', 'DT', 'CD', 'NNS', 'VBN', 'IN', '.']",30
relation_extraction,8,37,"To address the first problem , distant supervised relation extraction is treated as a multi-instance problem similar to previous studies .","['To', 'address', 'the', 'first', 'problem', ',', 'distant', 'supervised', 'relation', 'extraction', 'is', 'treated', 'as', 'a', 'multi-instance', 'problem', 'similar', 'to', 'previous', 'studies', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'JJ', 'NN', ',', 'NN', 'VBD', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'NN', 'JJ', 'TO', 'JJ', 'NNS', '.']",21
relation_extraction,8,40,We design an objective function at the bag level .,"['We', 'design', 'an', 'objective', 'function', 'at', 'the', 'bag', 'level', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",10
relation_extraction,8,41,"In the learning process , the uncertainty of instance labels can be taken into account ; this alleviates the wrong label problem .","['In', 'the', 'learning', 'process', ',', 'the', 'uncertainty', 'of', 'instance', 'labels', 'can', 'be', 'taken', 'into', 'account', ';', 'this', 'alleviates', 'the', 'wrong', 'label', 'problem', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'NN', ',', 'DT', 'NN', 'IN', 'NN', 'NNS', 'MD', 'VB', 'VBN', 'IN', 'NN', ':', 'DT', 'VBZ', 'DT', 'JJ', 'NN', 'NN', '.']",23
relation_extraction,8,42,"To address the second problem , we adopt convolutional architecture to automatically learn relevant features without complicated NLP preprocessing inspired by .","['To', 'address', 'the', 'second', 'problem', ',', 'we', 'adopt', 'convolutional', 'architecture', 'to', 'automatically', 'learn', 'relevant', 'features', 'without', 'complicated', 'NLP', 'preprocessing', 'inspired', 'by', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'TO', 'RB', 'VB', 'JJ', 'NNS', 'IN', 'VBN', 'NNP', 'VBG', 'VBN', 'IN', '.']",22
relation_extraction,8,51,"To capture structural and other latent information , we divide the convolution results into three segments based on the positions of the two given entities and devise a piecewise max pooling layer instead of the single max pooling layer .","['To', 'capture', 'structural', 'and', 'other', 'latent', 'information', ',', 'we', 'divide', 'the', 'convolution', 'results', 'into', 'three', 'segments', 'based', 'on', 'the', 'positions', 'of', 'the', 'two', 'given', 'entities', 'and', 'devise', 'a', 'piecewise', 'max', 'pooling', 'layer', 'instead', 'of', 'the', 'single', 'max', 'pooling', 'layer', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'JJ', 'CC', 'JJ', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NNS', 'IN', 'CD', 'NNS', 'VBN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'CD', 'VBN', 'NNS', 'CC', 'VB', 'DT', 'NN', 'NN', 'VBG', 'JJ', 'RB', 'IN', 'DT', 'JJ', 'NN', 'VBG', 'NN', '.']",40
relation_extraction,8,52,The piecewise max pooling procedure returns the maximum value in each segment instead of a single maximum value over the entire sentence .,"['The', 'piecewise', 'max', 'pooling', 'procedure', 'returns', 'the', 'maximum', 'value', 'in', 'each', 'segment', 'instead', 'of', 'a', 'single', 'maximum', 'value', 'over', 'the', 'entire', 'sentence', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBG', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'RB', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",23
relation_extraction,8,202,"In this paper , we use the Skip - gram model ( word2 vec ) 5 to train the word embeddings on the NYT corpus .","['In', 'this', 'paper', ',', 'we', 'use', 'the', 'Skip', '-', 'gram', 'model', '(', 'word2', 'vec', ')', '5', 'to', 'train', 'the', 'word', 'embeddings', 'on', 'the', 'NYT', 'corpus', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', ':', 'NN', 'NN', '(', 'JJ', 'NN', ')', 'CD', 'TO', 'VB', 'DT', 'NN', 'VBZ', 'IN', 'DT', 'NNP', 'NN', '.']",26
relation_extraction,8,213,"Following , we tune all of the models using three - fold validation on the training set .","['Following', ',', 'we', 'tune', 'all', 'of', 'the', 'models', 'using', 'three', '-', 'fold', 'validation', 'on', 'the', 'training', 'set', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['VBG', ',', 'PRP', 'VBP', 'DT', 'IN', 'DT', 'NNS', 'VBG', 'CD', ':', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",18
relation_extraction,8,214,"We use a grid search to determine the optimal parameters and manually specify subsets of the parameter spaces : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .","['We', 'use', 'a', 'grid', 'search', 'to', 'determine', 'the', 'optimal', 'parameters', 'and', 'manually', 'specify', 'subsets', 'of', 'the', 'parameter', 'spaces', ':', 'w', '?', '{', '1', ',', '2', ',', '3', ',', ',', '7', '}', 'and', 'n', '?', '{', '50', ',', '60', ',', ',', '300}.', 'shows', 'all', 'parameters', 'used', 'in', 'the', 'experiments', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'CC', 'RB', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NNS', ':', 'NN', '.', '(', 'CD', ',', 'CD', ',', 'CD', ',', ',', 'CD', ')', 'CC', 'RB', '.', '(', 'CD', ',', 'CD', ',', ',', 'CD', 'NNS', 'DT', 'NNS', 'VBN', 'IN', 'DT', 'NNS', '.']",49
relation_extraction,8,215,"Because the position dimension has little effect on the result , we heuristically choose d p = 5 .","['Because', 'the', 'position', 'dimension', 'has', 'little', 'effect', 'on', 'the', 'result', ',', 'we', 'heuristically', 'choose', 'd', 'p', '=', '5', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NN', 'NN', 'VBZ', 'JJ', 'NN', 'IN', 'DT', 'NN', ',', 'PRP', 'RB', 'VBP', 'JJ', 'NN', 'VBD', 'CD', '.']",19
relation_extraction,8,216,The batch size is fixed to 50 .,"['The', 'batch', 'size', 'is', 'fixed', 'to', '50', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",8
relation_extraction,8,217,"We use Adadelta in the update procedure ; it relies on two main parameters , ? and ? , which do not significantly affect the performance .","['We', 'use', 'Adadelta', 'in', 'the', 'update', 'procedure', ';', 'it', 'relies', 'on', 'two', 'main', 'parameters', ',', '?', 'and', '?', ',', 'which', 'do', 'not', 'significantly', 'affect', 'the', 'performance', '.']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'NNP', 'IN', 'DT', 'JJ', 'NN', ':', 'PRP', 'VBZ', 'IN', 'CD', 'JJ', 'NNS', ',', '.', 'CC', '.', ',', 'WDT', 'VBP', 'RB', 'RB', 'VBP', 'DT', 'NN', '.']",27
relation_extraction,8,219,"In the dropout operation , we randomly set the hidden unit activities to zero with a probability of 0.5 during training .","['In', 'the', 'dropout', 'operation', ',', 'we', 'randomly', 'set', 'the', 'hidden', 'unit', 'activities', 'to', 'zero', 'with', 'a', 'probability', 'of', '0.5', 'during', 'training', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NN', 'NN', ',', 'PRP', 'VBP', 'VBN', 'DT', 'JJ', 'NN', 'NNS', 'TO', 'VB', 'IN', 'DT', 'NN', 'IN', 'CD', 'IN', 'NN', '.']",22
relation_extraction,8,226,Mintz represents a traditional distantsupervision - based model that was proposed by .,"['Mintz', 'represents', 'a', 'traditional', 'distantsupervision', '-', 'based', 'model', 'that', 'was', 'proposed', 'by', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', ':', 'VBN', 'NN', 'WDT', 'VBD', 'VBN', 'IN', '.']",13
relation_extraction,8,227,MultiR is a multi-instance learning method that was proposed by .,"['MultiR', 'is', 'a', 'multi-instance', 'learning', 'method', 'that', 'was', 'proposed', 'by', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'NN', 'NN', 'WDT', 'VBD', 'VBN', 'IN', '.']",11
relation_extraction,8,228,MIML is a multi-instance multilabel model that was proposed by .,"['MIML', 'is', 'a', 'multi-instance', 'multilabel', 'model', 'that', 'was', 'proposed', 'by', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBD', 'VBN', 'IN', '.']",11
relation_extraction,8,229,"shows the precision - recall curves for each method , where PCNNs + MIL denotes our method , and demonstrates that PCNNs + MIL achieves higher precision over the entire range of recall .","['shows', 'the', 'precision', '-', 'recall', 'curves', 'for', 'each', 'method', ',', 'where', 'PCNNs', '+', 'MIL', 'denotes', 'our', 'method', ',', 'and', 'demonstrates', 'that', 'PCNNs', '+', 'MIL', 'achieves', 'higher', 'precision', 'over', 'the', 'entire', 'range', 'of', 'recall', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['VBZ', 'DT', 'NN', ':', 'NN', 'NNS', 'IN', 'DT', 'NN', ',', 'WRB', 'NNP', 'NNP', 'NNP', 'VBZ', 'PRP$', 'NN', ',', 'CC', 'VBZ', 'IN', 'NNP', 'NNP', 'NNP', 'VBZ', 'JJR', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', '.']",34
relation_extraction,8,230,PCNNs + MIL enhances the recall to ap - proximately 34 % without any loss of precision .,"['PCNNs', '+', 'MIL', 'enhances', 'the', 'recall', 'to', 'ap', '-', 'proximately', '34', '%', 'without', 'any', 'loss', 'of', 'precision', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O']","['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'NN', 'TO', 'VB', ':', 'RB', 'CD', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', '.']",18
relation_extraction,8,231,"In terms of both precision and recall , PCNNs + MIL outperforms all other evaluated approaches .","['In', 'terms', 'of', 'both', 'precision', 'and', 'recall', ',', 'PCNNs', '+', 'MIL', 'outperforms', 'all', 'other', 'evaluated', 'approaches', '.']","['B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NN', ',', 'NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'JJ', 'NNS', '.']",17
relation_extraction,8,235,Automatically learning features via PCNNs can alleviate the error propagation that occurs in traditional feature extraction .,"['Automatically', 'learning', 'features', 'via', 'PCNNs', 'can', 'alleviate', 'the', 'error', 'propagation', 'that', 'occurs', 'in', 'traditional', 'feature', 'extraction', '.']","['B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'VBG', 'NNS', 'IN', 'NNP', 'MD', 'VB', 'DT', 'NN', 'NN', 'WDT', 'VBZ', 'IN', 'JJ', 'NN', 'NN', '.']",17
relation_extraction,8,236,Incorporating multi-instance learning into a convolutional neural network is an effective means of addressing the wrong label problem .,"['Incorporating', 'multi-instance', 'learning', 'into', 'a', 'convolutional', 'neural', 'network', 'is', 'an', 'effective', 'means', 'of', 'addressing', 'the', 'wrong', 'label', 'problem', '.']","['B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['VBG', 'NN', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NNS', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'NN', '.']",19
relation_extraction,9,2,Relation Classification via Multi - Level Attention CNNs,"['Relation', 'Classification', 'via', 'Multi', '-', 'Level', 'Attention', 'CNNs']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'IN', 'NNP', ':', 'NN', 'NNP', 'NNP']",8
relation_extraction,9,24,We propose a novel CNN architecture that addresses some of the shortcomings of previous approaches .,"['We', 'propose', 'a', 'novel', 'CNN', 'architecture', 'that', 'addresses', 'some', 'of', 'the', 'shortcomings', 'of', 'previous', 'approaches', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NNP', 'NN', 'WDT', 'VBZ', 'DT', 'IN', 'DT', 'NNS', 'IN', 'JJ', 'NNS', '.']",16
relation_extraction,9,27,"Our CNN architecture relies on a novel multi-level attention mechanism to capture both entity - specific attention ( primary attention at the input level , with respect to the target entities ) and relation - specific pooling attention ( secondary attention with respect to the target relations ) .","['Our', 'CNN', 'architecture', 'relies', 'on', 'a', 'novel', 'multi-level', 'attention', 'mechanism', 'to', 'capture', 'both', 'entity', '-', 'specific', 'attention', '(', 'primary', 'attention', 'at', 'the', 'input', 'level', ',', 'with', 'respect', 'to', 'the', 'target', 'entities', ')', 'and', 'relation', '-', 'specific', 'pooling', 'attention', '(', 'secondary', 'attention', 'with', 'respect', 'to', 'the', 'target', 'relations', ')', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O']","['PRP$', 'NNP', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', ':', 'JJ', 'NN', '(', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'IN', 'NN', 'TO', 'DT', 'NN', 'NNS', ')', 'CC', 'NN', ':', 'JJ', 'VBG', 'NN', '(', 'JJ', 'NN', 'IN', 'NN', 'TO', 'DT', 'NN', 'NNS', ')', '.']",49
relation_extraction,9,29,2 . We introduce a novel pair - wise margin - based objective function that proves superior to standard loss functions .,"['2', '.', 'We', 'introduce', 'a', 'novel', 'pair', '-', 'wise', 'margin', '-', 'based', 'objective', 'function', 'that', 'proves', 'superior', 'to', 'standard', 'loss', 'functions', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['CD', '.', 'PRP', 'VBP', 'DT', 'NN', 'NN', ':', 'NN', 'NN', ':', 'VBN', 'JJ', 'NN', 'WDT', 'VBZ', 'JJ', 'TO', 'JJ', 'NN', 'NNS', '.']",22
relation_extraction,9,149,We use the word2 vec skip - gram model to learn initial word representations on Wikipedia .,"['We', 'use', 'the', 'word2', 'vec', 'skip', '-', 'gram', 'model', 'to', 'learn', 'initial', 'word', 'representations', 'on', 'Wikipedia', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'SYM', ':', 'NN', 'NN', 'TO', 'VB', 'JJ', 'NN', 'NNS', 'IN', 'NNP', '.']",17
relation_extraction,9,150,Other matrices are initialized with random values following a Gaussian distribution .,"['Other', 'matrices', 'are', 'initialized', 'with', 'random', 'values', 'following', 'a', 'Gaussian', 'distribution', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['JJ', 'NNS', 'VBP', 'VBN', 'IN', 'JJ', 'NNS', 'VBG', 'DT', 'JJ', 'NN', '.']",12
relation_extraction,9,151,We apply a cross-validation procedure on the training data to select suitable hyperparameters .,"['We', 'apply', 'a', 'cross-validation', 'procedure', 'on', 'the', 'training', 'data', 'to', 'select', 'suitable', 'hyperparameters', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NNS', 'TO', 'VB', 'JJ', 'NNS', '.']",14
relation_extraction,9,153,We observe that our novel attentionbased architecture achieves new state - of - the - art results on this relation classification dataset .,"['We', 'observe', 'that', 'our', 'novel', 'attentionbased', 'architecture', 'achieves', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'on', 'this', 'relation', 'classification', 'dataset', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'PRP$', 'NN', 'VBN', 'NN', 'VBZ', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'NN', '.']",23
relation_extraction,9,154,"Att - Input - CNN relies only on the primal attention at the input level , performing standard max - pooling after the convolution layer to generate the network output w O , in which the new objective function is utilized .","['Att', '-', 'Input', '-', 'CNN', 'relies', 'only', 'on', 'the', 'primal', 'attention', 'at', 'the', 'input', 'level', ',', 'performing', 'standard', 'max', '-', 'pooling', 'after', 'the', 'convolution', 'layer', 'to', 'generate', 'the', 'network', 'output', 'w', 'O', ',', 'in', 'which', 'the', 'new', 'objective', 'function', 'is', 'utilized', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NNP', ':', 'NN', 'VBZ', 'RB', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'VBG', 'JJ', 'NN', ':', 'NN', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'NN', 'NNP', ',', 'IN', 'WDT', 'DT', 'JJ', 'JJ', 'NN', 'VBZ', 'JJ', '.']",42
relation_extraction,9,155,"With Att - Input - CNN , we achieve an F1-score of 87.5 % , thus already outperforming not only the original winner of the SemEval task , an SVM - based approach ( 82.2 % ) , but also the wellknown CR - CNN model ( 84.1 % ) with a relative improvement of 4.04 % , and the newly released DRNNs ( 85.8 % ) with a relative improvement of 2.0 % , although the latter approach depends on the Stanford parser to obtain dependency parse information .","['With', 'Att', '-', 'Input', '-', 'CNN', ',', 'we', 'achieve', 'an', 'F1-score', 'of', '87.5', '%', ',', 'thus', 'already', 'outperforming', 'not', 'only', 'the', 'original', 'winner', 'of', 'the', 'SemEval', 'task', ',', 'an', 'SVM', '-', 'based', 'approach', '(', '82.2', '%', ')', ',', 'but', 'also', 'the', 'wellknown', 'CR', '-', 'CNN', 'model', '(', '84.1', '%', ')', 'with', 'a', 'relative', 'improvement', 'of', '4.04', '%', ',', 'and', 'the', 'newly', 'released', 'DRNNs', '(', '85.8', '%', ')', 'with', 'a', 'relative', 'improvement', 'of', '2.0', '%', ',', 'although', 'the', 'latter', 'approach', 'depends', 'on', 'the', 'Stanford', 'parser', 'to', 'obtain', 'dependency', 'parse', 'information', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ':', 'NNP', ':', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'CD', 'NN', ',', 'RB', 'RB', 'VBG', 'RB', 'RB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', ',', 'DT', 'NNP', ':', 'VBN', 'NN', '(', 'CD', 'NN', ')', ',', 'CC', 'RB', 'DT', 'JJ', 'NNP', ':', 'NNP', 'NN', '(', 'CD', 'NN', ')', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NN', ',', 'CC', 'DT', 'RB', 'VBN', 'NNP', '(', 'CD', 'NN', ')', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NN', ',', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'IN', 'DT', 'NNP', 'NN', 'TO', 'VB', 'NN', 'JJ', 'NN', '.']",90
relation_extraction,9,156,Our full dual attention model Att - Pooling - CNN achieves an even more favorable F1- score of 88 % .,"['Our', 'full', 'dual', 'attention', 'model', 'Att', '-', 'Pooling', '-', 'CNN', 'achieves', 'an', 'even', 'more', 'favorable', 'F1-', 'score', 'of', '88', '%', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP$', 'JJ', 'JJ', 'NN', 'NN', 'NNP', ':', 'VBG', ':', 'NNP', 'VBZ', 'DT', 'RB', 'RBR', 'JJ', 'JJ', 'NN', 'IN', 'CD', 'NN', '.']",21
relation_extraction,9,159,"To better quantify the contribution of the different components of our model , we also conduct an ablation study evaluating several simplified models .","['To', 'better', 'quantify', 'the', 'contribution', 'of', 'the', 'different', 'components', 'of', 'our', 'model', ',', 'we', 'also', 'conduct', 'an', 'ablation', 'study', 'evaluating', 'several', 'simplified', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'RBR', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'PRP$', 'NN', ',', 'PRP', 'RB', 'VBP', 'DT', 'NN', 'NN', 'VBG', 'JJ', 'JJ', 'NNS', '.']",24
relation_extraction,9,160,The first simplification is to use our model without the input attention mechanism but with the pooling attention layer .,"['The', 'first', 'simplification', 'is', 'to', 'use', 'our', 'model', 'without', 'the', 'input', 'attention', 'mechanism', 'but', 'with', 'the', 'pooling', 'attention', 'layer', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'TO', 'VB', 'PRP$', 'NN', 'IN', 'DT', 'NN', 'NN', 'NN', 'CC', 'IN', 'DT', 'VBG', 'NN', 'NN', '.']",20
relation_extraction,9,161,The second removes both attention mechanisms .,"['The', 'second', 'removes', 'both', 'attention', 'mechanisms', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NNS', 'DT', 'NN', 'NN', '.']",7
relation_extraction,9,162,The third removes both forms of attention and additionally uses a regular objective function based on the inner product s = r w for a sentence representation r and relation class embedding w.,"['The', 'third', 'removes', 'both', 'forms', 'of', 'attention', 'and', 'additionally', 'uses', 'a', 'regular', 'objective', 'function', 'based', 'on', 'the', 'inner', 'product', 's', '=', 'r', 'w', 'for', 'a', 'sentence', 'representation', 'r', 'and', 'relation', 'class', 'embedding', 'w.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n']","['DT', 'JJ', 'NNS', 'DT', 'NNS', 'IN', 'NN', 'CC', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NNP', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'NN', 'CC', 'NN', 'NN', 'VBG', 'NN']",33
relation_extraction,9,163,We observe that all three of our components lead to noticeable improvements over these baselines .,"['We', 'observe', 'that', 'all', 'three', 'of', 'our', 'components', 'lead', 'to', 'noticeable', 'improvements', 'over', 'these', 'baselines', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'CD', 'IN', 'PRP$', 'NNS', 'VBP', 'TO', 'JJ', 'NNS', 'IN', 'DT', 'NNS', '.']",16
relation_extraction,1,2,Simple BERT Models for Relation Extraction and Semantic Role Labeling,"['Simple', 'BERT', 'Models', 'for', 'Relation', 'Extraction', 'and', 'Semantic', 'Role', 'Labeling']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'CC', 'NNP', 'NNP', 'VBG']",10
relation_extraction,1,10,Relation extraction and semantic role labeling ( SRL ) are two fundamental tasks in natural language understanding .,"['Relation', 'extraction', 'and', 'semantic', 'role', 'labeling', '(', 'SRL', ')', 'are', 'two', 'fundamental', 'tasks', 'in', 'natural', 'language', 'understanding', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'CC', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBP', 'CD', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NN', '.']",18
relation_extraction,1,14,"For SRL , the task is to extract the predicate - argument structure of a sentence , determining "" who did what to whom "" , "" when "" , "" where "" , etc .","['For', 'SRL', ',', 'the', 'task', 'is', 'to', 'extract', 'the', 'predicate', '-', 'argument', 'structure', 'of', 'a', 'sentence', ',', 'determining', '""', 'who', 'did', 'what', 'to', 'whom', '""', ',', '""', 'when', '""', ',', '""', 'where', '""', ',', 'etc', '.']","['O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ',', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NN', ':', 'NN', 'NN', 'IN', 'DT', 'NN', ',', 'VBG', 'NNS', 'WP', 'VBD', 'WP', 'TO', 'WP', 'NNP', ',', 'NN', 'WRB', 'NN', ',', 'NNP', 'WRB', 'NN', ',', 'FW', '.']",36
relation_extraction,1,24,We show that simple neural architectures built on top of BERT yields state - of - the - art performance on a variety of benchmark datasets for these two tasks .,"['We', 'show', 'that', 'simple', 'neural', 'architectures', 'built', 'on', 'top', 'of', 'BERT', 'yields', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'on', 'a', 'variety', 'of', 'benchmark', 'datasets', 'for', 'these', 'two', 'tasks', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'JJ', 'JJ', 'NNS', 'VBN', 'IN', 'NN', 'IN', 'NNP', 'NNS', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'CD', 'NNS', '.']",31
relation_extraction,1,64,We conduct experiments on two SRL tasks : and the predicate indicator embedding size is 10 .,"['We', 'conduct', 'experiments', 'on', 'two', 'SRL', 'tasks', ':', 'and', 'the', 'predicate', 'indicator', 'embedding', 'size', 'is', '10', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'NNS', 'IN', 'CD', 'NNP', 'NNS', ':', 'CC', 'DT', 'NN', 'NN', 'VBG', 'NN', 'VBZ', 'CD', '.']",17
relation_extraction,1,65,The learning rate is 5 10 ?5 . BERT base - cased and large - cased models are used in our experiments .,"['The', 'learning', 'rate', 'is', '5', '10', '?5', '.', 'BERT', 'base', '-', 'cased', 'and', 'large', '-', 'cased', 'models', 'are', 'used', 'in', 'our', 'experiments', '.']","['O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'CD', 'CD', 'NN', '.', 'NNP', 'NN', ':', 'VBN', 'CC', 'JJ', ':', 'VBN', 'NNS', 'VBP', 'VBN', 'IN', 'PRP$', 'NNS', '.']",23
relation_extraction,1,66,The position embeddings are randomly initialized and fine - tuned during the training process .,"['The', 'position', 'embeddings', 'are', 'randomly', 'initialized', 'and', 'fine', '-', 'tuned', 'during', 'the', 'training', 'process', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'CC', 'JJ', ':', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",15
relation_extraction,1,79,We see that the BERT - LSTM - large model achieves the state - of - the - art F 1 score among single models and outperforms the ensemble model on the CoNLL 2005 in - domain and out - of - domain tests .,"['We', 'see', 'that', 'the', 'BERT', '-', 'LSTM', '-', 'large', 'model', 'achieves', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'F', '1', 'score', 'among', 'single', 'models', 'and', 'outperforms', 'the', 'ensemble', 'model', 'on', 'the', 'CoNLL', '2005', 'in', '-', 'domain', 'and', 'out', '-', 'of', '-', 'domain', 'tests', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NNP', ':', 'NNP', ':', 'JJ', 'NN', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNP', 'CD', 'NN', 'IN', 'JJ', 'NNS', 'CC', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'CD', 'IN', ':', 'NN', 'CC', 'IN', ':', 'IN', ':', 'NN', 'NNS', '.']",45
relation_extraction,1,80,"However , it falls short on the CoNLL 2012 benchmark because the model of obtains very high precision .","['However', ',', 'it', 'falls', 'short', 'on', 'the', 'CoNLL', '2012', 'benchmark', 'because', 'the', 'model', 'of', 'obtains', 'very', 'high', 'precision', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBZ', 'JJ', 'IN', 'DT', 'NNP', 'CD', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNS', 'RB', 'JJ', 'NN', '.']",19
relation_extraction,5,2,Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,"['Graph', 'Convolution', 'over', 'Pruned', 'Dependency', 'Trees', 'Improves', 'Relation', 'Extraction']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'VBZ', 'NNP', 'NNP']",9
relation_extraction,5,28,"In this work , we propose a novel extension of the graph convolutional network ) that is tailored for relation extraction .","['In', 'this', 'work', ',', 'we', 'propose', 'a', 'novel', 'extension', 'of', 'the', 'graph', 'convolutional', 'network', ')', 'that', 'is', 'tailored', 'for', 'relation', 'extraction', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'JJ', 'NN', ')', 'WDT', 'VBZ', 'VBN', 'IN', 'NN', 'NN', '.']",22
relation_extraction,5,29,"Our model encodes the dependency structure over the input sentence with efficient graph convolution operations , then extracts entity - centric representations to make robust relation predictions .","['Our', 'model', 'encodes', 'the', 'dependency', 'structure', 'over', 'the', 'input', 'sentence', 'with', 'efficient', 'graph', 'convolution', 'operations', ',', 'then', 'extracts', 'entity', '-', 'centric', 'representations', 'to', 'make', 'robust', 'relation', 'predictions', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', 'NNS', ',', 'RB', 'VBZ', 'NN', ':', 'JJ', 'NNS', 'TO', 'VB', 'JJ', 'NN', 'NNS', '.']",28
relation_extraction,5,30,"We also apply a novel path - centric pruning technique to remove irrelevant information from the tree while maximally keeping relevant content , which further improves the performance of several dependencybased models including ours .","['We', 'also', 'apply', 'a', 'novel', 'path', '-', 'centric', 'pruning', 'technique', 'to', 'remove', 'irrelevant', 'information', 'from', 'the', 'tree', 'while', 'maximally', 'keeping', 'relevant', 'content', ',', 'which', 'further', 'improves', 'the', 'performance', 'of', 'several', 'dependencybased', 'models', 'including', 'ours', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'JJ', 'NN', ':', 'JJ', 'NN', 'NN', 'TO', 'VB', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'RB', 'VBG', 'JJ', 'NN', ',', 'WDT', 'RBR', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'JJ', 'NNS', 'VBG', 'NNS', '.']",35
relation_extraction,5,124,Dependency - based models .,"['Dependency', '-', 'based', 'models', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'VBN', 'NNS', '.']",5
relation_extraction,5,126,( 1 ) A logistic regression ( LR ) classifier which combines dependencybased features with other lexical features .,"['(', '1', ')', 'A', 'logistic', 'regression', '(', 'LR', ')', 'classifier', 'which', 'combines', 'dependencybased', 'features', 'with', 'other', 'lexical', 'features', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'NNP', 'JJ', 'NN', '(', 'NNP', ')', 'NN', 'WDT', 'NNS', 'VBD', 'NNS', 'IN', 'JJ', 'JJ', 'NNS', '.']",19
relation_extraction,5,127,"( 2 ) Shortest Dependency Path LSTM ( SDP - LSTM ) , which applies a neural sequence model on the shortest path between the subject and object entities in the dependency tree .","['(', '2', ')', 'Shortest', 'Dependency', 'Path', 'LSTM', '(', 'SDP', '-', 'LSTM', ')', ',', 'which', 'applies', 'a', 'neural', 'sequence', 'model', 'on', 'the', 'shortest', 'path', 'between', 'the', 'subject', 'and', 'object', 'entities', 'in', 'the', 'dependency', 'tree', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['(', 'CD', ')', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ':', 'NNP', ')', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NN', 'CC', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",34
relation_extraction,5,128,"Tree - LSTM , which is a recursive model that generalizes the LSTM to arbitrary tree structures .","['Tree', '-', 'LSTM', ',', 'which', 'is', 'a', 'recursive', 'model', 'that', 'generalizes', 'the', 'LSTM', 'to', 'arbitrary', 'tree', 'structures', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'DT', 'NNP', 'TO', 'VB', 'JJ', 'NNS', '.']",18
relation_extraction,5,132,Neural sequence model .,"['Neural', 'sequence', 'model', '.']","['B-n', 'I-n', 'I-n', 'O']","['JJ', 'NN', 'NN', '.']",4
relation_extraction,5,133,"Our group presented a competitive sequence model that employs a position - aware attention mechanism over LSTM outputs ( PA - LSTM ) , and showed that it outperforms several CNN and dependency - based models by a substantial margin .","['Our', 'group', 'presented', 'a', 'competitive', 'sequence', 'model', 'that', 'employs', 'a', 'position', '-', 'aware', 'attention', 'mechanism', 'over', 'LSTM', 'outputs', '(', 'PA', '-', 'LSTM', ')', ',', 'and', 'showed', 'that', 'it', 'outperforms', 'several', 'CNN', 'and', 'dependency', '-', 'based', 'models', 'by', 'a', 'substantial', 'margin', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'VBD', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'NN', ':', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'NNS', '(', 'NNP', ':', 'NNP', ')', ',', 'CC', 'VBD', 'IN', 'PRP', 'VBZ', 'JJ', 'NNP', 'CC', 'NN', ':', 'VBN', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",41
relation_extraction,5,149,Results on the TACRED Dataset,"['Results', 'on', 'the', 'TACRED', 'Dataset']","['O', 'B-p', 'O', 'B-n', 'I-n']","['NNS', 'IN', 'DT', 'NNP', 'NNP']",5
relation_extraction,5,151,We observe that our GCN model Our Model ( C - GCN ) 84.8 * 76.5 * outperforms all dependency - based models by at least 1.6 F 1 .,"['We', 'observe', 'that', 'our', 'GCN', 'model', 'Our', 'Model', '(', 'C', '-', 'GCN', ')', '84.8', '*', '76.5', '*', 'outperforms', 'all', 'dependency', '-', 'based', 'models', 'by', 'at', 'least', '1.6', 'F', '1', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'PRP$', 'NNP', 'NN', 'PRP$', 'NNP', '(', 'NNP', ':', 'NNP', ')', 'CD', '$', 'CD', 'NN', 'NNS', 'DT', 'NN', ':', 'VBN', 'NNS', 'IN', 'IN', 'JJS', 'CD', 'NNP', 'CD', '.']",30
relation_extraction,5,152,"By using contextualized word representations , the C - GCN model further outperforms the strong PA - LSTM model by 1.3 F 1 , and achieves a new state of the art .","['By', 'using', 'contextualized', 'word', 'representations', ',', 'the', 'C', '-', 'GCN', 'model', 'further', 'outperforms', 'the', 'strong', 'PA', '-', 'LSTM', 'model', 'by', '1.3', 'F', '1', ',', 'and', 'achieves', 'a', 'new', 'state', 'of', 'the', 'art', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBG', 'VBN', 'NN', 'NNS', ',', 'DT', 'NNP', ':', 'NNP', 'NN', 'RB', 'VBZ', 'DT', 'JJ', 'NNP', ':', 'NNP', 'NN', 'IN', 'CD', 'NNP', 'CD', ',', 'CC', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",33
relation_extraction,5,153,"In addition , we find our model improves upon other dependencybased models in both precision and recall .","['In', 'addition', ',', 'we', 'find', 'our', 'model', 'improves', 'upon', 'other', 'dependencybased', 'models', 'in', 'both', 'precision', 'and', 'recall', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'PRP$', 'NN', 'VBZ', 'IN', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NN', '.']",18
relation_extraction,5,154,"Comparing the C - GCN model with the GCN model , we find that the gain mainly comes from improved recall .","['Comparing', 'the', 'C', '-', 'GCN', 'model', 'with', 'the', 'GCN', 'model', ',', 'we', 'find', 'that', 'the', 'gain', 'mainly', 'comes', 'from', 'improved', 'recall', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['VBG', 'DT', 'NNP', ':', 'NNP', 'NN', 'IN', 'DT', 'NNP', 'NN', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'RB', 'VBZ', 'IN', 'VBN', 'NN', '.']",22
relation_extraction,5,156,"As we will show in Section 6.2 , we find that our GCN models have complementary strengths when compared to the PA - LSTM .","['As', 'we', 'will', 'show', 'in', 'Section', '6.2', ',', 'we', 'find', 'that', 'our', 'GCN', 'models', 'have', 'complementary', 'strengths', 'when', 'compared', 'to', 'the', 'PA', '-', 'LSTM', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'PRP', 'MD', 'VB', 'IN', 'NNP', 'CD', ',', 'PRP', 'VBP', 'IN', 'PRP$', 'NNP', 'NNS', 'VBP', 'JJ', 'NNS', 'WRB', 'VBN', 'TO', 'DT', 'NNP', ':', 'NN', '.']",25
relation_extraction,5,161,"This simple interpolation between a GCN and a PA - LSTM achieves an F 1 score of 67.1 , outperforming each model alone by at least 2.0 F 1 .","['This', 'simple', 'interpolation', 'between', 'a', 'GCN', 'and', 'a', 'PA', '-', 'LSTM', 'achieves', 'an', 'F', '1', 'score', 'of', '67.1', ',', 'outperforming', 'each', 'model', 'alone', 'by', 'at', 'least', '2.0', 'F', '1', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'CC', 'DT', 'NNP', ':', 'NNP', 'VBZ', 'DT', 'NNP', 'CD', 'NN', 'IN', 'CD', ',', 'VBG', 'DT', 'NN', 'RB', 'IN', 'IN', 'JJS', 'CD', 'NNP', 'CD', '.']",30
relation_extraction,5,162,An interpolation between a C - GCN and a PA - LSTM further improves the result to 68.2 .,"['An', 'interpolation', 'between', 'a', 'C', '-', 'GCN', 'and', 'a', 'PA', '-', 'LSTM', 'further', 'improves', 'the', 'result', 'to', '68.2', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'DT', 'NNP', ':', 'NNP', 'CC', 'DT', 'NNP', ':', 'NNP', 'RBR', 'VBZ', 'DT', 'NN', 'TO', 'CD', '.']",19
relation_extraction,5,163,Results on the SemEval Dataset,"['Results', 'on', 'the', 'SemEval', 'Dataset']","['O', 'O', 'O', 'B-n', 'I-n']","['NNS', 'IN', 'DT', 'NNP', 'NNP']",5
relation_extraction,5,165,"We find that under the conventional with- entity evaluation , our C - GCN model outperforms all existing dependency - based neural models on this sep - arate dataset .","['We', 'find', 'that', 'under', 'the', 'conventional', 'with-', 'entity', 'evaluation', ',', 'our', 'C', '-', 'GCN', 'model', 'outperforms', 'all', 'existing', 'dependency', '-', 'based', 'neural', 'models', 'on', 'this', 'sep', '-', 'arate', 'dataset', '.']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'NN', ',', 'PRP$', 'NNP', ':', 'NNP', 'NN', 'VBZ', 'DT', 'VBG', 'NN', ':', 'VBN', 'JJ', 'NNS', 'IN', 'DT', 'JJ', ':', 'NN', 'NN', '.']",30
relation_extraction,5,166,"Notably , by properly incorporating off - path information , our model outperforms the previous shortest dependency path - based model ( SDP - LSTM ) .","['Notably', ',', 'by', 'properly', 'incorporating', 'off', '-', 'path', 'information', ',', 'our', 'model', 'outperforms', 'the', 'previous', 'shortest', 'dependency', 'path', '-', 'based', 'model', '(', 'SDP', '-', 'LSTM', ')', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'IN', 'RB', 'VBG', 'RP', ':', 'NN', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'NN', ':', 'VBN', 'NN', '(', 'NNP', ':', 'NN', ')', '.']",27
relation_extraction,5,167,"Under the mask - entity evaluation , our C - GCN model also outperforms PA - LSTM by a substantial margin , suggesting its generalizability even when entities are not seen .","['Under', 'the', 'mask', '-', 'entity', 'evaluation', ',', 'our', 'C', '-', 'GCN', 'model', 'also', 'outperforms', 'PA', '-', 'LSTM', 'by', 'a', 'substantial', 'margin', ',', 'suggesting', 'its', 'generalizability', 'even', 'when', 'entities', 'are', 'not', 'seen', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ':', 'NN', 'NN', ',', 'PRP$', 'NNP', ':', 'NNP', 'NN', 'RB', 'VBZ', 'NNP', ':', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'VBG', 'PRP$', 'NN', 'RB', 'WRB', 'NNS', 'VBP', 'RB', 'VBN', '.']",32
relation_extraction,5,170,"To show the effectiveness of path - centric pruning , we compare the two GCN models and the Tree - LSTM when the pruning distance K is varied .","['To', 'show', 'the', 'effectiveness', 'of', 'path', '-', 'centric', 'pruning', ',', 'we', 'compare', 'the', 'two', 'GCN', 'models', 'and', 'the', 'Tree', '-', 'LSTM', 'when', 'the', 'pruning', 'distance', 'K', 'is', 'varied', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'NN', 'IN', 'NN', ':', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'CD', 'NNP', 'NNS', 'CC', 'DT', 'NNP', ':', 'NN', 'WRB', 'DT', 'NN', 'NN', 'NNP', 'VBZ', 'VBN', '.']",29
relation_extraction,5,172,"As shown in , the performance of all three models peaks when K = 1 , outperforming their respective dependency path - based counterpart ( K = 0 ) .","['As', 'shown', 'in', ',', 'the', 'performance', 'of', 'all', 'three', 'models', 'peaks', 'when', 'K', '=', '1', ',', 'outperforming', 'their', 'respective', 'dependency', 'path', '-', 'based', 'counterpart', '(', 'K', '=', '0', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'IN', ',', 'DT', 'NN', 'IN', 'DT', 'CD', 'NNS', 'VBP', 'WRB', 'NNP', 'NNP', 'CD', ',', 'VBG', 'PRP$', 'JJ', 'NN', 'NN', ':', 'VBN', 'NN', '(', 'NNP', 'NNP', 'CD', ')', '.']",30
relation_extraction,5,176,"We find that all three models are less effective when the entire dependency tree is present , indicating that including extra information hurts performance .","['We', 'find', 'that', 'all', 'three', 'models', 'are', 'less', 'effective', 'when', 'the', 'entire', 'dependency', 'tree', 'is', 'present', ',', 'indicating', 'that', 'including', 'extra', 'information', 'hurts', 'performance', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'DT', 'CD', 'NNS', 'VBP', 'RBR', 'JJ', 'WRB', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'JJ', ',', 'VBG', 'IN', 'VBG', 'JJ', 'NN', 'VBZ', 'NN', '.']",25
relation_extraction,5,177,"Finally , we note that contextualizing the GCN makes it less sensitive to changes in the tree structures provided , presumably because the model can use word sequence information in the LSTM layer to recover any off - path information that it needs for correct relation extraction .","['Finally', ',', 'we', 'note', 'that', 'contextualizing', 'the', 'GCN', 'makes', 'it', 'less', 'sensitive', 'to', 'changes', 'in', 'the', 'tree', 'structures', 'provided', ',', 'presumably', 'because', 'the', 'model', 'can', 'use', 'word', 'sequence', 'information', 'in', 'the', 'LSTM', 'layer', 'to', 'recover', 'any', 'off', '-', 'path', 'information', 'that', 'it', 'needs', 'for', 'correct', 'relation', 'extraction', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'IN', 'VBG', 'DT', 'NNP', 'VBZ', 'PRP', 'RBR', 'JJ', 'TO', 'NNS', 'IN', 'DT', 'JJ', 'NNS', 'VBN', ',', 'RB', 'IN', 'DT', 'NN', 'MD', 'VB', 'NN', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', 'TO', 'VB', 'DT', 'RP', ':', 'NN', 'NN', 'IN', 'PRP', 'VBZ', 'IN', 'JJ', 'NN', 'NN', '.']",48
relation_extraction,5,181,We find that : The entity representations and feedforward layers contribute 1.0 F 1 .,"['We', 'find', 'that', ':', 'The', 'entity', 'representations', 'and', 'feedforward', 'layers', 'contribute', '1.0', 'F', '1', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', ':', 'DT', 'NN', 'NNS', 'CC', 'JJ', 'NNS', 'VBP', 'CD', 'NNP', 'CD', '.']",15
relation_extraction,5,182,"( 2 ) When we remove the dependency structure ( i.e. , setting to I ) , the score drops by 3.2 F 1 .","['(', '2', ')', 'When', 'we', 'remove', 'the', 'dependency', 'structure', '(', 'i.e.', ',', 'setting', 'to', 'I', ')', ',', 'the', 'score', 'drops', 'by', '3.2', 'F', '1', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'WRB', 'PRP', 'VBP', 'DT', 'NN', 'NN', '(', 'JJ', ',', 'VBG', 'TO', 'PRP', ')', ',', 'DT', 'NN', 'NNS', 'IN', 'CD', 'NNP', 'CD', '.']",25
relation_extraction,5,183,"( 3 ) F 1 drops by 10.3 when we remove the feedforward layers , the LSTM component and the dependency structure altogether .","['(', '3', ')', 'F', '1', 'drops', 'by', '10.3', 'when', 'we', 'remove', 'the', 'feedforward', 'layers', ',', 'the', 'LSTM', 'component', 'and', 'the', 'dependency', 'structure', 'altogether', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O']","['(', 'CD', ')', 'NNP', 'CD', 'NNS', 'IN', 'CD', 'WRB', 'PRP', 'VBP', 'DT', 'NN', 'NNS', ',', 'DT', 'NNP', 'NN', 'CC', 'DT', 'NN', 'NN', 'RB', '.']",24
relation_extraction,5,184,"( 4 ) Removing the pruning ( i.e. , using full trees as input ) further hurts the result by another 9.7 F 1 .","['(', '4', ')', 'Removing', 'the', 'pruning', '(', 'i.e.', ',', 'using', 'full', 'trees', 'as', 'input', ')', 'further', 'hurts', 'the', 'result', 'by', 'another', '9.7', 'F', '1', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'VBG', 'DT', 'NN', '(', 'FW', ',', 'VBG', 'JJ', 'NNS', 'IN', 'NN', ')', 'RBR', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'CD', 'NNP', 'CD', '.']",25
relation_extraction,11,2,RESIDE : Improving Distantly - Supervised Neural Relation Extraction using Side Information,"['RESIDE', ':', 'Improving', 'Distantly', '-', 'Supervised', 'Neural', 'Relation', 'Extraction', 'using', 'Side', 'Information']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['NN', ':', 'VBG', 'NNP', ':', 'VBD', 'JJ', 'NNP', 'NNP', 'VBG', 'NNP', 'NN']",12
relation_extraction,11,4,Distantly - supervised Relation Extraction ( RE ) methods train an extractor by automatically aligning relation instances in a Knowledge Base ( KB ) with unstructured text .,"['Distantly', '-', 'supervised', 'Relation', 'Extraction', '(', 'RE', ')', 'methods', 'train', 'an', 'extractor', 'by', 'automatically', 'aligning', 'relation', 'instances', 'in', 'a', 'Knowledge', 'Base', '(', 'KB', ')', 'with', 'unstructured', 'text', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ':', 'VBN', 'NNP', 'NNP', '(', 'NNP', ')', 'NNS', 'VBP', 'DT', 'NN', 'IN', 'RB', 'VBG', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'NNP', '(', 'NNP', ')', 'IN', 'JJ', 'NN', '.']",28
relation_extraction,11,6,RE models usually ignore such readily available side information .,"['RE', 'models', 'usually', 'ignore', 'such', 'readily', 'available', 'side', 'information', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNS', 'RB', 'VBP', 'JJ', 'RB', 'JJ', 'NN', 'NN', '.']",10
relation_extraction,11,15,Relation Extraction ( RE ) attempts to fill this gap by extracting semantic relationships between entity pairs from plain text .,"['Relation', 'Extraction', '(', 'RE', ')', 'attempts', 'to', 'fill', 'this', 'gap', 'by', 'extracting', 'semantic', 'relationships', 'between', 'entity', 'pairs', 'from', 'plain', 'text', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NNP', '(', 'NNP', ')', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'NN', 'NNS', 'IN', 'JJ', 'NN', '.']",21
relation_extraction,11,39,"In this paper , we propose RESIDE , a novel distant supervised relation extraction method which utilizes additional supervision from KB through its neural network based architecture .","['In', 'this', 'paper', ',', 'we', 'propose', 'RESIDE', ',', 'a', 'novel', 'distant', 'supervised', 'relation', 'extraction', 'method', 'which', 'utilizes', 'additional', 'supervision', 'from', 'KB', 'through', 'its', 'neural', 'network', 'based', 'architecture', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'NNP', ',', 'DT', 'JJ', 'NN', 'VBD', 'NN', 'NN', 'NN', 'WDT', 'VBZ', 'JJ', 'NN', 'IN', 'NNP', 'IN', 'PRP$', 'JJ', 'NN', 'VBN', 'NN', '.']",28
relation_extraction,11,40,"RESIDE makes principled use of entity type and relation alias information from KBs , to impose soft constraints while predicting the relation .","['RESIDE', 'makes', 'principled', 'use', 'of', 'entity', 'type', 'and', 'relation', 'alias', 'information', 'from', 'KBs', ',', 'to', 'impose', 'soft', 'constraints', 'while', 'predicting', 'the', 'relation', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['NNP', 'VBZ', 'VBN', 'NN', 'IN', 'NN', 'NN', 'CC', 'NN', 'NNS', 'NN', 'IN', 'NNP', ',', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'VBG', 'DT', 'NN', '.']",23
relation_extraction,11,41,"It uses encoded syntactic information obtained from Graph Convolution Networks ( GCN ) , along with embedded side information , to improve neural relation extraction .","['It', 'uses', 'encoded', 'syntactic', 'information', 'obtained', 'from', 'Graph', 'Convolution', 'Networks', '(', 'GCN', ')', ',', 'along', 'with', 'embedded', 'side', 'information', ',', 'to', 'improve', 'neural', 'relation', 'extraction', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'VBD', 'JJ', 'NN', 'VBN', 'IN', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'IN', 'IN', 'JJ', 'NN', 'NN', ',', 'TO', 'VB', 'JJ', 'NN', 'NN', '.']",26
relation_extraction,11,45,RESIDE 's source code and datasets used in the paper are available at http://github.com / malllabiisc / RESIDE .,"['RESIDE', ""'s"", 'source', 'code', 'and', 'datasets', 'used', 'in', 'the', 'paper', 'are', 'available', 'at', 'http://github.com', '/', 'malllabiisc', '/', 'RESIDE', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'POS', 'NN', 'NN', 'CC', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'VBP', 'JJ', 'IN', 'NN', 'NNP', 'NN', 'NNP', 'NNP', '.']",19
relation_extraction,11,198,Mintz : Multi-class logistic regression model proposed by for distant supervision paradigm .,"['Mintz', ':', 'Multi-class', 'logistic', 'regression', 'model', 'proposed', 'by', 'for', 'distant', 'supervision', 'paradigm', '.']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NN', ':', 'JJ', 'JJ', 'NN', 'NN', 'VBN', 'IN', 'IN', 'JJ', 'NN', 'NN', '.']",13
relation_extraction,11,199,MultiR : Probabilistic graphical model for multi instance learning by MIMLRE :,"['MultiR', ':', 'Probabilistic', 'graphical', 'model', 'for', 'multi', 'instance', 'learning', 'by', 'MIMLRE', ':']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O']","['NN', ':', 'JJ', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', 'IN', 'NNP', ':']",12
relation_extraction,11,200,A graphical model which jointly models multiple instances and multiple labels .,"['A', 'graphical', 'model', 'which', 'jointly', 'models', 'multiple', 'instances', 'and', 'multiple', 'labels', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'WDT', 'RB', 'NNS', 'JJ', 'NNS', 'CC', 'JJ', 'NNS', '.']",12
relation_extraction,11,201,More details in . PCNN : A CNN based relation extraction model by which uses piecewise max - pooling for sentence representation .,"['More', 'details', 'in', '.', 'PCNN', ':', 'A', 'CNN', 'based', 'relation', 'extraction', 'model', 'by', 'which', 'uses', 'piecewise', 'max', '-', 'pooling', 'for', 'sentence', 'representation', '.']","['O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['JJR', 'NNS', 'IN', '.', 'NN', ':', 'DT', 'NNP', 'VBN', 'NN', 'NN', 'NN', 'IN', 'WDT', 'VBZ', 'NN', 'SYM', ':', 'NN', 'IN', 'NN', 'NN', '.']",23
relation_extraction,11,202,PCNN + ATT : A piecewise max - pooling over CNN based model which is used by to get sentence representation followed by attention over sentences .,"['PCNN', '+', 'ATT', ':', 'A', 'piecewise', 'max', '-', 'pooling', 'over', 'CNN', 'based', 'model', 'which', 'is', 'used', 'by', 'to', 'get', 'sentence', 'representation', 'followed', 'by', 'attention', 'over', 'sentences', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NNP', ':', 'DT', 'NN', 'SYM', ':', 'NN', 'IN', 'NNP', 'VBN', 'NN', 'WDT', 'VBZ', 'VBN', 'IN', 'TO', 'VB', 'NN', 'NN', 'VBN', 'IN', 'NN', 'IN', 'NNS', '.']",27
relation_extraction,11,203,BGWA : Bi - GRU based relation extraction model with word and sentence level attention ) .,"['BGWA', ':', 'Bi', '-', 'GRU', 'based', 'relation', 'extraction', 'model', 'with', 'word', 'and', 'sentence', 'level', 'attention', ')', '.']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['NN', ':', 'NNP', ':', 'NNP', 'VBN', 'NN', 'NN', 'NN', 'IN', 'NN', 'CC', 'NN', 'NN', 'NN', ')', '.']",17
relation_extraction,11,219,"Overall , we find that RESIDE achieves higher precision over the entire recall range on both the datasets .","['Overall', ',', 'we', 'find', 'that', 'RESIDE', 'achieves', 'higher', 'precision', 'over', 'the', 'entire', 'recall', 'range', 'on', 'both', 'the', 'datasets', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['JJ', ',', 'PRP', 'VBP', 'IN', 'NNP', 'VBZ', 'JJR', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CC', 'DT', 'NNS', '.']",19
relation_extraction,11,220,All the non-neural baselines could not perform well as the features used by them are mostly derived from NLP tools which can be erroneous .,"['All', 'the', 'non-neural', 'baselines', 'could', 'not', 'perform', 'well', 'as', 'the', 'features', 'used', 'by', 'them', 'are', 'mostly', 'derived', 'from', 'NLP', 'tools', 'which', 'can', 'be', 'erroneous', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PDT', 'DT', 'JJ', 'NNS', 'MD', 'RB', 'VB', 'RB', 'IN', 'DT', 'NNS', 'VBN', 'IN', 'PRP', 'VBP', 'RB', 'VBN', 'IN', 'NNP', 'NNS', 'WDT', 'MD', 'VB', 'JJ', '.']",25
relation_extraction,11,221,RESIDE outperforms PCNN + ATT and BGWA which indicates that incorporating side information helps in improving the performance of the model .,"['RESIDE', 'outperforms', 'PCNN', '+', 'ATT', 'and', 'BGWA', 'which', 'indicates', 'that', 'incorporating', 'side', 'information', 'helps', 'in', 'improving', 'the', 'performance', 'of', 'the', 'model', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', 'WDT', 'VBZ', 'IN', 'VBG', 'JJ', 'NN', 'VBZ', 'IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",22
relation_extraction,11,222,The higher performance of BGWA and PCNN + ATT over PCNN shows that attention helps in distant supervised RE .,"['The', 'higher', 'performance', 'of', 'BGWA', 'and', 'PCNN', '+', 'ATT', 'over', 'PCNN', 'shows', 'that', 'attention', 'helps', 'in', 'distant', 'supervised', 'RE', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJR', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNS', 'IN', 'NN', 'VBZ', 'IN', 'NN', 'VBN', 'NNP', '.']",20
relation_extraction,11,230,The results validate that GCNs are effective at encoding syntactic information .,"['The', 'results', 'validate', 'that', 'GCNs', 'are', 'effective', 'at', 'encoding', 'syntactic', 'information', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'IN', 'NNP', 'VBP', 'JJ', 'IN', 'VBG', 'JJ', 'NN', '.']",12
relation_extraction,11,231,"Further , the improvement from side information shows that it is complementary to the features extracted from text , thus validating the central thesis of this paper , that inducing side information leads to improved relation extraction .","['Further', ',', 'the', 'improvement', 'from', 'side', 'information', 'shows', 'that', 'it', 'is', 'complementary', 'to', 'the', 'features', 'extracted', 'from', 'text', ',', 'thus', 'validating', 'the', 'central', 'thesis', 'of', 'this', 'paper', ',', 'that', 'inducing', 'side', 'information', 'leads', 'to', 'improved', 'relation', 'extraction', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'IN', 'PRP', 'VBZ', 'JJ', 'TO', 'DT', 'NNS', 'VBD', 'IN', 'NN', ',', 'RB', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', ',', 'IN', 'VBG', 'JJ', 'NN', 'VBZ', 'TO', 'VBN', 'NN', 'NN', '.']",38
relation_extraction,11,240,We find that the model performs best when aliases are provided by the KB itself .,"['We', 'find', 'that', 'the', 'model', 'performs', 'best', 'when', 'aliases', 'are', 'provided', 'by', 'the', 'KB', 'itself', '.']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NN', 'NNS', 'RB', 'WRB', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'NNP', 'PRP', '.']",16
relation_extraction,11,241,"Overall , we find that RESIDE gives competitive performance even when very limited amount of relation alias information is available .","['Overall', ',', 'we', 'find', 'that', 'RESIDE', 'gives', 'competitive', 'performance', 'even', 'when', 'very', 'limited', 'amount', 'of', 'relation', 'alias', 'information', 'is', 'available', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['JJ', ',', 'PRP', 'VBP', 'IN', 'NNP', 'VBZ', 'JJ', 'NN', 'RB', 'WRB', 'RB', 'JJ', 'NN', 'IN', 'NN', 'NN', 'NN', 'VBZ', 'JJ', '.']",21
relation_extraction,11,242,We observe that performance improves further with the availability of more alias information .,"['We', 'observe', 'that', 'performance', 'improves', 'further', 'with', 'the', 'availability', 'of', 'more', 'alias', 'information', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'VBZ', 'RBR', 'IN', 'DT', 'NN', 'IN', 'JJR', 'JJ', 'NN', '.']",14
relation_extraction,7,2,Neural Relation Extraction via Inner - Sentence Noise Reduction and Transfer Learning,"['Neural', 'Relation', 'Extraction', 'via', 'Inner', '-', 'Sentence', 'Noise', 'Reduction', 'and', 'Transfer', 'Learning']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', ':', 'NN', 'NNP', 'NNP', 'CC', 'NNP', 'NNP']",12
relation_extraction,7,13,Relation extraction aims to extract relations between pairs of marked entities in raw texts .,"['Relation', 'extraction', 'aims', 'to', 'extract', 'relations', 'between', 'pairs', 'of', 'marked', 'entities', 'in', 'raw', 'texts', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'VBZ', 'TO', 'VB', 'NNS', 'IN', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NN', '.']",15
relation_extraction,7,36,"In this paper , we propose a novel word - level approach for distant supervised relation extraction by reducing inner-sentence noise and improving robustness against noisy words .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', 'word', '-', 'level', 'approach', 'for', 'distant', 'supervised', 'relation', 'extraction', 'by', 'reducing', 'inner-sentence', 'noise', 'and', 'improving', 'robustness', 'against', 'noisy', 'words', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', ':', 'NN', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'VBG', 'JJ', 'NN', 'CC', 'VBG', 'NN', 'IN', 'JJ', 'NNS', '.']",28
relation_extraction,7,37,"To reduce innersentence noise , we utilize a novel Sub - Tree Parse ( STP ) method to remove irrelevant words by intercepting a subtree under the parent of entities ' lowest common ancestor .","['To', 'reduce', 'innersentence', 'noise', ',', 'we', 'utilize', 'a', 'novel', 'Sub', '-', 'Tree', 'Parse', '(', 'STP', ')', 'method', 'to', 'remove', 'irrelevant', 'words', 'by', 'intercepting', 'a', 'subtree', 'under', 'the', 'parent', 'of', 'entities', ""'"", 'lowest', 'common', 'ancestor', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NNP', ':', 'CD', 'NNP', '(', 'NNP', ')', 'NN', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'VBG', 'DT', 'JJ', 'IN', 'DT', 'NN', 'IN', 'NNS', 'POS', 'JJS', 'JJ', 'NN', '.']",35
relation_extraction,7,39,"Furthermore , the entity - wise attention is adopted to alleviate the influence of noisy words in the subtree and emphasize the task - relevant features .","['Furthermore', ',', 'the', 'entity', '-', 'wise', 'attention', 'is', 'adopted', 'to', 'alleviate', 'the', 'influence', 'of', 'noisy', 'words', 'in', 'the', 'subtree', 'and', 'emphasize', 'the', 'task', '-', 'relevant', 'features', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', ':', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'VB', 'DT', 'NN', ':', 'JJ', 'NNS', '.']",27
relation_extraction,7,40,"To tackle the second challenge , we initialize our model parameters with a priori knowledge learned from the entity type classification task by transfer learning .","['To', 'tackle', 'the', 'second', 'challenge', ',', 'we', 'initialize', 'our', 'model', 'parameters', 'with', 'a', 'priori', 'knowledge', 'learned', 'from', 'the', 'entity', 'type', 'classification', 'task', 'by', 'transfer', 'learning', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'PRP$', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'VBN', 'IN', 'DT', 'NN', 'JJ', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",26
relation_extraction,7,191,"In the experiment , we utilize word2vec 2 to train word embeddings on NYT corpus .","['In', 'the', 'experiment', ',', 'we', 'utilize', 'word2vec', '2', 'to', 'train', 'word', 'embeddings', 'on', 'NYT', 'corpus', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'CD', 'TO', 'VB', 'NN', 'NNS', 'IN', 'NNP', 'NN', '.']",16
relation_extraction,7,193,"The grid search approach is used to select optimal learning rate lr for Adam optimizer among { 0.1 , 0.001 , 0.0005 , 0.0001 } , GRU size m ? { 100 , 160 , 230 , 400 } , position embedding size l ? { 5 , 10 , 15 , 20}. shows all parameters for our task .","['The', 'grid', 'search', 'approach', 'is', 'used', 'to', 'select', 'optimal', 'learning', 'rate', 'lr', 'for', 'Adam', 'optimizer', 'among', '{', '0.1', ',', '0.001', ',', '0.0005', ',', '0.0001', '}', ',', 'GRU', 'size', 'm', '?', '{', '100', ',', '160', ',', '230', ',', '400', '}', ',', 'position', 'embedding', 'size', 'l', '?', '{', '5', ',', '10', ',', '15', ',', '20}.', 'shows', 'all', 'parameters', 'for', 'our', 'task', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'JJ', 'VBG', 'NN', 'NN', 'IN', 'NNP', 'NN', 'IN', '(', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ')', ',', 'NNP', 'NN', 'NN', '.', '(', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ')', ',', 'NN', 'VBG', 'NN', 'NN', '.', '(', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', 'NNS', 'DT', 'NNS', 'IN', 'PRP$', 'NN', '.']",60
relation_extraction,7,195,GRU size m 230,"['GRU', 'size', 'm', '230']","['B-p', 'I-p', 'I-p', 'B-n']","['NNP', 'NN', 'NN', 'CD']",4
relation_extraction,7,196,Word embedding dimension k 50 POS embedding dimension l 5 Batch size n 50 Entity - Task weights ( ?,"['Word', 'embedding', 'dimension', 'k', '50', 'POS', 'embedding', 'dimension', 'l', '5', 'Batch', 'size', 'n', '50', 'Entity', '-', 'Task', 'weights', '(', '?']","['B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'O']","['NNP', 'VBG', 'NN', 'VBD', 'CD', 'NNP', 'VBG', 'NN', 'NN', 'CD', 'NNP', 'NN', 'RB', 'CD', 'NNP', ':', 'NN', 'NNS', '(', '.']",20
relation_extraction,7,197,"head , ? tail ) 0.5,0.5 Entity - Relation Task weight ?","['head', ',', '?', 'tail', ')', '0.5,0.5', 'Entity', '-', 'Relation', 'Task', 'weight', '?']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O']","['NN', ',', '.', 'NN', ')', 'CD', 'NNP', ':', 'NN', 'NNP', 'NN', '.']",12
relation_extraction,7,198,0.3 Learning rate lr 0.001 Dropout probability p 0.5 l 2 penalty ?,"['0.3', 'Learning', 'rate', 'lr', '0.001', 'Dropout', 'probability', 'p', '0.5', 'l', '2', 'penalty', '?']","['B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'O']","['CD', 'VBG', 'NN', 'NNS', 'CD', 'NNP', 'NN', 'NN', 'CD', 'NN', 'CD', 'NN', '.']",13
relation_extraction,7,199,0.0001,['0.0001'],['B-n'],['CD'],1
relation_extraction,7,205,"From , we can observe that the model with the STP performs best , and the SDP model obtains an even worse result than the pure one .","['From', ',', 'we', 'can', 'observe', 'that', 'the', 'model', 'with', 'the', 'STP', 'performs', 'best', ',', 'and', 'the', 'SDP', 'model', 'obtains', 'an', 'even', 'worse', 'result', 'than', 'the', 'pure', 'one', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', ',', 'PRP', 'MD', 'VB', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NNS', 'JJS', ',', 'CC', 'DT', 'NNP', 'NN', 'VBZ', 'DT', 'RB', 'JJR', 'NN', 'IN', 'DT', 'JJ', 'CD', '.']",28
relation_extraction,7,206,"The PR curve areas of BGRU + SDP and BGRU are about 0.332 and 0.337 respectively , while BGRU + STP increases it to 0.366 .","['The', 'PR', 'curve', 'areas', 'of', 'BGRU', '+', 'SDP', 'and', 'BGRU', 'are', 'about', '0.332', 'and', '0.337', 'respectively', ',', 'while', 'BGRU', '+', 'STP', 'increases', 'it', 'to', '0.366', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['DT', 'NNP', 'NN', 'NNS', 'IN', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', 'VBP', 'IN', 'CD', 'CC', 'CD', 'RB', ',', 'IN', 'NNP', 'NNP', 'NNP', 'VBZ', 'PRP', 'TO', 'CD', '.']",26
relation_extraction,7,207,The result indicates : ( 1 ) Our STP can get rid of irrelevant words in each instance and obtain more precise sentence representation for relation extraction .,"['The', 'result', 'indicates', ':', '(', '1', ')', 'Our', 'STP', 'can', 'get', 'rid', 'of', 'irrelevant', 'words', 'in', 'each', 'instance', 'and', 'obtain', 'more', 'precise', 'sentence', 'representation', 'for', 'relation', 'extraction', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', ':', '(', 'CD', ')', 'PRP$', 'NN', 'MD', 'VB', 'JJ', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CC', 'VB', 'JJR', 'JJ', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",28
relation_extraction,7,209,( 2 ) The SDP method is not appropriate to handle low - quality sentences where key relation words are not in the SDP .,"['(', '2', ')', 'The', 'SDP', 'method', 'is', 'not', 'appropriate', 'to', 'handle', 'low', '-', 'quality', 'sentences', 'where', 'key', 'relation', 'words', 'are', 'not', 'in', 'the', 'SDP', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['(', 'CD', ')', 'DT', 'NNP', 'NN', 'VBZ', 'RB', 'JJ', 'TO', 'VB', 'JJ', ':', 'NN', 'NNS', 'WRB', 'JJ', 'NN', 'NNS', 'VBP', 'RB', 'IN', 'DT', 'NNP', '.']",25
relation_extraction,7,214,"From and , we can obtain : ( 1 ) Regardless of the dataset that we employ , BGRU - WLA ( + STP ) + EWA outperforms BGRU (+ STP ) .","['From', 'and', ',', 'we', 'can', 'obtain', ':', '(', '1', ')', 'Regardless', 'of', 'the', 'dataset', 'that', 'we', 'employ', ',', 'BGRU', '-', 'WLA', '(', '+', 'STP', ')', '+', 'EWA', 'outperforms', 'BGRU', '(+', 'STP', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'CC', ',', 'PRP', 'MD', 'VB', ':', '(', 'CD', ')', 'NNP', 'IN', 'DT', 'NN', 'IN', 'PRP', 'VBP', ',', 'NNP', ':', 'NNP', '(', 'NNP', 'NNP', ')', 'VBP', 'NNP', 'NNS', 'NNP', 'NNP', 'NNP', ')', '.']",33
relation_extraction,7,215,"To be more specific , the PR curve area has a relative improvement of over 2.3 % , which demonstrates that entity - wise hidden states in the BGRU present more precise relational features than other word states .","['To', 'be', 'more', 'specific', ',', 'the', 'PR', 'curve', 'area', 'has', 'a', 'relative', 'improvement', 'of', 'over', '2.3', '%', ',', 'which', 'demonstrates', 'that', 'entity', '-', 'wise', 'hidden', 'states', 'in', 'the', 'BGRU', 'present', 'more', 'precise', 'relational', 'features', 'than', 'other', 'word', 'states', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'RBR', 'JJ', ',', 'DT', 'NNP', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'IN', 'CD', 'NN', ',', 'WDT', 'VBZ', 'IN', 'NN', ':', 'NN', 'JJ', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'RBR', 'JJ', 'NN', 'NNS', 'IN', 'JJ', 'NN', 'NNS', '.']",39
relation_extraction,7,217,"EWA achieves further improvements and outperforms the baseline by over 4.6 % , because it considers more information than entity or relational words alone .","['EWA', 'achieves', 'further', 'improvements', 'and', 'outperforms', 'the', 'baseline', 'by', 'over', '4.6', '%', ',', 'because', 'it', 'considers', 'more', 'information', 'than', 'entity', 'or', 'relational', 'words', 'alone', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'JJ', 'NNS', 'CC', 'VBZ', 'DT', 'NN', 'IN', 'IN', 'CD', 'NN', ',', 'IN', 'PRP', 'VBZ', 'JJR', 'NN', 'IN', 'NN', 'CC', 'JJ', 'NNS', 'RB', '.']",25
relation_extraction,7,227,"( 1 ) Regardless of the dataset that we use , models with TL achieve better performance , which improve the PR curve area by over 4.7 % .","['(', '1', ')', 'Regardless', 'of', 'the', 'dataset', 'that', 'we', 'use', ',', 'models', 'with', 'TL', 'achieve', 'better', 'performance', ',', 'which', 'improve', 'the', 'PR', 'curve', 'area', 'by', 'over', '4.7', '%', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'NNP', 'IN', 'DT', 'NN', 'IN', 'PRP', 'VBP', ',', 'NNS', 'IN', 'NNP', 'VBP', 'JJR', 'NN', ',', 'WDT', 'VBP', 'DT', 'NNP', 'NN', 'NN', 'IN', 'IN', 'CD', 'NN', '.']",29
relation_extraction,7,229,"( 2 ) BGRU + STP + TL achieves the best performance and increases the area to 0.383 , while areas of BGRU , BGRU + STP and BGRU + TL are 0.337 , 0.366 and 0.372 respectively .","['(', '2', ')', 'BGRU', '+', 'STP', '+', 'TL', 'achieves', 'the', 'best', 'performance', 'and', 'increases', 'the', 'area', 'to', '0.383', ',', 'while', 'areas', 'of', 'BGRU', ',', 'BGRU', '+', 'STP', 'and', 'BGRU', '+', 'TL', 'are', '0.337', ',', '0.366', 'and', '0.372', 'respectively', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'CD', ')', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJS', 'NN', 'CC', 'VBZ', 'DT', 'NN', 'TO', 'CD', ',', 'IN', 'NNS', 'IN', 'NNP', ',', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', 'NNP', 'NNP', 'VBP', 'CD', ',', 'CD', 'CC', 'CD', 'RB', '.']",39
relation_extraction,7,233,Mintz proposes the humandesigned feature model .,"['Mintz', 'proposes', 'the', 'humandesigned', 'feature', 'model', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', 'NN', '.']",7
relation_extraction,7,234,MultiR puts forward a graphical model .,"['MultiR', 'puts', 'forward', 'a', 'graphical', 'model', '.']","['B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'RB', 'DT', 'JJ', 'NN', '.']",7
relation_extraction,7,235,MIML proposes a multi -instance multi-label model .,"['MIML', 'proposes', 'a', 'multi', '-instance', 'multi-label', 'model', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', 'JJ', 'NN', '.']",8
relation_extraction,7,236,PCNN puts forward a piecewise CNN for relation extraction .,"['PCNN', 'puts', 'forward', 'a', 'piecewise', 'CNN', 'for', 'relation', 'extraction', '.']","['B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'RB', 'DT', 'NN', 'NNP', 'IN', 'NN', 'NN', '.']",10
relation_extraction,7,237,PCNN + ATT proposes the selective attention mechanism with PCNN .,"['PCNN', '+', 'ATT', 'proposes', 'the', 'selective', 'attention', 'mechanism', 'with', 'PCNN', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NNP', '.']",11
relation_extraction,7,238,BGRU proposes a BGRU with the word - level attention mechanism .,"['BGRU', 'proposes', 'a', 'BGRU', 'with', 'the', 'word', '-', 'level', 'attention', 'mechanism', '.']","['B-n', 'B-p', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'NNP', 'IN', 'DT', 'NN', ':', 'NN', 'NN', 'NN', '.']",12
relation_extraction,4,5,"However , the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly .","['However', ',', 'the', 'ability', 'to', 'populate', 'knowledge', 'bases', 'with', 'facts', 'automatically', 'extracted', 'from', 'documents', 'has', 'improved', 'frustratingly', 'slowly', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'DT', 'NN', 'TO', 'VB', 'NN', 'NNS', 'IN', 'NNS', 'RB', 'VBN', 'IN', 'NNS', 'VBZ', 'VBN', 'RB', 'RB', '.']",19
relation_extraction,4,12,A basic but highly important challenge in natural language understanding is being able to populate a knowledge base with relational facts contained in a piece of text .,"['A', 'basic', 'but', 'highly', 'important', 'challenge', 'in', 'natural', 'language', 'understanding', 'is', 'being', 'able', 'to', 'populate', 'a', 'knowledge', 'base', 'with', 'relational', 'facts', 'contained', 'in', 'a', 'piece', 'of', 'text', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'CC', 'RB', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', 'VBZ', 'VBG', 'JJ', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'JJ', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NN', '.']",28
relation_extraction,4,21,"Existing work on relation extraction ( e.g. , has been unable to achieve sufficient recall or precision for the results to be usable versus hand - constructed knowledge bases .","['Existing', 'work', 'on', 'relation', 'extraction', '(', 'e.g.', ',', 'has', 'been', 'unable', 'to', 'achieve', 'sufficient', 'recall', 'or', 'precision', 'for', 'the', 'results', 'to', 'be', 'usable', 'versus', 'hand', '-', 'constructed', 'knowledge', 'bases', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NN', 'IN', 'NN', 'NN', '(', 'NN', ',', 'VBZ', 'VBN', 'JJ', 'TO', 'VB', 'JJ', 'NN', 'CC', 'NN', 'IN', 'DT', 'NNS', 'TO', 'VB', 'JJ', 'NN', 'NN', ':', 'VBN', 'NN', 'NNS', '.']",30
relation_extraction,4,30,"We propose a new , effective neural network sequence model for relation classification .","['We', 'propose', 'a', 'new', ',', 'effective', 'neural', 'network', 'sequence', 'model', 'for', 'relation', 'classification', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', ',', 'JJ', 'JJ', 'NN', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",14
relation_extraction,4,31,Its architecture is better customized for the slot filling task : the word representations are augmented by extra distributed representations of word position relative to the subject and object of the putative relation .,"['Its', 'architecture', 'is', 'better', 'customized', 'for', 'the', 'slot', 'filling', 'task', ':', 'the', 'word', 'representations', 'are', 'augmented', 'by', 'extra', 'distributed', 'representations', 'of', 'word', 'position', 'relative', 'to', 'the', 'subject', 'and', 'object', 'of', 'the', 'putative', 'relation', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'RBR', 'VBN', 'IN', 'DT', 'NN', 'VBG', 'NN', ':', 'DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'JJ', 'JJ', 'NNS', 'IN', 'NN', 'NN', 'NN', 'TO', 'DT', 'NN', 'CC', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",34
relation_extraction,4,32,This means that the neural attention model can effectively exploit the combination of semantic similarity - based attention and positionbased attention .,"['This', 'means', 'that', 'the', 'neural', 'attention', 'model', 'can', 'effectively', 'exploit', 'the', 'combination', 'of', 'semantic', 'similarity', '-', 'based', 'attention', 'and', 'positionbased', 'attention', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'NN', 'MD', 'RB', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NN', ':', 'VBN', 'NN', 'CC', 'JJ', 'NN', '.']",22
relation_extraction,4,33,"Secondly , we markedly improve the availability of supervised training data by using Mechanical Turk crowd annotation to produce a large supervised training dataset , suitable for the common relations between people , organizations and locations which are used in the TAC KBP evaluations .","['Secondly', ',', 'we', 'markedly', 'improve', 'the', 'availability', 'of', 'supervised', 'training', 'data', 'by', 'using', 'Mechanical', 'Turk', 'crowd', 'annotation', 'to', 'produce', 'a', 'large', 'supervised', 'training', 'dataset', ',', 'suitable', 'for', 'the', 'common', 'relations', 'between', 'people', ',', 'organizations', 'and', 'locations', 'which', 'are', 'used', 'in', 'the', 'TAC', 'KBP', 'evaluations', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'RB', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NNS', 'IN', 'VBG', 'NNP', 'NNP', 'VBP', 'NN', 'TO', 'VB', 'DT', 'JJ', 'JJ', 'NN', 'NN', ',', 'JJ', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NNS', ',', 'NNS', 'CC', 'NNS', 'WDT', 'VBP', 'VBN', 'IN', 'DT', 'NNP', 'NNP', 'NNS', '.']",45
relation_extraction,4,34,"We name this dataset the TAC Relation Extraction Dataset ( TACRED ) , and will make it available through the Linguistic Data Consortium ( LDC ) in order to respect copyrights on the underlying text .","['We', 'name', 'this', 'dataset', 'the', 'TAC', 'Relation', 'Extraction', 'Dataset', '(', 'TACRED', ')', ',', 'and', 'will', 'make', 'it', 'available', 'through', 'the', 'Linguistic', 'Data', 'Consortium', '(', 'LDC', ')', 'in', 'order', 'to', 'respect', 'copyrights', 'on', 'the', 'underlying', 'text', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'VBZ', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'CC', 'MD', 'VB', 'PRP', 'JJ', 'IN', 'DT', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'IN', 'NN', 'TO', 'VB', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",36
relation_extraction,4,120,We map words that occur less than 2 times in the training set to a special < UNK > token .,"['We', 'map', 'words', 'that', 'occur', 'less', 'than', '2', 'times', 'in', 'the', 'training', 'set', 'to', 'a', 'special', '<', 'UNK', '>', 'token', '.']","['O', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNS', 'WDT', 'VBP', 'JJR', 'IN', 'CD', 'NNS', 'IN', 'DT', 'NN', 'VBN', 'TO', 'DT', 'JJ', 'NNP', 'NNP', 'NNP', 'NN', '.']",21
relation_extraction,4,121,We use the pre-trained GloVe vectors to initialize word embeddings .,"['We', 'use', 'the', 'pre-trained', 'GloVe', 'vectors', 'to', 'initialize', 'word', 'embeddings', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NNP', 'NNS', 'TO', 'VB', 'NN', 'NNS', '.']",11
relation_extraction,4,122,"For all the LSTM layers , we find that 2 - layer stacked LSTMs generally work better than one - layer LSTMs .","['For', 'all', 'the', 'LSTM', 'layers', ',', 'we', 'find', 'that', '2', '-', 'layer', 'stacked', 'LSTMs', 'generally', 'work', 'better', 'than', 'one', '-', 'layer', 'LSTMs', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'PDT', 'DT', 'NNP', 'NNS', ',', 'PRP', 'VBP', 'IN', 'CD', ':', 'NN', 'VBD', 'NNP', 'RB', 'VBP', 'JJR', 'IN', 'CD', ':', 'NN', 'NNP', '.']",23
relation_extraction,4,123,We minimize cross - entropy loss over all 42 relations using AdaGrad .,"['We', 'minimize', 'cross', '-', 'entropy', 'loss', 'over', 'all', '42', 'relations', 'using', 'AdaGrad', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'JJ', ':', 'JJ', 'NN', 'IN', 'DT', 'CD', 'NNS', 'VBG', 'NNP', '.']",13
relation_extraction,4,124,We apply Dropout with p = 0.5 to CNNs and LSTMs .,"['We', 'apply', 'Dropout', 'with', 'p', '=', '0.5', 'to', 'CNNs', 'and', 'LSTMs', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'IN', 'NN', '$', 'CD', 'TO', 'NNP', 'CC', 'NNP', '.']",12
relation_extraction,4,125,During training we also find a word dropout strategy to be very effective : we randomly set a token to be < UNK > with a probability p.,"['During', 'training', 'we', 'also', 'find', 'a', 'word', 'dropout', 'strategy', 'to', 'be', 'very', 'effective', ':', 'we', 'randomly', 'set', 'a', 'token', 'to', 'be', '<', 'UNK', '>', 'with', 'a', 'probability', 'p.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n']","['IN', 'NN', 'PRP', 'RB', 'VBP', 'DT', 'NN', 'NN', 'NN', 'TO', 'VB', 'RB', 'JJ', ':', 'PRP', 'VBP', 'VBN', 'DT', 'NN', 'TO', 'VB', 'JJ', 'NNP', 'NN', 'IN', 'DT', 'NN', 'NN']",28
relation_extraction,4,126,We set p to be 0.06 for the SDP - LSTM model and 0.04 for all other models .,"['We', 'set', 'p', 'to', 'be', '0.06', 'for', 'the', 'SDP', '-', 'LSTM', 'model', 'and', '0.04', 'for', 'all', 'other', 'models', '.']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNS', 'TO', 'VB', 'CD', 'IN', 'DT', 'NNP', ':', 'NNP', 'NN', 'CC', 'CD', 'IN', 'DT', 'JJ', 'NNS', '.']",19
relation_extraction,4,141,"We observe that all neural models achieve higher F 1 scores than the logistic regression and patterns systems , which demonstrates the effectiveness of neural models for relation extraction .","['We', 'observe', 'that', 'all', 'neural', 'models', 'achieve', 'higher', 'F', '1', 'scores', 'than', 'the', 'logistic', 'regression', 'and', 'patterns', 'systems', ',', 'which', 'demonstrates', 'the', 'effectiveness', 'of', 'neural', 'models', 'for', 'relation', 'extraction', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'JJ', 'NNS', 'VBP', 'JJR', 'NNP', 'CD', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'CC', 'NNS', 'NNS', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NN', 'NN', '.']",30
relation_extraction,4,142,"Although positional embeddings help increase the F 1 by around 2 % over the plain CNN model , a simple ( 2 - layer ) LSTM model performs surprisingly better than CNN and dependency - based models .","['Although', 'positional', 'embeddings', 'help', 'increase', 'the', 'F', '1', 'by', 'around', '2', '%', 'over', 'the', 'plain', 'CNN', 'model', ',', 'a', 'simple', '(', '2', '-', 'layer', ')', 'LSTM', 'model', 'performs', 'surprisingly', 'better', 'than', 'CNN', 'and', 'dependency', '-', 'based', 'models', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', 'NNS', 'NN', 'VB', 'DT', 'NNP', 'CD', 'IN', 'IN', 'CD', 'NN', 'IN', 'DT', 'NN', 'NNP', 'NN', ',', 'DT', 'NN', '(', 'CD', ':', 'NN', ')', 'NNP', 'NN', 'NNS', 'RB', 'JJR', 'IN', 'NNP', 'CC', 'NN', ':', 'VBN', 'NNS', '.']",38
relation_extraction,4,143,"Lastly , our proposed position - aware mechanism is very effective and achieves an F 1 score of 65.4 % , with an absolute increase of 3.9 % over the best baseline neural model ( LSTM ) and 7.9 % over the baseline logistic regression system .","['Lastly', ',', 'our', 'proposed', 'position', '-', 'aware', 'mechanism', 'is', 'very', 'effective', 'and', 'achieves', 'an', 'F', '1', 'score', 'of', '65.4', '%', ',', 'with', 'an', 'absolute', 'increase', 'of', '3.9', '%', 'over', 'the', 'best', 'baseline', 'neural', 'model', '(', 'LSTM', ')', 'and', '7.9', '%', 'over', 'the', 'baseline', 'logistic', 'regression', 'system', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'VBN', 'NN', ':', 'JJ', 'NN', 'VBZ', 'RB', 'JJ', 'CC', 'VBZ', 'DT', 'NNP', 'CD', 'NN', 'IN', 'CD', 'NN', ',', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NN', 'IN', 'DT', 'JJS', 'JJ', 'JJ', 'NN', '(', 'NNP', ')', 'CC', 'CD', 'NN', 'IN', 'DT', 'NN', 'JJ', 'NN', 'NN', '.']",47
relation_extraction,4,144,We also run an ensemble of our position - aware attention model which takes majority votes from 5 runs with random initializations and it further pushes the F 1 score up by 1.6 % .,"['We', 'also', 'run', 'an', 'ensemble', 'of', 'our', 'position', '-', 'aware', 'attention', 'model', 'which', 'takes', 'majority', 'votes', 'from', '5', 'runs', 'with', 'random', 'initializations', 'and', 'it', 'further', 'pushes', 'the', 'F', '1', 'score', 'up', 'by', '1.6', '%', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', 'IN', 'PRP$', 'NN', ':', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'NN', 'NNS', 'IN', 'CD', 'NNS', 'IN', 'JJ', 'NNS', 'CC', 'PRP', 'RB', 'VBZ', 'DT', 'NNP', 'CD', 'NN', 'RB', 'IN', 'CD', 'NN', '.']",35
relation_extraction,4,146,CNN - based models tend to have higher precision ; RNN - based models have better recall .,"['CNN', '-', 'based', 'models', 'tend', 'to', 'have', 'higher', 'precision', ';', 'RNN', '-', 'based', 'models', 'have', 'better', 'recall', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ':', 'VBN', 'NNS', 'VBP', 'TO', 'VB', 'JJR', 'NN', ':', 'NNP', ':', 'VBN', 'NNS', 'VBP', 'JJR', 'NN', '.']",18
relation_extraction,4,156,"Evaluating relation extraction systems on slot filling is particularly challenging in that : ( 1 ) Endto - end cold start slot filling scores conflate the performance of all modules in the system ( i.e. , entity recognizer , entity linker and relation extractor ) .","['Evaluating', 'relation', 'extraction', 'systems', 'on', 'slot', 'filling', 'is', 'particularly', 'challenging', 'in', 'that', ':', '(', '1', ')', 'Endto', '-', 'end', 'cold', 'start', 'slot', 'filling', 'scores', 'conflate', 'the', 'performance', 'of', 'all', 'modules', 'in', 'the', 'system', '(', 'i.e.', ',', 'entity', 'recognizer', ',', 'entity', 'linker', 'and', 'relation', 'extractor', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'NN', 'NN', 'NNS', 'IN', 'NN', 'NN', 'VBZ', 'RB', 'JJ', 'IN', 'DT', ':', '(', 'CD', ')', 'NNP', ':', 'NN', 'JJ', 'NN', 'NN', 'VBG', 'NNS', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', '(', 'FW', ',', 'NN', 'NN', ',', 'NN', 'NN', 'CC', 'NN', 'NN', ')', '.']",46
relation_extraction,4,157,( 2 ) Errors in hop - 0 predictions can easily propagate to hop - 1 predictions .,"['(', '2', ')', 'Errors', 'in', 'hop', '-', '0', 'predictions', 'can', 'easily', 'propagate', 'to', 'hop', '-', '1', 'predictions', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'NNS', 'IN', 'NN', ':', 'CD', 'NNS', 'MD', 'RB', 'VB', 'TO', 'VB', ':', 'CD', 'NNS', '.']",18
relation_extraction,4,163,"We find that : ( 1 ) by only training our logistic regression model on TACRED ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -","['We', 'find', 'that', ':', '(', '1', ')', 'by', 'only', 'training', 'our', 'logistic', 'regression', 'model', 'on', 'TACRED', '(', 'in', 'contrast', 'to', 'on', 'the', '2', 'million', 'bootstrapped', 'examples', 'used', 'in', 'the', '2015', 'Stanford', 'system', ')', 'and', 'combining', 'it', 'with', 'patterns', ',', 'we', 'obtain', 'a', 'higher', 'hop', '-', '0', 'F', '1', 'score', 'than', 'the', '2015', 'Stanford', 'sys', '-']","['O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', ':', '(', 'CD', ')', 'IN', 'RB', 'VBG', 'PRP$', 'JJ', 'NN', 'NN', 'IN', 'NNP', '(', 'IN', 'NN', 'TO', 'IN', 'DT', 'CD', 'CD', 'VBD', 'NNS', 'VBN', 'IN', 'DT', 'CD', 'NNP', 'NN', ')', 'CC', 'VBG', 'PRP', 'IN', 'NNS', ',', 'PRP', 'VB', 'DT', 'JJR', 'NN', ':', 'CD', 'NNP', 'CD', 'NN', 'IN', 'DT', 'CD', 'NNP', 'VBD', ':']",55
relation_extraction,4,166,presents the results of an ablation test of our position - aware attention model on the development set of TACRED .,"['presents', 'the', 'results', 'of', 'an', 'ablation', 'test', 'of', 'our', 'position', '-', 'aware', 'attention', 'model', 'on', 'the', 'development', 'set', 'of', 'TACRED', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'PRP$', 'NN', ':', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'NNP', '.']",21
relation_extraction,4,167,"The entire attention mechanism contributes about 1.5 % F 1 , where the position - aware term in Eq.","['The', 'entire', 'attention', 'mechanism', 'contributes', 'about', '1.5', '%', 'F', '1', ',', 'where', 'the', 'position', '-', 'aware', 'term', 'in', 'Eq.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'IN', 'CD', 'NN', 'NNP', 'CD', ',', 'WRB', 'DT', 'NN', ':', 'JJ', 'NN', 'IN', 'NNP']",19
relation_extraction,4,168,( 3 ) alone contributes about 1 % F 1 score .,"['(', '3', ')', 'alone', 'contributes', 'about', '1', '%', 'F', '1', 'score', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'RB', 'VBZ', 'IN', 'CD', 'NN', 'NNP', 'CD', 'NN', '.']",12
relation_extraction,4,170,"shows how the slot filling evaluation scores change as we change the amount of negative ( i.e. , no relation ) training data provided to our proposed model .","['shows', 'how', 'the', 'slot', 'filling', 'evaluation', 'scores', 'change', 'as', 'we', 'change', 'the', 'amount', 'of', 'negative', '(', 'i.e.', ',', 'no', 'relation', ')', 'training', 'data', 'provided', 'to', 'our', 'proposed', 'model', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['VBZ', 'WRB', 'DT', 'NN', 'VBG', 'NN', 'NNS', 'VBP', 'IN', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'JJ', '(', 'FW', ',', 'DT', 'NN', ')', 'NN', 'NNS', 'VBD', 'TO', 'PRP$', 'VBN', 'NN', '.']",29
relation_extraction,4,171,"We find that : ( 1 ) At hop - 0 level , precision increases as we provide more negative examples , while recall stays almost unchanged .","['We', 'find', 'that', ':', '(', '1', ')', 'At', 'hop', '-', '0', 'level', ',', 'precision', 'increases', 'as', 'we', 'provide', 'more', 'negative', 'examples', ',', 'while', 'recall', 'stays', 'almost', 'unchanged', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', ':', '(', 'CD', ')', 'IN', 'NN', ':', 'CD', 'NN', ',', 'NN', 'NNS', 'IN', 'PRP', 'VBP', 'RBR', 'JJ', 'NNS', ',', 'IN', 'NN', 'VBZ', 'RB', 'JJ', '.']",28
relation_extraction,4,172,F 1 score keeps increasing .,"['F', '1', 'score', 'keeps', 'increasing', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', 'CD', 'NN', 'NNS', 'VBG', '.']",6
relation_extraction,4,173,"( 2 ) At hop - all level , F 1 score increases by Performance by sentence length .","['(', '2', ')', 'At', 'hop', '-', 'all', 'level', ',', 'F', '1', 'score', 'increases', 'by', 'Performance', 'by', 'sentence', 'length', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['(', 'CD', ')', 'IN', 'NN', ':', 'DT', 'NN', ',', 'NNP', 'CD', 'NN', 'NNS', 'IN', 'NNP', 'IN', 'NN', 'NN', '.']",19
relation_extraction,4,175,We find that : ( 1 ) Performance of all models degrades substantially as the sentences get longer .,"['We', 'find', 'that', ':', '(', '1', ')', 'Performance', 'of', 'all', 'models', 'degrades', 'substantially', 'as', 'the', 'sentences', 'get', 'longer', '.']","['O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'IN', ':', '(', 'CD', ')', 'NN', 'IN', 'DT', 'NNS', 'NNS', 'RB', 'IN', 'DT', 'NNS', 'VBP', 'RB', '.']",19
relation_extraction,4,178,"When compared with the CNN - PE model , our position - aware attention model achieves improved F 1 scores on 30 out of the 41 slot types , with the top 5 slot types being org : members , per: country of death , org : shareholders , per:children and per:religion .","['When', 'compared', 'with', 'the', 'CNN', '-', 'PE', 'model', ',', 'our', 'position', '-', 'aware', 'attention', 'model', 'achieves', 'improved', 'F', '1', 'scores', 'on', '30', 'out', 'of', 'the', '41', 'slot', 'types', ',', 'with', 'the', 'top', '5', 'slot', 'types', 'being', 'org', ':', 'members', ',', 'per:', 'country', 'of', 'death', ',', 'org', ':', 'shareholders', ',', 'per:children', 'and', 'per:religion', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O']","['WRB', 'VBN', 'IN', 'DT', 'NNP', ':', 'NNP', 'NN', ',', 'PRP$', 'NN', ':', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'NNP', 'CD', 'NNS', 'IN', 'CD', 'IN', 'IN', 'DT', 'CD', 'NN', 'NNS', ',', 'IN', 'DT', 'JJ', 'CD', 'NN', 'NNS', 'VBG', 'NN', ':', 'NNS', ',', 'JJ', 'NN', 'IN', 'NN', ',', 'NN', ':', 'NNS', ',', 'NNS', 'CC', 'NN', '.']",53
relation_extraction,4,179,"When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and per: alternate names .","['When', 'compared', 'with', 'SDP', '-', 'LSTM', 'model', ',', 'our', 'model', 'achieves', 'improved', 'F', '1', 'scores', 'on', '26', 'out', 'of', 'the', '41', 'slot', 'types', ',', 'with', 'the', 'top', '5', 'slot', 'types', 'being', 'org', ':', 'political', '/', 'religious', 'affiliation', ',', 'per:', 'country', 'of', 'death', ',', 'org', ':', 'alternate', 'names', ',', 'per:religion', 'and', 'per:', 'alternate', 'names', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['WRB', 'VBN', 'IN', 'NNP', ':', 'NNP', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'VBN', 'NNP', 'CD', 'NNS', 'IN', 'CD', 'IN', 'IN', 'DT', 'CD', 'NN', 'NNS', ',', 'IN', 'DT', 'JJ', 'CD', 'NN', 'NNS', 'VBG', 'NN', ':', 'JJ', 'NN', 'JJ', 'NN', ',', 'JJ', 'NN', 'IN', 'NN', ',', 'NN', ':', 'NN', 'NNS', ',', 'NN', 'CC', 'NN', 'NN', 'NNS', '.']",54
relation_extraction,4,180,We observe that slot types with relatively sparse training examples tend to be improved by using the position - aware attention model .,"['We', 'observe', 'that', 'slot', 'types', 'with', 'relatively', 'sparse', 'training', 'examples', 'tend', 'to', 'be', 'improved', 'by', 'using', 'the', 'position', '-', 'aware', 'attention', 'model', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'VBZ', 'IN', 'RB', 'JJ', 'NN', 'NNS', 'VBP', 'TO', 'VB', 'VBN', 'IN', 'VBG', 'DT', 'NN', ':', 'JJ', 'NN', 'NN', '.']",23
relation_extraction,4,183,"We find that the model learns to pay more attention to words that are informative for the relation ( e.g. , "" graduated from "" , "" niece "" and "" chairman "" ) , though it still makes mistakes ( e.g. , "" refused to name the three "" ) .","['We', 'find', 'that', 'the', 'model', 'learns', 'to', 'pay', 'more', 'attention', 'to', 'words', 'that', 'are', 'informative', 'for', 'the', 'relation', '(', 'e.g.', ',', '""', 'graduated', 'from', '""', ',', '""', 'niece', '""', 'and', '""', 'chairman', '""', ')', ',', 'though', 'it', 'still', 'makes', 'mistakes', '(', 'e.g.', ',', '""', 'refused', 'to', 'name', 'the', 'three', '""', ')', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'JJR', 'NN', 'TO', 'NNS', 'WDT', 'VBP', 'JJ', 'IN', 'DT', 'NN', '(', 'JJ', ',', 'RB', 'VBN', 'IN', 'NN', ',', 'NNP', 'CC', 'NNP', 'CC', 'NNP', 'NN', 'NN', ')', ',', 'IN', 'PRP', 'RB', 'VBZ', 'NNS', '(', 'NN', ',', 'NNP', 'VBD', 'TO', 'VB', 'DT', 'CD', 'NN', ')', '.']",52
relation_extraction,4,184,"We also observe that the model tends to put a lot of weight onto object entities , as the object NER signatures are very informative to the classification of relations .","['We', 'also', 'observe', 'that', 'the', 'model', 'tends', 'to', 'put', 'a', 'lot', 'of', 'weight', 'onto', 'object', 'entities', ',', 'as', 'the', 'object', 'NER', 'signatures', 'are', 'very', 'informative', 'to', 'the', 'classification', 'of', 'relations', '.']","['O', 'O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'IN', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', 'IN', 'JJ', 'NNS', ',', 'IN', 'DT', 'NN', 'NNP', 'NNS', 'VBP', 'RB', 'JJ', 'TO', 'DT', 'NN', 'IN', 'NNS', '.']",31
relation_extraction,6,2,Context - Aware Representations for Knowledge Base Relation Extraction,"['Context', '-', 'Aware', 'Representations', 'for', 'Knowledge', 'Base', 'Relation', 'Extraction']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'JJ', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'NNP']",9
relation_extraction,6,4,We demonstrate that for sentence - level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation .,"['We', 'demonstrate', 'that', 'for', 'sentence', '-', 'level', 'relation', 'extraction', 'it', 'is', 'beneficial', 'to', 'consider', 'other', 'relations', 'in', 'the', 'sentential', 'context', 'while', 'predicting', 'the', 'target', 'relation', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'IN', 'NN', ':', 'NN', 'NN', 'NN', 'PRP', 'VBZ', 'JJ', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'NN', 'NN', '.']",26
relation_extraction,6,12,The main goal of relation extraction is to determine a type of relation between two target entities that appear together in a text .,"['The', 'main', 'goal', 'of', 'relation', 'extraction', 'is', 'to', 'determine', 'a', 'type', 'of', 'relation', 'between', 'two', 'target', 'entities', 'that', 'appear', 'together', 'in', 'a', 'text', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', 'IN', 'CD', 'NN', 'NNS', 'WDT', 'VBP', 'RB', 'IN', 'DT', 'NN', '.']",24
relation_extraction,6,13,"In this paper , we consider the sentential relation extraction task : to each occurrence of the target entity pair e 1 , e 2 in some sentence s one has to assign a relation type r from a given set R. A triple e 1 , r , e 2 is called a relation instance and we refer to the relation of the target entity pair as target relation .","['In', 'this', 'paper', ',', 'we', 'consider', 'the', 'sentential', 'relation', 'extraction', 'task', ':', 'to', 'each', 'occurrence', 'of', 'the', 'target', 'entity', 'pair', 'e', '1', ',', 'e', '2', 'in', 'some', 'sentence', 's', 'one', 'has', 'to', 'assign', 'a', 'relation', 'type', 'r', 'from', 'a', 'given', 'set', 'R.', 'A', 'triple', 'e', '1', ',', 'r', ',', 'e', '2', 'is', 'called', 'a', 'relation', 'instance', 'and', 'we', 'refer', 'to', 'the', 'relation', 'of', 'the', 'target', 'entity', 'pair', 'as', 'target', 'relation', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'NN', ':', 'TO', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'NN', 'VBZ', 'CD', ',', 'RB', 'CD', 'IN', 'DT', 'NN', 'VBD', 'CD', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'NN', 'NN', 'IN', 'DT', 'VBN', 'NN', 'NNP', 'NNP', 'JJ', 'NN', 'CD', ',', 'NN', ',', 'NN', 'CD', 'VBZ', 'VBN', 'DT', 'NN', 'NN', 'CC', 'PRP', 'VBP', 'TO', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",71
relation_extraction,6,20,We present a novel architecture that considers other relations in the sentence as a context for predicting the label of the target relation .,"['We', 'present', 'a', 'novel', 'architecture', 'that', 'considers', 'other', 'relations', 'in', 'the', 'sentence', 'as', 'a', 'context', 'for', 'predicting', 'the', 'label', 'of', 'the', 'target', 'relation', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'VBZ', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",24
relation_extraction,6,22,Our architecture uses an LSTM - based encoder to jointly learn representations for all relations in a single sentence .,"['Our', 'architecture', 'uses', 'an', 'LSTM', '-', 'based', 'encoder', 'to', 'jointly', 'learn', 'representations', 'for', 'all', 'relations', 'in', 'a', 'single', 'sentence', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'DT', 'NNP', ':', 'VBN', 'NN', 'TO', 'RB', 'VB', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",20
relation_extraction,6,23,The representation of the target relation and representations of the context relations are combined to make the final prediction .,"['The', 'representation', 'of', 'the', 'target', 'relation', 'and', 'representations', 'of', 'the', 'context', 'relations', 'are', 'combined', 'to', 'make', 'the', 'final', 'prediction', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'NNS', 'IN', 'DT', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NN', '.']",20
relation_extraction,6,99,All models were trained using the Adam optimizer with categorical crossentropy as the loss function .,"['All', 'models', 'were', 'trained', 'using', 'the', 'Adam', 'optimizer', 'with', 'categorical', 'crossentropy', 'as', 'the', 'loss', 'function', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'VBD', 'VBN', 'VBG', 'DT', 'NNP', 'NN', 'IN', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",16
relation_extraction,6,100,We use an early stopping criterion on the validation data to determine the number of training epochs .,"['We', 'use', 'an', 'early', 'stopping', 'criterion', 'on', 'the', 'validation', 'data', 'to', 'determine', 'the', 'number', 'of', 'training', 'epochs', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'NNS', 'TO', 'VB', 'DT', 'NN', 'IN', 'VBG', 'NN', '.']",18
relation_extraction,6,101,"The learning rate is fixed to 0.01 and the rest of the optimization parameters are set as recommended in : ? 1 = 0.9 , ? 2 = 0.999 , ? = 1e ? 08 . The training is performed in batches of 128 instances .","['The', 'learning', 'rate', 'is', 'fixed', 'to', '0.01', 'and', 'the', 'rest', 'of', 'the', 'optimization', 'parameters', 'are', 'set', 'as', 'recommended', 'in', ':', '?', '1', '=', '0.9', ',', '?', '2', '=', '0.999', ',', '?', '=', '1e', '?', '08', '.', 'The', 'training', 'is', 'performed', 'in', 'batches', 'of', '128', 'instances', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'CC', 'DT', 'NN', 'IN', 'DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'VBN', 'IN', ':', '.', 'CD', 'NN', 'CD', ',', '.', 'CD', 'NN', 'CD', ',', '.', '$', 'CD', '.', 'CD', '.', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'NNS', 'IN', 'CD', 'NNS', '.']",46
relation_extraction,6,102,We apply Dropout on the penultimate layer as well as on the embeddings layer with a probability of 0.5 .,"['We', 'apply', 'Dropout', 'on', 'the', 'penultimate', 'layer', 'as', 'well', 'as', 'on', 'the', 'embeddings', 'layer', 'with', 'a', 'probability', 'of', '0.5', '.']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'NNP', 'IN', 'DT', 'NN', 'NN', 'RB', 'RB', 'IN', 'IN', 'DT', 'NNS', 'NN', 'IN', 'DT', 'NN', 'IN', 'CD', '.']",20
relation_extraction,6,103,We choose the size of the layers ( RNN layer size o = 256 ) and entity marker embeddings ( d = 3 ) with a random search on the validation set .,"['We', 'choose', 'the', 'size', 'of', 'the', 'layers', '(', 'RNN', 'layer', 'size', 'o', '=', '256', ')', 'and', 'entity', 'marker', 'embeddings', '(', 'd', '=', '3', ')', 'with', 'a', 'random', 'search', 'on', 'the', 'validation', 'set', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NNS', '(', 'NNP', 'NN', 'NN', 'NN', 'NNP', 'CD', ')', 'CC', 'NN', 'NN', 'NNS', '(', 'VB', 'RB', 'CD', ')', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",33
relation_extraction,6,113,"The models that take the context into account perform similar to the baselines at the smallest recall numbers , but start to positively deviate from them at higher recall rates .","['The', 'models', 'that', 'take', 'the', 'context', 'into', 'account', 'perform', 'similar', 'to', 'the', 'baselines', 'at', 'the', 'smallest', 'recall', 'numbers', ',', 'but', 'start', 'to', 'positively', 'deviate', 'from', 'them', 'at', 'higher', 'recall', 'rates', '.']","['O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'WDT', 'VBP', 'DT', 'NN', 'IN', 'NN', 'NN', 'JJ', 'TO', 'DT', 'NNS', 'IN', 'DT', 'JJS', 'NN', 'NNS', ',', 'CC', 'VBP', 'TO', 'RB', 'VB', 'IN', 'PRP', 'IN', 'JJR', 'NN', 'NNS', '.']",31
relation_extraction,6,114,"In particular , the ContextAtt model performs better than any other system in our study over the entire recall range .","['In', 'particular', ',', 'the', 'ContextAtt', 'model', 'performs', 'better', 'than', 'any', 'other', 'system', 'in', 'our', 'study', 'over', 'the', 'entire', 'recall', 'range', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', ',', 'DT', 'NNP', 'NN', 'NNS', 'RBR', 'IN', 'DT', 'JJ', 'NN', 'IN', 'PRP$', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",21
relation_extraction,6,115,"Compared to the competitive LSTM - baseline that uses the same relation encoder , the ContextAtt model achieves a 24 % reduction of the average error : from 0.2096 0.002 to 0.1590 0.002 .","['Compared', 'to', 'the', 'competitive', 'LSTM', '-', 'baseline', 'that', 'uses', 'the', 'same', 'relation', 'encoder', ',', 'the', 'ContextAtt', 'model', 'achieves', 'a', '24', '%', 'reduction', 'of', 'the', 'average', 'error', ':', 'from', '0.2096', '0.002', 'to', '0.1590', '0.002', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'TO', 'DT', 'JJ', 'NNP', ':', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'NN', ',', 'DT', 'NNP', 'NN', 'VBZ', 'DT', 'CD', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'IN', 'CD', 'CD', 'TO', 'CD', 'CD', '.']",34
relation_extraction,6,118,shows that the ContextAtt model performs best over all relation types .,"['shows', 'that', 'the', 'ContextAtt', 'model', 'performs', 'best', 'over', 'all', 'relation', 'types', '.']","['B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNS', 'IN', 'DT', 'NNP', 'NN', 'NNS', 'RBS', 'IN', 'DT', 'NN', 'NNS', '.']",12
relation_extraction,6,119,One can also see that the ContextSum does n't universally outperforms the LSTM - baseline .,"['One', 'can', 'also', 'see', 'that', 'the', 'ContextSum', 'does', ""n't"", 'universally', 'outperforms', 'the', 'LSTM', '-', 'baseline', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['CD', 'MD', 'RB', 'VB', 'IN', 'DT', 'NNP', 'VBZ', 'RB', 'RB', 'VBZ', 'DT', 'NNP', ':', 'NN', '.']",16
relation_extraction,6,120,It demonstrates again that using attention is crucial to extract relevant information from the context relations .,"['It', 'demonstrates', 'again', 'that', 'using', 'attention', 'is', 'crucial', 'to', 'extract', 'relevant', 'information', 'from', 'the', 'context', 'relations', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBZ', 'RB', 'IN', 'VBG', 'NN', 'VBZ', 'JJ', 'TO', 'VB', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NNS', '.']",17
relation_extraction,6,121,"On the relation - specific results we observe that the context - enabled model demonstrates the most improvement on precision and seems to be especially useful for taxonomy relations ( see SUBCLASS OF , PART OF ) .","['On', 'the', 'relation', '-', 'specific', 'results', 'we', 'observe', 'that', 'the', 'context', '-', 'enabled', 'model', 'demonstrates', 'the', 'most', 'improvement', 'on', 'precision', 'and', 'seems', 'to', 'be', 'especially', 'useful', 'for', 'taxonomy', 'relations', '(', 'see', 'SUBCLASS', 'OF', ',', 'PART', 'OF', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ':', 'JJ', 'NNS', 'PRP', 'VBP', 'IN', 'DT', 'NN', ':', 'JJ', 'NN', 'VBZ', 'DT', 'RBS', 'NN', 'IN', 'NN', 'CC', 'VBZ', 'TO', 'VB', 'RB', 'JJ', 'IN', 'JJ', 'NNS', '(', 'VB', 'NNP', 'NNP', ',', 'NNP', 'NNP', ')', '.']",38
relation_extraction,10,2,Span - Level Model for Relation Extraction,"['Span', '-', 'Level', 'Model', 'for', 'Relation', 'Extraction']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', ':', 'NN', 'NNP', 'IN', 'NNP', 'NNP']",7
relation_extraction,10,13,"This paper focuses on Relation Extraction ( RE ) , which is the task of entity mention detection and classifying the relations between each pair of those mentions .","['This', 'paper', 'focuses', 'on', 'Relation', 'Extraction', '(', 'RE', ')', ',', 'which', 'is', 'the', 'task', 'of', 'entity', 'mention', 'detection', 'and', 'classifying', 'the', 'relations', 'between', 'each', 'pair', 'of', 'those', 'mentions', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'IN', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'NN', 'NN', 'NN', 'CC', 'VBG', 'DT', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNS', '.']",29
relation_extraction,10,21,"Since , work on RE has revolved around end - to - end systems : single models which first perform entity mention detection and then relation extraction .","['Since', ',', 'work', 'on', 'RE', 'has', 'revolved', 'around', 'end', '-', 'to', '-', 'end', 'systems', ':', 'single', 'models', 'which', 'first', 'perform', 'entity', 'mention', 'detection', 'and', 'then', 'relation', 'extraction', '.']","['O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', ',', 'NN', 'IN', 'NNP', 'VBZ', 'VBN', 'IN', 'NN', ':', 'TO', ':', 'NN', 'NNS', ':', 'JJ', 'NNS', 'WDT', 'RB', 'VBP', 'NN', 'NN', 'NN', 'CC', 'RB', 'NN', 'NN', '.']",28
relation_extraction,10,36,We propose a simple bi - LSTM based model which generates span representations for each possible span .,"['We', 'propose', 'a', 'simple', 'bi', '-', 'LSTM', 'based', 'model', 'which', 'generates', 'span', 'representations', 'for', 'each', 'possible', 'span', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', ':', 'NNP', 'VBN', 'NN', 'WDT', 'VBZ', 'VBP', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",18
relation_extraction,10,37,The span representations are used to perform entity mention detection on all spans in parallel .,"['The', 'span', 'representations', 'are', 'used', 'to', 'perform', 'entity', 'mention', 'detection', 'on', 'all', 'spans', 'in', 'parallel', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'VB', 'NN', 'NN', 'NN', 'IN', 'DT', 'NNS', 'IN', 'NN', '.']",16
relation_extraction,10,38,The same span representations are then used to perform relation extraction on all pairs of detected entity mentions .,"['The', 'same', 'span', 'representations', 'are', 'then', 'used', 'to', 'perform', 'relation', 'extraction', 'on', 'all', 'pairs', 'of', 'detected', 'entity', 'mentions', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'TO', 'VB', 'NN', 'NN', 'IN', 'DT', 'NNS', 'IN', 'JJ', 'NN', 'NNS', '.']",19
relation_extraction,10,143,"The learned character embeddings are of size 8 . 1 - dimensional convolutions of window size 3 , 4,5 are applied per-token with 50 filters of each window size .","['The', 'learned', 'character', 'embeddings', 'are', 'of', 'size', '8', '.', '1', '-', 'dimensional', 'convolutions', 'of', 'window', 'size', '3', ',', '4,5', 'are', 'applied', 'per-token', 'with', '50', 'filters', 'of', 'each', 'window', 'size', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NNS', 'VBP', 'IN', 'NN', 'CD', '.', 'CD', ':', 'JJ', 'NNS', 'IN', 'NN', 'NN', 'CD', ',', 'CD', 'VBP', 'VBN', 'JJ', 'IN', 'CD', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",30
relation_extraction,10,146,Our stacked bi - LSTMs ( Section 3.1 ) has 3 layers with 200 - dimensional hidden states and highway connections .,"['Our', 'stacked', 'bi', '-', 'LSTMs', '(', 'Section', '3.1', ')', 'has', '3', 'layers', 'with', '200', '-', 'dimensional', 'hidden', 'states', 'and', 'highway', 'connections', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'VBD', 'SYM', ':', 'NNP', '(', 'NNP', 'CD', ')', 'VBZ', 'CD', 'NNS', 'IN', 'CD', ':', 'JJ', 'JJ', 'NNS', 'CC', 'NN', 'NNS', '.']",22
relation_extraction,10,147,"All Multi Layer Perceptrons ( MLP ) has two hidden layers with 500 dimensions , each followed by ReLU activation .","['All', 'Multi', 'Layer', 'Perceptrons', '(', 'MLP', ')', 'has', 'two', 'hidden', 'layers', 'with', '500', 'dimensions', ',', 'each', 'followed', 'by', 'ReLU', 'activation', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'VBZ', 'CD', 'JJ', 'NNS', 'IN', 'CD', 'NNS', ',', 'DT', 'VBN', 'IN', 'NNP', 'NN', '.']",21
relation_extraction,10,152,We only consider spans that are entirely within a sentence and limit spans to a max length of L = 10 .,"['We', 'only', 'consider', 'spans', 'that', 'are', 'entirely', 'within', 'a', 'sentence', 'and', 'limit', 'spans', 'to', 'a', 'max', 'length', 'of', 'L', '=', '10', '.']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VB', 'NNS', 'WDT', 'VBP', 'RB', 'IN', 'DT', 'NN', 'CC', 'NN', 'NNS', 'TO', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'CD', '.']",22
relation_extraction,10,155,"Regularization Dropout is applied with dropout rate 0.2 to all hidden layers of all MLPs and feature encodings , with dropout rate 0.5 to all word and character embeddings and with dropout rate 0.4 to all LSTM layer outputs .","['Regularization', 'Dropout', 'is', 'applied', 'with', 'dropout', 'rate', '0.2', 'to', 'all', 'hidden', 'layers', 'of', 'all', 'MLPs', 'and', 'feature', 'encodings', ',', 'with', 'dropout', 'rate', '0.5', 'to', 'all', 'word', 'and', 'character', 'embeddings', 'and', 'with', 'dropout', 'rate', '0.4', 'to', 'all', 'LSTM', 'layer', 'outputs', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'NNP', 'VBZ', 'VBN', 'IN', 'NN', 'NN', 'CD', 'TO', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NNP', 'CC', 'NN', 'NNS', ',', 'IN', 'NN', 'NN', 'CD', 'TO', 'DT', 'NN', 'CC', 'NN', 'NNS', 'CC', 'IN', 'NN', 'NN', 'CD', 'TO', 'DT', 'NNP', 'NN', 'NNS', '.']",40
relation_extraction,10,156,"Learning Learning is done with Adam ( Kingma and Ba , 2015 ) with default parameters .","['Learning', 'Learning', 'is', 'done', 'with', 'Adam', '(', 'Kingma', 'and', 'Ba', ',', '2015', ')', 'with', 'default', 'parameters', '.']","['B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['VBG', 'NNP', 'VBZ', 'VBN', 'IN', 'NNP', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'IN', 'NN', 'NNS', '.']",17
relation_extraction,10,157,The learning rate is annealed by 1 % every 100 iterations .,"['The', 'learning', 'rate', 'is', 'annealed', 'by', '1', '%', 'every', '100', 'iterations', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', 'NN', 'DT', 'CD', 'NNS', '.']",12
relation_extraction,10,158,Minibatch Size is 1 .,"['Minibatch', 'Size', 'is', '1', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', 'NNP', 'VBZ', 'CD', '.']",5
relation_extraction,10,159,Early Stopping of 20 evaluations on the dev set is used .,"['Early', 'Stopping', 'of', '20', 'evaluations', 'on', 'the', 'dev', 'set', 'is', 'used', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['JJ', 'NNP', 'IN', 'CD', 'NNS', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'VBN', '.']",12
relation_extraction,10,181,"Our proposed model achieves a new SOTA on RE with a F 1 of 62. 83 , more than 2.3 F 1 above the previous SOTA .","['Our', 'proposed', 'model', 'achieves', 'a', 'new', 'SOTA', 'on', 'RE', 'with', 'a', 'F', '1', 'of', '62.', '83', ',', 'more', 'than', '2.3', 'F', '1', 'above', 'the', 'previous', 'SOTA', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'VBN', 'NN', 'VBZ', 'DT', 'JJ', 'NNP', 'IN', 'NNP', 'IN', 'DT', 'NNP', 'CD', 'IN', 'CD', 'CD', ',', 'JJR', 'IN', 'CD', 'NNP', 'CD', 'IN', 'DT', 'JJ', 'NNP', '.']",27
relation_extraction,10,182,Our proposed model also beats a multitask model which uses signals from additional tasks by more than 1.5 F 1 points .,"['Our', 'proposed', 'model', 'also', 'beats', 'a', 'multitask', 'model', 'which', 'uses', 'signals', 'from', 'additional', 'tasks', 'by', 'more', 'than', '1.5', 'F', '1', 'points', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'VBN', 'NN', 'RB', 'VBZ', 'DT', 'NN', 'NN', 'WDT', 'VBZ', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'JJR', 'IN', 'CD', 'NNP', 'CD', 'NNS', '.']",22
relation_extraction,10,183,"For both tasks , our model 's Precision is close to and Recall is significantly higher than previous works .","['For', 'both', 'tasks', ',', 'our', 'model', ""'s"", 'Precision', 'is', 'close', 'to', 'and', 'Recall', 'is', 'significantly', 'higher', 'than', 'previous', 'works', '.']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'PRP$', 'NN', 'POS', 'NNP', 'VBZ', 'JJ', 'TO', 'CC', 'NNP', 'VBZ', 'RB', 'JJR', 'IN', 'JJ', 'NNS', '.']",20
relation_extraction,10,184,The Recall gains for RE ( 4.3 absolute points ) are much higher than for EMD ( 0.6 absolute points ) .,"['The', 'Recall', 'gains', 'for', 'RE', '(', '4.3', 'absolute', 'points', ')', 'are', 'much', 'higher', 'than', 'for', 'EMD', '(', '0.6', 'absolute', 'points', ')', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['DT', 'NNP', 'NNS', 'IN', 'NNP', '(', 'CD', 'NN', 'NNS', ')', 'VBP', 'JJ', 'JJR', 'IN', 'IN', 'NNP', '(', 'CD', 'NN', 'NNS', ')', '.']",22
relation_extraction,10,187,"Thus , our large gains in RE Recall ( and F 1 ) showcase the effectiveness of our simple modeling of ordered span pairs for relation extraction ( Section 3.3 ) .","['Thus', ',', 'our', 'large', 'gains', 'in', 'RE', 'Recall', '(', 'and', 'F', '1', ')', 'showcase', 'the', 'effectiveness', 'of', 'our', 'simple', 'modeling', 'of', 'ordered', 'span', 'pairs', 'for', 'relation', 'extraction', '(', 'Section', '3.3', ')', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP$', 'JJ', 'NNS', 'IN', 'NNP', 'NNP', '(', 'CC', 'NNP', 'CD', ')', 'VBD', 'DT', 'NN', 'IN', 'PRP$', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NNS', 'IN', 'NN', 'NN', '(', 'NNP', 'CD', ')', '.']",32
relation_extraction,2,2,Enriching Pre-trained Language Model with Entity Information for Relation Classification,"['Enriching', 'Pre-trained', 'Language', 'Model', 'with', 'Entity', 'Information', 'for', 'Relation', 'Classification']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['VBG', 'JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",10
relation_extraction,2,25,"In this paper , we apply the pretrained BERT model for relation classification .","['In', 'this', 'paper', ',', 'we', 'apply', 'the', 'pretrained', 'BERT', 'model', 'for', 'relation', 'classification', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNP', 'NN', 'IN', 'NN', 'NN', '.']",14
relation_extraction,2,26,"We insert special tokens before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the BERT model .","['We', 'insert', 'special', 'tokens', 'before', 'and', 'after', 'the', 'target', 'entities', 'before', 'feeding', 'the', 'text', 'to', 'BERT', 'for', 'fine', '-', 'tuning', ',', 'in', 'order', 'to', 'identify', 'the', 'locations', 'of', 'the', 'two', 'target', 'entities', 'and', 'transfer', 'the', 'information', 'into', 'the', 'BERT', 'model', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NNS', 'IN', 'CC', 'IN', 'DT', 'NN', 'NNS', 'IN', 'VBG', 'DT', 'NN', 'TO', 'NNP', 'IN', 'JJ', ':', 'NN', ',', 'IN', 'NN', 'TO', 'VB', 'DT', 'NNS', 'IN', 'DT', 'CD', 'NN', 'NNS', 'CC', 'VB', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",41
relation_extraction,2,27,We then locate the positions of the two target entities in the output embedding from BERT model .,"['We', 'then', 'locate', 'the', 'positions', 'of', 'the', 'two', 'target', 'entities', 'in', 'the', 'output', 'embedding', 'from', 'BERT', 'model', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NNS', 'IN', 'DT', 'CD', 'NN', 'NNS', 'IN', 'DT', 'NN', 'VBG', 'IN', 'NNP', 'NN', '.']",18
relation_extraction,2,28,We use their embeddings as well as the sentence encoding ( embedding of the special first token in the setting of BERT ) as the input to a multi - layer neural network for classification .,"['We', 'use', 'their', 'embeddings', 'as', 'well', 'as', 'the', 'sentence', 'encoding', '(', 'embedding', 'of', 'the', 'special', 'first', 'token', 'in', 'the', 'setting', 'of', 'BERT', ')', 'as', 'the', 'input', 'to', 'a', 'multi', '-', 'layer', 'neural', 'network', 'for', 'classification', '.']","['O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'PRP$', 'NNS', 'RB', 'RB', 'IN', 'DT', 'NN', 'NN', '(', 'VBG', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ')', 'IN', 'DT', 'NN', 'TO', 'DT', 'NN', ':', 'NN', 'JJ', 'NN', 'IN', 'NN', '.']",36
relation_extraction,2,95,We add dropout before each add - on layer .,"['We', 'add', 'dropout', 'before', 'each', 'add', '-', 'on', 'layer', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'RB', 'IN', 'DT', 'NN', ':', 'IN', 'NN', '.']",10
relation_extraction,2,96,"For the pre-trained BERT model , we use the uncased basic model .","['For', 'the', 'pre-trained', 'BERT', 'model', ',', 'we', 'use', 'the', 'uncased', 'basic', 'model', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NNP', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', '.']",13
relation_extraction,2,100,"We compare our method , R - BERT , against results by multiple methods recently published for the SemEval - 2010 Task 8 dataset , including SVM , RNN , MVRNN , CNN + Softmax , FCM , CR - CNN , Attention - CNN , Entity Attention Bi-LSTM .","['We', 'compare', 'our', 'method', ',', 'R', '-', 'BERT', ',', 'against', 'results', 'by', 'multiple', 'methods', 'recently', 'published', 'for', 'the', 'SemEval', '-', '2010', 'Task', '8', 'dataset', ',', 'including', 'SVM', ',', 'RNN', ',', 'MVRNN', ',', 'CNN', '+', 'Softmax', ',', 'FCM', ',', 'CR', '-', 'CNN', ',', 'Attention', '-', 'CNN', ',', 'Entity', 'Attention', 'Bi-LSTM', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'NN', ',', 'NNP', ':', 'NNP', ',', 'IN', 'NNS', 'IN', 'JJ', 'NNS', 'RB', 'VBN', 'IN', 'DT', 'NNP', ':', 'CD', 'NNP', 'CD', 'NN', ',', 'VBG', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', 'NNP', 'NNP', ',', 'NNP', ',', 'NNP', ':', 'NN', ',', 'NNP', ':', 'NNP', ',', 'NNP', 'NNP', 'NNP', '.']",50
relation_extraction,2,103,We can see that R - BERT significantly beats all the baseline methods .,"['We', 'can', 'see', 'that', 'R', '-', 'BERT', 'significantly', 'beats', 'all', 'the', 'baseline', 'methods', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'DT', 'NNP', ':', 'NNP', 'RB', 'VBZ', 'PDT', 'DT', 'NN', 'NNS', '.']",14
relation_extraction,2,104,"The MACRO F1 value of R - BERT is 89. 25 , which is much better than the previous best solution on this dataset .","['The', 'MACRO', 'F1', 'value', 'of', 'R', '-', 'BERT', 'is', '89.', '25', ',', 'which', 'is', 'much', 'better', 'than', 'the', 'previous', 'best', 'solution', 'on', 'this', 'dataset', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NNP', 'NN', 'IN', 'NNP', ':', 'NNP', 'VBZ', 'CD', 'CD', ',', 'WDT', 'VBZ', 'RB', 'JJR', 'IN', 'DT', 'JJ', 'JJS', 'NN', 'IN', 'DT', 'NN', '.']",25
relation_extraction,2,124,We observe that the three methods all perform worse than R - BERT .,"['We', 'observe', 'that', 'the', 'three', 'methods', 'all', 'perform', 'worse', 'than', 'R', '-', 'BERT', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'CD', 'NNS', 'DT', 'VBP', 'JJR', 'IN', 'NNP', ':', 'NNP', '.']",14
relation_extraction,2,125,"Of the methods , BERT - NO - SEP - NO - ENT performs worst , with its F1 8.16 absolute points worse than R - BERT .","['Of', 'the', 'methods', ',', 'BERT', '-', 'NO', '-', 'SEP', '-', 'NO', '-', 'ENT', 'performs', 'worst', ',', 'with', 'its', 'F1', '8.16', 'absolute', 'points', 'worse', 'than', 'R', '-', 'BERT', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'NNP', ':', 'SYM', ':', 'NNP', ':', 'SYM', ':', 'NN', 'NNS', 'VBP', ',', 'IN', 'PRP$', 'NNP', 'CD', 'NN', 'NNS', 'JJR', 'IN', 'NNP', ':', 'NNP', '.']",28
relation_extraction,2,126,This ablation study demonstrates that both the special separate tokens and the hidden entity vectors make important contributions to our approach .,"['This', 'ablation', 'study', 'demonstrates', 'that', 'both', 'the', 'special', 'separate', 'tokens', 'and', 'the', 'hidden', 'entity', 'vectors', 'make', 'important', 'contributions', 'to', 'our', 'approach', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', 'VBZ', 'IN', 'DT', 'DT', 'JJ', 'JJ', 'NNS', 'CC', 'DT', 'JJ', 'NN', 'NNS', 'VBP', 'JJ', 'NNS', 'TO', 'PRP$', 'NN', '.']",22
relation_extraction,2,128,BERT without special separate tokens can not locate the target entities and lose this key information .,"['BERT', 'without', 'special', 'separate', 'tokens', 'can', 'not', 'locate', 'the', 'target', 'entities', 'and', 'lose', 'this', 'key', 'information', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'IN', 'JJ', 'JJ', 'NNS', 'MD', 'RB', 'VB', 'DT', 'NN', 'NNS', 'CC', 'VB', 'DT', 'JJ', 'NN', '.']",17
relation_extraction,2,130,"On the other hand , incorporating the output of the target entity vectors further enriches the information and helps to make more accurate prediction .","['On', 'the', 'other', 'hand', ',', 'incorporating', 'the', 'output', 'of', 'the', 'target', 'entity', 'vectors', 'further', 'enriches', 'the', 'information', 'and', 'helps', 'to', 'make', 'more', 'accurate', 'prediction', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'NNS', 'RBR', 'VBZ', 'DT', 'NN', 'CC', 'VBZ', 'TO', 'VB', 'JJR', 'JJ', 'NN', '.']",25
relation_extraction,3,2,Extracting Multiple - Relations in One - Pass with Pre-Trained Transformers,"['Extracting', 'Multiple', '-', 'Relations', 'in', 'One', '-', 'Pass', 'with', 'Pre-Trained', 'Transformers']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NNP', ':', 'NNPS', 'IN', 'CD', ':', 'NN', 'IN', 'JJ', 'NNS']",11
relation_extraction,3,4,The state - of - the - art solutions for extracting multiple entity - relations from an input paragraph always require a multiple - pass encoding on the input .,"['The', 'state', '-', 'of', '-', 'the', '-', 'art', 'solutions', 'for', 'extracting', 'multiple', 'entity', '-', 'relations', 'from', 'an', 'input', 'paragraph', 'always', 'require', 'a', 'multiple', '-', 'pass', 'encoding', 'on', 'the', 'input', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'VBG', 'JJ', 'NN', ':', 'NNS', 'IN', 'DT', 'NN', 'NN', 'RB', 'VB', 'DT', 'JJ', ':', 'NN', 'VBG', 'IN', 'DT', 'NN', '.']",30
relation_extraction,3,5,"This paper proposes a new solution that can complete the multiple entityrelations extraction task with only one - pass encoding on the input corpus , and achieve a new state - of - the - art accuracy performance , as demonstrated in the ACE 2005 benchmark .","['This', 'paper', 'proposes', 'a', 'new', 'solution', 'that', 'can', 'complete', 'the', 'multiple', 'entityrelations', 'extraction', 'task', 'with', 'only', 'one', '-', 'pass', 'encoding', 'on', 'the', 'input', 'corpus', ',', 'and', 'achieve', 'a', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'accuracy', 'performance', ',', 'as', 'demonstrated', 'in', 'the', 'ACE', '2005', 'benchmark', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'WDT', 'MD', 'VB', 'DT', 'JJ', 'NNS', 'NN', 'NN', 'IN', 'RB', 'CD', ':', 'NN', 'VBG', 'IN', 'DT', 'NN', 'NN', ',', 'CC', 'VB', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'NN', ',', 'IN', 'VBN', 'IN', 'DT', 'NNP', 'CD', 'NN', '.']",47
relation_extraction,3,10,Relation extraction ( RE ) aims to find the semantic relation between a pair of entity mentions from an input paragraph .,"['Relation', 'extraction', '(', 'RE', ')', 'aims', 'to', 'find', 'the', 'semantic', 'relation', 'between', 'a', 'pair', 'of', 'entity', 'mentions', 'from', 'an', 'input', 'paragraph', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', '(', 'NNP', ')', 'VBZ', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",22
relation_extraction,3,12,One particular type of the RE task is multiplerelations extraction ( MRE ) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph .,"['One', 'particular', 'type', 'of', 'the', 'RE', 'task', 'is', 'multiplerelations', 'extraction', '(', 'MRE', ')', 'that', 'aims', 'to', 'recognize', 'relations', 'of', 'multiple', 'pairs', 'of', 'entity', 'mentions', 'from', 'an', 'input', 'paragraph', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['CD', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', 'VBZ', 'NNS', 'NN', '(', 'NNP', ')', 'WDT', 'VBZ', 'TO', 'VB', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",29
relation_extraction,3,13,"Because in real - world applications , whose input paragraphs dominantly contain multiple pairs of entities , an efficient and effective solution for MRE has more important and more practical implications .","['Because', 'in', 'real', '-', 'world', 'applications', ',', 'whose', 'input', 'paragraphs', 'dominantly', 'contain', 'multiple', 'pairs', 'of', 'entities', ',', 'an', 'efficient', 'and', 'effective', 'solution', 'for', 'MRE', 'has', 'more', 'important', 'and', 'more', 'practical', 'implications', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'IN', 'JJ', ':', 'NN', 'NNS', ',', 'WP$', 'NN', 'VBP', 'RB', 'VBP', 'JJ', 'NNS', 'IN', 'NNS', ',', 'DT', 'NN', 'CC', 'JJ', 'NN', 'IN', 'NNP', 'VBZ', 'JJR', 'JJ', 'CC', 'RBR', 'JJ', 'NNS', '.']",32
relation_extraction,3,16,"This work presents a solution that can resolve the inefficient multiple - passes issue of existing solutions for MRE by encoding the input only once , which significantly increases the efficiency and scalability .","['This', 'work', 'presents', 'a', 'solution', 'that', 'can', 'resolve', 'the', 'inefficient', 'multiple', '-', 'passes', 'issue', 'of', 'existing', 'solutions', 'for', 'MRE', 'by', 'encoding', 'the', 'input', 'only', 'once', ',', 'which', 'significantly', 'increases', 'the', 'efficiency', 'and', 'scalability', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'DT', 'NN', 'WDT', 'MD', 'VB', 'DT', 'NN', 'SYM', ':', 'NNS', 'NN', 'IN', 'VBG', 'NNS', 'IN', 'NNP', 'IN', 'VBG', 'DT', 'NN', 'RB', 'RB', ',', 'WDT', 'RB', 'VBZ', 'DT', 'NN', 'CC', 'NN', '.']",34
relation_extraction,3,17,"Specifically , the proposed solution is built on top of the existing transformer - based , pretrained general - purposed language encoders .","['Specifically', ',', 'the', 'proposed', 'solution', 'is', 'built', 'on', 'top', 'of', 'the', 'existing', 'transformer', '-', 'based', ',', 'pretrained', 'general', '-', 'purposed', 'language', 'encoders', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'VBN', 'NN', 'VBZ', 'VBN', 'IN', 'NN', 'IN', 'DT', 'VBG', 'NN', ':', 'VBN', ',', 'VBN', 'JJ', ':', 'JJ', 'NN', 'NNS', '.']",23
relation_extraction,3,18,"In this paper we use Bidirectional Encoder Representations from Transformers ( BERT ) as the transformer - based encoder , but this solution is not limited to using BERT alone .","['In', 'this', 'paper', 'we', 'use', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', '(', 'BERT', ')', 'as', 'the', 'transformer', '-', 'based', 'encoder', ',', 'but', 'this', 'solution', 'is', 'not', 'limited', 'to', 'using', 'BERT', 'alone', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'JJ', 'NNP', 'NNP', 'IN', 'NNP', '(', 'NNP', ')', 'IN', 'DT', 'NN', ':', 'VBN', 'NN', ',', 'CC', 'DT', 'NN', 'VBZ', 'RB', 'VBN', 'TO', 'VBG', 'NNP', 'RB', '.']",31
relation_extraction,3,19,The two novel modifications to the original BERT architecture are : ( 1 ) we introduce a structured prediction layer for predicting multiple relations for different entity pairs ; and ( 2 ) we make the selfattention layers aware of the positions of all en-tities in the input paragraph .,"['The', 'two', 'novel', 'modifications', 'to', 'the', 'original', 'BERT', 'architecture', 'are', ':', '(', '1', ')', 'we', 'introduce', 'a', 'structured', 'prediction', 'layer', 'for', 'predicting', 'multiple', 'relations', 'for', 'different', 'entity', 'pairs', ';', 'and', '(', '2', ')', 'we', 'make', 'the', 'selfattention', 'layers', 'aware', 'of', 'the', 'positions', 'of', 'all', 'en-tities', 'in', 'the', 'input', 'paragraph', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'CD', 'JJ', 'NNS', 'TO', 'DT', 'JJ', 'NNP', 'NN', 'VBP', ':', '(', 'CD', ')', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NNS', ':', 'CC', '(', 'CD', ')', 'PRP', 'VBP', 'DT', 'NN', 'NNS', 'VBP', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",50
relation_extraction,3,77,"BERT SP : BERT with structured prediction only , which includes proposed improvement in 3.1 .","['BERT', 'SP', ':', 'BERT', 'with', 'structured', 'prediction', 'only', ',', 'which', 'includes', 'proposed', 'improvement', 'in', '3.1', '.']","['B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', ':', 'NN', 'IN', 'JJ', 'NN', 'RB', ',', 'WDT', 'VBZ', 'VBN', 'NN', 'IN', 'CD', '.']",16
relation_extraction,3,78,"Entity - Aware BERT SP : our full model , which includes both improvements in 3.1 and 3.2 .","['Entity', '-', 'Aware', 'BERT', 'SP', ':', 'our', 'full', 'model', ',', 'which', 'includes', 'both', 'improvements', 'in', '3.1', 'and', '3.2', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NNP', 'NNP', 'NNP', ':', 'PRP$', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NNS', 'IN', 'CD', 'CC', 'CD', '.']",19
relation_extraction,3,79,BERT SP with position embedding on the final attention layer .,"['BERT', 'SP', 'with', 'position', 'embedding', 'on', 'the', 'final', 'attention', 'layer', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'IN', 'NN', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",11
relation_extraction,3,80,This is a more straightforward way to achieve MRE in one - pass derived from previous works using position embeddings .,"['This', 'is', 'a', 'more', 'straightforward', 'way', 'to', 'achieve', 'MRE', 'in', 'one', '-', 'pass', 'derived', 'from', 'previous', 'works', 'using', 'position', 'embeddings', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'VBZ', 'DT', 'RBR', 'JJ', 'NN', 'TO', 'VB', 'NNP', 'IN', 'CD', ':', 'NN', 'VBN', 'IN', 'JJ', 'NNS', 'VBG', 'NN', 'NNS', '.']",21
relation_extraction,3,83,"BERT SP with entity indicators on input layer : it replaces our structured attention layer , and adds indicators of entities ( transformed to embeddings )","['BERT', 'SP', 'with', 'entity', 'indicators', 'on', 'input', 'layer', ':', 'it', 'replaces', 'our', 'structured', 'attention', 'layer', ',', 'and', 'adds', 'indicators', 'of', 'entities', '(', 'transformed', 'to', 'embeddings', ')']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', 'IN', 'NN', 'NNS', 'IN', 'NN', 'NN', ':', 'PRP', 'VBZ', 'PRP$', 'JJ', 'NN', 'NN', ',', 'CC', 'VBZ', 'NNS', 'IN', 'NNS', '(', 'VBN', 'TO', 'NNS', ')']",26
relation_extraction,3,86,The first observation is that our model architecture achieves much better results compared to the previous state - of - the - art methods .,"['The', 'first', 'observation', 'is', 'that', 'our', 'model', 'architecture', 'achieves', 'much', 'better', 'results', 'compared', 'to', 'the', 'previous', 'state', '-', 'of', '-', 'the', '-', 'art', 'methods', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'IN', 'PRP$', 'NN', 'NN', 'VBZ', 'RB', 'JJR', 'NNS', 'VBN', 'TO', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', '.']",25
relation_extraction,3,87,"Note that our method was not designed for domain adaptation , it still outperforms those methods with domain adaptation .","['Note', 'that', 'our', 'method', 'was', 'not', 'designed', 'for', 'domain', 'adaptation', ',', 'it', 'still', 'outperforms', 'those', 'methods', 'with', 'domain', 'adaptation', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['NN', 'IN', 'PRP$', 'NN', 'VBD', 'RB', 'VBN', 'IN', 'NN', 'NN', ',', 'PRP', 'RB', 'VBZ', 'DT', 'NNS', 'IN', 'NN', 'NN', '.']",20
relation_extraction,3,89,"Among all the BERT - based approaches , finetuning the off - the - shelf BERT does not give a satisfying result , because the sentence embeddings can not distinguish different entity pairs .","['Among', 'all', 'the', 'BERT', '-', 'based', 'approaches', ',', 'finetuning', 'the', 'off', '-', 'the', '-', 'shelf', 'BERT', 'does', 'not', 'give', 'a', 'satisfying', 'result', ',', 'because', 'the', 'sentence', 'embeddings', 'can', 'not', 'distinguish', 'different', 'entity', 'pairs', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PDT', 'DT', 'NNP', ':', 'VBN', 'NNS', ',', 'VBG', 'DT', 'RP', ':', 'DT', ':', 'NN', 'NNP', 'VBZ', 'RB', 'VB', 'DT', 'NN', 'NN', ',', 'IN', 'DT', 'NN', 'NNS', 'MD', 'RB', 'VB', 'JJ', 'NN', 'NNS', '.']",34
relation_extraction,3,90,"The simpler version of our approach , BERT SP , can successfully adapt the pre-trained BERT to the MRE task , and achieves comparable performance at the 3 Note the usage of relative position embeddings does notwork for one - pass MRE , since each word corresponds to a varying number of position embedding vectors .","['The', 'simpler', 'version', 'of', 'our', 'approach', ',', 'BERT', 'SP', ',', 'can', 'successfully', 'adapt', 'the', 'pre-trained', 'BERT', 'to', 'the', 'MRE', 'task', ',', 'and', 'achieves', 'comparable', 'performance', 'at', 'the', '3', 'Note', 'the', 'usage', 'of', 'relative', 'position', 'embeddings', 'does', 'notwork', 'for', 'one', '-', 'pass', 'MRE', ',', 'since', 'each', 'word', 'corresponds', 'to', 'a', 'varying', 'number', 'of', 'position', 'embedding', 'vectors', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', 'IN', 'PRP$', 'NN', ',', 'NNP', 'NNP', ',', 'MD', 'RB', 'VB', 'DT', 'JJ', 'NNP', 'TO', 'DT', 'NNP', 'NN', ',', 'CC', 'VBZ', 'JJ', 'NN', 'IN', 'DT', 'CD', 'NNP', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NNS', 'VBZ', 'NN', 'IN', 'CD', ':', 'NN', 'NNP', ',', 'IN', 'DT', 'NN', 'VBZ', 'TO', 'DT', 'VBG', 'NN', 'IN', 'NN', 'VBG', 'NNS', '.']",56
relation_extraction,3,92,"It works for the singlerelation per pass setting , but the performance lags behind using only indicators of the two target entities .","['It', 'works', 'for', 'the', 'singlerelation', 'per', 'pass', 'setting', ',', 'but', 'the', 'performance', 'lags', 'behind', 'using', 'only', 'indicators', 'of', 'the', 'two', 'target', 'entities', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', ',', 'CC', 'DT', 'NN', 'NN', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'DT', 'CD', 'NN', 'NNS', '.']",23
relation_extraction,3,94,"Our full model , with the structured fine - tuning of attention layers , brings further improvement of about 5.5 % , in the MRE one - pass setting , and achieves a new state - of - the - art performance when compared to the methods with domain adaptation .","['Our', 'full', 'model', ',', 'with', 'the', 'structured', 'fine', '-', 'tuning', 'of', 'attention', 'layers', ',', 'brings', 'further', 'improvement', 'of', 'about', '5.5', '%', ',', 'in', 'the', 'MRE', 'one', '-', 'pass', 'setting', ',', 'and', 'achieves', 'a', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'when', 'compared', 'to', 'the', 'methods', 'with', 'domain', 'adaptation', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'JJ', 'NN', ',', 'IN', 'DT', 'JJ', 'JJ', ':', 'NN', 'IN', 'NN', 'NNS', ',', 'VBZ', 'JJ', 'NN', 'IN', 'RB', 'CD', 'NN', ',', 'IN', 'DT', 'NNP', 'CD', ':', 'NN', 'NN', ',', 'CC', 'VBZ', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'WRB', 'VBN', 'TO', 'DT', 'NNS', 'IN', 'NN', 'NN', '.']",51
relation_extraction,3,99,"For BERT SP with entity indicators on inputs , it is expected to perform slightly better in the single - relation setting , because of the mixture of information from multiple pairs .","['For', 'BERT', 'SP', 'with', 'entity', 'indicators', 'on', 'inputs', ',', 'it', 'is', 'expected', 'to', 'perform', 'slightly', 'better', 'in', 'the', 'single', '-', 'relation', 'setting', ',', 'because', 'of', 'the', 'mixture', 'of', 'information', 'from', 'multiple', 'pairs', '.']","['B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', 'NNP', 'IN', 'NN', 'NNS', 'IN', 'NNS', ',', 'PRP', 'VBZ', 'VBN', 'TO', 'VB', 'RB', 'RBR', 'IN', 'DT', 'JJ', ':', 'NN', 'NN', ',', 'IN', 'IN', 'DT', 'NN', 'IN', 'NN', 'IN', 'JJ', 'NNS', '.']",33
relation_extraction,3,100,A 2 % gap is observed as expected .,"['A', '2', '%', 'gap', 'is', 'observed', 'as', 'expected', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O']","['DT', 'CD', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'VBN', '.']",9
relation_extraction,3,103,"For BERT SP with position embeddings on the final attention layer , we train the model in the single - relation setting and test with two different settings , so the results are the same .","['For', 'BERT', 'SP', 'with', 'position', 'embeddings', 'on', 'the', 'final', 'attention', 'layer', ',', 'we', 'train', 'the', 'model', 'in', 'the', 'single', '-', 'relation', 'setting', 'and', 'test', 'with', 'two', 'different', 'settings', ',', 'so', 'the', 'results', 'are', 'the', 'same', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']","['IN', 'NNP', 'NNP', 'IN', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'JJ', ':', 'NN', 'NN', 'CC', 'NN', 'IN', 'CD', 'JJ', 'NNS', ',', 'IN', 'DT', 'NNS', 'VBP', 'DT', 'JJ', '.']",36
relation_extraction,3,117,"Our Entity - Aware BERT SP gives comparable results to the top - ranked system in the shared task , with slightly lower Macro - F1 , which is the official metric of the task , and slightly higher Micro - F1 .","['Our', 'Entity', '-', 'Aware', 'BERT', 'SP', 'gives', 'comparable', 'results', 'to', 'the', 'top', '-', 'ranked', 'system', 'in', 'the', 'shared', 'task', ',', 'with', 'slightly', 'lower', 'Macro', '-', 'F1', ',', 'which', 'is', 'the', 'official', 'metric', 'of', 'the', 'task', ',', 'and', 'slightly', 'higher', 'Micro', '-', 'F1', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'VBZ', 'JJ', 'NNS', 'TO', 'DT', 'JJ', ':', 'JJ', 'NN', 'IN', 'DT', 'VBN', 'NN', ',', 'IN', 'RB', 'JJR', 'NNP', ':', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'JJ', 'IN', 'DT', 'NN', ',', 'CC', 'RB', 'JJR', 'NNP', ':', 'NN', '.']",43
relation_extraction,3,118,"When predicting multiple relations in one - pass , we have 0.9 % drop on Macro - F1 , but a further 0.8 % improvement on Micro - F1 .","['When', 'predicting', 'multiple', 'relations', 'in', 'one', '-', 'pass', ',', 'we', 'have', '0.9', '%', 'drop', 'on', 'Macro', '-', 'F1', ',', 'but', 'a', 'further', '0.8', '%', 'improvement', 'on', 'Micro', '-', 'F1', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['WRB', 'VBG', 'JJ', 'NNS', 'IN', 'CD', ':', 'NN', ',', 'PRP', 'VBP', 'CD', 'NN', 'NN', 'IN', 'NNP', ':', 'NN', ',', 'CC', 'DT', 'JJ', 'CD', 'NN', 'NN', 'IN', 'NNP', ':', 'NN', '.']",30
relation_extraction,3,120,"On the other hand , compared to the top singlemodel result , which makes use of additional word and entity embeddings pretrained on in - domain data , our methods demonstrate clear advantage as a single model .","['On', 'the', 'other', 'hand', ',', 'compared', 'to', 'the', 'top', 'singlemodel', 'result', ',', 'which', 'makes', 'use', 'of', 'additional', 'word', 'and', 'entity', 'embeddings', 'pretrained', 'on', 'in', '-', 'domain', 'data', ',', 'our', 'methods', 'demonstrate', 'clear', 'advantage', 'as', 'a', 'single', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'VBN', 'TO', 'DT', 'JJ', 'NN', 'NN', ',', 'WDT', 'VBZ', 'NN', 'IN', 'JJ', 'NN', 'CC', 'NN', 'NNS', 'VBN', 'IN', 'IN', ':', 'NN', 'NNS', ',', 'PRP$', 'NNS', 'VBP', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",38
relation_extraction,12,2,Attention Guided Graph Convolutional Networks for Relation Extraction,"['Attention', 'Guided', 'Graph', 'Convolutional', 'Networks', 'for', 'Relation', 'Extraction']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",8
relation_extraction,12,32,"In this paper , we propose the novel Attention Guided Graph Convolutional Networks ( AGGCNs ) , which operate directly on the full tree .","['In', 'this', 'paper', ',', 'we', 'propose', 'the', 'novel', 'Attention', 'Guided', 'Graph', 'Convolutional', 'Networks', '(', 'AGGCNs', ')', ',', 'which', 'operate', 'directly', 'on', 'the', 'full', 'tree', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'WDT', 'VBP', 'RB', 'IN', 'DT', 'JJ', 'NN', '.']",25
relation_extraction,12,33,"Intuitively , we develop a "" soft pruning "" strategy that transforms the original dependency tree into a fully connected edgeweighted graph .","['Intuitively', ',', 'we', 'develop', 'a', '""', 'soft', 'pruning', '""', 'strategy', 'that', 'transforms', 'the', 'original', 'dependency', 'tree', 'into', 'a', 'fully', 'connected', 'edgeweighted', 'graph', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'JJ', 'VBG', 'JJ', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'RB', 'VBN', 'JJ', 'NN', '.']",23
relation_extraction,12,34,"These weights can be viewed as the strength of relatedness between nodes , which can be learned in an end - to - end fashion by using self - attention mechanism .","['These', 'weights', 'can', 'be', 'viewed', 'as', 'the', 'strength', 'of', 'relatedness', 'between', 'nodes', ',', 'which', 'can', 'be', 'learned', 'in', 'an', 'end', '-', 'to', '-', 'end', 'fashion', 'by', 'using', 'self', '-', 'attention', 'mechanism', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'MD', 'VB', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NN', 'IN', 'NNS', ',', 'WDT', 'MD', 'VB', 'VBN', 'IN', 'DT', 'NN', ':', 'TO', ':', 'NN', 'NN', 'IN', 'VBG', 'PRP', ':', 'NN', 'NN', '.']",32
relation_extraction,12,41,we next introduce dense connections ) to the GCN model following .,"['we', 'next', 'introduce', 'dense', 'connections', ')', 'to', 'the', 'GCN', 'model', 'following', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O']","['PRP', 'JJ', 'NN', 'NN', 'NNS', ')', 'TO', 'DT', 'NNP', 'NN', 'VBG', '.']",12
relation_extraction,12,42,"For GCNs , L layers will be needed in order to capture neighborhood information that is L hops away .","['For', 'GCNs', ',', 'L', 'layers', 'will', 'be', 'needed', 'in', 'order', 'to', 'capture', 'neighborhood', 'information', 'that', 'is', 'L', 'hops', 'away', '.']","['B-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', ',', 'NNP', 'NNS', 'MD', 'VB', 'VBN', 'IN', 'NN', 'TO', 'VB', 'JJ', 'NN', 'WDT', 'VBZ', 'NNP', 'VBZ', 'RB', '.']",20
relation_extraction,12,45,"With the help of dense connections , we are able to train the AGGCN model with a large depth , allowing rich local and non-local dependency information to be captured .","['With', 'the', 'help', 'of', 'dense', 'connections', ',', 'we', 'are', 'able', 'to', 'train', 'the', 'AGGCN', 'model', 'with', 'a', 'large', 'depth', ',', 'allowing', 'rich', 'local', 'and', 'non-local', 'dependency', 'information', 'to', 'be', 'captured', '.']","['B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'JJ', 'TO', 'VB', 'DT', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'VBG', 'JJ', 'JJ', 'CC', 'JJ', 'NN', 'NN', 'TO', 'VB', 'VBN', '.']",31
relation_extraction,12,49,Our code is available at https://github.com/Cartus / AGGCN_TACRED,"['Our', 'code', 'is', 'available', 'at', 'https://github.com/Cartus', '/', 'AGGCN_TACRED']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['PRP$', 'NN', 'VBZ', 'JJ', 'IN', 'NN', 'NN', 'NNP']",8
relation_extraction,12,174,"We choose the number of heads N for attention guided layer from { 1 , 2 , 3 , 4 } , the block number M from { 1 , 2 , 3 } , the number of sub - layers L in each densely connected layer from { 2 , 3 , 4 }.","['We', 'choose', 'the', 'number', 'of', 'heads', 'N', 'for', 'attention', 'guided', 'layer', 'from', '{', '1', ',', '2', ',', '3', ',', '4', '}', ',', 'the', 'block', 'number', 'M', 'from', '{', '1', ',', '2', ',', '3', '}', ',', 'the', 'number', 'of', 'sub', '-', 'layers', 'L', 'in', 'each', 'densely', 'connected', 'layer', 'from', '{', '2', ',', '3', ',', '4', '}.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'NNS', 'NNP', 'IN', 'NN', 'VBN', 'NN', 'IN', '(', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ')', ',', 'DT', 'NN', 'NN', 'NNP', 'IN', '(', 'CD', ',', 'CD', ',', 'CD', ')', ',', 'DT', 'NN', 'IN', 'JJ', ':', 'NNS', 'NNP', 'IN', 'DT', 'RB', 'VBN', 'NN', 'IN', '(', 'CD', ',', 'CD', ',', 'CD', 'NN']",55
relation_extraction,12,176,Glo Ve vectors are used as the initialization for word embeddings .,"['Glo', 'Ve', 'vectors', 'are', 'used', 'as', 'the', 'initialization', 'for', 'word', 'embeddings', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'NNP', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', '.']",12
relation_extraction,12,182,"For cross - sentence n- ary relation extraction task , we consider three kinds of models as baselines :","['For', 'cross', '-', 'sentence', 'n-', 'ary', 'relation', 'extraction', 'task', ',', 'we', 'consider', 'three', 'kinds', 'of', 'models', 'as', 'baselines', ':']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ':', 'NN', 'JJ', 'JJ', 'NN', 'NN', 'NN', ',', 'PRP', 'VBP', 'CD', 'NNS', 'IN', 'NNS', 'IN', 'NNS', ':']",19
relation_extraction,12,183,"1 ) a feature - based classifier based on shortest dependency paths between all entity pairs , 2 ) Graph - structured LSTM methods , including Graph LSTM , bidirectional DAG LSTM ( Bidir DAG LSTM ) and Graph State LSTM ( GS GLSTM ) .","['1', ')', 'a', 'feature', '-', 'based', 'classifier', 'based', 'on', 'shortest', 'dependency', 'paths', 'between', 'all', 'entity', 'pairs', ',', '2', ')', 'Graph', '-', 'structured', 'LSTM', 'methods', ',', 'including', 'Graph', 'LSTM', ',', 'bidirectional', 'DAG', 'LSTM', '(', 'Bidir', 'DAG', 'LSTM', ')', 'and', 'Graph', 'State', 'LSTM', '(', 'GS', 'GLSTM', ')', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', ')', 'DT', 'NN', ':', 'VBN', 'NN', 'VBN', 'IN', 'JJS', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NNS', ',', 'CD', ')', 'NNP', ':', 'VBD', 'NNP', 'NNS', ',', 'VBG', 'NNP', 'NNP', ',', 'JJ', 'NNP', 'NNP', '(', 'NNP', 'NNP', 'NNP', ')', 'CC', 'NNP', 'NNP', 'NNP', '(', 'NNP', 'NNP', ')', '.']",46
relation_extraction,12,184,"These methods extend LSTM to encode graphs constructed from input sentences with dependency edges , 3 ) Graph convolutional networks ( GCN ) with pruned trees , 6 https://nlp.stanford.edu/projects/","['These', 'methods', 'extend', 'LSTM', 'to', 'encode', 'graphs', 'constructed', 'from', 'input', 'sentences', 'with', 'dependency', 'edges', ',', '3', ')', 'Graph', 'convolutional', 'networks', '(', 'GCN', ')', 'with', 'pruned', 'trees', ',', '6', 'https://nlp.stanford.edu/projects/']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['DT', 'NNS', 'VBP', 'NNP', 'TO', 'VB', 'NNS', 'VBN', 'IN', 'NN', 'NNS', 'IN', 'NN', 'NNS', ',', 'CD', ')', 'NNP', 'JJ', 'NNS', '(', 'NNP', ')', 'IN', 'VBN', 'NNS', ',', 'CD', 'NN']",29
relation_extraction,12,202,"For ternary relation extraction ( first two columns in ) , our AGGCN model achieves accuracies of 87.1 and 87.0 on instances within single sentence ( Single ) and on all instances ( Cross ) , respectively , which outperform all the baselines .","['For', 'ternary', 'relation', 'extraction', '(', 'first', 'two', 'columns', 'in', ')', ',', 'our', 'AGGCN', 'model', 'achieves', 'accuracies', 'of', '87.1', 'and', '87.0', 'on', 'instances', 'within', 'single', 'sentence', '(', 'Single', ')', 'and', 'on', 'all', 'instances', '(', 'Cross', ')', ',', 'respectively', ',', 'which', 'outperform', 'all', 'the', 'baselines', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', 'NN', 'NN', '(', 'JJ', 'CD', 'NN', 'IN', ')', ',', 'PRP$', 'NNP', 'NN', 'VBZ', 'NNS', 'IN', 'CD', 'CC', 'CD', 'IN', 'NNS', 'IN', 'JJ', 'NN', '(', 'NNP', ')', 'CC', 'IN', 'DT', 'NNS', '(', 'NNP', ')', ',', 'RB', ',', 'WDT', 'VBP', 'PDT', 'DT', 'NNS', '.']",44
relation_extraction,12,203,"More specifically , our AG - GCN model surpasses the state - of - the - art Graphstructured LSTM model ( GS GLSTM ) by 6.8 and 3.8 points for the Single and Cross settings , respectively .","['More', 'specifically', ',', 'our', 'AG', '-', 'GCN', 'model', 'surpasses', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'Graphstructured', 'LSTM', 'model', '(', 'GS', 'GLSTM', ')', 'by', '6.8', 'and', '3.8', 'points', 'for', 'the', 'Single', 'and', 'Cross', 'settings', ',', 'respectively', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['RBR', 'RB', ',', 'PRP$', 'NNP', ':', 'NNP', 'NN', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNP', 'NNP', 'NN', '(', 'NNP', 'NNP', ')', 'IN', 'CD', 'CC', 'CD', 'NNS', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NNS', ',', 'RB', '.']",38
relation_extraction,12,204,"Compared to GCN models , our model obtains 1.3 and 1.2 points higher than the best performing model with pruned tree ( K=1 ) .","['Compared', 'to', 'GCN', 'models', ',', 'our', 'model', 'obtains', '1.3', 'and', '1.2', 'points', 'higher', 'than', 'the', 'best', 'performing', 'model', 'with', 'pruned', 'tree', '(', 'K=1', ')', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'TO', 'NNP', 'NNS', ',', 'PRP$', 'NN', 'VBZ', 'CD', 'CC', 'CD', 'NNS', 'JJR', 'IN', 'DT', 'JJS', 'NN', 'NN', 'IN', 'JJ', 'NN', '(', 'NNP', ')', '.']",25
relation_extraction,12,205,"For binary relation extraction ( third and fourth columns in ) , AGGCN consistently outperforms GS GLSTM and GCN as well .","['For', 'binary', 'relation', 'extraction', '(', 'third', 'and', 'fourth', 'columns', 'in', ')', ',', 'AGGCN', 'consistently', 'outperforms', 'GS', 'GLSTM', 'and', 'GCN', 'as', 'well', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O']","['IN', 'JJ', 'NN', 'NN', '(', 'JJ', 'CC', 'JJ', 'NNS', 'IN', ')', ',', 'NNP', 'RB', 'VBZ', 'NNP', 'NNP', 'CC', 'NNP', 'RB', 'RB', '.']",22
relation_extraction,12,207,"AGGCN also performs better than GCNs , although its performance can be boosted via pruned trees .","['AGGCN', 'also', 'performs', 'better', 'than', 'GCNs', ',', 'although', 'its', 'performance', 'can', 'be', 'boosted', 'via', 'pruned', 'trees', '.']","['B-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'RB', 'VBZ', 'JJR', 'IN', 'NNP', ',', 'IN', 'PRP$', 'NN', 'MD', 'VB', 'VBN', 'IN', 'JJ', 'NNS', '.']",17
relation_extraction,12,215,"However , our AGGCN model still obtains 8.0 and 5.7 points higher than the GS GLSTM model for ternary and binary relations , respectively .","['However', ',', 'our', 'AGGCN', 'model', 'still', 'obtains', '8.0', 'and', '5.7', 'points', 'higher', 'than', 'the', 'GS', 'GLSTM', 'model', 'for', 'ternary', 'and', 'binary', 'relations', ',', 'respectively', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['RB', ',', 'PRP$', 'NNP', 'NN', 'RB', 'VBZ', 'CD', 'CC', 'CD', 'NNS', 'JJR', 'IN', 'DT', 'NNP', 'NNP', 'NN', 'IN', 'JJ', 'CC', 'JJ', 'NNS', ',', 'RB', '.']",25
relation_extraction,12,216,"We also notice that our AGGCN achieves a better test accuracy than all GCN models , which further demonstrates its ability to learn better representations from full trees .","['We', 'also', 'notice', 'that', 'our', 'AGGCN', 'achieves', 'a', 'better', 'test', 'accuracy', 'than', 'all', 'GCN', 'models', ',', 'which', 'further', 'demonstrates', 'its', 'ability', 'to', 'learn', 'better', 'representations', 'from', 'full', 'trees', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'IN', 'PRP$', 'NNP', 'VBZ', 'DT', 'JJR', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NNS', ',', 'WDT', 'RB', 'VBZ', 'PRP$', 'NN', 'TO', 'VB', 'JJR', 'NNS', 'IN', 'JJ', 'NNS', '.']",29
relation_extraction,12,227,"Our C - AGGCN model achieves an F1 score of 68.2 , which outperforms the state - ofart C - GCN model by 1.8 points .","['Our', 'C', '-', 'AGGCN', 'model', 'achieves', 'an', 'F1', 'score', 'of', '68.2', ',', 'which', 'outperforms', 'the', 'state', '-', 'ofart', 'C', '-', 'GCN', 'model', 'by', '1.8', 'points', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP$', 'NNP', ':', 'NNP', 'NN', 'VBZ', 'DT', 'NNP', 'NN', 'IN', 'CD', ',', 'WDT', 'VBZ', 'DT', 'NN', ':', 'NN', 'NNP', ':', 'NNP', 'NN', 'IN', 'CD', 'NNS', '.']",26
relation_extraction,12,228,"We also notice that AGGCN and C - AGGCN achieve better precision and recall scores than GCN and C - GCN , respectively .","['We', 'also', 'notice', 'that', 'AGGCN', 'and', 'C', '-', 'AGGCN', 'achieve', 'better', 'precision', 'and', 'recall', 'scores', 'than', 'GCN', 'and', 'C', '-', 'GCN', ',', 'respectively', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'IN', 'NNP', 'CC', 'NNP', ':', 'NNP', 'VBP', 'JJR', 'NN', 'CC', 'NN', 'NNS', 'IN', 'NNP', 'CC', 'NNP', ':', 'NNP', ',', 'RB', '.']",24
relation_extraction,12,229,The performance gap between GCNs with pruned trees and AGGCNs with full trees empirically show that the AGGCN model is better at distinguishing relevant from irrelevant information for learning a better graph representation .,"['The', 'performance', 'gap', 'between', 'GCNs', 'with', 'pruned', 'trees', 'and', 'AGGCNs', 'with', 'full', 'trees', 'empirically', 'show', 'that', 'the', 'AGGCN', 'model', 'is', 'better', 'at', 'distinguishing', 'relevant', 'from', 'irrelevant', 'information', 'for', 'learning', 'a', 'better', 'graph', 'representation', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'IN', 'NNP', 'IN', 'JJ', 'NNS', 'CC', 'NNP', 'IN', 'JJ', 'NNS', 'RB', 'VBP', 'IN', 'DT', 'NNP', 'NN', 'VBZ', 'RBR', 'IN', 'VBG', 'NN', 'IN', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'JJR', 'NN', 'NN', '.']",34
relation_extraction,12,230,We also evaluate our model on the SemEval dataset under the same settings as .,"['We', 'also', 'evaluate', 'our', 'model', 'on', 'the', 'SemEval', 'dataset', 'under', 'the', 'same', 'settings', 'as', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'PRP$', 'NN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'IN', '.']",15
relation_extraction,12,233,"Our C - AGGCN model ( 85.7 ) consistently outperforms the C - GCN model ( 84.8 ) , showing the good generalizability .","['Our', 'C', '-', 'AGGCN', 'model', '(', '85.7', ')', 'consistently', 'outperforms', 'the', 'C', '-', 'GCN', 'model', '(', '84.8', ')', ',', 'showing', 'the', 'good', 'generalizability', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NNP', ':', 'NNP', 'NN', '(', 'CD', ')', 'RB', 'VBZ', 'DT', 'NNP', ':', 'NNP', 'NN', '(', 'CD', ')', ',', 'VBG', 'DT', 'JJ', 'NN', '.']",24
relation_extraction,12,238,We can observe that adding either attention guided layers or densely connected layers improves the performance of the model .,"['We', 'can', 'observe', 'that', 'adding', 'either', 'attention', 'guided', 'layers', 'or', 'densely', 'connected', 'layers', 'improves', 'the', 'performance', 'of', 'the', 'model', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']","['PRP', 'MD', 'VB', 'DT', 'VBG', 'DT', 'NN', 'VBD', 'NNS', 'CC', 'RB', 'JJ', 'NNS', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",20
relation_extraction,12,240,We also notice that the feed - forward layer is effective in our model .,"['We', 'also', 'notice', 'that', 'the', 'feed', '-', 'forward', 'layer', 'is', 'effective', 'in', 'our', 'model', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'IN', 'DT', 'NN', ':', 'NN', 'NN', 'VBZ', 'JJ', 'IN', 'PRP$', 'NN', '.']",15
relation_extraction,12,241,"Without the feed - forward layer , the result drops to an F1 score of 67.8 .","['Without', 'the', 'feed', '-', 'forward', 'layer', ',', 'the', 'result', 'drops', 'to', 'an', 'F1', 'score', 'of', '67.8', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ':', 'NN', 'NN', ',', 'DT', 'NN', 'VBZ', 'TO', 'DT', 'NNP', 'NN', 'IN', 'CD', '.']",17
relation_extraction,12,244,We can observe that all the C - AGGCN models with varied values of K are able to outperform the state - of - the - art C - GCN model ( reported in ) .,"['We', 'can', 'observe', 'that', 'all', 'the', 'C', '-', 'AGGCN', 'models', 'with', 'varied', 'values', 'of', 'K', 'are', 'able', 'to', 'outperform', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'C', '-', 'GCN', 'model', '(', 'reported', 'in', ')', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'MD', 'VB', 'IN', 'PDT', 'DT', 'NNP', ':', 'NNP', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'NNP', 'VBP', 'JJ', 'TO', 'VB', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNP', ':', 'NNP', 'NN', '(', 'VBN', 'IN', ')', '.']",36
relation_extraction,12,247,"In addition , we notice that the performance of C - AGGCN with full trees outperforms all C - AGGCNs with pruned trees .","['In', 'addition', ',', 'we', 'notice', 'that', 'the', 'performance', 'of', 'C', '-', 'AGGCN', 'with', 'full', 'trees', 'outperforms', 'all', 'C', '-', 'AGGCNs', 'with', 'pruned', 'trees', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'IN', 'NNP', ':', 'NN', 'IN', 'JJ', 'NNS', 'VBP', 'DT', 'NNP', ':', 'NN', 'IN', 'JJ', 'NNS', '.']",24
relation_extraction,12,252,"In general , C - AGGCN with full trees outperforms C - AGGCN with pruned trees and C - GCN against various sentence lengths .","['In', 'general', ',', 'C', '-', 'AGGCN', 'with', 'full', 'trees', 'outperforms', 'C', '-', 'AGGCN', 'with', 'pruned', 'trees', 'and', 'C', '-', 'GCN', 'against', 'various', 'sentence', 'lengths', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', ',', 'NNP', ':', 'NN', 'IN', 'JJ', 'NNS', 'VBP', 'NNP', ':', 'NN', 'IN', 'JJ', 'NNS', 'CC', 'NNP', ':', 'NNP', 'IN', 'JJ', 'NN', 'NNS', '.']",25
relation_extraction,12,254,"Moreover , the improvement achieved by C - AGGCN with pruned trees decays when the sentence length increases .","['Moreover', ',', 'the', 'improvement', 'achieved', 'by', 'C', '-', 'AGGCN', 'with', 'pruned', 'trees', 'decays', 'when', 'the', 'sentence', 'length', 'increases', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O']","['RB', ',', 'DT', 'NN', 'VBN', 'IN', 'NNP', ':', 'NN', 'IN', 'JJ', 'NNS', 'NNS', 'WRB', 'DT', 'NN', 'NN', 'NNS', '.']",19
relation_extraction,12,257,This suggests that C - AGGCN can benefit more from larger graphs ( full tree ) .,"['This', 'suggests', 'that', 'C', '-', 'AGGCN', 'can', 'benefit', 'more', 'from', 'larger', 'graphs', '(', 'full', 'tree', ')', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'IN', 'NNP', ':', 'NN', 'MD', 'VB', 'RBR', 'IN', 'JJR', 'NN', '(', 'JJ', 'NN', ')', '.']",17
relation_extraction,12,261,C - AGGCN consistently outperforms C - GCN under the same amount of training data .,"['C', '-', 'AGGCN', 'consistently', 'outperforms', 'C', '-', 'GCN', 'under', 'the', 'same', 'amount', 'of', 'training', 'data', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'RB', 'VBZ', 'NNP', ':', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', '.']",16
relation_extraction,12,262,"When the size of training data increases , we can observe that the performance gap becomes more obvious .","['When', 'the', 'size', 'of', 'training', 'data', 'increases', ',', 'we', 'can', 'observe', 'that', 'the', 'performance', 'gap', 'becomes', 'more', 'obvious', '.']","['B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['WRB', 'DT', 'NN', 'IN', 'VBG', 'NN', 'NNS', ',', 'PRP', 'MD', 'VB', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'RBR', 'JJ', '.']",19
relation_extraction,12,263,"Particularly , using 80 % of the training data , the C - AGGCN model is able to achieve a F 1 score of 66.5 , higher than C - GCN trained on the whole dataset .","['Particularly', ',', 'using', '80', '%', 'of', 'the', 'training', 'data', ',', 'the', 'C', '-', 'AGGCN', 'model', 'is', 'able', 'to', 'achieve', 'a', 'F', '1', 'score', 'of', '66.5', ',', 'higher', 'than', 'C', '-', 'GCN', 'trained', 'on', 'the', 'whole', 'dataset', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'VBG', 'CD', 'NN', 'IN', 'DT', 'NN', 'NNS', ',', 'DT', 'NNP', ':', 'NNP', 'NN', 'VBZ', 'JJ', 'TO', 'VB', 'DT', 'NNP', 'CD', 'NN', 'IN', 'CD', ',', 'JJR', 'IN', 'NNP', ':', 'NNP', 'VBD', 'IN', 'DT', 'JJ', 'NN', '.']",37
relation_extraction,0,2,Going out on a limb : Joint Extraction of Entity Mentions and Relations without Dependency Trees,"['Going', 'out', 'on', 'a', 'limb', ':', 'Joint', 'Extraction', 'of', 'Entity', 'Mentions', 'and', 'Relations', 'without', 'Dependency', 'Trees']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['VBG', 'RP', 'IN', 'DT', 'NN', ':', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'CC', 'NNP', 'IN', 'NNP', 'NNP']",16
relation_extraction,0,10,Extraction of entities and their relations from text belongs to a very well - studied family of structured prediction tasks in NLP .,"['Extraction', 'of', 'entities', 'and', 'their', 'relations', 'from', 'text', 'belongs', 'to', 'a', 'very', 'well', '-', 'studied', 'family', 'of', 'structured', 'prediction', 'tasks', 'in', 'NLP', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'IN', 'NNS', 'CC', 'PRP$', 'NNS', 'IN', 'JJ', 'NNS', 'TO', 'DT', 'RB', 'RB', ':', 'VBN', 'NN', 'IN', 'JJ', 'NN', 'NNS', 'IN', 'NNP', '.']",23
relation_extraction,0,12,Several methods have been proposed for entity mention and relation extraction at the sentencelevel .,"['Several', 'methods', 'have', 'been', 'proposed', 'for', 'entity', 'mention', 'and', 'relation', 'extraction', 'at', 'the', 'sentencelevel', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'NNS', 'VBP', 'VBN', 'VBN', 'IN', 'NN', 'NN', 'CC', 'NN', 'NN', 'IN', 'DT', 'NN', '.']",15
relation_extraction,0,24,"In this paper , we propose a novel RNN - based model for the joint extraction of entity mentions and relations .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', 'RNN', '-', 'based', 'model', 'for', 'the', 'joint', 'extraction', 'of', 'entity', 'mentions', 'and', 'relations', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NNP', ':', 'VBN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', 'CC', 'NNS', '.']",22
relation_extraction,0,25,"Unlike other models , our model does not depend on any dependency tree information .","['Unlike', 'other', 'models', ',', 'our', 'model', 'does', 'not', 'depend', 'on', 'any', 'dependency', 'tree', 'information', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', 'NNS', ',', 'PRP$', 'NN', 'VBZ', 'RB', 'VB', 'IN', 'DT', 'NN', 'JJ', 'NN', '.']",15
relation_extraction,0,26,Our RNN - based model is a multi - layer bidirectional LSTM over a sequence .,"['Our', 'RNN', '-', 'based', 'model', 'is', 'a', 'multi', '-', 'layer', 'bidirectional', 'LSTM', 'over', 'a', 'sequence', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['PRP$', 'NNP', ':', 'VBN', 'NN', 'VBZ', 'DT', 'JJ', ':', 'NN', 'JJ', 'NNP', 'IN', 'DT', 'NN', '.']",16
relation_extraction,0,27,We encode the output sequence from left - to - right .,"['We', 'encode', 'the', 'output', 'sequence', 'from', 'left', '-', 'to', '-', 'right', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'VBN', ':', 'TO', ':', 'NN', '.']",12
relation_extraction,0,28,"At each time step , we use an attention - like model on the previously decoded time steps , to identify the tokens in a specified relation with the current token .","['At', 'each', 'time', 'step', ',', 'we', 'use', 'an', 'attention', '-', 'like', 'model', 'on', 'the', 'previously', 'decoded', 'time', 'steps', ',', 'to', 'identify', 'the', 'tokens', 'in', 'a', 'specified', 'relation', 'with', 'the', 'current', 'token', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', ':', 'IN', 'NN', 'IN', 'DT', 'RB', 'VBN', 'NN', 'NNS', ',', 'TO', 'VB', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",32
relation_extraction,0,29,We also add an additional layer to our network to encode the output sequence from right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,"['We', 'also', 'add', 'an', 'additional', 'layer', 'to', 'our', 'network', 'to', 'encode', 'the', 'output', 'sequence', 'from', 'right', '-', 'to', '-', 'left', 'and', 'find', 'significant', 'improvement', 'on', 'the', 'performance', 'of', 'relation', 'identification', 'using', 'bi-directional', 'encoding', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'JJ', 'NN', 'TO', 'PRP$', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'JJ', ':', 'TO', ':', 'NN', 'CC', 'VB', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', 'VBG', 'JJ', 'NN', '.']",34
relation_extraction,0,170,The model proposed by is a feature - based structured perceptron model with efficient beam - search .,"['The', 'model', 'proposed', 'by', 'is', 'a', 'feature', '-', 'based', 'structured', 'perceptron', 'model', 'with', 'efficient', 'beam', '-', 'search', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBN', 'IN', 'VBZ', 'DT', 'NN', ':', 'VBN', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'NN', ':', 'NN', '.']",18
relation_extraction,0,171,They employ a segment - based decoder instead of token - based decoding .,"['They', 'employ', 'a', 'segment', '-', 'based', 'decoder', 'instead', 'of', 'token', '-', 'based', 'decoding', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', ':', 'VBN', 'NN', 'RB', 'IN', 'JJ', ':', 'VBN', 'NN', '.']",14
relation_extraction,0,173,"( SPTree ) recently proposed a LSTM - based model with a sequence layer for entity identification , and a tree - based dependency layer which identifies relations between pairs of candidate entities using the shortest dependency path between them .","['(', 'SPTree', ')', 'recently', 'proposed', 'a', 'LSTM', '-', 'based', 'model', 'with', 'a', 'sequence', 'layer', 'for', 'entity', 'identification', ',', 'and', 'a', 'tree', '-', 'based', 'dependency', 'layer', 'which', 'identifies', 'relations', 'between', 'pairs', 'of', 'candidate', 'entities', 'using', 'the', 'shortest', 'dependency', 'path', 'between', 'them', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'NNP', ')', 'RB', 'VBD', 'DT', 'NNP', ':', 'VBN', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'NN', 'NN', ',', 'CC', 'DT', 'NN', ':', 'VBN', 'NN', 'NN', 'WDT', 'VBZ', 'NNS', 'IN', 'NNS', 'IN', 'NN', 'NNS', 'VBG', 'DT', 'JJS', 'NN', 'NN', 'IN', 'PRP', '.']",41
relation_extraction,0,174,We also employed our previous approach for extraction of opinion entities and relations to this task .,"['We', 'also', 'employed', 'our', 'previous', 'approach', 'for', 'extraction', 'of', 'opinion', 'entities', 'and', 'relations', 'to', 'this', 'task', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBD', 'PRP$', 'JJ', 'NN', 'IN', 'NN', 'IN', 'NN', 'NNS', 'CC', 'NNS', 'TO', 'DT', 'NN', '.']",17
relation_extraction,0,180,We train our model using Adadelta with gradient clipping .,"['We', 'train', 'our', 'model', 'using', 'Adadelta', 'with', 'gradient', 'clipping', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'VBG', 'NNP', 'IN', 'JJ', 'NN', '.']",10
relation_extraction,0,181,We regularize our network using dropout with the drop - out rate tuned using development set .,"['We', 'regularize', 'our', 'network', 'using', 'dropout', 'with', 'the', 'drop', '-', 'out', 'rate', 'tuned', 'using', 'development', 'set', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'VBG', 'NN', 'IN', 'DT', 'NN', ':', 'RP', 'NN', 'VBD', 'VBG', 'NN', 'NN', '.']",17
relation_extraction,0,186,We have 3 hidden layers in our network and the dimensionality of the hidden units is 100 .,"['We', 'have', '3', 'hidden', 'layers', 'in', 'our', 'network', 'and', 'the', 'dimensionality', 'of', 'the', 'hidden', 'units', 'is', '100', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'CD', 'JJ', 'NNS', 'IN', 'PRP$', 'NN', 'CC', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'VBZ', 'CD', '.']",18
relation_extraction,0,187,All the weights in the network are initialized from small random uniform noise .,"['All', 'the', 'weights', 'in', 'the', 'network', 'are', 'initialized', 'from', 'small', 'random', 'uniform', 'noise', '.']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PDT', 'DT', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'VBN', 'IN', 'JJ', 'NN', 'JJ', 'NN', '.']",14
relation_extraction,0,188,We tune our hyperparameters based on ACE05 development set and use them for training on ACE04 dataset .,"['We', 'tune', 'our', 'hyperparameters', 'based', 'on', 'ACE05', 'development', 'set', 'and', 'use', 'them', 'for', 'training', 'on', 'ACE04', 'dataset', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'NNS', 'VBN', 'IN', 'NNP', 'NN', 'NN', 'CC', 'VB', 'PRP', 'IN', 'VBG', 'IN', 'NNP', 'NN', '.']",18
relation_extraction,0,196,Multiple Relations,"['Multiple', 'Relations']","['B-n', 'I-n']","['JJ', 'NNS']",2
relation_extraction,0,197,"We find that modifying our objective to include multiple relations improves the recall of our system on relations , leading to slight improvement on the over all performance on relations .","['We', 'find', 'that', 'modifying', 'our', 'objective', 'to', 'include', 'multiple', 'relations', 'improves', 'the', 'recall', 'of', 'our', 'system', 'on', 'relations', ',', 'leading', 'to', 'slight', 'improvement', 'on', 'the', 'over', 'all', 'performance', 'on', 'relations', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'VBG', 'PRP$', 'NN', 'TO', 'VB', 'JJ', 'NNS', 'VBZ', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'IN', 'NNS', ',', 'VBG', 'TO', 'JJ', 'NN', 'IN', 'DT', 'IN', 'DT', 'NN', 'IN', 'NNS', '.']",31
relation_extraction,0,201,"By adding bidirectional encoding to our system , we find that we can significantly improve the performance of our system compared to left - to - right encoding .","['By', 'adding', 'bidirectional', 'encoding', 'to', 'our', 'system', ',', 'we', 'find', 'that', 'we', 'can', 'significantly', 'improve', 'the', 'performance', 'of', 'our', 'system', 'compared', 'to', 'left', '-', 'to', '-', 'right', 'encoding', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBG', 'JJ', 'NN', 'TO', 'PRP$', 'NN', ',', 'PRP', 'VBP', 'IN', 'PRP', 'MD', 'RB', 'VB', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'VBN', 'TO', 'VB', ':', 'TO', ':', 'NN', 'NN', '.']",29
relation_extraction,0,202,It also improves precision compared to left - toright decoding combined with multiple relations objective .,"['It', 'also', 'improves', 'precision', 'compared', 'to', 'left', '-', 'toright', 'decoding', 'combined', 'with', 'multiple', 'relations', 'objective', '.']","['O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBZ', 'NN', 'VBN', 'TO', 'VB', ':', 'NN', 'VBG', 'VBN', 'IN', 'JJ', 'NNS', 'VBP', '.']",16
relation_extraction,0,203,We find that for some relations it is easier to detect them with respect to one of the entities in the entity pair .,"['We', 'find', 'that', 'for', 'some', 'relations', 'it', 'is', 'easier', 'to', 'detect', 'them', 'with', 'respect', 'to', 'one', 'of', 'the', 'entities', 'in', 'the', 'entity', 'pair', '.']","['O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'IN', 'DT', 'NNS', 'PRP', 'VBZ', 'JJR', 'TO', 'VB', 'PRP', 'IN', 'NN', 'TO', 'CD', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",24
relation_extraction,0,204,PHYS relation is easier identified with respect to GPE entity than PER entity .,"['PHYS', 'relation', 'is', 'easier', 'identified', 'with', 'respect', 'to', 'GPE', 'entity', 'than', 'PER', 'entity', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'NN', 'VBZ', 'JJR', 'VBN', 'IN', 'NN', 'TO', 'NNP', 'NN', 'IN', 'NNP', 'NN', '.']",14
sentence_classification,1,2,Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts,"['Hierarchical', 'Neural', 'Networks', 'for', 'Sequential', 'Sentence', 'Classification', 'in', 'Medical', 'Scientific', 'Abstracts']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",11
sentence_classification,1,5,"This hampers the traditional sentence classification approaches to the problem of sequential sentence classification , where structured prediction is needed for better over all classification performance .","['This', 'hampers', 'the', 'traditional', 'sentence', 'classification', 'approaches', 'to', 'the', 'problem', 'of', 'sequential', 'sentence', 'classification', ',', 'where', 'structured', 'prediction', 'is', 'needed', 'for', 'better', 'over', 'all', 'classification', 'performance', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'NNS', 'TO', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', ',', 'WRB', 'VBN', 'NN', 'VBZ', 'VBN', 'IN', 'JJR', 'IN', 'DT', 'NN', 'NN', '.']",27
sentence_classification,1,21,"In this work , we present a hierarchical neural network model for the sequential sentence classification task , which we call a hierarchical sequential labeling network ( HSLN ) .","['In', 'this', 'work', ',', 'we', 'present', 'a', 'hierarchical', 'neural', 'network', 'model', 'for', 'the', 'sequential', 'sentence', 'classification', 'task', ',', 'which', 'we', 'call', 'a', 'hierarchical', 'sequential', 'labeling', 'network', '(', 'HSLN', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', ',', 'WDT', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'NN', '(', 'NNP', ')', '.']",30
sentence_classification,1,22,"Our model first uses a RNN or CNN layer to individually encode the sentence representation from the sequence of word embeddings , then uses another bi - LSTM layer to take as input the individual sentence representation and output the contextualized sentence representation , subsequently uses a single - hidden - layer feed - forward network to transform the sentence representation to the probability vector , and finally optimizes the predicted label sequence jointly via a CRF layer .","['Our', 'model', 'first', 'uses', 'a', 'RNN', 'or', 'CNN', 'layer', 'to', 'individually', 'encode', 'the', 'sentence', 'representation', 'from', 'the', 'sequence', 'of', 'word', 'embeddings', ',', 'then', 'uses', 'another', 'bi', '-', 'LSTM', 'layer', 'to', 'take', 'as', 'input', 'the', 'individual', 'sentence', 'representation', 'and', 'output', 'the', 'contextualized', 'sentence', 'representation', ',', 'subsequently', 'uses', 'a', 'single', '-', 'hidden', '-', 'layer', 'feed', '-', 'forward', 'network', 'to', 'transform', 'the', 'sentence', 'representation', 'to', 'the', 'probability', 'vector', ',', 'and', 'finally', 'optimizes', 'the', 'predicted', 'label', 'sequence', 'jointly', 'via', 'a', 'CRF', 'layer', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'JJ', 'VBZ', 'DT', 'NNP', 'CC', 'NNP', 'NN', 'TO', 'RB', 'VB', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', ',', 'RB', 'VBZ', 'DT', 'NN', ':', 'NNP', 'NN', 'TO', 'VB', 'IN', 'NN', 'DT', 'JJ', 'NN', 'NN', 'CC', 'NN', 'DT', 'JJ', 'NN', 'NN', ',', 'RB', 'VBZ', 'DT', 'JJ', ':', 'JJ', ':', 'NN', 'NN', ':', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'TO', 'DT', 'NN', 'NN', ',', 'CC', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'RB', 'IN', 'DT', 'NNP', 'NN', '.']",79
sentence_classification,1,110,"The token embeddings were pre-trained on a large corpus combining Wikipedia , PubMed , and PMC texts ( Moen and Ananiadou , 2013 ) using the word2vec tool 4 ( denoted as "" Word2vec- wiki+P.M. "" ) .","['The', 'token', 'embeddings', 'were', 'pre-trained', 'on', 'a', 'large', 'corpus', 'combining', 'Wikipedia', ',', 'PubMed', ',', 'and', 'PMC', 'texts', '(', 'Moen', 'and', 'Ananiadou', ',', '2013', ')', 'using', 'the', 'word2vec', 'tool', '4', '(', 'denoted', 'as', '""', 'Word2vec-', 'wiki+P.M.', '""', ')', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NNS', 'VBD', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'VBG', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'VBG', 'DT', 'NN', 'NN', 'CD', '(', 'VBN', 'IN', 'JJ', 'NNP', 'NN', 'NN', ')', '.']",38
sentence_classification,1,111,They are fixed during the training phase to avoid over-fitting .,"['They', 'are', 'fixed', 'during', 'the', 'training', 'phase', 'to', 'avoid', 'over-fitting', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'VBN', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', 'NN', '.']",11
sentence_classification,1,115,"The model is trained using the Adam optimization method ( Kingma and Ba , 2014 ) .","['The', 'model', 'is', 'trained', 'using', 'the', 'Adam', 'optimization', 'method', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'VBG', 'DT', 'NNP', 'NN', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', '.']",17
sentence_classification,1,116,The learning rate is initially set as 0.003 and decayed by 0.9 after each epoch .,"['The', 'learning', 'rate', 'is', 'initially', 'set', 'as', '0.003', 'and', 'decayed', 'by', '0.9', 'after', 'each', 'epoch', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'CD', 'CC', 'VBN', 'IN', 'CD', 'IN', 'DT', 'NN', '.']",16
sentence_classification,1,117,"For regularization , dropout ( Srivastava et al. , 2014 ) is applied to each layer .","['For', 'regularization', ',', 'dropout', '(', 'Srivastava', 'et', 'al.', ',', '2014', ')', 'is', 'applied', 'to', 'each', 'layer', '.']","['B-p', 'B-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'NN', '(', 'NNP', 'RB', 'RB', ',', 'CD', ')', 'VBZ', 'VBN', 'TO', 'DT', 'NN', '.']",17
sentence_classification,1,119,"To reduce this gap , we adopted the dropout with expectation - linear regularization introduced by to explicitly control the inference gap and thus improve the generaliza - tion performance .","['To', 'reduce', 'this', 'gap', ',', 'we', 'adopted', 'the', 'dropout', 'with', 'expectation', '-', 'linear', 'regularization', 'introduced', 'by', 'to', 'explicitly', 'control', 'the', 'inference', 'gap', 'and', 'thus', 'improve', 'the', 'generaliza', '-', 'tion', 'performance', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBD', 'DT', 'NN', 'IN', 'NN', ':', 'JJ', 'NN', 'VBN', 'IN', 'TO', 'RB', 'VB', 'DT', 'NN', 'NN', 'CC', 'RB', 'VB', 'DT', 'NN', ':', 'NN', 'NN', '.']",31
sentence_classification,1,120,Hyperparameters were optimized via grid search based on the validation set and the best configuration is shown in .,"['Hyperparameters', 'were', 'optimized', 'via', 'grid', 'search', 'based', 'on', 'the', 'validation', 'set', 'and', 'the', 'best', 'configuration', 'is', 'shown', 'in', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNS', 'VBD', 'VBN', 'IN', 'JJ', 'NN', 'VBN', 'IN', 'DT', 'NN', 'NN', 'CC', 'DT', 'JJS', 'NN', 'VBZ', 'VBN', 'IN', '.']",19
sentence_classification,1,121,"The window sizes of the CNN encoder in the sentence encoding layer are 2 , 3 , 4 and 5 .","['The', 'window', 'sizes', 'of', 'the', 'CNN', 'encoder', 'in', 'the', 'sentence', 'encoding', 'layer', 'are', '2', ',', '3', ',', '4', 'and', '5', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'IN', 'DT', 'NN', 'VBG', 'NN', 'VBP', 'CD', ',', 'CD', ',', 'CD', 'CC', 'CD', '.']",21
sentence_classification,1,129,"As can be seen from , our HSLN - CNN model uni-formly suffers a little more from the component removal than the HSLN - RNN model , indicating that the HSLN - RNN model is more robust .","['As', 'can', 'be', 'seen', 'from', ',', 'our', 'HSLN', '-', 'CNN', 'model', 'uni-formly', 'suffers', 'a', 'little', 'more', 'from', 'the', 'component', 'removal', 'than', 'the', 'HSLN', '-', 'RNN', 'model', ',', 'indicating', 'that', 'the', 'HSLN', '-', 'RNN', 'model', 'is', 'more', 'robust', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'MD', 'VB', 'VBN', 'IN', ',', 'PRP$', 'NNP', ':', 'NNP', 'FW', 'JJ', 'NNS', 'DT', 'RB', 'RBR', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NNP', ':', 'NNP', 'NN', ',', 'VBG', 'IN', 'DT', 'NNP', ':', 'NNP', 'NN', 'VBZ', 'RBR', 'JJ', '.']",38
sentence_classification,1,130,"When the context enriching layer is removed , both models experience the most significant performance drop and can only be on par with the previous stateof - the - art results , strongly demonstrating that this proposed component is the key to the performance improvement of our model .","['When', 'the', 'context', 'enriching', 'layer', 'is', 'removed', ',', 'both', 'models', 'experience', 'the', 'most', 'significant', 'performance', 'drop', 'and', 'can', 'only', 'be', 'on', 'par', 'with', 'the', 'previous', 'stateof', '-', 'the', '-', 'art', 'results', ',', 'strongly', 'demonstrating', 'that', 'this', 'proposed', 'component', 'is', 'the', 'key', 'to', 'the', 'performance', 'improvement', 'of', 'our', 'model', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['WRB', 'DT', 'NN', 'VBG', 'NN', 'VBZ', 'VBN', ',', 'DT', 'NNS', 'VBP', 'DT', 'RBS', 'JJ', 'NN', 'NN', 'CC', 'MD', 'RB', 'VB', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'DT', ':', 'NN', 'NNS', ',', 'RB', 'VBG', 'IN', 'DT', 'VBN', 'NN', 'VBZ', 'DT', 'NN', 'TO', 'DT', 'NN', 'NN', 'IN', 'PRP$', 'NN', '.']",49
sentence_classification,1,131,"Furthermore , even without the label sequence optimization layer , our model still significantly outperforms the best published methods that are empowered by this layer , indicating that the context enriching layer we propose can help optimize the label sequence by considering the context information from the surrounding sentences .","['Furthermore', ',', 'even', 'without', 'the', 'label', 'sequence', 'optimization', 'layer', ',', 'our', 'model', 'still', 'significantly', 'outperforms', 'the', 'best', 'published', 'methods', 'that', 'are', 'empowered', 'by', 'this', 'layer', ',', 'indicating', 'that', 'the', 'context', 'enriching', 'layer', 'we', 'propose', 'can', 'help', 'optimize', 'the', 'label', 'sequence', 'by', 'considering', 'the', 'context', 'information', 'from', 'the', 'surrounding', 'sentences', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'RB', 'IN', 'DT', 'NN', 'NN', 'NN', 'NN', ',', 'PRP$', 'NN', 'RB', 'RB', 'VBZ', 'DT', 'JJS', 'VBN', 'NNS', 'WDT', 'VBP', 'VBN', 'IN', 'DT', 'NN', ',', 'VBG', 'IN', 'DT', 'NN', 'VBG', 'NN', 'PRP', 'VBP', 'MD', 'VB', 'VB', 'DT', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'NN', 'IN', 'DT', 'VBG', 'NNS', '.']",50
sentence_classification,1,132,"Last but not the least , the dropout regularization and attention - based pooling components we add to our system can help further improve the model in a limited extent . :","['Last', 'but', 'not', 'the', 'least', ',', 'the', 'dropout', 'regularization', 'and', 'attention', '-', 'based', 'pooling', 'components', 'we', 'add', 'to', 'our', 'system', 'can', 'help', 'further', 'improve', 'the', 'model', 'in', 'a', 'limited', 'extent', '.', ':']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O']","['JJ', 'CC', 'RB', 'DT', 'JJS', ',', 'DT', 'NN', 'NN', 'CC', 'NN', ':', 'VBN', 'JJ', 'NNS', 'PRP', 'VBP', 'TO', 'PRP$', 'NN', 'MD', 'VB', 'RB', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', '.', ':']",32
sentence_classification,2,2,Translations as Additional Contexts for Sentence Classification,"['Translations', 'as', 'Additional', 'Contexts', 'for', 'Sentence', 'Classification']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNS', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",7
sentence_classification,2,20,"In this paper , we propose the usage of translations as compelling and effective domain - free contexts , or contexts that are always available no matter what the task domain is .","['In', 'this', 'paper', ',', 'we', 'propose', 'the', 'usage', 'of', 'translations', 'as', 'compelling', 'and', 'effective', 'domain', '-', 'free', 'contexts', ',', 'or', 'contexts', 'that', 'are', 'always', 'available', 'no', 'matter', 'what', 'the', 'task', 'domain', 'is', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NNS', 'IN', 'NN', 'CC', 'JJ', 'NN', ':', 'JJ', 'NN', ',', 'CC', 'NN', 'WDT', 'VBP', 'RB', 'JJ', 'DT', 'NN', 'WP', 'DT', 'NN', 'NN', 'VBZ', '.']",33
sentence_classification,2,37,"In this paper , we propose a method to mitigate the possible problems when using translated sentences as context based on the following observations .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'method', 'to', 'mitigate', 'the', 'possible', 'problems', 'when', 'using', 'translated', 'sentences', 'as', 'context', 'based', 'on', 'the', 'following', 'observations', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'WRB', 'VBG', 'JJ', 'NNS', 'IN', 'NN', 'VBN', 'IN', 'DT', 'JJ', 'NNS', '.']",25
sentence_classification,2,42,"Based on these observations , we present a neural attentionbased multiple context fixing attachment ( MCFA ) .","['Based', 'on', 'these', 'observations', ',', 'we', 'present', 'a', 'neural', 'attentionbased', 'multiple', 'context', 'fixing', 'attachment', '(', 'MCFA', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'VBG', 'NN', '(', 'NNP', ')', '.']",18
sentence_classification,2,43,"MCFA is a series of modules that uses all the sentence vectors ( e.g. Arabic , English , Korean , etc. ) as context to fix a sentence vector ( e.g. Korean ) .","['MCFA', 'is', 'a', 'series', 'of', 'modules', 'that', 'uses', 'all', 'the', 'sentence', 'vectors', '(', 'e.g.', 'Arabic', ',', 'English', ',', 'Korean', ',', 'etc.', ')', 'as', 'context', 'to', 'fix', 'a', 'sentence', 'vector', '(', 'e.g.', 'Korean', ')', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'IN', 'NNS', 'WDT', 'VBZ', 'PDT', 'DT', 'NN', 'NNS', '(', 'JJ', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NN', ')', 'IN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', '(', 'JJ', 'NNP', ')', '.']",34
sentence_classification,2,46,MCFA computes two sentence usability metrics to control the noise when fixing vectors : ( a ) self usability ? i ( a ) weighs the confidence of using sentence a in solving the task .,"['MCFA', 'computes', 'two', 'sentence', 'usability', 'metrics', 'to', 'control', 'the', 'noise', 'when', 'fixing', 'vectors', ':', '(', 'a', ')', 'self', 'usability', '?', 'i', '(', 'a', ')', 'weighs', 'the', 'confidence', 'of', 'using', 'sentence', 'a', 'in', 'solving', 'the', 'task', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['NNP', 'VBZ', 'CD', 'NN', 'NN', 'NNS', 'TO', 'VB', 'DT', 'NN', 'WRB', 'VBG', 'NNS', ':', '(', 'DT', ')', 'NN', 'NN', '.', 'NN', '(', 'DT', ')', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'NN', 'DT', 'IN', 'VBG', 'DT', 'NN', '.']",36
sentence_classification,2,47,"( b ) relative usability ? r ( a , b ) weighs the confidence of using sentence a in fixing sentence b.","['(', 'b', ')', 'relative', 'usability', '?', 'r', '(', 'a', ',', 'b', ')', 'weighs', 'the', 'confidence', 'of', 'using', 'sentence', 'a', 'in', 'fixing', 'sentence', 'b.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n']","['(', 'NN', ')', 'NN', 'NN', '.', 'NN', '(', 'DT', ',', 'NN', ')', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'NN', 'DT', 'IN', 'VBG', 'NN', 'NN']",23
sentence_classification,2,49,"( 1 ) MCFA is attached after encoding the sentence , which makes it widely adaptable to other models .","['(', '1', ')', 'MCFA', 'is', 'attached', 'after', 'encoding', 'the', 'sentence', ',', 'which', 'makes', 'it', 'widely', 'adaptable', 'to', 'other', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'NNP', 'VBZ', 'VBN', 'IN', 'VBG', 'DT', 'NN', ',', 'WDT', 'VBZ', 'PRP', 'RB', 'JJ', 'TO', 'JJ', 'NNS', '.']",20
sentence_classification,2,51,"( 3 ) MCFA moves the vectors inside the same space , thus preserves the meaning of vector dimensions .","['(', '3', ')', 'MCFA', 'moves', 'the', 'vectors', 'inside', 'the', 'same', 'space', ',', 'thus', 'preserves', 'the', 'meaning', 'of', 'vector', 'dimensions', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['(', 'CD', ')', 'NNP', 'VBZ', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', ',', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'NN', 'NNS', '.']",20
sentence_classification,2,150,Tokenization is done using the polyglot library 7 .,"['Tokenization', 'is', 'done', 'using', 'the', 'polyglot', 'library', '7', '.']","['B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O']","['NN', 'VBZ', 'VBN', 'VBG', 'DT', 'NN', 'JJ', 'CD', '.']",9
sentence_classification,2,151,We experiment on using only one additional context ( N = 1 ) and using all ten languages at once ( N = 10 ) .,"['We', 'experiment', 'on', 'using', 'only', 'one', 'additional', 'context', '(', 'N', '=', '1', ')', 'and', 'using', 'all', 'ten', 'languages', 'at', 'once', '(', 'N', '=', '10', ')', '.']","['O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'VBG', 'RB', 'CD', 'JJ', 'NN', '(', 'NNP', 'NNP', 'CD', ')', 'CC', 'VBG', 'DT', 'JJ', 'NNS', 'IN', 'RB', '(', 'NNP', 'NNP', 'CD', ')', '.']",26
sentence_classification,2,153,"For our CNN , we use rectified linear units and three filters with different window sizes h = 3 , 4 , 5 with 100 feature maps each , following .","['For', 'our', 'CNN', ',', 'we', 'use', 'rectified', 'linear', 'units', 'and', 'three', 'filters', 'with', 'different', 'window', 'sizes', 'h', '=', '3', ',', '4', ',', '5', 'with', '100', 'feature', 'maps', 'each', ',', 'following', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PRP$', 'NNP', ',', 'PRP', 'VBP', 'JJ', 'JJ', 'NNS', 'CC', 'CD', 'NNS', 'IN', 'JJ', 'NN', 'NNS', 'VBP', 'JJ', 'CD', ',', 'CD', ',', 'CD', 'IN', 'CD', 'NN', 'NNS', 'DT', ',', 'VBG', '.']",31
sentence_classification,2,154,"For the final sentence vector , we concatenate the feature maps to get a 300 - dimension vector .","['For', 'the', 'final', 'sentence', 'vector', ',', 'we', 'concatenate', 'the', 'feature', 'maps', 'to', 'get', 'a', '300', '-', 'dimension', 'vector', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'CD', ':', 'NN', 'NN', '.']",19
sentence_classification,2,155,We use dropout on all nonlinear connections with a dropout rate of 0.5 .,"['We', 'use', 'dropout', 'on', 'all', 'nonlinear', 'connections', 'with', 'a', 'dropout', 'rate', 'of', '0.5', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'IN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",14
sentence_classification,2,156,"We also use an l 2 constraint of 3 , following for accurate comparisons .","['We', 'also', 'use', 'an', 'l', '2', 'constraint', 'of', '3', ',', 'following', 'for', 'accurate', 'comparisons', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', 'CD', 'NN', 'IN', 'CD', ',', 'VBG', 'IN', 'JJ', 'NNS', '.']",15
sentence_classification,2,157,We use FastText pre-trained vectors 8 for all our data sets and their corresponding additional context .,"['We', 'use', 'FastText', 'pre-trained', 'vectors', '8', 'for', 'all', 'our', 'data', 'sets', 'and', 'their', 'corresponding', 'additional', 'context', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', 'JJ', 'NNS', 'CD', 'IN', 'DT', 'PRP$', 'NNS', 'NNS', 'CC', 'PRP$', 'JJ', 'JJ', 'NN', '.']",17
sentence_classification,2,158,"During training , we use mini-batch size of 50 .","['During', 'training', ',', 'we', 'use', 'mini-batch', 'size', 'of', '50', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'IN', 'CD', '.']",10
sentence_classification,2,159,Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule .,"['Training', 'is', 'done', 'via', 'stochastic', 'gradient', 'descent', 'over', 'shuffled', 'mini-batches', 'with', 'the', 'Adadelta', 'update', 'rule', '.']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NN', 'VBZ', 'VBN', 'IN', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'NN', '.']",16
sentence_classification,2,160,We perform early stopping using a random 10 % of the training set as the development set .,"['We', 'perform', 'early', 'stopping', 'using', 'a', 'random', '10', '%', 'of', 'the', 'training', 'set', 'as', 'the', 'development', 'set', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'VBG', 'DT', 'JJ', 'CD', 'NN', 'IN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",18
sentence_classification,2,169,We show that CNN + MCFA achieves state of the art performance on three of the four data sets and performs competitively on one data set .,"['We', 'show', 'that', 'CNN', '+', 'MCFA', 'achieves', 'state', 'of', 'the', 'art', 'performance', 'on', 'three', 'of', 'the', 'four', 'data', 'sets', 'and', 'performs', 'competitively', 'on', 'one', 'data', 'set', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'NNP', 'NNP', 'NNP', 'VBZ', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', 'IN', 'DT', 'CD', 'NNS', 'NNS', 'CC', 'NNS', 'RB', 'IN', 'CD', 'NN', 'NN', '.']",27
sentence_classification,2,170,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .","['When', 'N', '=', '1', ',', 'MCFA', 'increases', 'the', 'performance', 'of', 'a', 'normal', 'CNN', 'from', '85.0', 'to', '87.6', ',', 'beating', 'the', 'current', 'state', 'of', 'the', 'art', 'on', 'the', 'CR', 'data', 'set', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['WRB', 'NNP', 'VBZ', 'CD', ',', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'IN', 'CD', 'TO', 'CD', ',', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NN', '.']",31
sentence_classification,2,171,"When N = 10 , MCFA additionally beats the state of the art on the TREC data set .","['When', 'N', '=', '10', ',', 'MCFA', 'additionally', 'beats', 'the', 'state', 'of', 'the', 'art', 'on', 'the', 'TREC', 'data', 'set', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['WRB', 'NNP', 'VBZ', 'CD', ',', 'NNP', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NN', '.']",19
sentence_classification,2,172,"Finally , our ensemble classifier additionally outperforms all competing models on the MR data set .","['Finally', ',', 'our', 'ensemble', 'classifier', 'additionally', 'outperforms', 'all', 'competing', 'models', 'on', 'the', 'MR', 'data', 'set', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'JJ', 'NN', 'RB', 'VBZ', 'DT', 'VBG', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'NN', '.']",16
sentence_classification,2,186,"On all data sets except SUBJ , the accuracy of CNN + B1 decreases from the base CNN accuracy , while the accuracy of our model always improves from the base CNN accuracy .","['On', 'all', 'data', 'sets', 'except', 'SUBJ', ',', 'the', 'accuracy', 'of', 'CNN', '+', 'B1', 'decreases', 'from', 'the', 'base', 'CNN', 'accuracy', ',', 'while', 'the', 'accuracy', 'of', 'our', 'model', 'always', 'improves', 'from', 'the', 'base', 'CNN', 'accuracy', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNS', 'NNS', 'IN', 'NNP', ',', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'VBZ', 'IN', 'DT', 'NN', 'NNP', 'NN', ',', 'IN', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'RB', 'VBZ', 'IN', 'DT', 'NN', 'NNP', 'NN', '.']",34
sentence_classification,2,188,"We also compare two different kinds of additional context : topics ( TopCNN ) and translations ( CNN + B1 , CNN + B2 , CNN + MCFA ) .","['We', 'also', 'compare', 'two', 'different', 'kinds', 'of', 'additional', 'context', ':', 'topics', '(', 'TopCNN', ')', 'and', 'translations', '(', 'CNN', '+', 'B1', ',', 'CNN', '+', 'B2', ',', 'CNN', '+', 'MCFA', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'CD', 'JJ', 'NNS', 'IN', 'JJ', 'NN', ':', 'NNS', '(', 'NNP', ')', 'CC', 'NNS', '(', 'NNP', 'NNP', 'NNP', ',', 'NNP', 'NNP', 'NNP', ',', 'NNP', 'NNP', 'NNP', ')', '.']",30
sentence_classification,2,189,"Overall , we conclude that translations are better additional contexts than topics .","['Overall', ',', 'we', 'conclude', 'that', 'translations', 'are', 'better', 'additional', 'contexts', 'than', 'topics', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O']","['JJ', ',', 'PRP', 'VBP', 'IN', 'NNS', 'VBP', 'RBR', 'JJ', 'NN', 'IN', 'NNS', '.']",13
sentence_classification,2,190,"When using a single context ( i.e. TopCNN word , TopCNN sent , and our models when N = 1 ) , translations always outperform topics even when using the baseline methods .","['When', 'using', 'a', 'single', 'context', '(', 'i.e.', 'TopCNN', 'word', ',', 'TopCNN', 'sent', ',', 'and', 'our', 'models', 'when', 'N', '=', '1', ')', ',', 'translations', 'always', 'outperform', 'topics', 'even', 'when', 'using', 'the', 'baseline', 'methods', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['WRB', 'VBG', 'DT', 'JJ', 'NN', '(', 'JJ', 'NNP', 'NN', ',', 'NNP', 'VBD', ',', 'CC', 'PRP$', 'NNS', 'WRB', 'NNP', 'NNP', 'CD', ')', ',', 'NNS', 'RB', 'VBP', 'NNS', 'RB', 'WRB', 'VBG', 'DT', 'NN', 'NNS', '.']",33
sentence_classification,2,191,"Using topics as additional context also decreases the performance of the CNN classifier on most data sets , giving an adverse effect to the CNN classifier .","['Using', 'topics', 'as', 'additional', 'context', 'also', 'decreases', 'the', 'performance', 'of', 'the', 'CNN', 'classifier', 'on', 'most', 'data', 'sets', ',', 'giving', 'an', 'adverse', 'effect', 'to', 'the', 'CNN', 'classifier', '.']","['B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NNS', 'IN', 'JJ', 'NN', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'JJS', 'NNS', 'NNS', ',', 'VBG', 'DT', 'JJ', 'NN', 'TO', 'DT', 'NNP', 'NN', '.']",27
sentence_classification,0,2,Structural Scaffolds for Citation Intent Classification in Scientific Publications,"['Structural', 'Scaffolds', 'for', 'Citation', 'Intent', 'Classification', 'in', 'Scientific', 'Publications']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['JJ', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",9
sentence_classification,0,4,"Identifying the intent of a citation in scientific papers ( e.g. , background information , use of methods , comparing results ) is critical for machine reading of individual publications and automated analysis of the scientific literature .","['Identifying', 'the', 'intent', 'of', 'a', 'citation', 'in', 'scientific', 'papers', '(', 'e.g.', ',', 'background', 'information', ',', 'use', 'of', 'methods', ',', 'comparing', 'results', ')', 'is', 'critical', 'for', 'machine', 'reading', 'of', 'individual', 'publications', 'and', 'automated', 'analysis', 'of', 'the', 'scientific', 'literature', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', '(', 'NN', ',', 'NN', 'NN', ',', 'NN', 'IN', 'NNS', ',', 'VBG', 'NNS', ')', 'VBZ', 'JJ', 'IN', 'NN', 'NN', 'IN', 'JJ', 'NNS', 'CC', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",38
sentence_classification,0,8,Our code and data are available at : https://github.com/ allenai/scicite .,"['Our', 'code', 'and', 'data', 'are', 'available', 'at', ':', 'https://github.com/', 'allenai/scicite', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'CC', 'NNS', 'VBP', 'JJ', 'IN', ':', 'NN', 'NN', '.']",11
sentence_classification,0,19,"In this work , we approach the problem of citation intent classification by modeling the language expressed in the citation context .","['In', 'this', 'work', ',', 'we', 'approach', 'the', 'problem', 'of', 'citation', 'intent', 'classification', 'by', 'modeling', 'the', 'language', 'expressed', 'in', 'the', 'citation', 'context', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",22
sentence_classification,0,23,"To this end , we propose a neural multitask learning framework to incorporate knowledge into citations from the structure of scientific papers .","['To', 'this', 'end', ',', 'we', 'propose', 'a', 'neural', 'multitask', 'learning', 'framework', 'to', 'incorporate', 'knowledge', 'into', 'citations', 'from', 'the', 'structure', 'of', 'scientific', 'papers', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBG', 'NN', 'TO', 'VB', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', '.']",23
sentence_classification,0,24,"In particular , we propose two auxiliary tasks as structural scaffolds to improve citation intent prediction : 1 ( 1 ) predicting the section title in which the citation occurs and ( 2 ) predicting whether a sentence needs a citation .","['In', 'particular', ',', 'we', 'propose', 'two', 'auxiliary', 'tasks', 'as', 'structural', 'scaffolds', 'to', 'improve', 'citation', 'intent', 'prediction', ':', '1', '(', '1', ')', 'predicting', 'the', 'section', 'title', 'in', 'which', 'the', 'citation', 'occurs', 'and', '(', '2', ')', 'predicting', 'whether', 'a', 'sentence', 'needs', 'a', 'citation', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', ',', 'PRP', 'VBP', 'CD', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', 'TO', 'VB', 'NN', 'NN', 'NN', ':', 'CD', '(', 'CD', ')', 'VBG', 'DT', 'NN', 'NN', 'IN', 'WDT', 'DT', 'NN', 'VBZ', 'CC', '(', 'CD', ')', 'VBG', 'IN', 'DT', 'NN', 'VBZ', 'DT', 'NN', '.']",42
sentence_classification,0,26,"On two datasets , we show that the proposed neural scaffold model outperforms existing methods by large margins .","['On', 'two', 'datasets', ',', 'we', 'show', 'that', 'the', 'proposed', 'neural', 'scaffold', 'model', 'outperforms', 'existing', 'methods', 'by', 'large', 'margins', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'CD', 'NNS', ',', 'PRP', 'VBP', 'IN', 'DT', 'VBN', 'JJ', 'JJ', 'NN', 'NNS', 'VBG', 'NNS', 'IN', 'JJ', 'NNS', '.']",19
sentence_classification,0,27,"Our contributions are : ( i ) we propose a neural scaffold framework for citation intent classification to incorporate into citations knowledge from structure of scientific papers ; ( ii ) we achieve a new state - of - the - art of 67.9 % F1 on the ACL - ARC citations benchmark , an absolute 13.3 % increase over the previous state - of - the - art ; and ( iii ) we introduce SciCite , a new dataset of citation intents which is at least five times as large as existing datasets and covers a variety of scientific domains .","['Our', 'contributions', 'are', ':', '(', 'i', ')', 'we', 'propose', 'a', 'neural', 'scaffold', 'framework', 'for', 'citation', 'intent', 'classification', 'to', 'incorporate', 'into', 'citations', 'knowledge', 'from', 'structure', 'of', 'scientific', 'papers', ';', '(', 'ii', ')', 'we', 'achieve', 'a', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'of', '67.9', '%', 'F1', 'on', 'the', 'ACL', '-', 'ARC', 'citations', 'benchmark', ',', 'an', 'absolute', '13.3', '%', 'increase', 'over', 'the', 'previous', 'state', '-', 'of', '-', 'the', '-', 'art', ';', 'and', '(', 'iii', ')', 'we', 'introduce', 'SciCite', ',', 'a', 'new', 'dataset', 'of', 'citation', 'intents', 'which', 'is', 'at', 'least', 'five', 'times', 'as', 'large', 'as', 'existing', 'datasets', 'and', 'covers', 'a', 'variety', 'of', 'scientific', 'domains', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NNS', 'VBP', ':', '(', 'NN', ')', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NN', 'NN', 'NN', 'TO', 'VB', 'IN', 'NNS', 'VBP', 'IN', 'NN', 'IN', 'JJ', 'NNS', ':', '(', 'NN', ')', 'PRP', 'VBP', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'CD', 'NN', 'NNP', 'IN', 'DT', 'NNP', ':', 'NNP', 'NNS', 'NN', ',', 'DT', 'JJ', 'CD', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', ':', 'CC', '(', 'NN', ')', 'PRP', 'VBP', 'NNP', ',', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', 'WDT', 'VBZ', 'IN', 'JJS', 'CD', 'NNS', 'RB', 'JJ', 'IN', 'VBG', 'NNS', 'CC', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NNS', '.']",103
sentence_classification,0,121,"To address these limitations , we introduce Sci - Cite , a new dataset of citation intents that is significantly larger , more coarse - grained and generaldomain compared with existing datasets .","['To', 'address', 'these', 'limitations', ',', 'we', 'introduce', 'Sci', '-', 'Cite', ',', 'a', 'new', 'dataset', 'of', 'citation', 'intents', 'that', 'is', 'significantly', 'larger', ',', 'more', 'coarse', '-', 'grained', 'and', 'generaldomain', 'compared', 'with', 'existing', 'datasets', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NNS', ',', 'PRP', 'VBP', 'NNP', ':', 'NNP', ',', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', 'WDT', 'VBZ', 'RB', 'JJR', ',', 'RBR', 'JJ', ':', 'VBN', 'CC', 'VB', 'VBN', 'IN', 'VBG', 'NNS', '.']",33
sentence_classification,0,125,"We consider three intent categories outlined in : BACK - GROUND , METHOD and RESULTCOMPARISON .","['We', 'consider', 'three', 'intent', 'categories', 'outlined', 'in', ':', 'BACK', '-', 'GROUND', ',', 'METHOD', 'and', 'RESULTCOMPARISON', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O']","['PRP', 'VBP', 'CD', 'NN', 'NNS', 'VBN', 'IN', ':', 'NNP', ':', 'NN', ',', 'NNP', 'CC', 'NNP', '.']",16
sentence_classification,0,128,Citation intent of sentence extractions was labeled through the crowdsourcing platform .,"['Citation', 'intent', 'of', 'sentence', 'extractions', 'was', 'labeled', 'through', 'the', 'crowdsourcing', 'platform', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['NNP', 'NN', 'IN', 'NN', 'NNS', 'VBD', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",12
sentence_classification,0,142,"Citation contexts were annotated by 850 crowdsource workers who made a total of 29,926 annotations and individually made between 4 and 240 annotations .","['Citation', 'contexts', 'were', 'annotated', 'by', '850', 'crowdsource', 'workers', 'who', 'made', 'a', 'total', 'of', '29,926', 'annotations', 'and', 'individually', 'made', 'between', '4', 'and', '240', 'annotations', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'NNS', 'VBD', 'VBN', 'IN', 'CD', 'NN', 'NNS', 'WP', 'VBD', 'DT', 'NN', 'IN', 'CD', 'NNS', 'CC', 'RB', 'VBD', 'IN', 'CD', 'CC', 'CD', 'NNS', '.']",24
sentence_classification,0,143,"Each sentence was annotated , on average , 3.74 times .","['Each', 'sentence', 'was', 'annotated', ',', 'on', 'average', ',', '3.74', 'times', '.']","['O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBD', 'VBN', ',', 'IN', 'NN', ',', 'CD', 'NNS', '.']",11
sentence_classification,0,144,"This resulted in a total 9,159 crowdsourced instances which were divided to training and validation sets with 90 % of the data used for the training set .","['This', 'resulted', 'in', 'a', 'total', '9,159', 'crowdsourced', 'instances', 'which', 'were', 'divided', 'to', 'training', 'and', 'validation', 'sets', 'with', '90', '%', 'of', 'the', 'data', 'used', 'for', 'the', 'training', 'set', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'VBD', 'IN', 'DT', 'JJ', 'CD', 'VBD', 'NNS', 'WDT', 'VBD', 'VBN', 'TO', 'NN', 'CC', 'NN', 'NNS', 'IN', 'CD', 'NN', 'IN', 'DT', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",28
sentence_classification,0,155,We implement our proposed scaffold framework using the AllenNLP library .,"['We', 'implement', 'our', 'proposed', 'scaffold', 'framework', 'using', 'the', 'AllenNLP', 'library', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'VBN', 'NN', 'NN', 'VBG', 'DT', 'NNP', 'NN', '.']",11
sentence_classification,0,156,"For word representations , we use 100 - dimensional GloVe vectors trained on a corpus of 6B tokens from Wikipedia and Gigaword .","['For', 'word', 'representations', ',', 'we', 'use', '100', '-', 'dimensional', 'GloVe', 'vectors', 'trained', 'on', 'a', 'corpus', 'of', '6B', 'tokens', 'from', 'Wikipedia', 'and', 'Gigaword', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'NNS', ',', 'PRP', 'VBP', 'CD', ':', 'JJ', 'NNP', 'NNS', 'VBD', 'IN', 'DT', 'NN', 'IN', 'CD', 'NNS', 'IN', 'NNP', 'CC', 'NNP', '.']",23
sentence_classification,0,157,"For contextual representations , we use ELMo vectors released by with output dimension size of 1,024 which have been trained on a dataset of 5.5 B tokens .","['For', 'contextual', 'representations', ',', 'we', 'use', 'ELMo', 'vectors', 'released', 'by', 'with', 'output', 'dimension', 'size', 'of', '1,024', 'which', 'have', 'been', 'trained', 'on', 'a', 'dataset', 'of', '5.5', 'B', 'tokens', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'VBN', 'IN', 'IN', 'NN', 'NN', 'NN', 'IN', 'CD', 'WDT', 'VBP', 'VBN', 'VBN', 'IN', 'DT', 'NN', 'IN', 'CD', 'NNP', 'NNS', '.']",28
sentence_classification,0,158,We use a single - layer BiLSTM with a hidden dimension size of 50 for each direction 11 .,"['We', 'use', 'a', 'single', '-', 'layer', 'BiLSTM', 'with', 'a', 'hidden', 'dimension', 'size', 'of', '50', 'for', 'each', 'direction', '11', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', ':', 'NN', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', 'CD', '.']",19
sentence_classification,0,159,"For each of scaffold tasks , we use a single - layer MLP with 20 hidden nodes , ReLU activation and a Dropout rate of 0.2 between the hidden and input layers .","['For', 'each', 'of', 'scaffold', 'tasks', ',', 'we', 'use', 'a', 'single', '-', 'layer', 'MLP', 'with', '20', 'hidden', 'nodes', ',', 'ReLU', 'activation', 'and', 'a', 'Dropout', 'rate', 'of', '0.2', 'between', 'the', 'hidden', 'and', 'input', 'layers', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'IN', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', ':', 'NN', 'NNP', 'IN', 'CD', 'JJ', 'NNS', ',', 'NNP', 'NN', 'CC', 'DT', 'NNP', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', 'CC', 'NN', 'NNS', '.']",33
sentence_classification,0,164,Batch size is 8 for ACL - ARC dataset and 32 for SciCite dataset ( recall that SciCite is larger than ACL - ARC ) .,"['Batch', 'size', 'is', '8', 'for', 'ACL', '-', 'ARC', 'dataset', 'and', '32', 'for', 'SciCite', 'dataset', '(', 'recall', 'that', 'SciCite', 'is', 'larger', 'than', 'ACL', '-', 'ARC', ')', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'VBZ', 'CD', 'IN', 'NNP', ':', 'NNP', 'NN', 'CC', 'CD', 'IN', 'NNP', 'NN', '(', 'VB', 'DT', 'NNP', 'VBZ', 'JJR', 'IN', 'NNP', ':', 'NNP', ')', '.']",26
sentence_classification,0,165,We use Beaker 12 for running the experiments .,"['We', 'use', 'Beaker', '12', 'for', 'running', 'the', 'experiments', '.']","['O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', 'CD', 'IN', 'VBG', 'DT', 'NNS', '.']",9
sentence_classification,0,173,BiLSTM Attention ( with and without ELMo ) .,"['BiLSTM', 'Attention', '(', 'with', 'and', 'without', 'ELMo', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', '(', 'IN', 'CC', 'IN', 'NNP', ')', '.']",9
sentence_classification,0,174,"This baseline uses a similar architecture to our proposed neural multitask learning framework , except that it only optimizes the network for the main loss regarding the citation intent classification ( L 1 ) and does not include the structural scaffolds .","['This', 'baseline', 'uses', 'a', 'similar', 'architecture', 'to', 'our', 'proposed', 'neural', 'multitask', 'learning', 'framework', ',', 'except', 'that', 'it', 'only', 'optimizes', 'the', 'network', 'for', 'the', 'main', 'loss', 'regarding', 'the', 'citation', 'intent', 'classification', '(', 'L', '1', ')', 'and', 'does', 'not', 'include', 'the', 'structural', 'scaffolds', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'TO', 'PRP$', 'VBN', 'JJ', 'NN', 'VBG', 'NN', ',', 'IN', 'IN', 'PRP', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBG', 'DT', 'NN', 'NN', 'NN', '(', 'NNP', 'CD', ')', 'CC', 'VBZ', 'RB', 'VB', 'DT', 'JJ', 'NNS', '.']",42
sentence_classification,0,181,We observe that our scaffold - enhanced models achieve clear improvements over the state - of - the - art approach on this task .,"['We', 'observe', 'that', 'our', 'scaffold', '-', 'enhanced', 'models', 'achieve', 'clear', 'improvements', 'over', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'approach', 'on', 'this', 'task', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'PRP$', 'JJ', ':', 'JJ', 'NNS', 'VBP', 'JJ', 'NNS', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'DT', 'NN', '.']",25
sentence_classification,0,182,"Starting with the ' BiLSTM - Attn ' baseline with a macro F1 score of 51.8 , adding the first scaffold task in ' BiLSTM - Attn + section title scaffold ' improves the F1 score to 56.9 (?= 5.1 ) .","['Starting', 'with', 'the', ""'"", 'BiLSTM', '-', 'Attn', ""'"", 'baseline', 'with', 'a', 'macro', 'F1', 'score', 'of', '51.8', ',', 'adding', 'the', 'first', 'scaffold', 'task', 'in', ""'"", 'BiLSTM', '-', 'Attn', '+', 'section', 'title', 'scaffold', ""'"", 'improves', 'the', 'F1', 'score', 'to', '56.9', '(?=', '5.1', ')', '.']","['B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'IN', 'DT', 'POS', 'NNP', ':', 'NNP', 'POS', 'NN', 'IN', 'DT', 'NN', 'NNP', 'NN', 'IN', 'CD', ',', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'IN', ""''"", 'NNP', ':', 'NNP', 'NNP', 'NN', 'NN', 'NN', ""''"", 'VBZ', 'DT', 'NNP', 'NN', 'TO', 'CD', 'NNS', 'CD', ')', '.']",42
sentence_classification,0,183,Adding the second scaffold in ' BiLSTM - Attn + citation worthiness scaffold ' also results in similar improvements : 56.3 (?= 4.5 ) .,"['Adding', 'the', 'second', 'scaffold', 'in', ""'"", 'BiLSTM', '-', 'Attn', '+', 'citation', 'worthiness', 'scaffold', ""'"", 'also', 'results', 'in', 'similar', 'improvements', ':', '56.3', '(?=', '4.5', ')', '.']","['B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'DT', 'JJ', 'NN', 'IN', ""''"", 'NNP', ':', 'NNP', 'NNP', 'NN', 'NN', 'NN', ""''"", 'RB', 'NNS', 'IN', 'JJ', 'NNS', ':', 'CD', '$', 'CD', ')', '.']",25
sentence_classification,0,184,"When both scaffolds are used simultaneously in ' BiLSTM - Attn + both scaffolds ' , the F1 score further improves to 63.1 ( ?= 11.3 ) , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .","['When', 'both', 'scaffolds', 'are', 'used', 'simultaneously', 'in', ""'"", 'BiLSTM', '-', 'Attn', '+', 'both', 'scaffolds', ""'"", ',', 'the', 'F1', 'score', 'further', 'improves', 'to', '63.1', '(', '?=', '11.3', ')', ',', 'suggesting', 'that', 'the', 'two', 'tasks', 'provide', 'complementary', 'signal', 'that', 'is', 'useful', 'for', 'citation', 'intent', 'prediction', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['WRB', 'DT', 'NNS', 'VBP', 'VBN', 'RB', 'IN', ""''"", 'NNP', ':', 'NNP', 'NNP', 'DT', 'NNS', 'POS', ',', 'DT', 'NNP', 'NN', 'JJ', 'NNS', 'TO', 'CD', '(', '$', 'CD', ')', ',', 'VBG', 'IN', 'DT', 'CD', 'NNS', 'VBP', 'JJ', 'NN', 'WDT', 'VBZ', 'JJ', 'IN', 'NN', 'NN', 'NN', '.']",44
sentence_classification,0,185,"The best result is achieved when we also add ELMo vectors to the input representations in ' BiLSTM - Attn w / ELMo + both scaffolds ' , achieving an F1 of 67.9 , a major improvement from the previous state - of - the - art results of 54.6 ( ?= 13.3 ) .","['The', 'best', 'result', 'is', 'achieved', 'when', 'we', 'also', 'add', 'ELMo', 'vectors', 'to', 'the', 'input', 'representations', 'in', ""'"", 'BiLSTM', '-', 'Attn', 'w', '/', 'ELMo', '+', 'both', 'scaffolds', ""'"", ',', 'achieving', 'an', 'F1', 'of', '67.9', ',', 'a', 'major', 'improvement', 'from', 'the', 'previous', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'of', '54.6', '(', '?=', '13.3', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJS', 'NN', 'VBZ', 'VBN', 'WRB', 'PRP', 'RB', 'VBP', 'NNP', 'NNS', 'TO', 'DT', 'NN', 'NNS', 'IN', ""''"", 'NNP', ':', 'NNP', 'VBP', 'NNP', 'NNP', 'NNP', 'DT', 'NNS', 'POS', ',', 'VBG', 'DT', 'NNP', 'IN', 'CD', ',', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'CD', '(', '$', 'CD', ')', '.']",55
sentence_classification,0,186,"We note that the scaffold tasks provide major contributions on top of the ELMo - enabled baseline ( ?= 13.6 ) , demonstrating the efficacy of using structural scaffolds for citation intent prediction .","['We', 'note', 'that', 'the', 'scaffold', 'tasks', 'provide', 'major', 'contributions', 'on', 'top', 'of', 'the', 'ELMo', '-', 'enabled', 'baseline', '(', '?=', '13.6', ')', ',', 'demonstrating', 'the', 'efficacy', 'of', 'using', 'structural', 'scaffolds', 'for', 'citation', 'intent', 'prediction', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'DT', 'JJ', 'NNS', 'VBP', 'JJ', 'NNS', 'IN', 'NN', 'IN', 'DT', 'NNP', ':', 'VBD', 'NN', '(', 'JJ', 'CD', ')', ',', 'VBG', 'DT', 'NN', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'NN', 'NN', 'NN', '.']",34
sentence_classification,0,188,"We also experimented with adding features used in to our best model and not only we did not see any improvements , but we observed at least 1.7 % decline in performance .","['We', 'also', 'experimented', 'with', 'adding', 'features', 'used', 'in', 'to', 'our', 'best', 'model', 'and', 'not', 'only', 'we', 'did', 'not', 'see', 'any', 'improvements', ',', 'but', 'we', 'observed', 'at', 'least', '1.7', '%', 'decline', 'in', 'performance', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'RB', 'VBD', 'IN', 'VBG', 'NNS', 'VBN', 'IN', 'TO', 'PRP$', 'JJS', 'NN', 'CC', 'RB', 'RB', 'PRP', 'VBD', 'RB', 'VB', 'DT', 'NNS', ',', 'CC', 'PRP', 'VBD', 'IN', 'JJS', 'CD', 'NN', 'NN', 'IN', 'NN', '.']",33
sentence_classification,0,191,Each scaffold task improves model performance .,"['Each', 'scaffold', 'task', 'improves', 'model', 'performance', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'JJ', 'NN', '.']",7
sentence_classification,0,192,Adding both scaffolds results in further improvements .,"['Adding', 'both', 'scaffolds', 'results', 'in', 'further', 'improvements', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['VBG', 'DT', 'NNS', 'NNS', 'IN', 'JJ', 'NNS', '.']",8
sentence_classification,0,193,And the best results are obtained by using ELMo representation in addition to both scaffolds .,"['And', 'the', 'best', 'results', 'are', 'obtained', 'by', 'using', 'ELMo', 'representation', 'in', 'addition', 'to', 'both', 'scaffolds', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['CC', 'DT', 'JJS', 'NNS', 'VBP', 'VBN', 'IN', 'VBG', 'NNP', 'NN', 'IN', 'NN', 'TO', 'DT', 'NNS', '.']",16
sentence_classification,0,197,Generally we observe that results on categories with more number of instances are higher .,"['Generally', 'we', 'observe', 'that', 'results', 'on', 'categories', 'with', 'more', 'number', 'of', 'instances', 'are', 'higher', '.']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['RB', 'PRP', 'VBP', 'DT', 'NNS', 'IN', 'NNS', 'IN', 'JJR', 'NN', 'IN', 'NNS', 'VBP', 'JJR', '.']",15
sentence_classification,0,198,"For example on ACL - ARC , the results on the BACKGROUND category are the highest as this category is the most common .","['For', 'example', 'on', 'ACL', '-', 'ARC', ',', 'the', 'results', 'on', 'the', 'BACKGROUND', 'category', 'are', 'the', 'highest', 'as', 'this', 'category', 'is', 'the', 'most', 'common', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', 'IN', 'NNP', ':', 'NNP', ',', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'VBP', 'DT', 'JJS', 'IN', 'DT', 'NN', 'VBZ', 'DT', 'RBS', 'JJ', '.']",24
sentence_classification,0,199,"Conversely , the results on the FUTUREWORK category are the lowest .","['Conversely', ',', 'the', 'results', 'on', 'the', 'FUTUREWORK', 'category', 'are', 'the', 'lowest', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['RB', ',', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'VBP', 'DT', 'JJS', '.']",12
sentence_classification,0,200,This category has the fewest data points ( see distribution of the categories in ) and thus it is harder for the model to learn the optimal parameters for correct classification in this category .,"['This', 'category', 'has', 'the', 'fewest', 'data', 'points', '(', 'see', 'distribution', 'of', 'the', 'categories', 'in', ')', 'and', 'thus', 'it', 'is', 'harder', 'for', 'the', 'model', 'to', 'learn', 'the', 'optimal', 'parameters', 'for', 'correct', 'classification', 'in', 'this', 'category', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJS', 'NN', 'NNS', '(', 'VB', 'NN', 'IN', 'DT', 'NNS', 'IN', ')', 'CC', 'RB', 'PRP', 'VBZ', 'RBR', 'IN', 'DT', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",35
smile_recognition,0,2,Deep Learning For Smile Recognition,"['Deep', 'Learning', 'For', 'Smile', 'Recognition']","['O', 'O', 'O', 'B-n', 'I-n']","['NNP', 'NNP', 'IN', 'NNP', 'NNP']",5
smile_recognition,0,4,"Inspired by recent successes of deep learning in computer vision , we propose a novel application of deep convolutional neural networks to facial expression recognition , in particular smile recognition .","['Inspired', 'by', 'recent', 'successes', 'of', 'deep', 'learning', 'in', 'computer', 'vision', ',', 'we', 'propose', 'a', 'novel', 'application', 'of', 'deep', 'convolutional', 'neural', 'networks', 'to', 'facial', 'expression', 'recognition', ',', 'in', 'particular', 'smile', 'recognition', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'IN', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'JJ', 'JJ', 'NNS', 'TO', 'JJ', 'NN', 'NN', ',', 'IN', 'JJ', 'NN', 'NN', '.']",31
smile_recognition,0,56,The input images are fed into a convolution comprising a convolutional and a subsampling layer .,"['The', 'input', 'images', 'are', 'fed', 'into', 'a', 'convolution', 'comprising', 'a', 'convolutional', 'and', 'a', 'subsampling', 'layer', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'NN', 'VBG', 'DT', 'JJ', 'CC', 'DT', 'NN', 'NN', '.']",16
smile_recognition,0,57,That convolution maybe followed by more convolutions to become gradually more invariant to distortions in the input .,"['That', 'convolution', 'maybe', 'followed', 'by', 'more', 'convolutions', 'to', 'become', 'gradually', 'more', 'invariant', 'to', 'distortions', 'in', 'the', 'input', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'O']","['DT', 'NN', 'RB', 'VBN', 'IN', 'JJR', 'NNS', 'TO', 'VB', 'RB', 'RBR', 'JJ', 'TO', 'NNS', 'IN', 'DT', 'NN', '.']",18
smile_recognition,0,58,"In the second stage , a regular neural network follows the convolutions in order to discriminate the features learned by the convolutions .","['In', 'the', 'second', 'stage', ',', 'a', 'regular', 'neural', 'network', 'follows', 'the', 'convolutions', 'in', 'order', 'to', 'discriminate', 'the', 'features', 'learned', 'by', 'the', 'convolutions', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'DT', 'JJ', 'JJ', 'NN', 'VBZ', 'DT', 'NNS', 'IN', 'NN', 'TO', 'VB', 'DT', 'NNS', 'VBN', 'IN', 'DT', 'NNS', '.']",23
smile_recognition,0,59,The output layer consists of two units for smile or no smile .,"['The', 'output', 'layer', 'consists', 'of', 'two', 'units', 'for', 'smile', 'or', 'no', 'smile', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'IN', 'CD', 'NNS', 'IN', 'NN', 'CC', 'DT', 'NN', '.']",13
smile_recognition,0,60,"The novelty of this approach is that the exact number of convolutions , number of hidden layers and size of hidden layers are not fixed but subject to extensive model selection in Sec. 4.3 .","['The', 'novelty', 'of', 'this', 'approach', 'is', 'that', 'the', 'exact', 'number', 'of', 'convolutions', ',', 'number', 'of', 'hidden', 'layers', 'and', 'size', 'of', 'hidden', 'layers', 'are', 'not', 'fixed', 'but', 'subject', 'to', 'extensive', 'model', 'selection', 'in', 'Sec.', '4.3', '.']","['O', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', ',', 'NN', 'IN', 'JJ', 'NNS', 'CC', 'NN', 'IN', 'JJ', 'NNS', 'VBP', 'RB', 'VBN', 'CC', 'JJ', 'TO', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'CD', '.']",35
smile_recognition,0,62,"Due to training time constraints , some parameters have been fixed to reasonable and empirical values , such as the size of convolutions ( 5 5 pixels , 32 feature maps ) and the size of subsamplings ( 2 2 pixels using max pooling ) .","['Due', 'to', 'training', 'time', 'constraints', ',', 'some', 'parameters', 'have', 'been', 'fixed', 'to', 'reasonable', 'and', 'empirical', 'values', ',', 'such', 'as', 'the', 'size', 'of', 'convolutions', '(', '5', '5', 'pixels', ',', '32', 'feature', 'maps', ')', 'and', 'the', 'size', 'of', 'subsamplings', '(', '2', '2', 'pixels', 'using', 'max', 'pooling', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'TO', 'VBG', 'NN', 'NNS', ',', 'DT', 'NNS', 'VBP', 'VBN', 'VBN', 'TO', 'VB', 'CC', 'JJ', 'NNS', ',', 'JJ', 'IN', 'DT', 'NN', 'IN', 'NNS', '(', 'CD', 'CD', 'NNS', ',', 'CD', 'NN', 'NNS', ')', 'CC', 'DT', 'NN', 'IN', 'NNS', '(', 'CD', 'CD', 'NNS', 'VBG', 'JJ', 'NN', ')', '.']",46
smile_recognition,0,63,"All layers use ReLU units , except of softmax being used in the output layer .","['All', 'layers', 'use', 'ReLU', 'units', ',', 'except', 'of', 'softmax', 'being', 'used', 'in', 'the', 'output', 'layer', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'NNP', 'NNS', ',', 'IN', 'IN', 'NN', 'VBG', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",16
smile_recognition,0,64,The learning rate is fixed to ? = 0.01 and not subject to model selection as it would significantly prolong the model selection .,"['The', 'learning', 'rate', 'is', 'fixed', 'to', '?', '=', '0.01', 'and', 'not', 'subject', 'to', 'model', 'selection', 'as', 'it', 'would', 'significantly', 'prolong', 'the', 'model', 'selection', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', '.', 'VB', 'CD', 'CC', 'RB', 'JJ', 'TO', 'VB', 'NN', 'IN', 'PRP', 'MD', 'RB', 'VB', 'DT', 'NN', 'NN', '.']",24
smile_recognition,0,65,"The same considerations apply to the momentum , which is fixed to = 0.9 .","['The', 'same', 'considerations', 'apply', 'to', 'the', 'momentum', ',', 'which', 'is', 'fixed', 'to', '=', '0.9', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NNS', 'VBP', 'TO', 'DT', 'NN', ',', 'WDT', 'VBZ', 'VBN', 'TO', 'VB', 'CD', '.']",15
smile_recognition,0,66,The entire database has been randomly split into a 60% / 20 % / 20 % training / validation / test ratio .,"['The', 'entire', 'database', 'has', 'been', 'randomly', 'split', 'into', 'a', '60%', '/', '20', '%', '/', '20', '%', 'training', '/', 'validation', '/', 'test', 'ratio', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'VBN', 'RB', 'VBN', 'IN', 'DT', 'CD', 'JJ', 'CD', 'NN', 'JJ', 'CD', 'NN', 'NN', 'JJ', 'NN', 'NNP', 'NN', 'NN', '.']",23
smile_recognition,0,71,The model is implemented using Lasagne 4 and the generated CUDA code is executed on a Tesla K40c 9 as training on a GPU allows to perform a comprehensive model selection in a feasible amount of time .,"['The', 'model', 'is', 'implemented', 'using', 'Lasagne', '4', 'and', 'the', 'generated', 'CUDA', 'code', 'is', 'executed', 'on', 'a', 'Tesla', 'K40c', '9', 'as', 'training', 'on', 'a', 'GPU', 'allows', 'to', 'perform', 'a', 'comprehensive', 'model', 'selection', 'in', 'a', 'feasible', 'amount', 'of', 'time', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'VBG', 'NNP', 'CD', 'CC', 'DT', 'JJ', 'NNP', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NNP', 'NNP', 'CD', 'IN', 'NN', 'IN', 'DT', 'NNP', 'VBZ', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', '.']",38
smile_recognition,0,72,Stochastic gradient descent with a batch size of 500 is used .,"['Stochastic', 'gradient', 'descent', 'with', 'a', 'batch', 'size', 'of', '500', 'is', 'used', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O']","['JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', 'VBZ', 'VBN', '.']",12
smile_recognition,0,73,"contains the four parameters to be optimized : the number of convolutions , the number of hidden layers , the number of units per hidden layer and the dropout factor .","['contains', 'the', 'four', 'parameters', 'to', 'be', 'optimized', ':', 'the', 'number', 'of', 'convolutions', ',', 'the', 'number', 'of', 'hidden', 'layers', ',', 'the', 'number', 'of', 'units', 'per', 'hidden', 'layer', 'and', 'the', 'dropout', 'factor', '.']","['B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']","['VBZ', 'DT', 'CD', 'NNS', 'TO', 'VB', 'VBN', ':', 'DT', 'NN', 'IN', 'NNS', ',', 'DT', 'NN', 'IN', 'JJ', 'NNS', ',', 'DT', 'NN', 'IN', 'NNS', 'IN', 'NN', 'NN', 'CC', 'DT', 'NN', 'NN', '.']",31
smile_recognition,0,76,Each model was trained for 50 epochs in the model selection .,"['Each', 'model', 'was', 'trained', 'for', '50', 'epochs', 'in', 'the', 'model', 'selection', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBD', 'VBN', 'IN', 'CD', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",12
question_answering,1,2,BI - DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION,"['BI', '-', 'DIRECTIONAL', 'ATTENTION', 'FLOW', 'FOR', 'MACHINE', 'COMPREHENSION']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', ':', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",8
question_answering,1,4,"Machine comprehension ( MC ) , answering a query about a given context paragraph , requires modeling complex interactions between the context and the query .","['Machine', 'comprehension', '(', 'MC', ')', ',', 'answering', 'a', 'query', 'about', 'a', 'given', 'context', 'paragraph', ',', 'requires', 'modeling', 'complex', 'interactions', 'between', 'the', 'context', 'and', 'the', 'query', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', '(', 'NNP', ')', ',', 'VBG', 'DT', 'NN', 'IN', 'DT', 'VBN', 'NN', 'NN', ',', 'VBZ', 'VBG', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CC', 'DT', 'NN', '.']",26
question_answering,1,5,"Recently , attention mechanisms have been successfully extended to MC .","['Recently', ',', 'attention', 'mechanisms', 'have', 'been', 'successfully', 'extended', 'to', 'MC', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']","['RB', ',', 'NN', 'NNS', 'VBP', 'VBN', 'RB', 'VBN', 'TO', 'NNP', '.']",11
question_answering,1,10,The tasks of machine comprehension ( MC ) and question answering ( QA ) have gained significant popularity over the past few years within the natural language processing and computer vision communities .,"['The', 'tasks', 'of', 'machine', 'comprehension', '(', 'MC', ')', 'and', 'question', 'answering', '(', 'QA', ')', 'have', 'gained', 'significant', 'popularity', 'over', 'the', 'past', 'few', 'years', 'within', 'the', 'natural', 'language', 'processing', 'and', 'computer', 'vision', 'communities', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'NN', 'NN', '(', 'NNP', ')', 'VBP', 'VBN', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', 'CC', 'NN', 'NN', 'NNS', '.']",33
question_answering,1,17,"In this paper , we introduce the Bi- Directional Attention Flow ( BIDAF ) network , a hierarchical multi-stage architecture for modeling the representations of the context paragraph at different levels of granularity ) .","['In', 'this', 'paper', ',', 'we', 'introduce', 'the', 'Bi-', 'Directional', 'Attention', 'Flow', '(', 'BIDAF', ')', 'network', ',', 'a', 'hierarchical', 'multi-stage', 'architecture', 'for', 'modeling', 'the', 'representations', 'of', 'the', 'context', 'paragraph', 'at', 'different', 'levels', 'of', 'granularity', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', ',', 'DT', 'JJ', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NN', ')', '.']",35
question_answering,1,18,"BIDAF includes character - level , word - level , and contextual embeddings , and uses bi-directional attention flow to obtain a query - aware context representation .","['BIDAF', 'includes', 'character', '-', 'level', ',', 'word', '-', 'level', ',', 'and', 'contextual', 'embeddings', ',', 'and', 'uses', 'bi-directional', 'attention', 'flow', 'to', 'obtain', 'a', 'query', '-', 'aware', 'context', 'representation', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'JJR', ':', 'NN', ',', 'NN', ':', 'NN', ',', 'CC', 'JJ', 'NNS', ',', 'CC', 'VBZ', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', ':', 'JJ', 'JJ', 'NN', '.']",28
question_answering,1,21,"Instead , the attention is computed for every time step , and the attended vector at each time step , along with the representations from previous layers , is allowed to flow through to the subsequent modeling layer .","['Instead', ',', 'the', 'attention', 'is', 'computed', 'for', 'every', 'time', 'step', ',', 'and', 'the', 'attended', 'vector', 'at', 'each', 'time', 'step', ',', 'along', 'with', 'the', 'representations', 'from', 'previous', 'layers', ',', 'is', 'allowed', 'to', 'flow', 'through', 'to', 'the', 'subsequent', 'modeling', 'layer', '.']","['O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'NN', ',', 'CC', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'IN', 'IN', 'DT', 'NNS', 'IN', 'JJ', 'NNS', ',', 'VBZ', 'VBN', 'TO', 'VB', 'IN', 'TO', 'DT', 'JJ', 'NN', 'NN', '.']",39
question_answering,1,23,"Second , we use a memory - less attention mechanism .","['Second', ',', 'we', 'use', 'a', 'memory', '-', 'less', 'attention', 'mechanism', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', ',', 'PRP', 'VBP', 'DT', 'NN', ':', 'JJR', 'NN', 'NN', '.']",11
question_answering,1,26,"It forces the attention layer to focus on learning the attention between the query and the context , and enables the modeling layer to focus on learning the interaction within the query - aware context representation ( the output of the attention layer ) .","['It', 'forces', 'the', 'attention', 'layer', 'to', 'focus', 'on', 'learning', 'the', 'attention', 'between', 'the', 'query', 'and', 'the', 'context', ',', 'and', 'enables', 'the', 'modeling', 'layer', 'to', 'focus', 'on', 'learning', 'the', 'interaction', 'within', 'the', 'query', '-', 'aware', 'context', 'representation', '(', 'the', 'output', 'of', 'the', 'attention', 'layer', ')', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O']","['PRP', 'VBZ', 'DT', 'NN', 'NN', 'TO', 'VB', 'IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'CC', 'DT', 'NN', ',', 'CC', 'VBZ', 'DT', 'VBG', 'NN', 'TO', 'VB', 'IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', ':', 'JJ', 'JJ', 'NN', '(', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', ')', '.']",45
question_answering,1,27,It also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps .,"['It', 'also', 'allows', 'the', 'attention', 'at', 'each', 'time', 'step', 'to', 'be', 'unaffected', 'from', 'incorrect', 'attendances', 'at', 'previous', 'time', 'steps', '.']","['O', 'O', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', 'VBN', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NNS', '.']",20
question_answering,1,29,"Third , we use attention mechanisms in both directions , query - to - context and context - to - query , which provide complimentary information to each other .","['Third', ',', 'we', 'use', 'attention', 'mechanisms', 'in', 'both', 'directions', ',', 'query', '-', 'to', '-', 'context', 'and', 'context', '-', 'to', '-', 'query', ',', 'which', 'provide', 'complimentary', 'information', 'to', 'each', 'other', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ',', 'PRP', 'VBP', 'NN', 'NNS', 'IN', 'DT', 'NNS', ',', 'RB', ':', 'TO', ':', 'NN', 'CC', 'JJ', ':', 'TO', ':', 'NN', ',', 'WDT', 'VBP', 'JJ', 'NN', 'TO', 'DT', 'JJ', '.']",30
question_answering,1,174,Each paragraph and question are tokenized by a regular - expression - based word tokenizer ( PTB Tokenizer ) and fed into the model .,"['Each', 'paragraph', 'and', 'question', 'are', 'tokenized', 'by', 'a', 'regular', '-', 'expression', '-', 'based', 'word', 'tokenizer', '(', 'PTB', 'Tokenizer', ')', 'and', 'fed', 'into', 'the', 'model', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['DT', 'NN', 'CC', 'NN', 'VBP', 'VBN', 'IN', 'DT', 'JJ', ':', 'NN', ':', 'VBN', 'NN', 'NN', '(', 'NNP', 'NNP', ')', 'CC', 'VBN', 'IN', 'DT', 'NN', '.']",25
question_answering,1,175,"We use 100 1D filters for CNN char embedding , each with a width of 5 .","['We', 'use', '100', '1D', 'filters', 'for', 'CNN', 'char', 'embedding', ',', 'each', 'with', 'a', 'width', 'of', '5', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'CD', 'NNS', 'IN', 'NNP', 'NN', 'NN', ',', 'DT', 'IN', 'DT', 'NN', 'IN', 'CD', '.']",17
question_answering,1,176,The hidden state size ( d ) of the model is 100 .,"['The', 'hidden', 'state', 'size', '(', 'd', ')', 'of', 'the', 'model', 'is', '100', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['DT', 'JJ', 'NN', 'NN', '(', 'NN', ')', 'IN', 'DT', 'NN', 'VBZ', 'CD', '.']",13
question_answering,1,177,The model has about 2.6 million parameters .,"['The', 'model', 'has', 'about', '2.6', 'million', 'parameters', '.']","['B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'RB', 'CD', 'CD', 'NNS', '.']",8
question_answering,1,178,"We use the AdaDelta ( Zeiler , 2012 ) optimizer , with a minibatch size of 60 and an initial learning rate of 0.5 , for 12 epochs .","['We', 'use', 'the', 'AdaDelta', '(', 'Zeiler', ',', '2012', ')', 'optimizer', ',', 'with', 'a', 'minibatch', 'size', 'of', '60', 'and', 'an', 'initial', 'learning', 'rate', 'of', '0.5', ',', 'for', '12', 'epochs', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', '(', 'NNP', ',', 'CD', ')', 'NN', ',', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', 'CC', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', ',', 'IN', 'CD', 'NNS', '.']",29
question_answering,1,179,"A dropout ) rate of 0.2 is used for the CNN , all LSTM layers , and the linear transformation before the softmax for the answers .","['A', 'dropout', ')', 'rate', 'of', '0.2', 'is', 'used', 'for', 'the', 'CNN', ',', 'all', 'LSTM', 'layers', ',', 'and', 'the', 'linear', 'transformation', 'before', 'the', 'softmax', 'for', 'the', 'answers', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']","['DT', 'NN', ')', 'NN', 'IN', 'CD', 'VBZ', 'VBN', 'IN', 'DT', 'NNP', ',', 'DT', 'NNP', 'NNS', ',', 'CC', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNS', '.']",27
question_answering,1,180,"During training , the moving averages of all weights of the model are maintained with the exponential decay rate of 0.999 .","['During', 'training', ',', 'the', 'moving', 'averages', 'of', 'all', 'weights', 'of', 'the', 'model', 'are', 'maintained', 'with', 'the', 'exponential', 'decay', 'rate', 'of', '0.999', '.']","['B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'NN', ',', 'DT', 'VBG', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', '.']",22
question_answering,1,181,"At test time , the moving averages instead of the raw weights are used .","['At', 'test', 'time', ',', 'the', 'moving', 'averages', 'instead', 'of', 'the', 'raw', 'weights', 'are', 'used', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O']","['IN', 'NN', 'NN', ',', 'DT', 'VBG', 'NNS', 'RB', 'IN', 'DT', 'JJ', 'NNS', 'VBP', 'VBN', '.']",15
question_answering,1,182,The training process takes roughly 20 hours on a single Titan X GPU .,"['The', 'training', 'process', 'takes', 'roughly', '20', 'hours', 'on', 'a', 'single', 'Titan', 'X', 'GPU', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'RB', 'CD', 'NNS', 'IN', 'DT', 'JJ', 'NNP', 'NNP', 'NNP', '.']",14
question_answering,1,187,"BIDAF ( ensemble ) achieves an EM score of 73.3 and an F 1 score of 81.1 , outperforming all previous approaches .","['BIDAF', '(', 'ensemble', ')', 'achieves', 'an', 'EM', 'score', 'of', '73.3', 'and', 'an', 'F', '1', 'score', 'of', '81.1', ',', 'outperforming', 'all', 'previous', 'approaches', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', '(', 'JJ', ')', 'VBZ', 'DT', 'NNP', 'NN', 'IN', 'CD', 'CC', 'DT', 'NNP', 'CD', 'NN', 'IN', 'CD', ',', 'VBG', 'DT', 'JJ', 'NNS', '.']",23
question_answering,1,190,Both char - level and word - level embeddings contribute towards the model 's performance .,"['Both', 'char', '-', 'level', 'and', 'word', '-', 'level', 'embeddings', 'contribute', 'towards', 'the', 'model', ""'s"", 'performance', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'SYM', ':', 'NN', 'CC', 'NN', ':', 'NN', 'NNS', 'VBP', 'IN', 'DT', 'NN', 'POS', 'NN', '.']",16
question_answering,1,195,C2Q attention proves to be critical with a drop of more than 10 points on both metrics .,"['C2Q', 'attention', 'proves', 'to', 'be', 'critical', 'with', 'a', 'drop', 'of', 'more', 'than', '10', 'points', 'on', 'both', 'metrics', '.']","['B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'VBZ', 'TO', 'VB', 'JJ', 'IN', 'DT', 'NN', 'IN', 'JJR', 'IN', 'CD', 'NNS', 'IN', 'DT', 'NNS', '.']",18
question_answering,1,199,"Despite being a simpler attention mechanism , our proposed static attention outperforms the dynamically computed attention by more than 3 points .","['Despite', 'being', 'a', 'simpler', 'attention', 'mechanism', ',', 'our', 'proposed', 'static', 'attention', 'outperforms', 'the', 'dynamically', 'computed', 'attention', 'by', 'more', 'than', '3', 'points', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBG', 'DT', 'NN', 'NN', 'NN', ',', 'PRP$', 'VBN', 'JJ', 'NN', 'VBZ', 'DT', 'RB', 'VBN', 'NN', 'IN', 'JJR', 'IN', 'CD', 'NNS', '.']",22
question_answering,1,207,"At the word embedding layer , query words such as When , Where and Who are not well aligned to possible answers in the context , but this dramatically changes in the contextual embedding layer which has access to context from surrounding words and is just 1 layer below the attention layer .","['At', 'the', 'word', 'embedding', 'layer', ',', 'query', 'words', 'such', 'as', 'When', ',', 'Where', 'and', 'Who', 'are', 'not', 'well', 'aligned', 'to', 'possible', 'answers', 'in', 'the', 'context', ',', 'but', 'this', 'dramatically', 'changes', 'in', 'the', 'contextual', 'embedding', 'layer', 'which', 'has', 'access', 'to', 'context', 'from', 'surrounding', 'words', 'and', 'is', 'just', '1', 'layer', 'below', 'the', 'attention', 'layer', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'VBG', 'NN', ',', 'NN', 'NNS', 'JJ', 'IN', 'WRB', ',', 'NNP', 'CC', 'NNP', 'VBP', 'RB', 'RB', 'VBN', 'TO', 'JJ', 'NNS', 'IN', 'DT', 'NN', ',', 'CC', 'DT', 'RB', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'NN', 'TO', 'VB', 'IN', 'VBG', 'NNS', 'CC', 'VBZ', 'RB', 'CD', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",53
question_answering,5,2,EVIDENCE AGGREGATION FOR ANSWER RE - RANKING IN OPEN - DOMAIN QUESTION ANSWERING,"['EVIDENCE', 'AGGREGATION', 'FOR', 'ANSWER', 'RE', '-', 'RANKING', 'IN', 'OPEN', '-', 'DOMAIN', 'QUESTION', 'ANSWERING']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'NNP', 'NNP', ':', 'NN', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'NNP']",13
question_answering,5,16,Open-domain question answering ( QA ) aims to answer questions from a broad range of domains by effectively marshalling evidence from large open - domain knowledge sources .,"['Open-domain', 'question', 'answering', '(', 'QA', ')', 'aims', 'to', 'answer', 'questions', 'from', 'a', 'broad', 'range', 'of', 'domains', 'by', 'effectively', 'marshalling', 'evidence', 'from', 'large', 'open', '-', 'domain', 'knowledge', 'sources', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'TO', 'VB', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'RB', 'VBG', 'NN', 'IN', 'JJ', 'JJ', ':', 'NN', 'NN', 'NNS', '.']",28
question_answering,5,18,Recent work on open - domain QA has focused on using unstructured text retrieved from the web to build machine comprehension models .,"['Recent', 'work', 'on', 'open', '-', 'domain', 'QA', 'has', 'focused', 'on', 'using', 'unstructured', 'text', 'retrieved', 'from', 'the', 'web', 'to', 'build', 'machine', 'comprehension', 'models', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'IN', 'JJ', ':', 'NN', 'NNP', 'VBZ', 'VBN', 'IN', 'VBG', 'JJ', 'NN', 'VBN', 'IN', 'DT', 'NN', 'TO', 'VB', 'NN', 'NN', 'NNS', '.']",23
question_answering,5,23,"In this paper , we propose a method to improve open - domain QA by explicitly aggregating evidence from across multiple passages .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'method', 'to', 'improve', 'open', '-', 'domain', 'QA', 'by', 'explicitly', 'aggregating', 'evidence', 'from', 'across', 'multiple', 'passages', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'TO', 'VB', 'JJ', ':', 'NN', 'NNP', 'IN', 'RB', 'VBG', 'NN', 'IN', 'IN', 'JJ', 'NNS', '.']",23
question_answering,5,35,We formulate the above evidence aggregation as an answer re-ranking problem .,"['We', 'formulate', 'the', 'above', 'evidence', 'aggregation', 'as', 'an', 'answer', 're-ranking', 'problem', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'NN', '.']",12
question_answering,5,37,"Here we apply the idea of re-ranking ; for each answer candidate , we efficiently incorporate global information from multiple pieces of textual evidence without significantly increasing the complexity of the prediction of the RC model .","['Here', 'we', 'apply', 'the', 'idea', 'of', 're-ranking', ';', 'for', 'each', 'answer', 'candidate', ',', 'we', 'efficiently', 'incorporate', 'global', 'information', 'from', 'multiple', 'pieces', 'of', 'textual', 'evidence', 'without', 'significantly', 'increasing', 'the', 'complexity', 'of', 'the', 'prediction', 'of', 'the', 'RC', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O']","['RB', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', ':', 'IN', 'DT', 'NN', 'NN', ',', 'PRP', 'RB', 'VBP', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'IN', 'RB', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",37
question_answering,5,39,The re-rankers are :,"['The', 're-rankers', 'are', ':']","['O', 'B-n', 'B-p', 'O']","['DT', 'NNS', 'VBP', ':']",4
question_answering,5,40,"A strength - based re-ranker , which ranks the answer candidates according to how often their evidence occurs in different passages .","['A', 'strength', '-', 'based', 're-ranker', ',', 'which', 'ranks', 'the', 'answer', 'candidates', 'according', 'to', 'how', 'often', 'their', 'evidence', 'occurs', 'in', 'different', 'passages', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', ':', 'VBN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'NNS', 'VBG', 'TO', 'WRB', 'RB', 'PRP$', 'NN', 'VBZ', 'IN', 'JJ', 'NNS', '.']",22
question_answering,5,43,"A coverage - based re-ranker , which aims to rank an answer candidate higher if the union of all its contexts in different passages could cover more aspects included in the question .","['A', 'coverage', '-', 'based', 're-ranker', ',', 'which', 'aims', 'to', 'rank', 'an', 'answer', 'candidate', 'higher', 'if', 'the', 'union', 'of', 'all', 'its', 'contexts', 'in', 'different', 'passages', 'could', 'cover', 'more', 'aspects', 'included', 'in', 'the', 'question', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['DT', 'NN', ':', 'VBN', 'NN', ',', 'WDT', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'NN', 'JJR', 'IN', 'DT', 'NN', 'IN', 'PDT', 'PRP$', 'NN', 'IN', 'JJ', 'NNS', 'MD', 'VB', 'JJR', 'NNS', 'VBN', 'IN', 'DT', 'NN', '.']",33
question_answering,5,160,"Our baseline models 9 include the following : GA , a reading comprehension model with gated - attention ; BiDAF ) , a RC model with bidirectional attention flow ; AQA ) , a reinforced system learning to aggregate the answers generated by the re-written questions ; R 3 ) , a reinforced model making use of a ranker for selecting passages to train the RC model .","['Our', 'baseline', 'models', '9', 'include', 'the', 'following', ':', 'GA', ',', 'a', 'reading', 'comprehension', 'model', 'with', 'gated', '-', 'attention', ';', 'BiDAF', ')', ',', 'a', 'RC', 'model', 'with', 'bidirectional', 'attention', 'flow', ';', 'AQA', ')', ',', 'a', 'reinforced', 'system', 'learning', 'to', 'aggregate', 'the', 'answers', 'generated', 'by', 'the', 're-written', 'questions', ';', 'R', '3', ')', ',', 'a', 'reinforced', 'model', 'making', 'use', 'of', 'a', 'ranker', 'for', 'selecting', 'passages', 'to', 'train', 'the', 'RC', 'model', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'NNS', 'CD', 'VBP', 'DT', 'JJ', ':', 'NNP', ',', 'DT', 'NN', 'NN', 'NN', 'IN', 'VBN', ':', 'NN', ':', 'NNP', ')', ',', 'DT', 'NNP', 'NN', 'IN', 'JJ', 'NN', 'NN', ':', 'NNP', ')', ',', 'DT', 'JJ', 'NN', 'VBG', 'TO', 'VB', 'DT', 'NNS', 'VBN', 'IN', 'DT', 'JJ', 'NNS', ':', 'NNP', 'CD', ')', ',', 'DT', 'JJ', 'NN', 'VBG', 'NN', 'IN', 'DT', 'NN', 'IN', 'VBG', 'NNS', 'TO', 'VB', 'DT', 'NNP', 'NN', '.']",68
question_answering,5,167,"We first use a pre-trained R 3 model , which gets the state - of - the - art performance on the three public datasets we consider , to generate the top 50 candidate spans for the training , development and test datasets , and we use them for further ranking .","['We', 'first', 'use', 'a', 'pre-trained', 'R', '3', 'model', ',', 'which', 'gets', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'on', 'the', 'three', 'public', 'datasets', 'we', 'consider', ',', 'to', 'generate', 'the', 'top', '50', 'candidate', 'spans', 'for', 'the', 'training', ',', 'development', 'and', 'test', 'datasets', ',', 'and', 'we', 'use', 'them', 'for', 'further', 'ranking', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'JJ', 'NNP', 'CD', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'DT', 'CD', 'JJ', 'VBZ', 'PRP', 'VBP', ',', 'TO', 'VB', 'DT', 'JJ', 'CD', 'NN', 'NNS', 'IN', 'DT', 'NN', ',', 'NN', 'CC', 'NN', 'NNS', ',', 'CC', 'PRP', 'VBP', 'PRP', 'IN', 'JJ', 'NN', '.']",52
question_answering,5,169,"For the coverage - based re-ranker , we use Adam to optimize the model .","['For', 'the', 'coverage', '-', 'based', 're-ranker', ',', 'we', 'use', 'Adam', 'to', 'optimize', 'the', 'model', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['IN', 'DT', 'NN', ':', 'VBN', 'NN', ',', 'PRP', 'VBP', 'NNP', 'TO', 'VB', 'DT', 'NN', '.']",15
question_answering,5,171,We set all the words beyond Glove as zero vectors .,"['We', 'set', 'all', 'the', 'words', 'beyond', 'Glove', 'as', 'zero', 'vectors', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'PDT', 'DT', 'NNS', 'IN', 'NNP', 'IN', 'NN', 'NNS', '.']",11
question_answering,5,172,"We set l to 300 , batch size to 30 , learning rate to 0.002 .","['We', 'set', 'l', 'to', '300', ',', 'batch', 'size', 'to', '30', ',', 'learning', 'rate', 'to', '0.002', '.']","['O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'TO', 'CD', ',', 'NN', 'NN', 'TO', 'CD', ',', 'VBG', 'NN', 'TO', 'CD', '.']",16
question_answering,5,173,"We tune the dropout probability from 0 to 0.5 and the number of candidate answers for re-ranking ( K ) in [ 3 , 5 , 10 ] 11 .","['We', 'tune', 'the', 'dropout', 'probability', 'from', '0', 'to', '0.5', 'and', 'the', 'number', 'of', 'candidate', 'answers', 'for', 're-ranking', '(', 'K', ')', 'in', '[', '3', ',', '5', ',', '10', ']', '11', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CD', 'TO', 'CD', 'CC', 'DT', 'NN', 'IN', 'NN', 'NNS', 'IN', 'NN', '(', 'NNP', ')', 'IN', '$', 'CD', ',', 'CD', ',', 'CD', 'NN', 'CD', '.']",30
question_answering,5,181,"The results showed that R 3 achieved F1 56.0 , EM 50.9 on Wiki domain and F1 68.5 , EM 63.0 on Web domain , which is competitive to the state - of - the - arts .","['The', 'results', 'showed', 'that', 'R', '3', 'achieved', 'F1', '56.0', ',', 'EM', '50.9', 'on', 'Wiki', 'domain', 'and', 'F1', '68.5', ',', 'EM', '63.0', 'on', 'Web', 'domain', ',', 'which', 'is', 'competitive', 'to', 'the', 'state', '-', 'of', '-', 'the', '-', 'arts', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBD', 'IN', 'NNP', 'CD', 'VBD', 'NNP', 'CD', ',', 'NNP', 'CD', 'IN', 'NNP', 'NN', 'CC', 'NNP', 'CD', ',', 'NNP', 'CD', 'IN', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'JJ', 'TO', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NNS', '.']",38
question_answering,5,184,Our code will be released under https://github.com/shuohangwang/mprc.,"['Our', 'code', 'will', 'be', 'released', 'under', 'https://github.com/shuohangwang/mprc.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n']","['PRP$', 'NN', 'MD', 'VB', 'VBN', 'IN', 'NN']",7
question_answering,5,190,"From the results , we can clearly see that the full re-ranker , the combination of different re-rankers , significantly outperforms the previous best performance by a large margin , especially on Quasar - T and Search QA .","['From', 'the', 'results', ',', 'we', 'can', 'clearly', 'see', 'that', 'the', 'full', 're-ranker', ',', 'the', 'combination', 'of', 'different', 're-rankers', ',', 'significantly', 'outperforms', 'the', 'previous', 'best', 'performance', 'by', 'a', 'large', 'margin', ',', 'especially', 'on', 'Quasar', '-', 'T', 'and', 'Search', 'QA', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'PRP', 'MD', 'RB', 'VB', 'IN', 'DT', 'JJ', 'NN', ',', 'DT', 'NN', 'IN', 'JJ', 'NNS', ',', 'RB', 'VBZ', 'DT', 'JJ', 'RBS', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'RB', 'IN', 'NNP', ':', 'NN', 'CC', 'NNP', 'NNP', '.']",39
question_answering,5,191,"Moreover , our model is much better than the human performance on the Search QA dataset .","['Moreover', ',', 'our', 'model', 'is', 'much', 'better', 'than', 'the', 'human', 'performance', 'on', 'the', 'Search', 'QA', 'dataset', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'NN', 'VBZ', 'RB', 'JJR', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'NN', '.']",17
question_answering,5,192,"In addition , we see that our coverage - based re-ranker achieves consistently good performance on the three datasets , even though its performance is marginally lower than the strength - based re-ranker on the Search QA dataset .","['In', 'addition', ',', 'we', 'see', 'that', 'our', 'coverage', '-', 'based', 're-ranker', 'achieves', 'consistently', 'good', 'performance', 'on', 'the', 'three', 'datasets', ',', 'even', 'though', 'its', 'performance', 'is', 'marginally', 'lower', 'than', 'the', 'strength', '-', 'based', 're-ranker', 'on', 'the', 'Search', 'QA', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'IN', 'PRP$', 'NN', ':', 'VBN', 'NN', 'NNS', 'RB', 'JJ', 'NN', 'IN', 'DT', 'CD', 'NNS', ',', 'RB', 'IN', 'PRP$', 'NN', 'VBZ', 'RB', 'JJR', 'IN', 'DT', 'NN', ':', 'VBN', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'NN', '.']",39
question_answering,4,2,Densely Connected Attention Propagation for Reading Comprehension,"['Densely', 'Connected', 'Attention', 'Propagation', 'for', 'Reading', 'Comprehension']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['RB', 'VBN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",7
question_answering,4,4,"We propose DECAPROP ( Densely Connected Attention Propagation ) , a new densely connected neural architecture for reading comprehension ( RC ) .","['We', 'propose', 'DECAPROP', '(', 'Densely', 'Connected', 'Attention', 'Propagation', ')', ',', 'a', 'new', 'densely', 'connected', 'neural', 'architecture', 'for', 'reading', 'comprehension', '(', 'RC', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', '(', 'RB', 'VBN', 'NNP', 'NNP', ')', ',', 'DT', 'JJ', 'RB', 'VBN', 'JJ', 'NN', 'IN', 'VBG', 'NN', '(', 'NNP', ')', '.']",23
question_answering,4,9,We conduct extensive experiments on four challenging RC benchmarks .,"['We', 'conduct', 'extensive', 'experiments', 'on', 'four', 'challenging', 'RC', 'benchmarks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O']","['PRP', 'VBP', 'JJ', 'NNS', 'IN', 'CD', 'VBG', 'NNP', 'NNS', '.']",10
question_answering,4,31,"Firstly , our network is densely connected , connecting every layer of P with every layer of Q .","['Firstly', ',', 'our', 'network', 'is', 'densely', 'connected', ',', 'connecting', 'every', 'layer', 'of', 'P', 'with', 'every', 'layer', 'of', 'Q', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP$', 'NN', 'VBZ', 'RB', 'VBN', ',', 'VBG', 'DT', 'NN', 'IN', 'NNP', 'IN', 'DT', 'NN', 'IN', 'NNP', '.']",19
question_answering,4,37,"To this end , we propose efficient Bidirectional Attention Connectors ( BAC ) as a base building block to connect two sequences at arbitrary layers .","['To', 'this', 'end', ',', 'we', 'propose', 'efficient', 'Bidirectional', 'Attention', 'Connectors', '(', 'BAC', ')', 'as', 'a', 'base', 'building', 'block', 'to', 'connect', 'two', 'sequences', 'at', 'arbitrary', 'layers', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['TO', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'IN', 'DT', 'NN', 'NN', 'NN', 'TO', 'VB', 'CD', 'NNS', 'IN', 'JJ', 'NNS', '.']",26
question_answering,4,38,"The key idea is to compress the attention outputs so that they can be small enough to propagate , yet enabling a connection between two sequences .","['The', 'key', 'idea', 'is', 'to', 'compress', 'the', 'attention', 'outputs', 'so', 'that', 'they', 'can', 'be', 'small', 'enough', 'to', 'propagate', ',', 'yet', 'enabling', 'a', 'connection', 'between', 'two', 'sequences', '.']","['O', 'O', 'O', 'B-p', 'B-p', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'VBZ', 'RB', 'IN', 'PRP', 'MD', 'VB', 'JJ', 'RB', 'TO', 'VB', ',', 'RB', 'VBG', 'DT', 'NN', 'IN', 'CD', 'NNS', '.']",27
question_answering,4,39,"The propagated features are collectively passed into prediction layers , which effectively connect shallow layers to deeper layers .","['The', 'propagated', 'features', 'are', 'collectively', 'passed', 'into', 'prediction', 'layers', ',', 'which', 'effectively', 'connect', 'shallow', 'layers', 'to', 'deeper', 'layers', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'NN', 'NNS', ',', 'WDT', 'RB', 'VBP', 'JJ', 'NNS', 'TO', 'VB', 'NNS', '.']",19
question_answering,4,41,"Overall , we propose DECAPROP ( Densely Connected Attention Propagation ) , a novel architecture for reading comprehension .","['Overall', ',', 'we', 'propose', 'DECAPROP', '(', 'Densely', 'Connected', 'Attention', 'Propagation', ')', ',', 'a', 'novel', 'architecture', 'for', 'reading', 'comprehension', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['JJ', ',', 'PRP', 'VBP', 'NNP', '(', 'RB', 'VBN', 'NNP', 'NNP', ')', ',', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'NN', '.']",19
question_answering,4,152,NewsQA,['NewsQA'],['B-n'],['NN'],1
question_answering,4,156,"On this dataset , the key competitors are BiDAF , Match - LSTM , FastQA / Fast QA - Ext , R2-BiLSTM , AMANDA .","['On', 'this', 'dataset', ',', 'the', 'key', 'competitors', 'are', 'BiDAF', ',', 'Match', '-', 'LSTM', ',', 'FastQA', '/', 'Fast', 'QA', '-', 'Ext', ',', 'R2-BiLSTM', ',', 'AMANDA', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O']","['IN', 'DT', 'NN', ',', 'DT', 'JJ', 'NNS', 'VBP', 'NNP', ',', 'NNP', ':', 'NNP', ',', 'NNP', 'NNP', 'NNP', 'NNP', ':', 'NN', ',', 'NNP', ',', 'NNP', '.']",25
question_answering,4,157,Quasar -T,"['Quasar', '-T']","['B-n', 'I-n']","['NNP', 'NN']",2
question_answering,4,159,The key competitors on this dataset are BiDAF and the Reinforced Ranker - Reader ( R 3 ) .,"['The', 'key', 'competitors', 'on', 'this', 'dataset', 'are', 'BiDAF', 'and', 'the', 'Reinforced', 'Ranker', '-', 'Reader', '(', 'R', '3', ')', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'NNP', 'CC', 'DT', 'NNP', 'NNP', ':', 'NN', '(', 'NNP', 'CD', ')', '.']",19
question_answering,4,161,SearchQA,['SearchQA'],['B-n'],['NN'],1
question_answering,4,165,"The competitor baselines on this dataset are Attention Sum Reader ( ASR ) , Focused Hierarchical RNNs ( FH - RNN ) , AMANDA , BiDAF , AQA and the Reinforced Ranker - Reader ( R 3 ) .","['The', 'competitor', 'baselines', 'on', 'this', 'dataset', 'are', 'Attention', 'Sum', 'Reader', '(', 'ASR', ')', ',', 'Focused', 'Hierarchical', 'RNNs', '(', 'FH', '-', 'RNN', ')', ',', 'AMANDA', ',', 'BiDAF', ',', 'AQA', 'and', 'the', 'Reinforced', 'Ranker', '-', 'Reader', '(', 'R', '3', ')', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'VBD', 'NNP', 'NNP', '(', 'NNP', ':', 'NN', ')', ',', 'NNP', ',', 'NNP', ',', 'NNP', 'CC', 'DT', 'NNP', 'NNP', ':', 'NN', '(', 'NNP', 'CD', ')', '.']",39
question_answering,4,166,Narrative QA ] is a recent QA dataset that involves comprehension over stories .,"['Narrative', 'QA', ']', 'is', 'a', 'recent', 'QA', 'dataset', 'that', 'involves', 'comprehension', 'over', 'stories', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NNP', 'NN', 'WDT', 'VBZ', 'NN', 'IN', 'NNS', '.']",14
question_answering,4,168,"We compare with the baselines in the original paper , namely Seq2Seq , Attention Sum Reader and BiDAF .","['We', 'compare', 'with', 'the', 'baselines', 'in', 'the', 'original', 'paper', ',', 'namely', 'Seq2Seq', ',', 'Attention', 'Sum', 'Reader', 'and', 'BiDAF', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', ',', 'RB', 'NNP', ',', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', '.']",19
question_answering,4,169,We also compare with the recent BiAttention + MRU model .,"['We', 'also', 'compare', 'with', 'the', 'recent', 'BiAttention', '+', 'MRU', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'IN', 'DT', 'JJ', 'NNP', 'NNP', 'NNP', 'NN', '.']",11
question_answering,4,178,Our model is implemented in Tensorflow .,"['Our', 'model', 'is', 'implemented', 'in', 'Tensorflow', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['PRP$', 'NN', 'VBZ', 'VBN', 'IN', 'NNP', '.']",7
question_answering,4,179,"The sequence lengths are capped at 800/700/1500/1100 for News QA , Search QA , Quasar - T and Narrative QA respectively .","['The', 'sequence', 'lengths', 'are', 'capped', 'at', '800/700/1500/1100', 'for', 'News', 'QA', ',', 'Search', 'QA', ',', 'Quasar', '-', 'T', 'and', 'Narrative', 'QA', 'respectively', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'CD', 'IN', 'NNP', 'NNP', ',', 'NNP', 'NNP', ',', 'NNP', ':', 'NN', 'CC', 'NNP', 'NNP', 'RB', '.']",22
question_answering,4,180,"We use Adadelta with ? = 0.5 for News QA , Adam with ? = 0.001 for Search QA , Quasar - T and Narrative QA .","['We', 'use', 'Adadelta', 'with', '?', '=', '0.5', 'for', 'News', 'QA', ',', 'Adam', 'with', '?', '=', '0.001', 'for', 'Search', 'QA', ',', 'Quasar', '-', 'T', 'and', 'Narrative', 'QA', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'IN', '.', '$', 'CD', 'IN', 'NNP', 'NNP', ',', 'NNP', 'IN', '.', '$', 'CD', 'IN', 'NNP', 'NNP', ',', 'NNP', ':', 'NN', 'CC', 'NNP', 'NNP', '.']",27
question_answering,4,181,"The choice of the RNN encoder is tuned between GRU and LSTM cells and the hidden size is tuned amongst { 32 , 50 , 64 , 75 } .","['The', 'choice', 'of', 'the', 'RNN', 'encoder', 'is', 'tuned', 'between', 'GRU', 'and', 'LSTM', 'cells', 'and', 'the', 'hidden', 'size', 'is', 'tuned', 'amongst', '{', '32', ',', '50', ',', '64', ',', '75', '}', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'DT', 'NNP', 'NN', 'VBZ', 'VBN', 'IN', 'NNP', 'CC', 'NNP', 'NNS', 'CC', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'RB', '(', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ')', '.']",30
question_answering,4,182,We use the CUDNN implementation of the RNN encoder .,"['We', 'use', 'the', 'CUDNN', 'implementation', 'of', 'the', 'RNN', 'encoder', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",10
question_answering,4,183,"Batch size is tuned amongst { 16 , 32 , 64 } .","['Batch', 'size', 'is', 'tuned', 'amongst', '{', '16', ',', '32', ',', '64', '}', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'NN', 'VBZ', 'VBN', 'RB', '(', 'CD', ',', 'CD', ',', 'CD', ')', '.']",13
question_answering,4,184,"Dropout rate is tuned amongst { 0.1 , 0.2 , 0.3 } and applied to all RNN and fully - connected layers .","['Dropout', 'rate', 'is', 'tuned', 'amongst', '{', '0.1', ',', '0.2', ',', '0.3', '}', 'and', 'applied', 'to', 'all', 'RNN', 'and', 'fully', '-', 'connected', 'layers', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'NN', 'VBZ', 'VBN', 'RB', '(', 'CD', ',', 'CD', ',', 'CD', ')', 'CC', 'VBN', 'TO', 'DT', 'NNP', 'CC', 'RB', ':', 'VBN', 'NNS', '.']",23
question_answering,4,185,We apply variational dropout in - between RNN layers .,"['We', 'apply', 'variational', 'dropout', 'in', '-', 'between', 'RNN', 'layers', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', ':', 'IN', 'NNP', 'NNS', '.']",10
question_answering,4,186,We initialize the word embeddings with 300D Glo Ve embeddings and are fixed during training .,"['We', 'initialize', 'the', 'word', 'embeddings', 'with', '300D', 'Glo', 'Ve', 'embeddings', 'and', 'are', 'fixed', 'during', 'training', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'VBZ', 'IN', 'CD', 'NNP', 'NNP', 'NNS', 'CC', 'VBP', 'VBN', 'IN', 'NN', '.']",16
question_answering,4,187,The size of the character embeddings is set to 8 and the character RNN is set to the same as the word - level RNN encoders .,"['The', 'size', 'of', 'the', 'character', 'embeddings', 'is', 'set', 'to', '8', 'and', 'the', 'character', 'RNN', 'is', 'set', 'to', 'the', 'same', 'as', 'the', 'word', '-', 'level', 'RNN', 'encoders', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'DT', 'NN', 'NNS', 'VBZ', 'VBN', 'TO', 'CD', 'CC', 'DT', 'NN', 'NNP', 'VBZ', 'VBN', 'TO', 'DT', 'JJ', 'IN', 'DT', 'NN', ':', 'NN', 'NNP', 'NNS', '.']",27
question_answering,4,188,The maximum characters per word is set to 16 .,"['The', 'maximum', 'characters', 'per', 'word', 'is', 'set', 'to', '16', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'JJ', 'NNS', 'IN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",10
question_answering,4,189,The number of layers in DECAENC is set to 3 and the number of factors in the factorization kernel is set to 64 .,"['The', 'number', 'of', 'layers', 'in', 'DECAENC', 'is', 'set', 'to', '3', 'and', 'the', 'number', 'of', 'factors', 'in', 'the', 'factorization', 'kernel', 'is', 'set', 'to', '64', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NNS', 'IN', 'NNP', 'VBZ', 'VBN', 'TO', 'CD', 'CC', 'DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",24
question_answering,4,190,We use a learning rate decay factor of 2 and patience of 3 epochs whenever the EM ( or ROUGE - L ) score on the development set does not increase .,"['We', 'use', 'a', 'learning', 'rate', 'decay', 'factor', 'of', '2', 'and', 'patience', 'of', '3', 'epochs', 'whenever', 'the', 'EM', '(', 'or', 'ROUGE', '-', 'L', ')', 'score', 'on', 'the', 'development', 'set', 'does', 'not', 'increase', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'JJ', 'NN', 'IN', 'CD', 'CC', 'NN', 'IN', 'CD', 'NNS', 'WRB', 'DT', 'NNP', '(', 'CC', 'NNP', ':', 'NN', ')', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'RB', 'VB', '.']",32
question_answering,4,192,"Overall , our results are optimistic and promising , with results indicating that DECAPROP achieves state - of - the - art performance 6 on all four datasets . 66.2 75.9 DCN + CoVE 71.3 79.9 R- NET 72.3 80.6 R - NET","['Overall', ',', 'our', 'results', 'are', 'optimistic', 'and', 'promising', ',', 'with', 'results', 'indicating', 'that', 'DECAPROP', 'achieves', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', '6', 'on', 'all', 'four', 'datasets', '.', '66.2', '75.9', 'DCN', '+', 'CoVE', '71.3', '79.9', 'R-', 'NET', '72.3', '80.6', 'R', '-', 'NET']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', ',', 'PRP$', 'NNS', 'VBP', 'JJ', 'CC', 'JJ', ',', 'IN', 'NNS', 'VBG', 'IN', 'NNP', 'VBZ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'CD', 'IN', 'DT', 'CD', 'NNS', '.', 'CD', 'CD', 'NNP', 'NNP', 'NNP', 'CD', 'CD', 'JJ', 'NNP', 'CD', 'CD', 'NNP', ':', 'NN']",43
question_answering,4,194,"On this dataset , DECAPROP outperforms the existing state - of - the - art , i.e. , the recent AMANDA model by ( + 4.7 % EM / + 2.6 % F1 ) .","['On', 'this', 'dataset', ',', 'DECAPROP', 'outperforms', 'the', 'existing', 'state', '-', 'of', '-', 'the', '-', 'art', ',', 'i.e.', ',', 'the', 'recent', 'AMANDA', 'model', 'by', '(', '+', '4.7', '%', 'EM', '/', '+', '2.6', '%', 'F1', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'DT', 'NN', ',', 'NNP', 'VBZ', 'DT', 'VBG', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', ',', 'NN', ',', 'DT', 'JJ', 'NNP', 'NN', 'IN', '(', 'JJ', 'CD', 'NN', 'NNP', 'NNP', 'VBD', 'CD', 'NN', 'NNP', ')', '.']",35
question_answering,4,196,"Moreover , our proposed model also outperforms well - established baselines such as Match - LSTM ( + 18 % EM / + 16.3 % F1 ) and BiDAF ( + 16 % EM / + 14 % F1 ) .","['Moreover', ',', 'our', 'proposed', 'model', 'also', 'outperforms', 'well', '-', 'established', 'baselines', 'such', 'as', 'Match', '-', 'LSTM', '(', '+', '18', '%', 'EM', '/', '+', '16.3', '%', 'F1', ')', 'and', 'BiDAF', '(', '+', '16', '%', 'EM', '/', '+', '14', '%', 'F1', ')', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'VBN', 'NN', 'RB', 'VBZ', 'RB', ':', 'VBN', 'NNS', 'JJ', 'IN', 'NNP', ':', 'NNP', '(', 'VB', 'CD', 'NN', 'NNP', 'NNP', 'VBD', 'CD', 'NN', 'NNP', ')', 'CC', 'NNP', '(', 'VB', 'CD', 'NN', 'NNP', 'NNP', 'VBD', 'CD', 'NN', 'NNP', ')', '.']",41
question_answering,4,197,reports the results on Quasar - T .,"['reports', 'the', 'results', 'on', 'Quasar', '-', 'T', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNS', 'DT', 'NNS', 'IN', 'NNP', ':', 'NN', '.']",8
question_answering,4,198,"Our model achieves state - of - the - art performance on this dataset , outperforming the state - of - the - art R 3 ( Reinforced Ranker Reader ) by a considerable margin of + 4.4 % EM / + 6 % F1 .","['Our', 'model', 'achieves', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'on', 'this', 'dataset', ',', 'outperforming', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'R', '3', '(', 'Reinforced', 'Ranker', 'Reader', ')', 'by', 'a', 'considerable', 'margin', 'of', '+', '4.4', '%', 'EM', '/', '+', '6', '%', 'F1', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'DT', 'NN', ',', 'VBG', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNP', 'CD', '(', 'NNP', 'NNP', 'NNP', ')', 'IN', 'DT', 'JJ', 'NN', 'IN', '$', 'CD', 'NN', 'NNP', 'NNP', 'VBD', 'CD', 'NN', 'NNP', '.']",46
question_answering,4,202,"On the original setting , our model outperforms AMANDA by + 15.4 % EM and + 14.2 % in terms of F1 score .","['On', 'the', 'original', 'setting', ',', 'our', 'model', 'outperforms', 'AMANDA', 'by', '+', '15.4', '%', 'EM', 'and', '+', '14.2', '%', 'in', 'terms', 'of', 'F1', 'score', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'PRP$', 'NN', 'NNS', 'NNP', 'IN', 'NNP', 'CD', 'NN', 'NNP', 'CC', 'VB', 'CD', 'NN', 'IN', 'NNS', 'IN', 'NNP', 'NN', '.']",24
question_answering,4,203,"On the over all setting , our model outperforms both AQA ( + 18.1 % EM / + 18 % F1 ) and Reinforced Reader Ranker ( + 7.8 % EM / + 8.3 % F1 ) .","['On', 'the', 'over', 'all', 'setting', ',', 'our', 'model', 'outperforms', 'both', 'AQA', '(', '+', '18.1', '%', 'EM', '/', '+', '18', '%', 'F1', ')', 'and', 'Reinforced', 'Reader', 'Ranker', '(', '+', '7.8', '%', 'EM', '/', '+', '8.3', '%', 'F1', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'DT', 'IN', 'DT', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'NNP', '(', 'VB', 'CD', 'NN', 'NNP', 'NNP', 'VBD', 'CD', 'NN', 'NNP', ')', 'CC', 'NNP', 'NNP', 'NNP', '(', 'VB', 'CD', 'NN', 'NNP', 'NNP', 'VBD', 'CD', 'NN', 'NNP', ')', '.']",38
question_answering,4,209,SQuAD reports dev scores 8 of our model against several representative models on the popular SQuAD benchmark .,"['SQuAD', 'reports', 'dev', 'scores', '8', 'of', 'our', 'model', 'against', 'several', 'representative', 'models', 'on', 'the', 'popular', 'SQuAD', 'benchmark', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNS', 'NN', 'VBZ', 'CD', 'IN', 'PRP$', 'NN', 'IN', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NNP', 'NN', '.']",18
question_answering,4,210,"While our model does not achieve state - of - the - art performance , our model can outperform the base R - NET ( both our implementation as well as the published score ) .","['While', 'our', 'model', 'does', 'not', 'achieve', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', ',', 'our', 'model', 'can', 'outperform', 'the', 'base', 'R', '-', 'NET', '(', 'both', 'our', 'implementation', 'as', 'well', 'as', 'the', 'published', 'score', ')', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PRP$', 'NN', 'VBZ', 'RB', 'VB', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', ',', 'PRP$', 'NN', 'MD', 'VB', 'DT', 'NN', 'NNP', ':', 'NN', '(', 'DT', 'PRP$', 'NN', 'RB', 'RB', 'IN', 'DT', 'VBN', 'NN', ')', '.']",36
question_answering,4,213,We conduct an ablation study on the New s QA development set .,"['We', 'conduct', 'an', 'ablation', 'study', 'on', 'the', 'New', 's', 'QA', 'development', 'set', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NNP', 'NN', 'NN', '.']",13
question_answering,4,221,"Finally , in ( 8 - 9 ) , we varied the FM with linear and nonlinear feed - forward layers . From ( 1 ) , we observe a significant gap in performance between DECAPROP and R - NET .","['Finally', ',', 'in', '(', '8', '-', '9', ')', ',', 'we', 'varied', 'the', 'FM', 'with', 'linear', 'and', 'nonlinear', 'feed', '-', 'forward', 'layers', '.', 'From', '(', '1', ')', ',', 'we', 'observe', 'a', 'significant', 'gap', 'in', 'performance', 'between', 'DECAPROP', 'and', 'R', '-', 'NET', '.']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'IN', '(', 'CD', ':', 'CD', ')', ',', 'PRP', 'VBD', 'DT', 'NNP', 'IN', 'JJ', 'CC', 'JJ', 'NN', ':', 'NN', 'NNS', '.', 'NNP', '(', 'CD', ')', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'NNP', 'CC', 'NNP', ':', 'NN', '.']",41
question_answering,4,223,"Overall , the key insight is that all model components are crucial to DECAPROP .","['Overall', ',', 'the', 'key', 'insight', 'is', 'that', 'all', 'model', 'components', 'are', 'crucial', 'to', 'DECAPROP', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['JJ', ',', 'DT', 'JJ', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'NNS', 'VBP', 'JJ', 'TO', 'NNP', '.']",15
question_answering,4,224,"Notably , the DECAENC seems to contribute the most to the over all performance .","['Notably', ',', 'the', 'DECAENC', 'seems', 'to', 'contribute', 'the', 'most', 'to', 'the', 'over', 'all', 'performance', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NNP', 'VBZ', 'TO', 'VB', 'DT', 'RBS', 'TO', 'DT', 'IN', 'DT', 'NN', '.']",15
question_answering,4,226,We observe that the superiority of DECAPROP over R - NET is consistent and relatively stable .,"['We', 'observe', 'that', 'the', 'superiority', 'of', 'DECAPROP', 'over', 'R', '-', 'NET', 'is', 'consistent', 'and', 'relatively', 'stable', '.']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NN', 'IN', 'NNP', 'IN', 'NNP', ':', 'NN', 'VBZ', 'JJ', 'CC', 'RB', 'JJ', '.']",17
question_answering,2,2,Focal Visual - Text Attention for Visual Question Answering,"['Focal', 'Visual', '-', 'Text', 'Attention', 'for', 'Visual', 'Question', 'Answering']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['JJ', 'NNP', ':', 'NN', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",9
question_answering,2,13,"Visual question answering ( VQA ) is a successful direction utilizing both computer vision and natural language processing techniques to solve an interesting problem : given a pair of image and a question ( in natural language ) , the goal is to learn an inference model that can the answer questions according to cues discovered from the image .","['Visual', 'question', 'answering', '(', 'VQA', ')', 'is', 'a', 'successful', 'direction', 'utilizing', 'both', 'computer', 'vision', 'and', 'natural', 'language', 'processing', 'techniques', 'to', 'solve', 'an', 'interesting', 'problem', ':', 'given', 'a', 'pair', 'of', 'image', 'and', 'a', 'question', '(', 'in', 'natural', 'language', ')', ',', 'the', 'goal', 'is', 'to', 'learn', 'an', 'inference', 'model', 'that', 'can', 'the', 'answer', 'questions', 'according', 'to', 'cues', 'discovered', 'from', 'the', 'image', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'DT', 'JJ', 'NN', 'VBG', 'DT', 'NN', 'NN', 'CC', 'JJ', 'NN', 'NN', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NN', ':', 'VBN', 'DT', 'NN', 'IN', 'NN', 'CC', 'DT', 'NN', '(', 'IN', 'JJ', 'NN', ')', ',', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'NN', 'WDT', 'MD', 'DT', 'NN', 'NNS', 'VBG', 'TO', 'NNS', 'VBN', 'IN', 'DT', 'NN', '.']",60
question_answering,2,15,"Extending from VQA on a single image , this paper considers the following problem :","['Extending', 'from', 'VQA', 'on', 'a', 'single', 'image', ',', 'this', 'paper', 'considers', 'the', 'following', 'problem', ':']","['O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'IN', 'NNP', 'IN', 'DT', 'JJ', 'NN', ',', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', ':']",15
question_answering,2,32,"To address these two challenges , we propose a focal visual - text attention ( FVTA ) model for sequential data","['To', 'address', 'these', 'two', 'challenges', ',', 'we', 'propose', 'a', 'focal', 'visual', '-', 'text', 'attention', '(', 'FVTA', ')', 'model', 'for', 'sequential', 'data']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n']","['TO', 'VB', 'DT', 'CD', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', ':', 'NN', 'NN', '(', 'NNP', ')', 'NN', 'IN', 'JJ', 'NNS']",21
question_answering,2,37,"Inspired by this process , FVTA first learns to localize relevant information within a few , small , temporally consecutive regions over the input sequences , and learns to infer an answer based on the cross-modal statistics pooled from these regions .","['Inspired', 'by', 'this', 'process', ',', 'FVTA', 'first', 'learns', 'to', 'localize', 'relevant', 'information', 'within', 'a', 'few', ',', 'small', ',', 'temporally', 'consecutive', 'regions', 'over', 'the', 'input', 'sequences', ',', 'and', 'learns', 'to', 'infer', 'an', 'answer', 'based', 'on', 'the', 'cross-modal', 'statistics', 'pooled', 'from', 'these', 'regions', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'NN', ',', 'NNP', 'RB', 'VBZ', 'TO', 'VB', 'JJ', 'NN', 'IN', 'DT', 'JJ', ',', 'JJ', ',', 'RB', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NNS', ',', 'CC', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'VBN', 'IN', 'DT', 'JJ', 'NNS', 'VBD', 'IN', 'DT', 'NNS', '.']",42
question_answering,2,38,FVTA proposes a novel kernel to compute the attention tensor that jointly models the latent information in three sources :,"['FVTA', 'proposes', 'a', 'novel', 'kernel', 'to', 'compute', 'the', 'attention', 'tensor', 'that', 'jointly', 'models', 'the', 'latent', 'information', 'in', 'three', 'sources', ':']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'WDT', 'RB', 'NNS', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NNS', ':']",20
question_answering,2,39,"1 ) answer - signaling words in the question , 2 ) temporal correlation within a sequence , and 3 ) cross-modal interaction between the text and image .","['1', ')', 'answer', '-', 'signaling', 'words', 'in', 'the', 'question', ',', '2', ')', 'temporal', 'correlation', 'within', 'a', 'sequence', ',', 'and', '3', ')', 'cross-modal', 'interaction', 'between', 'the', 'text', 'and', 'image', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['CD', ')', 'JJR', ':', 'NN', 'NNS', 'IN', 'DT', 'NN', ',', 'CD', ')', 'JJ', 'NN', 'IN', 'DT', 'NN', ',', 'CC', 'CD', ')', 'JJ', 'NN', 'IN', 'DT', 'NN', 'CC', 'NN', '.']",29
question_answering,2,40,"FVTA attention allows for collective reasoning by the attention kernel learned over a few , small , consecutive sub-sequences of text and image .","['FVTA', 'attention', 'allows', 'for', 'collective', 'reasoning', 'by', 'the', 'attention', 'kernel', 'learned', 'over', 'a', 'few', ',', 'small', ',', 'consecutive', 'sub-sequences', 'of', 'text', 'and', 'image', '.']","['B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NN', 'VBZ', 'IN', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBN', 'IN', 'DT', 'JJ', ',', 'JJ', ',', 'JJ', 'NNS', 'IN', 'NN', 'CC', 'NN', '.']",24
question_answering,2,44,We propose a novel attention kernel for VQA on visual - text data .,"['We', 'propose', 'a', 'novel', 'attention', 'kernel', 'for', 'VQA', 'on', 'visual', '-', 'text', 'data', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'IN', 'JJ', ':', 'NN', 'NNS', '.']",14
question_answering,2,183,Memex QA provides 4 answer choices and only one correct answer for each question .,"['Memex', 'QA', 'provides', '4', 'answer', 'choices', 'and', 'only', 'one', 'correct', 'answer', 'for', 'each', 'question', '.']","['B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'VBZ', 'CD', 'NN', 'NNS', 'CC', 'RB', 'CD', 'NN', 'NN', 'IN', 'DT', 'NN', '.']",15
question_answering,2,191,"We implement the following methods as baselines : Logistic Regression predicts the answer with concatenated image , question and metadata features as reported in .","['We', 'implement', 'the', 'following', 'methods', 'as', 'baselines', ':', 'Logistic', 'Regression', 'predicts', 'the', 'answer', 'with', 'concatenated', 'image', ',', 'question', 'and', 'metadata', 'features', 'as', 'reported', 'in', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NNS', 'IN', 'NNS', ':', 'JJ', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NN', ',', 'NN', 'CC', 'NN', 'NNS', 'IN', 'VBN', 'IN', '.']",25
question_answering,2,192,"Embedding + LSTM utilizes word embeddings and character embeddings , along with the same visual embeddings used in FVTA .","['Embedding', '+', 'LSTM', 'utilizes', 'word', 'embeddings', 'and', 'character', 'embeddings', ',', 'along', 'with', 'the', 'same', 'visual', 'embeddings', 'used', 'in', 'FVTA', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['VBG', 'JJ', 'NNP', 'VBZ', 'NN', 'NNS', 'CC', 'NN', 'NNS', ',', 'IN', 'IN', 'DT', 'JJ', 'JJ', 'NNS', 'VBN', 'IN', 'NNP', '.']",20
question_answering,2,194,Embedding + LSTM + Concat concatenates the last LSTM output from different modalities to produce the final output .,"['Embedding', '+', 'LSTM', '+', 'Concat', 'concatenates', 'the', 'last', 'LSTM', 'output', 'from', 'different', 'modalities', 'to', 'produce', 'the', 'final', 'output', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['VBG', 'JJ', 'NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NNP', 'NN', 'IN', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NN', '.']",19
question_answering,2,196,Classic Soft Attention uses classic one dimensional question - to - context attention to summarize context for question answering .,"['Classic', 'Soft', 'Attention', 'uses', 'classic', 'one', 'dimensional', 'question', '-', 'to', '-', 'context', 'attention', 'to', 'summarize', 'context', 'for', 'question', 'answering', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['JJ', 'NNP', 'NNP', 'VBZ', 'JJ', 'CD', 'JJ', 'NN', ':', 'TO', ':', 'NN', 'NN', 'TO', 'VB', 'NN', 'IN', 'NN', 'NN', '.']",20
question_answering,2,198,"DMN + is the improved dynamic memory networks , which is one of the representative architectures that achieve good performance on the VQA Task .","['DMN', '+', 'is', 'the', 'improved', 'dynamic', 'memory', 'networks', ',', 'which', 'is', 'one', 'of', 'the', 'representative', 'architectures', 'that', 'achieve', 'good', 'performance', 'on', 'the', 'VQA', 'Task', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'NNS', ',', 'WDT', 'VBZ', 'CD', 'IN', 'DT', 'NN', 'VBZ', 'IN', 'VBP', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NNP', '.']",25
question_answering,2,201,TGIF Temporal Attention is a recently proposed spatial - temporal reasoning network on sequential animated image QA .,"['TGIF', 'Temporal', 'Attention', 'is', 'a', 'recently', 'proposed', 'spatial', '-', 'temporal', 'reasoning', 'network', 'on', 'sequential', 'animated', 'image', 'QA', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'RB', 'VBN', 'JJ', ':', 'JJ', 'NN', 'NN', 'IN', 'NN', 'VBN', 'NN', 'NNP', '.']",18
question_answering,2,211,We encode GPS locations using words .,"['We', 'encode', 'GPS', 'locations', 'using', 'words', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'NNS', 'VBG', 'NNS', '.']",7
question_answering,2,213,"All questions , textual context and answers are tokenized using the Stanford word tokenizer .","['All', 'questions', ',', 'textual', 'context', 'and', 'answers', 'are', 'tokenized', 'using', 'the', 'Stanford', 'word', 'tokenizer', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', ',', 'JJ', 'NN', 'CC', 'NNS', 'VBP', 'VBN', 'VBG', 'DT', 'NNP', 'NN', 'NN', '.']",15
question_answering,2,214,"We use pre-trained Glo Ve word embeddings , which is fixed during training .","['We', 'use', 'pre-trained', 'Glo', 'Ve', 'word', 'embeddings', ',', 'which', 'is', 'fixed', 'during', 'training', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'NNP', 'NNP', 'NN', 'NNS', ',', 'WDT', 'VBZ', 'VBN', 'IN', 'NN', '.']",14
question_answering,2,215,"For image / video embedding , we extract fixed - size features using the pre-trained CNN model , Inception - ResNet , by concatenating the pool5 layer and classification layer 's output before softmax .","['For', 'image', '/', 'video', 'embedding', ',', 'we', 'extract', 'fixed', '-', 'size', 'features', 'using', 'the', 'pre-trained', 'CNN', 'model', ',', 'Inception', '-', 'ResNet', ',', 'by', 'concatenating', 'the', 'pool5', 'layer', 'and', 'classification', 'layer', ""'s"", 'output', 'before', 'softmax', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'NN', 'NNP', 'NN', 'NN', ',', 'PRP', 'VBP', 'VBN', ':', 'NN', 'NNS', 'VBG', 'DT', 'JJ', 'NNP', 'NN', ',', 'NNP', ':', 'NN', ',', 'IN', 'VBG', 'DT', 'NN', 'NN', 'CC', 'NN', 'NN', 'POS', 'NN', 'IN', 'NN', '.']",35
question_answering,2,216,We then use a linear transformation to compress the image feature into 100 dimensional .,"['We', 'then', 'use', 'a', 'linear', 'transformation', 'to', 'compress', 'the', 'image', 'feature', 'into', '100', 'dimensional', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'CD', 'JJ', '.']",15
question_answering,2,217,Then a bi-directional LSTM is used for each modality to obtain contextual representations .,"['Then', 'a', 'bi-directional', 'LSTM', 'is', 'used', 'for', 'each', 'modality', 'to', 'obtain', 'contextual', 'representations', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['RB', 'DT', 'JJ', 'NNP', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'TO', 'VB', 'JJ', 'NNS', '.']",14
question_answering,2,218,"Given a hidden state size of d , which is set to 50 , we concatenate the output of both directions of the LSTM and get a question matrix Q ?","['Given', 'a', 'hidden', 'state', 'size', 'of', 'd', ',', 'which', 'is', 'set', 'to', '50', ',', 'we', 'concatenate', 'the', 'output', 'of', 'both', 'directions', 'of', 'the', 'LSTM', 'and', 'get', 'a', 'question', 'matrix', 'Q', '?']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['VBN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NN', ',', 'WDT', 'VBZ', 'VBN', 'TO', 'CD', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'CC', 'VB', 'DT', 'NN', 'NN', 'NNP', '.']",31
question_answering,2,219,R 2 d M and context tensor H ?,"['R', '2', 'd', 'M', 'and', 'context', 'tensor', 'H', '?']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['$', 'CD', 'NN', 'NNP', 'CC', 'NN', 'NN', 'NNP', '.']",9
question_answering,2,220,R 2dV KN 6 for all media documents .,"['R', '2dV', 'KN', '6', 'for', 'all', 'media', 'documents', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['$', 'CD', 'NNP', 'CD', 'IN', 'DT', 'NNS', 'NNS', '.']",9
question_answering,2,221,We reshape the context tensor into H ? R 2 d T 6 .,"['We', 'reshape', 'the', 'context', 'tensor', 'into', 'H', '?', 'R', '2', 'd', 'T', '6', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'NNP', '.', 'NNP', 'CD', 'NN', 'NNP', 'CD', '.']",14
question_answering,2,222,"To select the best hyperparmeters , we randomly select 20 % of the official training set as the validation set .","['To', 'select', 'the', 'best', 'hyperparmeters', ',', 'we', 'randomly', 'select', '20', '%', 'of', 'the', 'official', 'training', 'set', 'as', 'the', 'validation', 'set', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'JJS', 'NNS', ',', 'PRP', 'VBP', 'JJ', 'CD', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",21
question_answering,2,223,We use the AdaDelta optimizer and an initial learning rate of 0.5 to train for 200 epochs with a dropout rate of 0.3 ..,"['We', 'use', 'the', 'AdaDelta', 'optimizer', 'and', 'an', 'initial', 'learning', 'rate', 'of', '0.5', 'to', 'train', 'for', '200', 'epochs', 'with', 'a', 'dropout', 'rate', 'of', '0.3', '..']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', 'CC', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'TO', 'VB', 'IN', 'CD', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', 'NNS']",24
question_answering,2,234,FVTA outperforms other attention models on finding the relevant photos for the question .,"['FVTA', 'outperforms', 'other', 'attention', 'models', 'on', 'finding', 'the', 'relevant', 'photos', 'for', 'the', 'question', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['NNP', 'VBZ', 'JJ', 'NN', 'NNS', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",14
question_answering,2,242,"To evaluate the FVTA attention mechanism , we first replace our kernel tensor with simple cosine similarity function .","['To', 'evaluate', 'the', 'FVTA', 'attention', 'mechanism', ',', 'we', 'first', 'replace', 'our', 'kernel', 'tensor', 'with', 'simple', 'cosine', 'similarity', 'function', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NNP', 'NN', 'NN', ',', 'PRP', 'RB', 'VB', 'PRP$', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NN', 'NN', '.']",19
question_answering,2,243,Results show that standard cosine similarity is inferior to our similarity function .,"['Results', 'show', 'that', 'standard', 'cosine', 'similarity', 'is', 'inferior', 'to', 'our', 'similarity', 'function', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNS', 'VBP', 'IN', 'JJ', 'NN', 'NN', 'VBZ', 'JJ', 'TO', 'PRP$', 'NN', 'NN', '.']",13
question_answering,2,244,"For ablating intra-sequence dependency , we use the representations from the last timestep of each context document .","['For', 'ablating', 'intra-sequence', 'dependency', ',', 'we', 'use', 'the', 'representations', 'from', 'the', 'last', 'timestep', 'of', 'each', 'context', 'document', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'VBG', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",18
question_answering,2,245,"For ablating cross sequence interaction , we average all attended context representation from different modalities to get the final context vector .","['For', 'ablating', 'cross', 'sequence', 'interaction', ',', 'we', 'average', 'all', 'attended', 'context', 'representation', 'from', 'different', 'modalities', 'to', 'get', 'the', 'final', 'context', 'vector', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'VBG', 'NN', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'VBD', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', '.']",22
question_answering,2,246,"Both aspects of correlation of the FVTA attention tensor contribute towards the model 's performance , while intra-sequence dependency shows more importance in this experiment .","['Both', 'aspects', 'of', 'correlation', 'of', 'the', 'FVTA', 'attention', 'tensor', 'contribute', 'towards', 'the', 'model', ""'s"", 'performance', ',', 'while', 'intra-sequence', 'dependency', 'shows', 'more', 'importance', 'in', 'this', 'experiment', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NN', 'NN', 'IN', 'DT', 'NN', 'POS', 'NN', ',', 'IN', 'NN', 'NN', 'VBZ', 'JJR', 'NN', 'IN', 'DT', 'NN', '.']",26
question_answering,2,247,We compare the effectiveness of context - aware question attention by removing the question attention and use the last timestep of the LSTM output from the question as the question representation .,"['We', 'compare', 'the', 'effectiveness', 'of', 'context', '-', 'aware', 'question', 'attention', 'by', 'removing', 'the', 'question', 'attention', 'and', 'use', 'the', 'last', 'timestep', 'of', 'the', 'LSTM', 'output', 'from', 'the', 'question', 'as', 'the', 'question', 'representation', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'JJ', ':', 'JJ', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'NN', 'CC', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",32
question_answering,2,248,It shows the question attention provides slight improvement .,"['It', 'shows', 'the', 'question', 'attention', 'provides', 'slight', 'improvement', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBZ', 'DT', 'NN', 'NN', 'VBZ', 'JJ', 'NN', '.']",9
question_answering,2,249,"Finally , we train FVTA without photos to see the contribution of visual information .","['Finally', ',', 'we', 'train', 'FVTA', 'without', 'photos', 'to', 'see', 'the', 'contribution', 'of', 'visual', 'information', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'CD', 'IN', 'NNS', 'TO', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NN', '.']",15
question_answering,2,250,"The result is quite good but it is perhaps not surprising due to the language bias in the questions and answers of the dataset , which is not uncommon in VQA dataset and in Visual7W .","['The', 'result', 'is', 'quite', 'good', 'but', 'it', 'is', 'perhaps', 'not', 'surprising', 'due', 'to', 'the', 'language', 'bias', 'in', 'the', 'questions', 'and', 'answers', 'of', 'the', 'dataset', ',', 'which', 'is', 'not', 'uncommon', 'in', 'VQA', 'dataset', 'and', 'in', 'Visual7W', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'RB', 'JJ', 'CC', 'PRP', 'VBZ', 'RB', 'RB', 'JJ', 'JJ', 'TO', 'DT', 'NN', 'NN', 'IN', 'DT', 'NNS', 'CC', 'NNS', 'IN', 'DT', 'NN', ',', 'WDT', 'VBZ', 'RB', 'JJ', 'IN', 'NNP', 'NN', 'CC', 'IN', 'NNP', '.']",36
question_answering,2,257,"In the MovieQA dataset , each QA is given a set of N movie clips of the same movie , and each clip comes with subtitles .","['In', 'the', 'MovieQA', 'dataset', ',', 'each', 'QA', 'is', 'given', 'a', 'set', 'of', 'N', 'movie', 'clips', 'of', 'the', 'same', 'movie', ',', 'and', 'each', 'clip', 'comes', 'with', 'subtitles', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O']","['IN', 'DT', 'NNP', 'NN', ',', 'DT', 'NNP', 'VBZ', 'VBN', 'DT', 'NN', 'IN', 'NNP', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', ',', 'CC', 'DT', 'NN', 'VBZ', 'IN', 'NNS', '.']",27
question_answering,2,258,We implement FVTA network for Movie QA task with modality number of 2 ( video & text ) .,"['We', 'implement', 'FVTA', 'network', 'for', 'Movie', 'QA', 'task', 'with', 'modality', 'number', 'of', '2', '(', 'video', '&', 'text', ')', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', 'NN', 'IN', 'NNP', 'NNP', 'NN', 'IN', 'NN', 'NN', 'IN', 'CD', '(', 'NN', 'CC', 'NN', ')', '.']",19
question_answering,2,259,"We set the maximum number of movie clips per question to N = 20 , the maximum number of frames to consider to F = 10 , the maximum number of subtitle sentences in a clip to K = 100 and the maximum words to V = 10 .","['We', 'set', 'the', 'maximum', 'number', 'of', 'movie', 'clips', 'per', 'question', 'to', 'N', '=', '20', ',', 'the', 'maximum', 'number', 'of', 'frames', 'to', 'consider', 'to', 'F', '=', '10', ',', 'the', 'maximum', 'number', 'of', 'subtitle', 'sentences', 'in', 'a', 'clip', 'to', 'K', '=', '100', 'and', 'the', 'maximum', 'words', 'to', 'V', '=', '10', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', 'IN', 'NN', 'TO', 'NNP', 'NNP', 'CD', ',', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'TO', 'VB', 'TO', 'NNP', 'NNP', 'CD', ',', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'TO', 'NNP', 'NNP', 'CD', 'CC', 'DT', 'JJ', 'NNS', 'TO', 'NNP', 'NNP', 'CD', '.']",49
question_answering,2,261,We use the AdaDelta optimizer with a minibatch of 16 and an initial learning rate of 0.5 to trained for 300 epochs .,"['We', 'use', 'the', 'AdaDelta', 'optimizer', 'with', 'a', 'minibatch', 'of', '16', 'and', 'an', 'initial', 'learning', 'rate', 'of', '0.5', 'to', 'trained', 'for', '300', 'epochs', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', 'IN', 'DT', 'NN', 'IN', 'CD', 'CC', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'TO', 'VBN', 'IN', 'CD', 'NNS', '.']",23
question_answering,2,264,FVTA model outperforms all baseline methods and achieves comparable performance to the state - of - the - art result 2 on the MovieQA test server .,"['FVTA', 'model', 'outperforms', 'all', 'baseline', 'methods', 'and', 'achieves', 'comparable', 'performance', 'to', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'result', '2', 'on', 'the', 'MovieQA', 'test', 'server', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NN', 'VBZ', 'DT', 'NN', 'NNS', 'CC', 'NNS', 'JJ', 'NN', 'TO', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'CD', 'IN', 'DT', 'NNP', 'NN', 'NN', '.']",27
question_answering,2,266,Our accuracy is 0.410 ( vs 0.387 by RWMN ) on the validation set and 0.373 ( vs 0.363 ) on the test set .,"['Our', 'accuracy', 'is', '0.410', '(', 'vs', '0.387', 'by', 'RWMN', ')', 'on', 'the', 'validation', 'set', 'and', '0.373', '(', 'vs', '0.363', ')', 'on', 'the', 'test', 'set', '.']","['O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'CD', '(', 'NN', 'CD', 'IN', 'NNP', ')', 'IN', 'DT', 'NN', 'NN', 'CC', 'CD', '(', 'FW', 'CD', ')', 'IN', 'DT', 'NN', 'NN', '.']",25
question_answering,2,267,"Benefiting from such modeling ability , FVTA consistently outperforms the classical attention models including soft attention , MCB and TGIF .","['Benefiting', 'from', 'such', 'modeling', 'ability', ',', 'FVTA', 'consistently', 'outperforms', 'the', 'classical', 'attention', 'models', 'including', 'soft', 'attention', ',', 'MCB', 'and', 'TGIF', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O']","['VBG', 'IN', 'JJ', 'VBG', 'NN', ',', 'NNP', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'NNS', 'VBG', 'JJ', 'NN', ',', 'NNP', 'CC', 'NNP', '.']",21
question_answering,3,2,Multi - Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension,"['Multi', '-', 'Granular', 'Sequence', 'Encoding', 'via', 'Dilated', 'Compositional', 'Units', 'for', 'Reading', 'Comprehension']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', ':', 'JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",12
question_answering,3,5,This paper presents a new compositional encoder for reading comprehension ( RC ) .,"['This', 'paper', 'presents', 'a', 'new', 'compositional', 'encoder', 'for', 'reading', 'comprehension', '(', 'RC', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'VBG', 'NN', '(', 'NNP', ')', '.']",14
question_answering,3,9,"We conduct experiments on three RC datasets , showing that our proposed encoder demonstrates very promising results both as a standalone encoder as well as a complementary building block .","['We', 'conduct', 'experiments', 'on', 'three', 'RC', 'datasets', ',', 'showing', 'that', 'our', 'proposed', 'encoder', 'demonstrates', 'very', 'promising', 'results', 'both', 'as', 'a', 'standalone', 'encoder', 'as', 'well', 'as', 'a', 'complementary', 'building', 'block', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'NNS', 'IN', 'CD', 'NNP', 'NNS', ',', 'VBG', 'IN', 'PRP$', 'VBN', 'NN', 'VBZ', 'RB', 'JJ', 'NNS', 'DT', 'IN', 'DT', 'NN', 'NN', 'RB', 'RB', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",30
question_answering,3,22,"To this end , we propose a new compositional encoder that can either be used in place of standard RNN encoders or serve as a new module that is complementary to existing neural architectures .","['To', 'this', 'end', ',', 'we', 'propose', 'a', 'new', 'compositional', 'encoder', 'that', 'can', 'either', 'be', 'used', 'in', 'place', 'of', 'standard', 'RNN', 'encoders', 'or', 'serve', 'as', 'a', 'new', 'module', 'that', 'is', 'complementary', 'to', 'existing', 'neural', 'architectures', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'WDT', 'MD', 'RB', 'VB', 'VBN', 'IN', 'NN', 'IN', 'JJ', 'NNP', 'NNS', 'CC', 'VBP', 'IN', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'JJ', 'TO', 'VBG', 'JJ', 'NNS', '.']",35
question_answering,3,23,Our proposed encoder leverages dilated compositions to model relationships across multiple granularities .,"['Our', 'proposed', 'encoder', 'leverages', 'dilated', 'compositions', 'to', 'model', 'relationships', 'across', 'multiple', 'granularities', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP$', 'VBN', 'NN', 'NNS', 'VBD', 'NNS', 'TO', 'VB', 'NNS', 'IN', 'JJ', 'NNS', '.']",13
question_answering,3,24,"That is , for a given word in the target sequence , our encoder exploits both long - term ( far ) and short - term ( near ) information to decide how much information to retain for it .","['That', 'is', ',', 'for', 'a', 'given', 'word', 'in', 'the', 'target', 'sequence', ',', 'our', 'encoder', 'exploits', 'both', 'long', '-', 'term', '(', 'far', ')', 'and', 'short', '-', 'term', '(', 'near', ')', 'information', 'to', 'decide', 'how', 'much', 'information', 'to', 'retain', 'for', 'it', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O']","['DT', 'VBZ', ',', 'IN', 'DT', 'VBN', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'JJ', ':', 'NN', '(', 'RB', ')', 'CC', 'JJ', ':', 'NN', '(', 'IN', ')', 'NN', 'TO', 'VB', 'WRB', 'JJ', 'NN', 'TO', 'VB', 'IN', 'PRP', '.']",40
question_answering,3,26,"The output of the dilated composition mechanism acts as gating functions , which are then used to learn compositional representations of the input sequence .","['The', 'output', 'of', 'the', 'dilated', 'composition', 'mechanism', 'acts', 'as', 'gating', 'functions', ',', 'which', 'are', 'then', 'used', 'to', 'learn', 'compositional', 'representations', 'of', 'the', 'input', 'sequence', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'IN', 'VBG', 'NNS', ',', 'WDT', 'VBP', 'RB', 'VBN', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",25
question_answering,3,178,RACE,['RACE'],['B-n'],['NN'],1
question_answering,3,179,"The key competitors are the Stanford Attention Reader ( Stanford AR ) , Gated Attention Reader ( GA ) , and Dynamic Fusion Networks ( DFN ) .","['The', 'key', 'competitors', 'are', 'the', 'Stanford', 'Attention', 'Reader', '(', 'Stanford', 'AR', ')', ',', 'Gated', 'Attention', 'Reader', '(', 'GA', ')', ',', 'and', 'Dynamic', 'Fusion', 'Networks', '(', 'DFN', ')', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NNS', 'VBP', 'DT', 'NNP', 'NNP', 'NNP', '(', 'NNP', 'NNP', ')', ',', 'VBN', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'CC', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', '.']",28
question_answering,3,185,SearchQA,['SearchQA'],['B-n'],['NN'],1
question_answering,3,186,The main competitor baseline is the AMANDA model proposed by .,"['The', 'main', 'competitor', 'baseline', 'is', 'the', 'AMANDA', 'model', 'proposed', 'by', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'DT', 'NNP', 'NN', 'VBN', 'IN', '.']",11
question_answering,3,190,NarrativeQA,['NarrativeQA'],['B-n'],['NN'],1
question_answering,3,192,"We compete on the summaries setting , in which the baselines are a context - less sequence to sequence ( seq2seq ) model , ASR and BiDAF .","['We', 'compete', 'on', 'the', 'summaries', 'setting', ',', 'in', 'which', 'the', 'baselines', 'are', 'a', 'context', '-', 'less', 'sequence', 'to', 'sequence', '(', 'seq2seq', ')', 'model', ',', 'ASR', 'and', 'BiDAF', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NNS', 'VBG', ',', 'IN', 'WDT', 'DT', 'NNS', 'VBP', 'DT', 'JJ', ':', 'JJR', 'NN', 'TO', 'VB', '(', 'NN', ')', 'NN', ',', 'NNP', 'CC', 'NNP', '.']",28
question_answering,3,211,We implement all models in TensorFlow .,"['We', 'implement', 'all', 'models', 'in', 'TensorFlow', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNS', 'IN', 'NNP', '.']",7
question_answering,3,212,Word embeddings are initialized with 300d Glo Ve vectors and are not fine - tuned during training .,"['Word', 'embeddings', 'are', 'initialized', 'with', '300d', 'Glo', 'Ve', 'vectors', 'and', 'are', 'not', 'fine', '-', 'tuned', 'during', 'training', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O']","['NNP', 'NNS', 'VBP', 'VBN', 'IN', 'CD', 'NNP', 'NNP', 'NNS', 'CC', 'VBP', 'RB', 'JJ', ':', 'VBN', 'IN', 'NN', '.']",18
question_answering,3,213,"Dropout rate is tuned amongst { 0.1 , 0.2 , 0.3 } on all layers including the embedding layer .","['Dropout', 'rate', 'is', 'tuned', 'amongst', '{', '0.1', ',', '0.2', ',', '0.3', '}', 'on', 'all', 'layers', 'including', 'the', 'embedding', 'layer', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NN', 'NN', 'VBZ', 'VBN', 'RB', '(', 'CD', ',', 'CD', ',', 'CD', ')', 'IN', 'DT', 'NNS', 'VBG', 'DT', 'NN', 'NN', '.']",20
question_answering,3,216,"We adopt the Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 0.0003/ 0.001/0.001 for RACE / SearchQA / Narrative QA respectively .","['We', 'adopt', 'the', 'Adam', 'optimizer', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', 'with', 'a', 'learning', 'rate', 'of', '0.0003/', '0.001/0.001', 'for', 'RACE', '/', 'SearchQA', '/', 'Narrative', 'QA', 'respectively', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', 'CD', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'RB', '.']",28
question_answering,3,217,The batch size is set to 64/256/32 accordingly .,"['The', 'batch', 'size', 'is', 'set', 'to', '64/256/32', 'accordingly', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'RB', '.']",9
question_answering,3,218,The maximum sequence lengths are 500/200/1100 respectively .,"['The', 'maximum', 'sequence', 'lengths', 'are', '500/200/1100', 'respectively', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O']","['DT', 'JJ', 'NN', 'NNS', 'VBP', 'CD', 'RB', '.']",8
question_answering,3,219,"For Narrative QA , we use the Rouge - L score to find the best approximate answer relative to the human written answer for training the span model .","['For', 'Narrative', 'QA', ',', 'we', 'use', 'the', 'Rouge', '-', 'L', 'score', 'to', 'find', 'the', 'best', 'approximate', 'answer', 'relative', 'to', 'the', 'human', 'written', 'answer', 'for', 'training', 'the', 'span', 'model', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NNP', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NNP', ':', 'NNP', 'NN', 'TO', 'VB', 'DT', 'JJS', 'NN', 'NN', 'NN', 'TO', 'DT', 'JJ', 'VBN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'NN', '.']",29
question_answering,3,220,All models are trained and all runtime benchmarks are based on a TitanXP GPU .,"['All', 'models', 'are', 'trained', 'and', 'all', 'runtime', 'benchmarks', 'are', 'based', 'on', 'a', 'TitanXP', 'GPU', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'CC', 'DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'NNP', 'NNP', '.']",15
question_answering,3,221,reports our results on the RACE benchmark dataset .,"['reports', 'our', 'results', 'on', 'the', 'RACE', 'benchmark', 'dataset', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNS', 'PRP$', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'NN', '.']",9
question_answering,3,222,Our proposed DCU model achieves the best result for both single models and ensemble models .,"['Our', 'proposed', 'DCU', 'model', 'achieves', 'the', 'best', 'result', 'for', 'both', 'single', 'models', 'and', 'ensemble', 'models', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'VBN', 'NNP', 'NN', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'CC', 'JJ', 'NNS', '.']",16
question_answering,3,223,We outperform highly complex models such as DFN .,"['We', 'outperform', 'highly', 'complex', 'models', 'such', 'as', 'DFN', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'RB', 'JJ', 'NNS', 'JJ', 'IN', 'NNP', '.']",9
question_answering,3,224,We also pull ahead of other recent baselines such as ElimiNet and GA by at least 5 % .,"['We', 'also', 'pull', 'ahead', 'of', 'other', 'recent', 'baselines', 'such', 'as', 'ElimiNet', 'and', 'GA', 'by', 'at', 'least', '5', '%', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'RB', 'IN', 'JJ', 'JJ', 'NNS', 'JJ', 'IN', 'NNP', 'CC', 'NNP', 'IN', 'IN', 'JJS', 'CD', 'NN', '.']",19
question_answering,3,225,The best single model score from RACE - H and RACE - M alternates between Sim - DCU and DCU .,"['The', 'best', 'single', 'model', 'score', 'from', 'RACE', '-', 'H', 'and', 'RACE', '-', 'M', 'alternates', 'between', 'Sim', '-', 'DCU', 'and', 'DCU', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O']","['DT', 'JJS', 'JJ', 'NN', 'NN', 'IN', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NNP', 'VBZ', 'IN', 'NNP', ':', 'NN', 'CC', 'NNP', '.']",21
question_answering,3,243,Table 2 reports our results on the Search QA dataset .,"['Table', '2', 'reports', 'our', 'results', 'on', 'the', 'Search', 'QA', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['JJ', 'CD', 'NNS', 'PRP$', 'NNS', 'IN', 'DT', 'NNP', 'NNP', 'NN', '.']",11
question_answering,3,245,We achieve the same accuracy as AMANDA without using any LSTM or GRU encoder .,"['We', 'achieve', 'the', 'same', 'accuracy', 'as', 'AMANDA', 'without', 'using', 'any', 'LSTM', 'or', 'GRU', 'encoder', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'IN', 'VBG', 'DT', 'NNP', 'CC', 'NNP', 'NN', '.']",15
question_answering,3,248,"Finally , the hybrid combination , DCU - LSTM significantly outperforms AMANDA by 3 % .","['Finally', ',', 'the', 'hybrid', 'combination', ',', 'DCU', '-', 'LSTM', 'significantly', 'outperforms', 'AMANDA', 'by', '3', '%', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'DT', 'JJ', 'NN', ',', 'NNP', ':', 'NNP', 'RB', 'VBZ', 'NNP', 'IN', 'CD', 'NN', '.']",16
question_answering,3,249,"Contrary to MCQ - based datasets , we found that reports our results on the NarrativeQA benchmark .","['Contrary', 'to', 'MCQ', '-', 'based', 'datasets', ',', 'we', 'found', 'that', 'reports', 'our', 'results', 'on', 'the', 'NarrativeQA', 'benchmark', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['JJ', 'TO', 'NNP', ':', 'VBN', 'NNS', ',', 'PRP', 'VBD', 'IN', 'NNS', 'PRP$', 'NNS', 'IN', 'DT', 'NNP', 'NN', '.']",18
question_answering,3,250,"First , we observe that 300d DCU can achieve comparable performance with BiDAF .","['First', ',', 'we', 'observe', 'that', '300d', 'DCU', 'can', 'achieve', 'comparable', 'performance', 'with', 'BiDAF', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'CD', 'NNP', 'MD', 'VB', 'JJ', 'NN', 'IN', 'NNP', '.']",14
question_answering,3,256,"Finally , DCU - LSTM significantly outperforms all models in terms of ROUGE - L , including BiDAF on this dataset .","['Finally', ',', 'DCU', '-', 'LSTM', 'significantly', 'outperforms', 'all', 'models', 'in', 'terms', 'of', 'ROUGE', '-', 'L', ',', 'including', 'BiDAF', 'on', 'this', 'dataset', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['RB', ',', 'NNP', ':', 'NNP', 'RB', 'VBZ', 'DT', 'NNS', 'IN', 'NNS', 'IN', 'NNP', ':', 'NNP', ',', 'VBG', 'NNP', 'IN', 'DT', 'NN', '.']",22
question_answering,3,257,"Performance improvement over the vanilla BiLSTM model ranges from 1 % ? 3 % across all metrics , suggesting that DCU encoders are also effective as a complementary neural building block .","['Performance', 'improvement', 'over', 'the', 'vanilla', 'BiLSTM', 'model', 'ranges', 'from', '1', '%', '?', '3', '%', 'across', 'all', 'metrics', ',', 'suggesting', 'that', 'DCU', 'encoders', 'are', 'also', 'effective', 'as', 'a', 'complementary', 'neural', 'building', 'block', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'IN', 'DT', 'NN', 'NNP', 'NN', 'VBZ', 'IN', 'CD', 'NN', '.', 'CD', 'NN', 'IN', 'DT', 'NNS', ',', 'VBG', 'IN', 'NNP', 'NNS', 'VBP', 'RB', 'JJ', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'NN', '.']",32
question_answering,0,2,Modeling Semantics with Gated Graph Neural Networks for Knowledge Base Question Answering,"['Modeling', 'Semantics', 'with', 'Gated', 'Graph', 'Neural', 'Networks', 'for', 'Knowledge', 'Base', 'Question', 'Answering']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']","['VBG', 'NNS', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'NNP']",12
question_answering,0,11,Knowledge base question answering ( QA ) is an important natural language processing problem .,"['Knowledge', 'base', 'question', 'answering', '(', 'QA', ')', 'is', 'an', 'important', 'natural', 'language', 'processing', 'problem', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'NN', 'VBG', '(', 'NNP', ')', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'NN', '.']",15
question_answering,0,15,QA requires precise modeling of the question semantics through the entities and relations available in the KB in order to retrieve the correct answer .,"['QA', 'requires', 'precise', 'modeling', 'of', 'the', 'question', 'semantics', 'through', 'the', 'entities', 'and', 'relations', 'available', 'in', 'the', 'KB', 'in', 'order', 'to', 'retrieve', 'the', 'correct', 'answer', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NNS', 'CC', 'NNS', 'JJ', 'IN', 'DT', 'NNP', 'IN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', '.']",25
question_answering,0,19,"In this paper , we describe a semantic parsing approach to the problem of KB QA .","['In', 'this', 'paper', ',', 'we', 'describe', 'a', 'semantic', 'parsing', 'approach', 'to', 'the', 'problem', 'of', 'KB', 'QA', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'TO', 'DT', 'NN', 'IN', 'NNP', 'NNP', '.']",17
question_answering,0,20,"That is , for each input question , we construct an explicit structural semantic parse ( semantic graph ) , as in .","['That', 'is', ',', 'for', 'each', 'input', 'question', ',', 'we', 'construct', 'an', 'explicit', 'structural', 'semantic', 'parse', '(', 'semantic', 'graph', ')', ',', 'as', 'in', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'VBZ', ',', 'IN', 'DT', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'JJ', 'NN', '(', 'JJ', 'NN', ')', ',', 'IN', 'IN', '.']",23
question_answering,0,21,Semantic parses can be deterministically converted to a query to extract the answers from the KB .,"['Semantic', 'parses', 'can', 'be', 'deterministically', 'converted', 'to', 'a', 'query', 'to', 'extract', 'the', 'answers', 'from', 'the', 'KB', '.']","['B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']","['JJ', 'NNS', 'MD', 'VB', 'RB', 'VBN', 'TO', 'DT', 'NN', 'TO', 'VB', 'DT', 'NNS', 'IN', 'DT', 'NNP', '.']",17
question_answering,0,34,"In particular , we adapt Gated Graph Neural Networks ( GGNNs ) , described in , to process and score semantic parses .","['In', 'particular', ',', 'we', 'adapt', 'Gated', 'Graph', 'Neural', 'Networks', '(', 'GGNNs', ')', ',', 'described', 'in', ',', 'to', 'process', 'and', 'score', 'semantic', 'parses', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'JJ', ',', 'PRP', 'VBP', 'VBN', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'VBN', 'IN', ',', 'TO', 'VB', 'CC', 'VB', 'JJ', 'NNS', '.']",23
question_answering,0,48,https://github.com/UKPLab/coling2018-graph-neural-networks-question-answering.,['https://github.com/UKPLab/coling2018-graph-neural-networks-question-answering.'],['B-n'],['NN'],1
question_answering,0,171,3 . Pooled Edges model - We use the DCNN to encode the question and the label of each edge in the semantic graph .,"['3', '.', 'Pooled', 'Edges', 'model', '-', 'We', 'use', 'the', 'DCNN', 'to', 'encode', 'the', 'question', 'and', 'the', 'label', 'of', 'each', 'edge', 'in', 'the', 'semantic', 'graph', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['LS', '.', 'VBN', 'NNS', 'SYM', ':', 'PRP', 'VBP', 'DT', 'NNP', 'TO', 'VB', 'DT', 'NN', 'CC', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",25
question_answering,0,174,Graph Neural Network ( GNN ) -,"['Graph', 'Neural', 'Network', '(', 'GNN', ')', '-']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NNP', '(', 'NNP', ')', ':']",7
question_answering,0,175,"To judge the effect of the gated graph neural architecture , we also include a model variant that does not use the gating mechanism and directly computes the hidden state as a combination of the activations ( Eq 1 ) and the previous state .","['To', 'judge', 'the', 'effect', 'of', 'the', 'gated', 'graph', 'neural', 'architecture', ',', 'we', 'also', 'include', 'a', 'model', 'variant', 'that', 'does', 'not', 'use', 'the', 'gating', 'mechanism', 'and', 'directly', 'computes', 'the', 'hidden', 'state', 'as', 'a', 'combination', 'of', 'the', 'activations', '(', 'Eq', '1', ')', 'and', 'the', 'previous', 'state', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'JJ', 'NN', ',', 'PRP', 'RB', 'VBP', 'DT', 'NN', 'NN', 'WDT', 'VBZ', 'RB', 'VB', 'DT', 'NN', 'NN', 'CC', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNS', '(', 'NNP', 'CD', ')', 'CC', 'DT', 'JJ', 'NN', '.']",45
question_answering,0,176,"Gated Graph Neural Network ( GGNN ) - We use the GGNN to process semantic parses , as described in Section 3.2 .","['Gated', 'Graph', 'Neural', 'Network', '(', 'GGNN', ')', '-', 'We', 'use', 'the', 'GGNN', 'to', 'process', 'semantic', 'parses', ',', 'as', 'described', 'in', 'Section', '3.2', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', ':', 'PRP', 'VBP', 'DT', 'NNP', 'TO', 'VB', 'JJ', 'NNS', ',', 'IN', 'VBN', 'IN', 'NN', 'CD', '.']",23
question_answering,0,219,We compare the results on the WebQSP - WD data set in .,"['We', 'compare', 'the', 'results', 'on', 'the', 'WebQSP', '-', 'WD', 'data', 'set', 'in', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBP', 'DT', 'NNS', 'IN', 'DT', 'NNP', ':', 'NNP', 'NNS', 'NN', 'IN', '.']",13
question_answering,0,220,"As can be seen , the graph models outperform all other models across precision , recall and F-score , with GGNN showing the best over all result .","['As', 'can', 'be', 'seen', ',', 'the', 'graph', 'models', 'outperform', 'all', 'other', 'models', 'across', 'precision', ',', 'recall', 'and', 'F-score', ',', 'with', 'GGNN', 'showing', 'the', 'best', 'over', 'all', 'result', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'MD', 'VB', 'VBN', ',', 'DT', 'NN', 'NNS', 'VBP', 'DT', 'JJ', 'NNS', 'IN', 'NN', ',', 'NN', 'CC', 'NNP', ',', 'IN', 'NNP', 'VBG', 'DT', 'JJS', 'IN', 'DT', 'NN', '.']",28
question_answering,0,223,"The STAGG architecture delivers the worst results in our experiments , the main reason being supposedly that the model had to rely on manually defined features that are less flexible .","['The', 'STAGG', 'architecture', 'delivers', 'the', 'worst', 'results', 'in', 'our', 'experiments', ',', 'the', 'main', 'reason', 'being', 'supposedly', 'that', 'the', 'model', 'had', 'to', 'rely', 'on', 'manually', 'defined', 'features', 'that', 'are', 'less', 'flexible', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NN', 'VBZ', 'DT', 'JJS', 'NNS', 'IN', 'PRP$', 'NNS', ',', 'DT', 'JJ', 'NN', 'VBG', 'RB', 'IN', 'DT', 'NN', 'VBD', 'TO', 'VB', 'IN', 'RB', 'VBN', 'NNS', 'WDT', 'VBP', 'RBR', 'JJ', '.']",31
question_answering,0,224,The Single Edge model outperforms the more complex Pooled Edges model by a noticeable margin .,"['The', 'Single', 'Edge', 'model', 'outperforms', 'the', 'more', 'complex', 'Pooled', 'Edges', 'model', 'by', 'a', 'noticeable', 'margin', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNP', 'NNP', 'NN', 'VBZ', 'DT', 'RBR', 'JJ', 'NNP', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",16
question_answering,0,225,The Single Edge baseline prefers simple graphs that consist of a single edge which is a good strategy to achieve higher recall values .,"['The', 'Single', 'Edge', 'baseline', 'prefers', 'simple', 'graphs', 'that', 'consist', 'of', 'a', 'single', 'edge', 'which', 'is', 'a', 'good', 'strategy', 'to', 'achieve', 'higher', 'recall', 'values', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'NNP', 'NN', 'NNS', 'JJ', 'VBP', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'TO', 'VB', 'JJR', 'NN', 'NNS', '.']",24
question_answering,0,228,"In , we see that for the STAGG and Single Edge baselines the performance on more complex questions drops compared to the results on simpler questions .","['In', ',', 'we', 'see', 'that', 'for', 'the', 'STAGG', 'and', 'Single', 'Edge', 'baselines', 'the', 'performance', 'on', 'more', 'complex', 'questions', 'drops', 'compared', 'to', 'the', 'results', 'on', 'simpler', 'questions', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', ',', 'PRP', 'VBP', 'IN', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'RBR', 'JJ', 'NNS', 'NNS', 'VBN', 'TO', 'DT', 'NNS', 'IN', 'JJ', 'NNS', '.']",27
question_answering,0,229,"The Pooled Edges model maintains a better performance across questions of different complexity , which shows the benefits of encoding all graph edges .","['The', 'Pooled', 'Edges', 'model', 'maintains', 'a', 'better', 'performance', 'across', 'questions', 'of', 'different', 'complexity', ',', 'which', 'shows', 'the', 'benefits', 'of', 'encoding', 'all', 'graph', 'edges', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'NNP', 'NN', 'VBZ', 'DT', 'JJR', 'NN', 'IN', 'NNS', 'IN', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NNS', 'IN', 'VBG', 'DT', 'NN', 'NNS', '.']",24
question_answering,0,233,"We see that the GGNN model offers the best results both on simple and complex questions , as it effectively encodes the structure of semantic graphs .","['We', 'see', 'that', 'the', 'GGNN', 'model', 'offers', 'the', 'best', 'results', 'both', 'on', 'simple', 'and', 'complex', 'questions', ',', 'as', 'it', 'effectively', 'encodes', 'the', 'structure', 'of', 'semantic', 'graphs', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NNP', 'NN', 'VBZ', 'DT', 'JJS', 'NNS', 'DT', 'IN', 'NN', 'CC', 'JJ', 'NNS', ',', 'IN', 'PRP', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NN', '.']",27
text_summarization,13,2,Coarse-to-Fine Attention Models for Document Summarization,"['Coarse-to-Fine', 'Attention', 'Models', 'for', 'Document', 'Summarization']","['O', 'O', 'O', 'O', 'B-n', 'I-n']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",6
text_summarization,13,16,"Therefore , in order to scale attention models for this problem , we aim to prune down the length of the source sequence in an intelligent way .","['Therefore', ',', 'in', 'order', 'to', 'scale', 'attention', 'models', 'for', 'this', 'problem', ',', 'we', 'aim', 'to', 'prune', 'down', 'the', 'length', 'of', 'the', 'source', 'sequence', 'in', 'an', 'intelligent', 'way', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'IN', 'NN', 'TO', 'VB', 'NN', 'NNS', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'TO', 'VB', 'RP', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",28
text_summarization,13,17,"Instead of naively attending to all the words of the source at once , our solution is to use a two - layer hierarchical attention .","['Instead', 'of', 'naively', 'attending', 'to', 'all', 'the', 'words', 'of', 'the', 'source', 'at', 'once', ',', 'our', 'solution', 'is', 'to', 'use', 'a', 'two', '-', 'layer', 'hierarchical', 'attention', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', 'IN', 'RB', 'VBG', 'TO', 'PDT', 'DT', 'NNS', 'IN', 'DT', 'NN', 'IN', 'RB', ',', 'PRP$', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'CD', ':', 'NN', 'JJ', 'NN', '.']",26
text_summarization,13,18,"For document summarization , this means dividing the document into chunks of text , sparsely attending to one or a few chunks at a time using hard attention , then applying the usual full attention over those chunks - we call this method coarse - to - fine attention .","['For', 'document', 'summarization', ',', 'this', 'means', 'dividing', 'the', 'document', 'into', 'chunks', 'of', 'text', ',', 'sparsely', 'attending', 'to', 'one', 'or', 'a', 'few', 'chunks', 'at', 'a', 'time', 'using', 'hard', 'attention', ',', 'then', 'applying', 'the', 'usual', 'full', 'attention', 'over', 'those', 'chunks', '-', 'we', 'call', 'this', 'method', 'coarse', '-', 'to', '-', 'fine', 'attention', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', 'NN', ',', 'DT', 'VBZ', 'VBG', 'DT', 'NN', 'IN', 'NNS', 'IN', 'NN', ',', 'RB', 'VBG', 'TO', 'CD', 'CC', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'VBG', 'JJ', 'NN', ',', 'RB', 'VBG', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'DT', 'NNS', ':', 'PRP', 'VBP', 'DT', 'NN', 'SYM', ':', 'TO', ':', 'JJ', 'NN', '.']",50
text_summarization,13,185,"We train with minibatch stochastic gradient descent ( SGD ) with batch size 20 for 20 epochs , renormalizing gradients below norm 5 .","['We', 'train', 'with', 'minibatch', 'stochastic', 'gradient', 'descent', '(', 'SGD', ')', 'with', 'batch', 'size', '20', 'for', '20', 'epochs', ',', 'renormalizing', 'gradients', 'below', 'norm', '5', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'NN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'IN', 'NN', 'NN', 'CD', 'IN', 'CD', 'NNS', ',', 'VBG', 'NNS', 'IN', 'DT', 'CD', '.']",24
text_summarization,13,186,"We initialize the learning rate to 0.1 for the top - level encoder and 1 for the rest of the model , and begin decaying it by a factor of 0.5 each epoch after the validation perplexity stops decreasing .","['We', 'initialize', 'the', 'learning', 'rate', 'to', '0.1', 'for', 'the', 'top', '-', 'level', 'encoder', 'and', '1', 'for', 'the', 'rest', 'of', 'the', 'model', ',', 'and', 'begin', 'decaying', 'it', 'by', 'a', 'factor', 'of', '0.5', 'each', 'epoch', 'after', 'the', 'validation', 'perplexity', 'stops', 'decreasing', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'CD', 'IN', 'DT', 'JJ', ':', 'NN', 'NN', 'CC', 'CD', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'CC', 'VB', 'VBG', 'PRP', 'IN', 'DT', 'NN', 'IN', 'CD', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'VBG', '.']",40
text_summarization,13,187,"We use 2 layer LSTMs with 500 hidden units , and we initialize word embeddings with 300 dimensional word2vec embeddings .","['We', 'use', '2', 'layer', 'LSTMs', 'with', '500', 'hidden', 'units', ',', 'and', 'we', 'initialize', 'word', 'embeddings', 'with', '300', 'dimensional', 'word2vec', 'embeddings', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'NN', 'NNP', 'IN', 'CD', 'JJ', 'NNS', ',', 'CC', 'PRP', 'VBP', 'NN', 'NNS', 'IN', 'CD', 'JJ', 'NN', 'NNS', '.']",21
text_summarization,13,188,"We initialize all other parameters as uniform in the interval [ ? 0.1 , 0.1 ] .","['We', 'initialize', 'all', 'other', 'parameters', 'as', 'uniform', 'in', 'the', 'interval', '[', '?', '0.1', ',', '0.1', ']', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NNS', 'IN', 'NN', 'IN', 'DT', 'NN', 'NNP', '.', 'CD', ',', 'CD', 'NN', '.']",17
text_summarization,13,189,"For convolutional layers , we use a kernel width of 6 and 600 filters .","['For', 'convolutional', 'layers', ',', 'we', 'use', 'a', 'kernel', 'width', 'of', '6', 'and', '600', 'filters', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CD', 'CC', 'CD', 'NNS', '.']",15
text_summarization,13,190,Positional embeddings have dimension 25 .,"['Positional', 'embeddings', 'have', 'dimension', '25', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['JJ', 'NNS', 'VBP', 'NN', 'CD', '.']",6
text_summarization,13,191,We use dropout between stacked LSTM hidden states and before the final word generator layer to regularize ( with dropout probability 0.3 ) .,"['We', 'use', 'dropout', 'between', 'stacked', 'LSTM', 'hidden', 'states', 'and', 'before', 'the', 'final', 'word', 'generator', 'layer', 'to', 'regularize', '(', 'with', 'dropout', 'probability', '0.3', ')', '.']","['O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBP', 'IN', 'IN', 'VBN', 'NNP', 'JJ', 'NNS', 'CC', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', 'TO', 'VB', '(', 'IN', 'NN', 'NN', 'CD', ')', '.']",24
text_summarization,13,192,"At test time , we run beam search to produce the summary with a beam size of 5 .","['At', 'test', 'time', ',', 'we', 'run', 'beam', 'search', 'to', 'produce', 'the', 'summary', 'with', 'a', 'beam', 'size', 'of', '5', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'NN', ',', 'PRP', 'VBP', 'RB', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",19
text_summarization,13,193,Our models are implemented using Torch based on a past version of the Open NMT system,"['Our', 'models', 'are', 'implemented', 'using', 'Torch', 'based', 'on', 'a', 'past', 'version', 'of', 'the', 'Open', 'NMT', 'system']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['PRP$', 'NNS', 'VBP', 'VBN', 'VBG', 'NNP', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'NN']",16
text_summarization,13,194,4 . We ran our experiments on a 12GB Geforce GTX Titan X GPU .,"['4', '.', 'We', 'ran', 'our', 'experiments', 'on', 'a', '12GB', 'Geforce', 'GTX', 'Titan', 'X', 'GPU', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', '.', 'PRP', 'VBD', 'PRP$', 'NNS', 'IN', 'DT', 'CD', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', '.']",15
text_summarization,13,204,The ILP model ROUGE scores are surprisingly low .,"['The', 'ILP', 'model', 'ROUGE', 'scores', 'are', 'surprisingly', 'low', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNP', 'NN', 'NNP', 'NNS', 'VBP', 'RB', 'JJ', '.']",9
text_summarization,13,213,C2 F results are significantly worse than soft attention results .,"['C2', 'F', 'results', 'are', 'significantly', 'worse', 'than', 'soft', 'attention', 'results', '.']","['B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NNS', 'VBP', 'RB', 'JJR', 'IN', 'JJ', 'NN', 'NNS', '.']",11
text_summarization,13,234,Sharpness of Attention,"['Sharpness', 'of', 'Attention']","['B-n', 'I-n', 'I-n']","['NN', 'IN', 'NNP']",3
text_summarization,13,237,We compute the entropy numbers by averaging over all generated words in the validation set .,"['We', 'compute', 'the', 'entropy', 'numbers', 'by', 'averaging', 'over', 'all', 'generated', 'words', 'in', 'the', 'validation', 'set', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NNS', 'IN', 'VBG', 'IN', 'DT', 'VBN', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",16
text_summarization,13,239,We note that the entropy of C2F is very low ( before taking the argmax at test time ) .,"['We', 'note', 'that', 'the', 'entropy', 'of', 'C2F', 'is', 'very', 'low', '(', 'before', 'taking', 'the', 'argmax', 'at', 'test', 'time', ')', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NN', 'IN', 'NNP', 'VBZ', 'RB', 'JJ', '(', 'IN', 'VBG', 'DT', 'NN', 'IN', 'NN', 'NN', ')', '.']",20
text_summarization,13,240,This is exactly what we had hoped for - we will see that the model in fact learns to focus on only a few top - level chunks of the document over the course of generation .,"['This', 'is', 'exactly', 'what', 'we', 'had', 'hoped', 'for', '-', 'we', 'will', 'see', 'that', 'the', 'model', 'in', 'fact', 'learns', 'to', 'focus', 'on', 'only', 'a', 'few', 'top', '-', 'level', 'chunks', 'of', 'the', 'document', 'over', 'the', 'course', 'of', 'generation', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'RB', 'WP', 'PRP', 'VBD', 'VBN', 'IN', ':', 'PRP', 'MD', 'VB', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', 'TO', 'VB', 'IN', 'RB', 'DT', 'JJ', 'JJ', ':', 'NN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', '.']",37
text_summarization,13,245,Attention Heatmaps,"['Attention', 'Heatmaps']","['B-n', 'I-n']","['NN', 'NNS']",2
text_summarization,13,250,"In HIER , we observe that the attention becomes washed out ( in accord with its high entropy ) and is essentially averaging all of the encoder hidden states .","['In', 'HIER', ',', 'we', 'observe', 'that', 'the', 'attention', 'becomes', 'washed', 'out', '(', 'in', 'accord', 'with', 'its', 'high', 'entropy', ')', 'and', 'is', 'essentially', 'averaging', 'all', 'of', 'the', 'encoder', 'hidden', 'states', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'VBZ', 'VBN', 'RP', '(', 'IN', 'NN', 'IN', 'PRP$', 'JJ', 'NN', ')', 'CC', 'VBZ', 'RB', 'VBG', 'DT', 'IN', 'DT', 'NN', 'JJ', 'NNS', '.']",30
text_summarization,13,255,"In C2 F , we see that we get very sharp attention on some rows as we had hoped .","['In', 'C2', 'F', ',', 'we', 'see', 'that', 'we', 'get', 'very', 'sharp', 'attention', 'on', 'some', 'rows', 'as', 'we', 'had', 'hoped', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', 'NNP', ',', 'PRP', 'VBP', 'IN', 'PRP', 'VBP', 'RB', 'JJ', 'NN', 'IN', 'DT', 'NNS', 'IN', 'PRP', 'VBD', 'VBN', '.']",20
text_summarization,8,2,Bottom - Up Abstractive Summarization,"['Bottom', '-', 'Up', 'Abstractive', 'Summarization']","['O', 'O', 'O', 'B-n', 'I-n']","['NNP', ':', 'RB', 'NNP', 'NNP']",5
text_summarization,8,11,Text summarization systems aim to generate natural language summaries that compress the information in a longer text .,"['Text', 'summarization', 'systems', 'aim', 'to', 'generate', 'natural', 'language', 'summaries', 'that', 'compress', 'the', 'information', 'in', 'a', 'longer', 'text', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NNS', 'VBP', 'TO', 'VB', 'JJ', 'NN', 'NNS', 'WDT', 'VBP', 'DT', 'NN', 'IN', 'DT', 'JJR', 'NN', '.']",18
text_summarization,8,13,Current state - of - the - art neural abstractive summarization models combine extractive and abstractive techniques by using pointergenerator style models which can copy words from the source document .,"['Current', 'state', '-', 'of', '-', 'the', '-', 'art', 'neural', 'abstractive', 'summarization', 'models', 'combine', 'extractive', 'and', 'abstractive', 'techniques', 'by', 'using', 'pointergenerator', 'style', 'models', 'which', 'can', 'copy', 'words', 'from', 'the', 'source', 'document', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'JJ', 'JJ', 'NN', 'NNS', 'VBP', 'JJ', 'CC', 'JJ', 'NNS', 'IN', 'VBG', 'NN', 'NN', 'NNS', 'WDT', 'MD', 'VB', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",31
text_summarization,8,29,"Motivated by this approach , we consider bottom - up attention for neural abstractive summarization .","['Motivated', 'by', 'this', 'approach', ',', 'we', 'consider', 'bottom', '-', 'up', 'attention', 'for', 'neural', 'abstractive', 'summarization', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['VBN', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'SYM', ':', 'RP', 'NN', 'IN', 'JJ', 'JJ', 'NN', '.']",16
text_summarization,8,30,Our approach first selects a selection mask for the source document and then constrains a standard neural model by this mask .,"['Our', 'approach', 'first', 'selects', 'a', 'selection', 'mask', 'for', 'the', 'source', 'document', 'and', 'then', 'constrains', 'a', 'standard', 'neural', 'model', 'by', 'this', 'mask', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O']","['PRP$', 'NN', 'RB', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'RB', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",22
text_summarization,8,33,Our full model incorporates a separate content selection system to decide on relevant aspects of the source document .,"['Our', 'full', 'model', 'incorporates', 'a', 'separate', 'content', 'selection', 'system', 'to', 'decide', 'on', 'relevant', 'aspects', 'of', 'the', 'source', 'document', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'NN', 'TO', 'VB', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",19
text_summarization,8,34,"We frame this selection task as a sequence - tagging problem , with the objective of identifying tokens from a document that are part of its summary .","['We', 'frame', 'this', 'selection', 'task', 'as', 'a', 'sequence', '-', 'tagging', 'problem', ',', 'with', 'the', 'objective', 'of', 'identifying', 'tokens', 'from', 'a', 'document', 'that', 'are', 'part', 'of', 'its', 'summary', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', ':', 'NN', 'NN', ',', 'IN', 'DT', 'NN', 'IN', 'VBG', 'NNS', 'IN', 'DT', 'NN', 'WDT', 'VBP', 'NN', 'IN', 'PRP$', 'NN', '.']",28
text_summarization,8,36,"To incorporate bottom - up attention into abstractive summarization models , we employ masking to constrain copying words to the selected parts of the text , which produces grammatical outputs .","['To', 'incorporate', 'bottom', '-', 'up', 'attention', 'into', 'abstractive', 'summarization', 'models', ',', 'we', 'employ', 'masking', 'to', 'constrain', 'copying', 'words', 'to', 'the', 'selected', 'parts', 'of', 'the', 'text', ',', 'which', 'produces', 'grammatical', 'outputs', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['TO', 'VB', 'NN', ':', 'RB', 'NN', 'IN', 'JJ', 'NN', 'NNS', ',', 'PRP', 'VBP', 'VBG', 'TO', 'VB', 'JJ', 'NNS', 'TO', 'DT', 'VBN', 'NNS', 'IN', 'DT', 'NN', ',', 'WDT', 'VBZ', 'JJ', 'NNS', '.']",31
text_summarization,8,186,All inference parameters are tuned on a 200 example subset of the validation set .,"['All', 'inference', 'parameters', 'are', 'tuned', 'on', 'a', '200', 'example', 'subset', 'of', 'the', 'validation', 'set', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'CD', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",15
text_summarization,8,187,"Length penalty parameter ? and copy mask differ across models , with ? ranging from 0.6 to 1.4 , and ranging from 0.1 to 0.2 .","['Length', 'penalty', 'parameter', '?', 'and', 'copy', 'mask', 'differ', 'across', 'models', ',', 'with', '?', 'ranging', 'from', '0.6', 'to', '1.4', ',', 'and', 'ranging', 'from', '0.1', 'to', '0.2', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NN', 'NN', '.', 'CC', 'NN', 'NN', 'NN', 'IN', 'NNS', ',', 'IN', '.', 'VBG', 'IN', 'CD', 'TO', 'CD', ',', 'CC', 'VBG', 'IN', 'CD', 'TO', 'CD', '.']",26
text_summarization,8,188,The minimum length of the generated summary is set to 35 for CNN - DM and 6 for NYT .,"['The', 'minimum', 'length', 'of', 'the', 'generated', 'summary', 'is', 'set', 'to', '35', 'for', 'CNN', '-', 'DM', 'and', '6', 'for', 'NYT', '.']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'IN', 'NNP', ':', 'NNP', 'CC', 'CD', 'IN', 'NNP', '.']",20
text_summarization,8,190,"The coverage penalty parameter ? is set to 10 , and the copy attention normalization parameter ? to 2 for both approaches .","['The', 'coverage', 'penalty', 'parameter', '?', 'is', 'set', 'to', '10', ',', 'and', 'the', 'copy', 'attention', 'normalization', 'parameter', '?', 'to', '2', 'for', 'both', 'approaches', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', 'NN', '.', 'VBZ', 'VBN', 'TO', 'CD', ',', 'CC', 'DT', 'NN', 'NN', 'NN', 'NN', '.', 'TO', 'CD', 'IN', 'DT', 'NNS', '.']",23
text_summarization,8,191,"We use AllenNLP for the content selector , and Open NMT - py for the abstractive models .","['We', 'use', 'AllenNLP', 'for', 'the', 'content', 'selector', ',', 'and', 'Open', 'NMT', '-', 'py', 'for', 'the', 'abstractive', 'models', '.']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', 'IN', 'DT', 'NN', 'NN', ',', 'CC', 'NNP', 'NNP', ':', 'NN', 'IN', 'DT', 'JJ', 'NNS', '.']",18
text_summarization,8,192,"3 . shows our main results on the CNN - DM corpus , with abstractive models shown in the top , and bottom - up attention methods at the bottom .","['3', '.', 'shows', 'our', 'main', 'results', 'on', 'the', 'CNN', '-', 'DM', 'corpus', ',', 'with', 'abstractive', 'models', 'shown', 'in', 'the', 'top', ',', 'and', 'bottom', '-', 'up', 'attention', 'methods', 'at', 'the', 'bottom', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['LS', '.', 'VBZ', 'PRP$', 'JJ', 'NNS', 'IN', 'DT', 'NNP', ':', 'NNP', 'NN', ',', 'IN', 'JJ', 'NNS', 'VBN', 'IN', 'DT', 'JJ', ',', 'CC', 'SYM', ':', 'RP', 'NN', 'NNS', 'IN', 'DT', 'NN', '.']",31
text_summarization,8,193,"We first observe that using a coverage inference penalty scores the same as a full coverage mechanism , without requiring any additional model parameters or model fine - tuning .","['We', 'first', 'observe', 'that', 'using', 'a', 'coverage', 'inference', 'penalty', 'scores', 'the', 'same', 'as', 'a', 'full', 'coverage', 'mechanism', ',', 'without', 'requiring', 'any', 'additional', 'model', 'parameters', 'or', 'model', 'fine', '-', 'tuning', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'IN', 'VBG', 'DT', 'NN', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'NNS', 'CC', 'NN', 'JJ', ':', 'NN', '.']",30
text_summarization,8,194,"The results with the CopyTransformer and coverage penalty indicate a slight improvement across all three scores , but we observe no significant difference between Pointer - Generator and CopyTransformer with bottom - up attention .","['The', 'results', 'with', 'the', 'CopyTransformer', 'and', 'coverage', 'penalty', 'indicate', 'a', 'slight', 'improvement', 'across', 'all', 'three', 'scores', ',', 'but', 'we', 'observe', 'no', 'significant', 'difference', 'between', 'Pointer', '-', 'Generator', 'and', 'CopyTransformer', 'with', 'bottom', '-', 'up', 'attention', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'DT', 'NNP', 'CC', 'NN', 'NN', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'CD', 'NNS', ',', 'CC', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NNP', ':', 'NN', 'CC', 'NNP', 'IN', 'NN', ':', 'IN', 'NN', '.']",35
text_summarization,9,2,Abstractive Sentence Summarization with Attentive Recurrent Neural Networks,"['Abstractive', 'Sentence', 'Summarization', 'with', 'Attentive', 'Recurrent', 'Neural', 'Networks']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'NNP']",8
text_summarization,9,10,Generating a condensed version of a passage while preserving its meaning is known as text summarization .,"['Generating', 'a', 'condensed', 'version', 'of', 'a', 'passage', 'while', 'preserving', 'its', 'meaning', 'is', 'known', 'as', 'text', 'summarization', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'VBG', 'PRP$', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', 'NN', '.']",17
text_summarization,9,16,"Inspired by the recently proposed architectures for machine translation , our model consists of a conditional recurrent neural network , which acts as a decoder to generate the summary of an input sentence , much like a standard recurrent language model .","['Inspired', 'by', 'the', 'recently', 'proposed', 'architectures', 'for', 'machine', 'translation', ',', 'our', 'model', 'consists', 'of', 'a', 'conditional', 'recurrent', 'neural', 'network', ',', 'which', 'acts', 'as', 'a', 'decoder', 'to', 'generate', 'the', 'summary', 'of', 'an', 'input', 'sentence', ',', 'much', 'like', 'a', 'standard', 'recurrent', 'language', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'IN', 'DT', 'RB', 'VBN', 'NNS', 'IN', 'NN', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'IN', 'DT', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'RB', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', '.']",42
text_summarization,9,17,"In addition , at every time step the decoder also takes a conditioning input which is the output of an encoder module .","['In', 'addition', ',', 'at', 'every', 'time', 'step', 'the', 'decoder', 'also', 'takes', 'a', 'conditioning', 'input', 'which', 'is', 'the', 'output', 'of', 'an', 'encoder', 'module', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'IN', 'DT', 'NN', 'VB', 'DT', 'NN', 'RB', 'VBZ', 'DT', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",23
text_summarization,9,18,"Depending on the current state of the RNN , the encoder computes scores over the words in the input sentence .","['Depending', 'on', 'the', 'current', 'state', 'of', 'the', 'RNN', ',', 'the', 'encoder', 'computes', 'scores', 'over', 'the', 'words', 'in', 'the', 'input', 'sentence', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['VBG', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', ',', 'DT', 'NN', 'VBZ', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",21
text_summarization,9,20,Both the decoder and encoder are jointly trained on a data set consisting of sentence - summary pairs .,"['Both', 'the', 'decoder', 'and', 'encoder', 'are', 'jointly', 'trained', 'on', 'a', 'data', 'set', 'consisting', 'of', 'sentence', '-', 'summary', 'pairs', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'DT', 'NN', 'CC', 'NN', 'VBP', 'RB', 'VBN', 'IN', 'DT', 'NN', 'NN', 'NN', 'IN', 'NN', ':', 'JJ', 'NNS', '.']",19
text_summarization,9,24,"Lastly , our encoder uses a convolutional network to encode input words .","['Lastly', ',', 'our', 'encoder', 'uses', 'a', 'convolutional', 'network', 'to', 'encode', 'input', 'words', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'TO', 'VB', 'NN', 'NNS', '.']",13
text_summarization,9,96,We implemented our models in the Torch library ( http://torch.ch/),"['We', 'implemented', 'our', 'models', 'in', 'the', 'Torch', 'library', '(', 'http://torch.ch/)']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O']","['PRP', 'VBD', 'PRP$', 'NNS', 'IN', 'DT', 'NNP', 'NN', '(', 'NN']",10
text_summarization,9,97,2 . To optimize our loss ( Equation 5 ) we used stochastic gradient descent with mini-batches of size 32 .,"['2', '.', 'To', 'optimize', 'our', 'loss', '(', 'Equation', '5', ')', 'we', 'used', 'stochastic', 'gradient', 'descent', 'with', 'mini-batches', 'of', 'size', '32', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O']","['CD', '.', 'TO', 'VB', 'PRP$', 'NN', '(', 'NNP', 'CD', ')', 'PRP', 'VBD', 'JJ', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'NN', 'CD', '.']",21
text_summarization,9,98,"During training we measure the perplexity of the summaries in the validation set and adjust our hyper - parameters , such as the learning rate , based on this number .","['During', 'training', 'we', 'measure', 'the', 'perplexity', 'of', 'the', 'summaries', 'in', 'the', 'validation', 'set', 'and', 'adjust', 'our', 'hyper', '-', 'parameters', ',', 'such', 'as', 'the', 'learning', 'rate', ',', 'based', 'on', 'this', 'number', '.']","['B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', 'CC', 'VB', 'PRP$', 'JJR', ':', 'NNS', ',', 'JJ', 'IN', 'DT', 'NN', 'NN', ',', 'VBN', 'IN', 'DT', 'NN', '.']",31
text_summarization,9,99,For the decoder we experimented with both the Elman RNN and the Long - Short Term Memory ( LSTM ) architecture ( as discussed in 3.1 ) .,"['For', 'the', 'decoder', 'we', 'experimented', 'with', 'both', 'the', 'Elman', 'RNN', 'and', 'the', 'Long', '-', 'Short', 'Term', 'Memory', '(', 'LSTM', ')', 'architecture', '(', 'as', 'discussed', 'in', '3.1', ')', '.']","['B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBD', 'IN', 'DT', 'DT', 'NNP', 'NNP', 'CC', 'DT', 'NNP', ':', 'JJ', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', '(', 'IN', 'VBN', 'IN', 'CD', ')', '.']",28
text_summarization,9,100,We chose hyper - parameters based on a grid search and picked the one which gave the best perplexity on the validation set .,"['We', 'chose', 'hyper', '-', 'parameters', 'based', 'on', 'a', 'grid', 'search', 'and', 'picked', 'the', 'one', 'which', 'gave', 'the', 'best', 'perplexity', 'on', 'the', 'validation', 'set', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBD', 'JJR', ':', 'NNS', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'CC', 'VBD', 'DT', 'CD', 'WDT', 'VBD', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",24
text_summarization,9,104,"Our final Elman architecture ( RAS - Elman ) uses a single layer with H = 512 , ? = 0.5 , ? = 2 , and ? = 10 .","['Our', 'final', 'Elman', 'architecture', '(', 'RAS', '-', 'Elman', ')', 'uses', 'a', 'single', 'layer', 'with', 'H', '=', '512', ',', '?', '=', '0.5', ',', '?', '=', '2', ',', 'and', '?', '=', '10', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'JJ', 'NNP', 'NN', '(', 'NNP', ':', 'NN', ')', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'CD', ',', '.', 'UH', 'CD', ',', '.', 'UH', 'CD', ',', 'CC', '.', '$', 'CD', '.']",31
text_summarization,9,105,"The LSTM model ( RAS - LSTM ) also has a single layer with H = 512 , ? = 0.1 , ? = 2 , and ? = 10 .","['The', 'LSTM', 'model', '(', 'RAS', '-', 'LSTM', ')', 'also', 'has', 'a', 'single', 'layer', 'with', 'H', '=', '512', ',', '?', '=', '0.1', ',', '?', '=', '2', ',', 'and', '?', '=', '10', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'NN', '(', 'NNP', ':', 'NN', ')', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'CD', ',', '.', 'UH', 'CD', ',', '.', 'UH', 'CD', ',', 'CC', '.', '$', 'CD', '.']",31
text_summarization,9,112,shows that both our RAS - Elman and RAS - LSTM models achieve lower perplexity than ABS as well as other models reported in .,"['shows', 'that', 'both', 'our', 'RAS', '-', 'Elman', 'and', 'RAS', '-', 'LSTM', 'models', 'achieve', 'lower', 'perplexity', 'than', 'ABS', 'as', 'well', 'as', 'other', 'models', 'reported', 'in', '.']","['B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNS', 'IN', 'DT', 'PRP$', 'NNP', ':', 'NN', 'CC', 'NNP', ':', 'NNP', 'NNS', 'VBP', 'JJR', 'NN', 'IN', 'NNP', 'RB', 'RB', 'IN', 'JJ', 'NNS', 'VBN', 'IN', '.']",25
text_summarization,9,113,"The RAS - LSTM performs slightly worse than RAS - Elman , most likely due to over-fitting .","['The', 'RAS', '-', 'LSTM', 'performs', 'slightly', 'worse', 'than', 'RAS', '-', 'Elman', ',', 'most', 'likely', 'due', 'to', 'over-fitting', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', ':', 'NN', 'NNS', 'RB', 'JJR', 'IN', 'NNP', ':', 'NNP', ',', 'JJS', 'JJ', 'JJ', 'TO', 'JJ', '.']",18
text_summarization,9,115,The ROUGE results show that our models comfortably outperform both ABS and ABS + by a wide margin on all metrics .,"['The', 'ROUGE', 'results', 'show', 'that', 'our', 'models', 'comfortably', 'outperform', 'both', 'ABS', 'and', 'ABS', '+', 'by', 'a', 'wide', 'margin', 'on', 'all', 'metrics', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NNP', 'NNS', 'VBP', 'IN', 'PRP$', 'NNS', 'RB', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNS', '.']",22
text_summarization,9,118,On DUC - 2004 we report recall ROUGE as is customary on this dataset .,"['On', 'DUC', '-', '2004', 'we', 'report', 'recall', 'ROUGE', 'as', 'is', 'customary', 'on', 'this', 'dataset', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ':', 'CD', 'PRP', 'VBP', 'JJ', 'NNP', 'IN', 'VBZ', 'JJ', 'IN', 'DT', 'NN', '.']",15
text_summarization,9,119,The results ( Table 3 ) show that our models are better than ABS + .,"['The', 'results', '(', 'Table', '3', ')', 'show', 'that', 'our', 'models', 'are', 'better', 'than', 'ABS', '+', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NNS', '(', 'JJ', 'CD', ')', 'NN', 'IN', 'PRP$', 'NNS', 'VBP', 'JJR', 'IN', 'NNP', 'NNP', '.']",16
text_summarization,1,2,Mixture Content Selection for Diverse Sequence Generation,"['Mixture', 'Content', 'Selection', 'for', 'Diverse', 'Sequence', 'Generation']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",7
text_summarization,1,4,Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one - to - many relationships between source and the target sequences .,"['Generating', 'diverse', 'sequences', 'is', 'important', 'in', 'many', 'NLP', 'applications', 'such', 'as', 'question', 'generation', 'or', 'summarization', 'that', 'exhibit', 'semantically', 'one', '-', 'to', '-', 'many', 'relationships', 'between', 'source', 'and', 'the', 'target', 'sequences', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', 'NNS', 'VBZ', 'JJ', 'IN', 'JJ', 'NNP', 'NNS', 'JJ', 'IN', 'NN', 'NN', 'CC', 'NN', 'IN', 'NN', 'RB', 'CD', ':', 'TO', ':', 'JJ', 'NNS', 'IN', 'NN', 'CC', 'DT', 'NN', 'NNS', '.']",31
text_summarization,1,12,Generating target sequences given a source sequence has applications in a wide range of problems in NLP with different types of relationships between the source and target sequences .,"['Generating', 'target', 'sequences', 'given', 'a', 'source', 'sequence', 'has', 'applications', 'in', 'a', 'wide', 'range', 'of', 'problems', 'in', 'NLP', 'with', 'different', 'types', 'of', 'relationships', 'between', 'the', 'source', 'and', 'target', 'sequences', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NN', 'NNS', 'VBN', 'DT', 'NN', 'NN', 'VBZ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'NNP', 'IN', 'JJ', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NN', 'NNS', '.']",29
text_summarization,1,30,"Encoder - decoder models are widely used for sequence generation , most notably in machine translation where neural models are now often almost as good as human translators in some language pairs .","['Encoder', '-', 'decoder', 'models', 'are', 'widely', 'used', 'for', 'sequence', 'generation', ',', 'most', 'notably', 'in', 'machine', 'translation', 'where', 'neural', 'models', 'are', 'now', 'often', 'almost', 'as', 'good', 'as', 'human', 'translators', 'in', 'some', 'language', 'pairs', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'NN', 'NN', ',', 'RBS', 'RB', 'IN', 'NN', 'NN', 'WRB', 'JJ', 'NNS', 'VBP', 'RB', 'RB', 'RB', 'RB', 'JJ', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NNS', '.']",33
text_summarization,1,34,"In this paper , we present a method for diverse generation that separates diversification and generation stages .","['In', 'this', 'paper', ',', 'we', 'present', 'a', 'method', 'for', 'diverse', 'generation', 'that', 'separates', 'diversification', 'and', 'generation', 'stages', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'NN', 'WDT', 'VBZ', 'NN', 'CC', 'NN', 'NNS', '.']",18
text_summarization,1,35,"The diversification stage leverages content selection to map the source to multiple sequences , where each mapping is modeled by focusing on different tokens in the source ( oneto - many mapping ) .","['The', 'diversification', 'stage', 'leverages', 'content', 'selection', 'to', 'map', 'the', 'source', 'to', 'multiple', 'sequences', ',', 'where', 'each', 'mapping', 'is', 'modeled', 'by', 'focusing', 'on', 'different', 'tokens', 'in', 'the', 'source', '(', 'oneto', '-', 'many', 'mapping', ')', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', 'VBZ', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'TO', 'VB', 'NNS', ',', 'WRB', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'VBG', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', '(', 'SYM', ':', 'JJ', 'NN', ')', '.']",34
text_summarization,1,36,The generation stage uses a standard encoder - decoder model to generate a target sequence given each selected content from the source ( one - to - one mapping ) .,"['The', 'generation', 'stage', 'uses', 'a', 'standard', 'encoder', '-', 'decoder', 'model', 'to', 'generate', 'a', 'target', 'sequence', 'given', 'each', 'selected', 'content', 'from', 'the', 'source', '(', 'one', '-', 'to', '-', 'one', 'mapping', ')', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', ':', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'VBN', 'DT', 'VBN', 'NN', 'IN', 'DT', 'NN', '(', 'CD', ':', 'TO', ':', 'CD', 'NN', ')', '.']",31
text_summarization,1,37,We present a generic module called SELECTOR that is specialized for diversification .,"['We', 'present', 'a', 'generic', 'module', 'called', 'SELECTOR', 'that', 'is', 'specialized', 'for', 'diversification', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBN', 'NNP', 'WDT', 'VBZ', 'VBN', 'IN', 'NN', '.']",13
text_summarization,1,38,This module can be used as a plug - and - play to an arbitrary encoder - decoder model for generation without architecture change .,"['This', 'module', 'can', 'be', 'used', 'as', 'a', 'plug', '-', 'and', '-', 'play', 'to', 'an', 'arbitrary', 'encoder', '-', 'decoder', 'model', 'for', 'generation', 'without', 'architecture', 'change', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'MD', 'VB', 'VBN', 'IN', 'DT', 'NN', ':', 'CC', ':', 'NN', 'TO', 'DT', 'JJ', 'NN', ':', 'NN', 'NN', 'IN', 'NN', 'IN', 'NN', 'NN', '.']",25
text_summarization,1,161,Beam Search,"['Beam', 'Search']","['B-n', 'I-n']","['NNP', 'NNP']",2
text_summarization,1,162,This baseline keeps K hypotheses with highest log-probability scores at each decoding step .,"['This', 'baseline', 'keeps', 'K', 'hypotheses', 'with', 'highest', 'log-probability', 'scores', 'at', 'each', 'decoding', 'step', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'NNP', 'NNS', 'IN', 'JJS', 'NN', 'NNS', 'IN', 'DT', 'VBG', 'NN', '.']",14
text_summarization,1,163,Truncated Sampling,"['Truncated', 'Sampling']","['B-n', 'I-n']","['VBN', 'VBG']",2
text_summarization,1,164,This baseline randomly samples words from top - 10 candidates of the distribution at the decoding step .,"['This', 'baseline', 'randomly', 'samples', 'words', 'from', 'top', '-', '10', 'candidates', 'of', 'the', 'distribution', 'at', 'the', 'decoding', 'step', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'NNS', 'NNS', 'IN', 'JJ', ':', 'CD', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'VBG', 'NN', '.']",18
text_summarization,1,165,Mixture Decoder,"['Mixture', 'Decoder']","['B-n', 'I-n']","['NN', 'NN']",2
text_summarization,1,166,This baseline constructs a hard - MoE of K decoders with uniform mixing coefficient ( referred as hMup in ) and conducts parallel greedy decoding .,"['This', 'baseline', 'constructs', 'a', 'hard', '-', 'MoE', 'of', 'K', 'decoders', 'with', 'uniform', 'mixing', 'coefficient', '(', 'referred', 'as', 'hMup', 'in', ')', 'and', 'conducts', 'parallel', 'greedy', 'decoding', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', ':', 'NN', 'IN', 'NNP', 'NNS', 'IN', 'JJ', 'VBG', 'NN', '(', 'VBN', 'IN', 'NN', 'IN', ')', 'CC', 'VBZ', 'JJ', 'NN', 'NN', '.']",26
text_summarization,1,168,Mixture Selector ( Ours ),"['Mixture', 'Selector', '(', 'Ours', ')']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NN', 'NNP', '(', 'NNP', ')']",5
text_summarization,1,169,We construct a hard - MoE of K SELECTORs with uniform mixing coefficient that infers K different focus from source sequence .,"['We', 'construct', 'a', 'hard', '-', 'MoE', 'of', 'K', 'SELECTORs', 'with', 'uniform', 'mixing', 'coefficient', 'that', 'infers', 'K', 'different', 'focus', 'from', 'source', 'sequence', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', ':', 'NN', 'IN', 'NNP', 'NNP', 'IN', 'JJ', 'VBG', 'NN', 'IN', 'NNS', 'NNP', 'JJ', 'NN', 'IN', 'NN', 'NN', '.']",22
text_summarization,1,201,"For all experiments , we tie the weights of the encoder embedding , the decoder embedding , and the decoder output layers .","['For', 'all', 'experiments', ',', 'we', 'tie', 'the', 'weights', 'of', 'the', 'encoder', 'embedding', ',', 'the', 'decoder', 'embedding', ',', 'and', 'the', 'decoder', 'output', 'layers', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', ',', 'DT', 'NN', 'NN', ',', 'CC', 'DT', 'NN', 'NN', 'NNS', '.']",23
text_summarization,1,203,We train up to 20 epochs and select the checkpoint with the best oracle metric .,"['We', 'train', 'up', 'to', '20', 'epochs', 'and', 'select', 'the', 'checkpoint', 'with', 'the', 'best', 'oracle', 'metric', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'RB', 'TO', 'CD', 'NNS', 'CC', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJS', 'NN', 'JJ', '.']",16
text_summarization,1,204,"We use Adam ( Kingma and Ba , 2015 ) optimizer with learning rate 0.001 and momentum parmeters ? 1 = 0.9 and ? 2 = 0.999 .","['We', 'use', 'Adam', '(', 'Kingma', 'and', 'Ba', ',', '2015', ')', 'optimizer', 'with', 'learning', 'rate', '0.001', 'and', 'momentum', 'parmeters', '?', '1', '=', '0.9', 'and', '?', '2', '=', '0.999', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'NN', 'IN', 'VBG', 'NN', 'CD', 'CC', 'NN', 'NNS', '.', 'CD', 'JJ', 'CD', 'CC', '.', 'CD', 'JJ', 'CD', '.']",28
text_summarization,1,205,Minibatch size is 64 and 32 for question generation and abstractive summarization .,"['Minibatch', 'size', 'is', '64', 'and', '32', 'for', 'question', 'generation', 'and', 'abstractive', 'summarization', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NN', 'VBZ', 'CD', 'CC', 'CD', 'IN', 'NN', 'NN', 'CC', 'JJ', 'NN', '.']",13
text_summarization,1,206,"All models are implemented in PyTorch and trained on single Tesla P40 GPU , based on NAVER Smart Machine Learning ( NSML ) platform .","['All', 'models', 'are', 'implemented', 'in', 'PyTorch', 'and', 'trained', 'on', 'single', 'Tesla', 'P40', 'GPU', ',', 'based', 'on', 'NAVER', 'Smart', 'Machine', 'Learning', '(', 'NSML', ')', 'platform', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'CC', 'VBN', 'IN', 'JJ', 'NNP', 'NNP', 'NNP', ',', 'VBN', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', '.']",25
text_summarization,1,208,Diversity vs. Accuracy Trade - off compare our method with different diversitypromoting techniques in question generation and abstractive summarization .,"['Diversity', 'vs.', 'Accuracy', 'Trade', '-', 'off', 'compare', 'our', 'method', 'with', 'different', 'diversitypromoting', 'techniques', 'in', 'question', 'generation', 'and', 'abstractive', 'summarization', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'FW', 'NNP', 'NNP', ':', 'RP', 'VB', 'PRP$', 'NN', 'IN', 'JJ', 'VBG', 'NNS', 'IN', 'NN', 'NN', 'CC', 'JJ', 'NN', '.']",20
text_summarization,1,209,The tables show that our mixture SELECTOR method outperforms all baselines in Top - 1 and oracle metrics and achieves the best trade - off between diversity and accuracy .,"['The', 'tables', 'show', 'that', 'our', 'mixture', 'SELECTOR', 'method', 'outperforms', 'all', 'baselines', 'in', 'Top', '-', '1', 'and', 'oracle', 'metrics', 'and', 'achieves', 'the', 'best', 'trade', '-', 'off', 'between', 'diversity', 'and', 'accuracy', '.']","['O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'IN', 'PRP$', 'NN', 'NNP', 'NN', 'VBZ', 'DT', 'NNS', 'IN', 'NNP', ':', 'CD', 'CC', 'NN', 'NNS', 'CC', 'VBZ', 'DT', 'JJS', 'NN', ':', 'NN', 'IN', 'NN', 'CC', 'NN', '.']",30
text_summarization,1,213,"Notably , our method scores state - of - the - art BLEU - 4 in question generation on SQuAD and ROUGE comparable to state - of - the - art methods in abstractive summarization in CNN - DM ( See also for state - of - the - art results in CNN - DM ) .","['Notably', ',', 'our', 'method', 'scores', 'state', '-', 'of', '-', 'the', '-', 'art', 'BLEU', '-', '4', 'in', 'question', 'generation', 'on', 'SQuAD', 'and', 'ROUGE', 'comparable', 'to', 'state', '-', 'of', '-', 'the', '-', 'art', 'methods', 'in', 'abstractive', 'summarization', 'in', 'CNN', '-', 'DM', '(', 'See', 'also', 'for', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'in', 'CNN', '-', 'DM', ')', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP$', 'NN', 'NNS', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNP', ':', 'CD', 'IN', 'NN', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'JJ', 'TO', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'JJ', 'NN', 'IN', 'NNP', ':', 'NNP', '(', 'NNP', 'RB', 'IN', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'NNP', ':', 'NN', ')', '.']",57
text_summarization,1,214,Diversity vs. Number of Mixtures,"['Diversity', 'vs.', 'Number', 'of', 'Mixtures']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'IN', 'NNP', 'IN', 'NNP']",5
text_summarization,1,215,Here we compare the effect of number of mixtures in our SELECTOR and Mixture Decoder .,"['Here', 'we', 'compare', 'the', 'effect', 'of', 'number', 'of', 'mixtures', 'in', 'our', 'SELECTOR', 'and', 'Mixture', 'Decoder', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'IN', 'NNS', 'IN', 'PRP$', 'NNP', 'CC', 'NNP', 'NNP', '.']",16
text_summarization,1,216,show that pairwise similarity increases ( diversity ?) when the number of mixtures increases for Mixture Decoder .,"['show', 'that', 'pairwise', 'similarity', 'increases', '(', 'diversity', '?)', 'when', 'the', 'number', 'of', 'mixtures', 'increases', 'for', 'Mixture', 'Decoder', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NN', 'IN', 'NN', 'NN', 'NNS', '(', 'NN', 'NN', 'WRB', 'DT', 'NN', 'IN', 'NNS', 'NNS', 'IN', 'NNP', 'NNP', '.']",18
text_summarization,5,2,"Retrieve , Rerank and Rewrite : Soft Template Based Neural Summarization","['Retrieve', ',', 'Rerank', 'and', 'Rewrite', ':', 'Soft', 'Template', 'Based', 'Neural', 'Summarization']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', ',', 'NNP', 'CC', 'NNP', ':', 'JJ', 'NNP', 'VBD', 'NNP', 'NNP']",11
text_summarization,5,12,"In this paper , we focus on an increasingly intriguing task , i.e. , abstractive sentence summarization , which generates a shorter version of a given sentence while attempting to preserve its original meaning .","['In', 'this', 'paper', ',', 'we', 'focus', 'on', 'an', 'increasingly', 'intriguing', 'task', ',', 'i.e.', ',', 'abstractive', 'sentence', 'summarization', ',', 'which', 'generates', 'a', 'shorter', 'version', 'of', 'a', 'given', 'sentence', 'while', 'attempting', 'to', 'preserve', 'its', 'original', 'meaning', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'IN', 'DT', 'RB', 'JJ', 'NN', ',', 'FW', ',', 'JJ', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'JJR', 'NN', 'IN', 'DT', 'VBN', 'NN', 'IN', 'VBG', 'TO', 'VB', 'PRP$', 'JJ', 'NN', '.']",35
text_summarization,5,30,"Due to the strong rewriting ability of the seq2seq framework , in this paper , we propose to combine the seq2seq and template based summarization approaches .","['Due', 'to', 'the', 'strong', 'rewriting', 'ability', 'of', 'the', 'seq2seq', 'framework', ',', 'in', 'this', 'paper', ',', 'we', 'propose', 'to', 'combine', 'the', 'seq2seq', 'and', 'template', 'based', 'summarization', 'approaches', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'TO', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'TO', 'VB', 'DT', 'NN', 'CC', 'NN', 'VBN', 'NN', 'NNS', '.']",27
text_summarization,5,31,"We call our summarization system Re 3 Sum , which consists of three modules : Retrieve , Rerank and Rewrite .","['We', 'call', 'our', 'summarization', 'system', 'Re', '3', 'Sum', ',', 'which', 'consists', 'of', 'three', 'modules', ':', 'Retrieve', ',', 'Rerank', 'and', 'Rewrite', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'NN', 'NNP', 'CD', 'NNP', ',', 'WDT', 'VBZ', 'IN', 'CD', 'NNS', ':', 'NNP', ',', 'NNP', 'CC', 'NNP', '.']",21
text_summarization,5,32,We utilize a widely - used Information Retrieval ( IR ) platform to find out candidate soft templates from the training corpus .,"['We', 'utilize', 'a', 'widely', '-', 'used', 'Information', 'Retrieval', '(', 'IR', ')', 'platform', 'to', 'find', 'out', 'candidate', 'soft', 'templates', 'from', 'the', 'training', 'corpus', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'RB', ':', 'VBN', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', 'TO', 'VB', 'RP', 'NN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",23
text_summarization,5,33,"Then , we extend the seq2seq model to jointly learn template saliency measurement ( Rerank ) and final summary generation ( Rewrite ) .","['Then', ',', 'we', 'extend', 'the', 'seq2seq', 'model', 'to', 'jointly', 'learn', 'template', 'saliency', 'measurement', '(', 'Rerank', ')', 'and', 'final', 'summary', 'generation', '(', 'Rewrite', ')', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'RB', 'VB', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'JJ', 'NN', 'NN', '(', 'NNP', ')', '.']",24
text_summarization,5,34,"Specifically , a Recurrent Neural Network ( RNN ) encoder is applied to convert the input sentence and each candidate template into hidden states .","['Specifically', ',', 'a', 'Recurrent', 'Neural', 'Network', '(', 'RNN', ')', 'encoder', 'is', 'applied', 'to', 'convert', 'the', 'input', 'sentence', 'and', 'each', 'candidate', 'template', 'into', 'hidden', 'states', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'DT', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NN', 'NN', 'CC', 'DT', 'NN', 'NN', 'IN', 'JJ', 'NNS', '.']",25
text_summarization,5,35,"In Rerank , we measure the informativeness of a candidate template according to its hidden state relevance to the input sentence .","['In', 'Rerank', ',', 'we', 'measure', 'the', 'informativeness', 'of', 'a', 'candidate', 'template', 'according', 'to', 'its', 'hidden', 'state', 'relevance', 'to', 'the', 'input', 'sentence', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBG', 'TO', 'PRP$', 'JJ', 'NN', 'NN', 'TO', 'DT', 'NN', 'NN', '.']",22
text_summarization,5,36,The candidate template with the highest predicted informativeness is regarded as the actual soft template .,"['The', 'candidate', 'template', 'with', 'the', 'highest', 'predicted', 'informativeness', 'is', 'regarded', 'as', 'the', 'actual', 'soft', 'template', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'IN', 'DT', 'JJS', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'JJ', 'NN', '.']",16
text_summarization,5,37,"In Rewrite , the summary is generated according to the hidden states of both the sentence and template .","['In', 'Rewrite', ',', 'the', 'summary', 'is', 'generated', 'according', 'to', 'the', 'hidden', 'states', 'of', 'both', 'the', 'sentence', 'and', 'template', '.']","['O', 'B-n', 'O', 'O', 'B-n', 'O', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', ',', 'DT', 'NN', 'VBZ', 'VBN', 'VBG', 'TO', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'DT', 'NN', 'CC', 'NN', '.']",19
text_summarization,5,43,Code and results can be found at http://www4.comp.polyu.edu.hk/cszqcao/,"['Code', 'and', 'results', 'can', 'be', 'found', 'at', 'http://www4.comp.polyu.edu.hk/cszqcao/']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']","['NNP', 'CC', 'NNS', 'MD', 'VB', 'VBN', 'IN', 'NN']",8
text_summarization,5,139,We use the popular seq2seq framework Open - NMT 5 as the starting point .,"['We', 'use', 'the', 'popular', 'seq2seq', 'framework', 'Open', '-', 'NMT', '5', 'as', 'the', 'starting', 'point', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'NNP', ':', 'RB', 'CD', 'IN', 'DT', 'NN', 'NN', '.']",15
text_summarization,5,140,"To make our model more general , we retain the default settings of Open NMT to build the network architecture .","['To', 'make', 'our', 'model', 'more', 'general', ',', 'we', 'retain', 'the', 'default', 'settings', 'of', 'Open', 'NMT', 'to', 'build', 'the', 'network', 'architecture', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['TO', 'VB', 'PRP$', 'NN', 'RBR', 'JJ', ',', 'PRP', 'VBP', 'DT', 'NN', 'NNS', 'IN', 'NNP', 'NNP', 'TO', 'VB', 'DT', 'NN', 'NN', '.']",21
text_summarization,5,141,"Specifically , the dimensions of word embeddings and RNN are both 500 , and the encoder and decoder structures are two - layer bidirectional Long Short Term Memory Networks ( LSTMs ) .","['Specifically', ',', 'the', 'dimensions', 'of', 'word', 'embeddings', 'and', 'RNN', 'are', 'both', '500', ',', 'and', 'the', 'encoder', 'and', 'decoder', 'structures', 'are', 'two', '-', 'layer', 'bidirectional', 'Long', 'Short', 'Term', 'Memory', 'Networks', '(', 'LSTMs', ')', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NNS', 'IN', 'NN', 'NNS', 'CC', 'NNP', 'VBP', 'DT', 'CD', ',', 'CC', 'DT', 'NN', 'CC', 'NN', 'NNS', 'VBP', 'CD', ':', 'NN', 'JJ', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', '.']",33
text_summarization,5,144,"On our computer ( GPU : GTX 1080 , Memory : 16G , CPU : i7-7700 K ) , the training spends about 2 days .","['On', 'our', 'computer', '(', 'GPU', ':', 'GTX', '1080', ',', 'Memory', ':', '16G', ',', 'CPU', ':', 'i7-7700', 'K', ')', ',', 'the', 'training', 'spends', 'about', '2', 'days', '.']","['B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'PRP$', 'NN', '(', 'NNP', ':', 'NN', 'CD', ',', 'NN', ':', 'CD', ',', 'NNP', ':', 'JJ', 'NNP', ')', ',', 'DT', 'NN', 'VBZ', 'IN', 'CD', 'NNS', '.']",26
text_summarization,5,145,"During test , we use beam search of size 5 to generate summaries .","['During', 'test', ',', 'we', 'use', 'beam', 'search', 'of', 'size', '5', 'to', 'generate', 'summaries', '.']","['B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'IN', 'NN', 'CD', 'TO', 'VB', 'NNS', '.']",14
text_summarization,5,146,"We add the argument "" replace unk "" to replace the generated unknown words with the source word that holds the highest attention weight .","['We', 'add', 'the', 'argument', '""', 'replace', 'unk', '""', 'to', 'replace', 'the', 'generated', 'unknown', 'words', 'with', 'the', 'source', 'word', 'that', 'holds', 'the', 'highest', 'attention', 'weight', '.']","['O', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NNP', 'VB', 'JJ', 'NN', 'TO', 'VB', 'DT', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'JJS', 'NN', 'NN', '.']",25
text_summarization,5,147,"Since the generated summaries are often shorter than the actual ones , we introduce an additional length penalty argument "" alpha 1 "" to encourage longer generation , like .","['Since', 'the', 'generated', 'summaries', 'are', 'often', 'shorter', 'than', 'the', 'actual', 'ones', ',', 'we', 'introduce', 'an', 'additional', 'length', 'penalty', 'argument', '""', 'alpha', '1', '""', 'to', 'encourage', 'longer', 'generation', ',', 'like', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NNS', 'VBP', 'RB', 'JJR', 'IN', 'DT', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'NN', 'NNP', 'VBZ', 'CD', 'NN', 'TO', 'VB', 'JJR', 'NN', ',', 'IN', '.']",30
text_summarization,5,151,OpenNMT,['OpenNMT'],['B-n'],['NN'],1
text_summarization,5,152,We also implement the standard attentional seq2seq model with OpenNMT .,"['We', 'also', 'implement', 'the', 'standard', 'attentional', 'seq2seq', 'model', 'with', 'OpenNMT', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'NNP', '.']",11
text_summarization,5,156,FTSum encoded the facts extracted from the source sentence to improve both the faithfulness and informativeness of generated summaries .,"['FTSum', 'encoded', 'the', 'facts', 'extracted', 'from', 'the', 'source', 'sentence', 'to', 'improve', 'both', 'the', 'faithfulness', 'and', 'informativeness', 'of', 'generated', 'summaries', '.']","['B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-p', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O']","['NNP', 'VBD', 'DT', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', 'PDT', 'DT', 'NN', 'CC', 'NN', 'IN', 'JJ', 'NNS', '.']",20
text_summarization,5,157,"In addition , to evaluate the effectiveness of our joint learning framework , we develop a baseline named "" PIPELINE "" .","['In', 'addition', ',', 'to', 'evaluate', 'the', 'effectiveness', 'of', 'our', 'joint', 'learning', 'framework', ',', 'we', 'develop', 'a', 'baseline', 'named', '""', 'PIPELINE', '""', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O']","['IN', 'NN', ',', 'TO', 'VB', 'DT', 'NN', 'IN', 'PRP$', 'JJ', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBN', 'NNP', 'NNP', 'NNP', '.']",22
text_summarization,5,159,"However , it trains the Rerank module and Rewrite module in pipeline .","['However', ',', 'it', 'trains', 'the', 'Rerank', 'module', 'and', 'Rewrite', 'module', 'in', 'pipeline', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBZ', 'DT', 'NNP', 'NN', 'CC', 'NNP', 'NN', 'IN', 'NN', '.']",13
text_summarization,5,163,We also examine the performance of directly regarding soft templates as output summaries .,"['We', 'also', 'examine', 'the', 'performance', 'of', 'directly', 'regarding', 'soft', 'templates', 'as', 'output', 'summaries', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', 'IN', 'RB', 'VBG', 'JJ', 'NNS', 'IN', 'NN', 'NNS', '.']",14
text_summarization,5,164,We introduce five types of different soft templates :,"['We', 'introduce', 'five', 'types', 'of', 'different', 'soft', 'templates', ':']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'NNS', 'IN', 'JJ', 'JJ', 'NNS', ':']",9
text_summarization,5,176,"As shown in , the performance of Random is terrible , indicating it is impossible to use one summary template to fit various actual summaries .","['As', 'shown', 'in', ',', 'the', 'performance', 'of', 'Random', 'is', 'terrible', ',', 'indicating', 'it', 'is', 'impossible', 'to', 'use', 'one', 'summary', 'template', 'to', 'fit', 'various', 'actual', 'summaries', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBN', 'IN', ',', 'DT', 'NN', 'IN', 'NNP', 'VBZ', 'JJ', ',', 'VBG', 'PRP', 'VBZ', 'JJ', 'TO', 'VB', 'CD', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'JJ', 'NNS', '.']",26
text_summarization,5,177,"Rerank largely outperforms First , which verifies the effectiveness of the Rerank module .","['Rerank', 'largely', 'outperforms', 'First', ',', 'which', 'verifies', 'the', 'effectiveness', 'of', 'the', 'Rerank', 'module', '.']","['B-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'RB', 'VBZ', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",14
text_summarization,5,179,"Likewise , comparing Max and First , we observe that the improving capacity of the Retrieve module is high .","['Likewise', ',', 'comparing', 'Max', 'and', 'First', ',', 'we', 'observe', 'that', 'the', 'improving', 'capacity', 'of', 'the', 'Retrieve', 'module', 'is', 'high', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['RB', ',', 'VBG', 'NNP', 'CC', 'NNP', ',', 'PRP', 'VBP', 'IN', 'DT', 'VBG', 'NN', 'IN', 'DT', 'NNP', 'NN', 'VBZ', 'JJ', '.']",20
text_summarization,5,180,Notice that Optimal greatly exceeds all the state - of - the - art approaches .,"['Notice', 'that', 'Optimal', 'greatly', 'exceeds', 'all', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'approaches', '.']","['O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'IN', 'NNP', 'RB', 'VBZ', 'PDT', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', '.']",16
text_summarization,5,183,"We also measure the linguistic quality of generated summaries from various aspects , and the results are present in .","['We', 'also', 'measure', 'the', 'linguistic', 'quality', 'of', 'generated', 'summaries', 'from', 'various', 'aspects', ',', 'and', 'the', 'results', 'are', 'present', 'in', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', ',', 'CC', 'DT', 'NNS', 'VBP', 'JJ', 'IN', '.']",20
text_summarization,5,184,"As can be seen from the rows "" LEN DIF "" and "" LESS 3 "" , the performance of Re 3 Sum is almost the same as that of soft templates .","['As', 'can', 'be', 'seen', 'from', 'the', 'rows', '""', 'LEN', 'DIF', '""', 'and', '""', 'LESS', '3', '""', ',', 'the', 'performance', 'of', 'Re', '3', 'Sum', 'is', 'almost', 'the', 'same', 'as', 'that', 'of', 'soft', 'templates', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'O']","['IN', 'MD', 'VB', 'VBN', 'IN', 'DT', 'NNS', 'VBP', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', 'NNP', 'CD', 'NN', ',', 'DT', 'NN', 'IN', 'NNP', 'CD', 'NNP', 'VBZ', 'RB', 'DT', 'JJ', 'IN', 'DT', 'IN', 'JJ', 'NNS', '.']",33
text_summarization,5,204,"In this section , we investigate how soft templates affect our model .","['In', 'this', 'section', ',', 'we', 'investigate', 'how', 'soft', 'templates', 'affect', 'our', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'WRB', 'JJ', 'NNS', 'VBP', 'PRP$', 'NN', '.']",13
text_summarization,5,206,"As illustrated in , the more high - quality templates are provided , the higher ROUGE scores are achieved .","['As', 'illustrated', 'in', ',', 'the', 'more', 'high', '-', 'quality', 'templates', 'are', 'provided', ',', 'the', 'higher', 'ROUGE', 'scores', 'are', 'achieved', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O']","['IN', 'VBN', 'IN', ',', 'DT', 'JJR', 'JJ', ':', 'NN', 'NNS', 'VBP', 'VBN', ',', 'DT', 'JJR', 'NNP', 'NNS', 'VBP', 'VBN', '.']",20
text_summarization,5,210,"Next , we manually inspect the summaries generated by different methods .","['Next', ',', 'we', 'manually', 'inspect', 'the', 'summaries', 'generated', 'by', 'different', 'methods', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['JJ', ',', 'PRP', 'RB', 'VBP', 'DT', 'NNS', 'VBN', 'IN', 'JJ', 'NNS', '.']",12
text_summarization,5,211,We find the outputs of Re 3 Sum are usually longer and more flu - ent than the outputs of OpenNMT .,"['We', 'find', 'the', 'outputs', 'of', 'Re', '3', 'Sum', 'are', 'usually', 'longer', 'and', 'more', 'flu', '-', 'ent', 'than', 'the', 'outputs', 'of', 'OpenNMT', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNS', 'IN', 'NNP', 'CD', 'NNP', 'VBP', 'RB', 'JJR', 'CC', 'RBR', 'JJ', ':', 'NN', 'IN', 'DT', 'NNS', 'IN', 'NNP', '.']",22
text_summarization,5,222,"As can be seen , with different templates given , our model is likely to generate dissimilar summaries .","['As', 'can', 'be', 'seen', ',', 'with', 'different', 'templates', 'given', ',', 'our', 'model', 'is', 'likely', 'to', 'generate', 'dissimilar', 'summaries', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'MD', 'VB', 'VBN', ',', 'IN', 'JJ', 'NNS', 'VBN', ',', 'PRP$', 'NN', 'VBZ', 'JJ', 'TO', 'VB', 'JJ', 'NNS', '.']",19
text_summarization,11,2,Global Encoding for Abstractive Summarization,"['Global', 'Encoding', 'for', 'Abstractive', 'Summarization']","['O', 'O', 'O', 'B-n', 'I-n']","['JJ', 'NNP', 'IN', 'NNP', 'NNP']",5
text_summarization,11,10,"Therefore , sequence - to - sequence learning can be applied to neural abstractive summarization , whose model consists of an encoder and a decoder .","['Therefore', ',', 'sequence', '-', 'to', '-', 'sequence', 'learning', 'can', 'be', 'applied', 'to', 'neural', 'abstractive', 'summarization', ',', 'whose', 'model', 'consists', 'of', 'an', 'encoder', 'and', 'a', 'decoder', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'NN', ':', 'TO', ':', 'NN', 'NN', 'MD', 'VB', 'VBN', 'TO', 'JJ', 'JJ', 'NN', ',', 'WP$', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'CC', 'DT', 'NN', '.']",26
text_summarization,11,23,"To tackle this problem , we propose a model of global encoding for abstractive summarization .","['To', 'tackle', 'this', 'problem', ',', 'we', 'propose', 'a', 'model', 'of', 'global', 'encoding', 'for', 'abstractive', 'summarization', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'NN', 'IN', 'JJ', 'NN', '.']",16
text_summarization,11,24,We set a convolutional gated unit to perform global encoding on the source context .,"['We', 'set', 'a', 'convolutional', 'gated', 'unit', 'to', 'perform', 'global', 'encoding', 'on', 'the', 'source', 'context', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'VBG', 'IN', 'DT', 'NN', 'NN', '.']",15
text_summarization,11,25,"The gate based on convolutional neural network ( CNN ) filters each encoder output based on the global context due to the parameter sharing , so that the representations at each time step are refined with consideration of the global context .","['The', 'gate', 'based', 'on', 'convolutional', 'neural', 'network', '(', 'CNN', ')', 'filters', 'each', 'encoder', 'output', 'based', 'on', 'the', 'global', 'context', 'due', 'to', 'the', 'parameter', 'sharing', ',', 'so', 'that', 'the', 'representations', 'at', 'each', 'time', 'step', 'are', 'refined', 'with', 'consideration', 'of', 'the', 'global', 'context', '.']","['O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBN', 'IN', 'JJ', 'JJ', 'NN', '(', 'NNP', ')', 'VBZ', 'DT', 'NN', 'NN', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'JJ', 'TO', 'DT', 'NN', 'NN', ',', 'IN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', 'VBP', 'VBN', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",42
text_summarization,11,88,We implement our experiments in PyTorch on an NVIDIA 1080 Ti GPU .,"['We', 'implement', 'our', 'experiments', 'in', 'PyTorch', 'on', 'an', 'NVIDIA', '1080', 'Ti', 'GPU', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'NNS', 'IN', 'NNP', 'IN', 'DT', 'JJ', 'CD', 'NNP', 'NNP', '.']",13
text_summarization,11,89,The word embedding dimension and the number of hidden units are both 512 .,"['The', 'word', 'embedding', 'dimension', 'and', 'the', 'number', 'of', 'hidden', 'units', 'are', 'both', '512', '.']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'O']","['DT', 'NN', 'VBG', 'NN', 'CC', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'VBP', 'DT', 'CD', '.']",14
text_summarization,11,90,"In both experiments , the batch size is set to 64 .","['In', 'both', 'experiments', ',', 'the', 'batch', 'size', 'is', 'set', 'to', '64', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'DT', 'NNS', ',', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",12
text_summarization,11,91,"We use Adam optimizer ( Kingma and Ba , 2014 ) with the default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 10 ?8 .","['We', 'use', 'Adam', 'optimizer', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', 'with', 'the', 'default', 'setting', '?', '=', '0.001', ',', '?', '1', '=', '0.9', ',', '?', '2', '=', '0.999', 'and', '=', '1', '10', '?8', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'IN', 'DT', 'NN', 'VBG', '.', '$', 'CD', ',', '.', 'CD', 'NN', 'CD', ',', '.', 'CD', 'JJ', 'CD', 'CC', 'VBD', 'CD', 'CD', 'NN', '.']",34
text_summarization,11,92,The learning rate is halved every epoch .,"['The', 'learning', 'rate', 'is', 'halved', 'every', 'epoch', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'DT', 'NN', '.']",8
text_summarization,11,93,"Gradient clipping is applied with range [ - 10 , 10 ] .","['Gradient', 'clipping', 'is', 'applied', 'with', 'range', '[', '-', '10', ',', '10', ']', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NN', 'VBZ', 'VBN', 'IN', 'NN', 'NNP', ':', 'CD', ',', 'CD', 'NN', '.']",13
text_summarization,11,100,Baselines for LCSTS are introduced in the following .,"['Baselines', 'for', 'LCSTS', 'are', 'introduced', 'in', 'the', 'following', '.']","['O', 'B-p', 'B-n', 'B-p', 'O', 'O', 'O', 'O', 'O']","['NNS', 'IN', 'NNP', 'VBP', 'VBN', 'IN', 'DT', 'NN', '.']",9
text_summarization,11,101,"RNN and RNN - context are the RNNbased seq2seq models , without and with attention mechanism respectively .","['RNN', 'and', 'RNN', '-', 'context', 'are', 'the', 'RNNbased', 'seq2seq', 'models', ',', 'without', 'and', 'with', 'attention', 'mechanism', 'respectively', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O']","['NNP', 'CC', 'NNP', ':', 'NN', 'VBP', 'DT', 'JJ', 'NN', 'NNS', ',', 'IN', 'CC', 'IN', 'NN', 'NN', 'RB', '.']",18
text_summarization,11,102,Copy - Net is the attention - based seq2seq model with the copy mechanism .,"['Copy', '-', 'Net', 'is', 'the', 'attention', '-', 'based', 'seq2seq', 'model', 'with', 'the', 'copy', 'mechanism', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', ':', 'NN', 'VBZ', 'DT', 'NN', ':', 'VBN', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",15
text_summarization,11,103,SRB is a model that improves semantic relevance between source text and summary .,"['SRB', 'is', 'a', 'model', 'that', 'improves', 'semantic', 'relevance', 'between', 'source', 'text', 'and', 'summary', '.']","['B-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'WDT', 'VBZ', 'JJ', 'NN', 'IN', 'NN', 'NN', 'CC', 'NN', '.']",14
text_summarization,11,104,DRGD is the conventional seq2seq with a deep recurrent generative decoder .,"['DRGD', 'is', 'the', 'conventional', 'seq2seq', 'with', 'a', 'deep', 'recurrent', 'generative', 'decoder', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'JJ', 'NN', '.']",12
text_summarization,11,105,"As to the baselines for Gigaword , ABS and ABS + are the models with local attention and handcrafted features .","['As', 'to', 'the', 'baselines', 'for', 'Gigaword', ',', 'ABS', 'and', 'ABS', '+', 'are', 'the', 'models', 'with', 'local', 'attention', 'and', 'handcrafted', 'features', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'TO', 'DT', 'NNS', 'IN', 'NNP', ',', 'NNP', 'CC', 'NNP', 'NNP', 'VBP', 'DT', 'NNS', 'IN', 'JJ', 'NN', 'CC', 'JJ', 'NNS', '.']",21
text_summarization,11,106,Feats is a fully RNN seq2seq model with some specific methods to control the vocabulary size .,"['Feats', 'is', 'a', 'fully', 'RNN', 'seq2seq', 'model', 'with', 'some', 'specific', 'methods', 'to', 'control', 'the', 'vocabulary', 'size', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['NNS', 'VBZ', 'DT', 'RB', 'NNP', 'VBD', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NN', '.']",17
text_summarization,11,107,RAS - LSTM and RAS - Elman are seq2seq models with a convolutional encoder and an LSTM decoder and an Elman RNN decoder respectively .,"['RAS', '-', 'LSTM', 'and', 'RAS', '-', 'Elman', 'are', 'seq2seq', 'models', 'with', 'a', 'convolutional', 'encoder', 'and', 'an', 'LSTM', 'decoder', 'and', 'an', 'Elman', 'RNN', 'decoder', 'respectively', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NN', 'VBP', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'CC', 'DT', 'NNP', 'NN', 'CC', 'DT', 'NNP', 'NNP', 'NN', 'RB', '.']",25
text_summarization,11,108,SEASS is a seq2seq model with a selective gate mechanism .,"['SEASS', 'is', 'a', 'seq2seq', 'model', 'with', 'a', 'selective', 'gate', 'mechanism', '.']","['B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",11
text_summarization,11,109,DRGD is also a baseline for Gigaword .,"['DRGD', 'is', 'also', 'a', 'baseline', 'for', 'Gigaword', '.']","['B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['NNP', 'VBZ', 'RB', 'DT', 'NN', 'IN', 'NNP', '.']",8
text_summarization,11,115,"In the experiments on the two datasets , our model achieves advantages of ROUGE score over the baselines , and the advantages of ROUGE score on the LCSTS are significant .","['In', 'the', 'experiments', 'on', 'the', 'two', 'datasets', ',', 'our', 'model', 'achieves', 'advantages', 'of', 'ROUGE', 'score', 'over', 'the', 'baselines', ',', 'and', 'the', 'advantages', 'of', 'ROUGE', 'score', 'on', 'the', 'LCSTS', 'are', 'significant', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NNS', 'IN', 'DT', 'CD', 'NNS', ',', 'PRP$', 'NN', 'VBZ', 'NNS', 'IN', 'NNP', 'NN', 'IN', 'DT', 'NNS', ',', 'CC', 'DT', 'NNS', 'IN', 'NNP', 'NN', 'IN', 'DT', 'NNP', 'VBP', 'JJ', '.']",31
text_summarization,11,118,"Compared with the conventional seq2seq model , our model owns an advantage of ROUGE - 2 score 3.7 and 1.5 on the LCSTS and Gigaword respectively .","['Compared', 'with', 'the', 'conventional', 'seq2seq', 'model', ',', 'our', 'model', 'owns', 'an', 'advantage', 'of', 'ROUGE', '-', '2', 'score', '3.7', 'and', '1.5', 'on', 'the', 'LCSTS', 'and', 'Gigaword', 'respectively', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'NNP', ':', 'CD', 'NN', 'CD', 'CC', 'CD', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'RB', '.']",27
text_summarization,14,2,Ensure the Correctness of the Summary : Incorporate Entailment Knowledge into Abstractive Sentence Summarization,"['Ensure', 'the', 'Correctness', 'of', 'the', 'Summary', ':', 'Incorporate', 'Entailment', 'Knowledge', 'into', 'Abstractive', 'Sentence', 'Summarization']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NN', 'DT', 'NNP', 'IN', 'DT', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",14
text_summarization,14,4,"In this paper , we investigate the sentence summarization task that produces a summary from a source sentence .","['In', 'this', 'paper', ',', 'we', 'investigate', 'the', 'sentence', 'summarization', 'task', 'that', 'produces', 'a', 'summary', 'from', 'a', 'source', 'sentence', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",19
text_summarization,14,27,"To incorporate entailment knowledge into abstractive summarization models , we propose in this work an entailment - aware encoder and an entailment - aware decoder .","['To', 'incorporate', 'entailment', 'knowledge', 'into', 'abstractive', 'summarization', 'models', ',', 'we', 'propose', 'in', 'this', 'work', 'an', 'entailment', '-', 'aware', 'encoder', 'and', 'an', 'entailment', '-', 'aware', 'decoder', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NNS', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'DT', 'JJ', ':', 'JJ', 'NN', 'CC', 'DT', 'JJ', ':', 'JJ', 'NN', '.']",26
text_summarization,14,28,"We share the encoder of the summarization generation system with the entailment recognition system , so that the encoder can grasp both the gist of the source sentence and be aware of entailment relationships .","['We', 'share', 'the', 'encoder', 'of', 'the', 'summarization', 'generation', 'system', 'with', 'the', 'entailment', 'recognition', 'system', ',', 'so', 'that', 'the', 'encoder', 'can', 'grasp', 'both', 'the', 'gist', 'of', 'the', 'source', 'sentence', 'and', 'be', 'aware', 'of', 'entailment', 'relationships', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'NN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'NN', ',', 'IN', 'IN', 'DT', 'NN', 'MD', 'VB', 'DT', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'VB', 'JJ', 'IN', 'JJ', 'NNS', '.']",35
text_summarization,14,29,"Furthermore , we propose an entailment Reward Augmented Maximum Likelihood ( RAML ) training that encourages the decoder of the summarization system to produce summary entailed by the source .","['Furthermore', ',', 'we', 'propose', 'an', 'entailment', 'Reward', 'Augmented', 'Maximum', 'Likelihood', '(', 'RAML', ')', 'training', 'that', 'encourages', 'the', 'decoder', 'of', 'the', 'summarization', 'system', 'to', 'produce', 'summary', 'entailed', 'by', 'the', 'source', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', 'JJ', 'VBN', 'IN', 'DT', 'NN', '.']",30
text_summarization,14,151,ABS . first apply the seq2seq model to abstractive sentence summarization .,"['ABS', '.', 'first', 'apply', 'the', 'seq2seq', 'model', 'to', 'abstractive', 'sentence', 'summarization', '.']","['B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', '.', 'RB', 'VB', 'DT', 'NN', 'NN', 'TO', 'JJ', 'NN', 'NN', '.']",12
text_summarization,14,153,ABS +. propose a neural machine translation model with two - layer LSTMs for the encoder - decoder .,"['ABS', '+.', 'propose', 'a', 'neural', 'machine', 'translation', 'model', 'with', 'two', '-', 'layer', 'LSTMs', 'for', 'the', 'encoder', '-', 'decoder', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'VBD', 'DT', 'JJ', 'NN', 'NN', 'NN', 'IN', 'CD', ':', 'NN', 'NNP', 'IN', 'DT', 'NN', ':', 'NN', '.']",19
text_summarization,14,154,Seq2seq .,"['Seq2seq', '.']","['B-n', 'O']","['NNP', '.']",2
text_summarization,14,155,This is a standard seq2seq model with attention mechanism .,"['This', 'is', 'a', 'standard', 'seq2seq', 'model', 'with', 'attention', 'mechanism', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",10
text_summarization,14,156,Seq2seq + MTL .,"['Seq2seq', '+', 'MTL', '.']","['B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NNP', '.']",4
text_summarization,14,157,"This is our proposed model with entailment - aware encoder , which applies a multi-task learning ( MTL ) framework to seq2seq model .","['This', 'is', 'our', 'proposed', 'model', 'with', 'entailment', '-', 'aware', 'encoder', ',', 'which', 'applies', 'a', 'multi-task', 'learning', '(', 'MTL', ')', 'framework', 'to', 'seq2seq', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'VBZ', 'PRP$', 'VBN', 'NN', 'IN', 'JJ', ':', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', '(', 'NNP', ')', 'NN', 'TO', 'VB', 'NN', '.']",24
text_summarization,14,158,Seq2seq + MTL ( Share decoder ) .,"['Seq2seq', '+', 'MTL', '(', 'Share', 'decoder', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NNP', '(', 'NNP', 'NN', ')', '.']",8
text_summarization,14,159,propose a multi - task learning ( MTL ) framework in which the decoder is shared for summarization generation and entailment generation task .,"['propose', 'a', 'multi', '-', 'task', 'learning', '(', 'MTL', ')', 'framework', 'in', 'which', 'the', 'decoder', 'is', 'shared', 'for', 'summarization', 'generation', 'and', 'entailment', 'generation', 'task', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VB', 'DT', 'NN', ':', 'NN', 'NN', '(', 'NNP', ')', 'NN', 'IN', 'WDT', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'NN', 'NN', 'CC', 'JJ', 'NN', 'NN', '.']",24
text_summarization,14,160,Seq2seq + ERAML .,"['Seq2seq', '+', 'ERAML', '.']","['B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NNP', '.']",4
text_summarization,14,161,"This is our proposed model with entailment - aware decoder , which conducts an Entailment Reward Augmented Maximum Likelihood ( ERAML ) training framework .","['This', 'is', 'our', 'proposed', 'model', 'with', 'entailment', '-', 'aware', 'decoder', ',', 'which', 'conducts', 'an', 'Entailment', 'Reward', 'Augmented', 'Maximum', 'Likelihood', '(', 'ERAML', ')', 'training', 'framework', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'PRP$', 'VBN', 'NN', 'IN', 'JJ', ':', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', 'NN', '.']",25
text_summarization,14,162,Seq2seq + ROUGE -2 RAML .,"['Seq2seq', '+', 'ROUGE', '-2', 'RAML', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NNP', 'NNP', 'NNP', '.']",6
text_summarization,14,163,We apply ROUGE - 2 RAML training for seq2seq model .,"['We', 'apply', 'ROUGE', '-', '2', 'RAML', 'training', 'for', 'seq2seq', 'model', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', ':', 'CD', 'NNP', 'NN', 'IN', 'JJ', 'NN', '.']",11
text_summarization,14,164,Seq2seq + RL .,"['Seq2seq', '+', 'RL', '.']","['B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NNP', '.']",4
text_summarization,14,165,We implement Reinforcement Learning ( RL ) models ( policy gradient ) with reward metrics of Entailment and ROUGE - 2 .,"['We', 'implement', 'Reinforcement', 'Learning', '(', 'RL', ')', 'models', '(', 'policy', 'gradient', ')', 'with', 'reward', 'metrics', 'of', 'Entailment', 'and', 'ROUGE', '-', '2', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NNP', '(', 'NNP', ')', 'NNS', '(', 'NN', 'NN', ')', 'IN', 'NN', 'NNS', 'IN', 'NNP', 'CC', 'NNP', ':', 'CD', '.']",22
text_summarization,14,166,Seq2seq + selective .,"['Seq2seq', '+', 'selective', '.']","['B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NN', '.']",4
text_summarization,14,167,employ a selective encoding model to control the information flow from encoder to decoder .,"['employ', 'a', 'selective', 'encoding', 'model', 'to', 'control', 'the', 'information', 'flow', 'from', 'encoder', 'to', 'decoder', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['VB', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'NN', 'TO', 'VB', '.']",15
text_summarization,14,172,Experimental Results : Gigaword Corpus,"['Experimental', 'Results', ':', 'Gigaword', 'Corpus']","['O', 'O', 'O', 'B-n', 'I-n']","['JJ', 'NNS', ':', 'NNP', 'NNP']",5
text_summarization,14,176,Our model performs better than the previous works .,"['Our', 'model', 'performs', 'better', 'than', 'the', 'previous', 'works', '.']","['B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'JJR', 'IN', 'DT', 'JJ', 'NNS', '.']",9
text_summarization,14,177,Experimental Results : DUC 2004,"['Experimental', 'Results', ':', 'DUC', '2004']","['O', 'O', 'O', 'B-n', 'I-n']","['JJ', 'NNS', ':', 'NNP', 'CD']",5
text_summarization,14,181,"In , experimental results also show our Seq2seq + selective + MTL + ERAML model achieves significant improvements over baseline models , surpassing Feats2s by 0.98 % ROUGE - 1 , 0.78 % ROUGE - 2 and 0.65 % ROUGE - L without fine - tuning on DUC data .","['In', ',', 'experimental', 'results', 'also', 'show', 'our', 'Seq2seq', '+', 'selective', '+', 'MTL', '+', 'ERAML', 'model', 'achieves', 'significant', 'improvements', 'over', 'baseline', 'models', ',', 'surpassing', 'Feats2s', 'by', '0.98', '%', 'ROUGE', '-', '1', ',', '0.78', '%', 'ROUGE', '-', '2', 'and', '0.65', '%', 'ROUGE', '-', 'L', 'without', 'fine', '-', 'tuning', 'on', 'DUC', 'data', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O']","['IN', ',', 'JJ', 'NNS', 'RB', 'VBP', 'PRP$', 'NNP', 'NNP', 'JJ', 'NNP', 'NNP', 'NNP', 'NNP', 'NN', 'VBZ', 'JJ', 'NNS', 'IN', 'NN', 'NNS', ',', 'VBG', 'NNP', 'IN', 'CD', 'NN', 'NNP', ':', 'CD', ',', 'CD', 'NN', 'NNP', ':', 'CD', 'CC', 'CD', 'NN', 'NNP', ':', 'NN', 'IN', 'JJ', ':', 'NN', 'IN', 'NNP', 'NNS', '.']",50
text_summarization,14,189,Does our summarization model learn entailment knowledge ?,"['Does', 'our', 'summarization', 'model', 'learn', 'entailment', 'knowledge', '?']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['VBZ', 'PRP$', 'NN', 'NN', 'JJ', 'NN', 'NN', '.']",8
text_summarization,14,192,"For the test set of , the average entailment score for the reference is 0.72 , while for the basic seq2seq model , the entailment score is only 0.46 .","['For', 'the', 'test', 'set', 'of', ',', 'the', 'average', 'entailment', 'score', 'for', 'the', 'reference', 'is', '0.72', ',', 'while', 'for', 'the', 'basic', 'seq2seq', 'model', ',', 'the', 'entailment', 'score', 'is', 'only', '0.46', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['IN', 'DT', 'NN', 'NN', 'IN', ',', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'CD', ',', 'IN', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'DT', 'NN', 'NN', 'VBZ', 'RB', 'CD', '.']",30
text_summarization,14,193,"When we adopt entailmentbased strategies , the entailment score rises to 0.63 for seq2seq model .","['When', 'we', 'adopt', 'entailmentbased', 'strategies', ',', 'the', 'entailment', 'score', 'rises', 'to', '0.63', 'for', 'seq2seq', 'model', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['WRB', 'PRP', 'VBP', 'JJ', 'NNS', ',', 'DT', 'NN', 'NN', 'VBZ', 'TO', 'CD', 'IN', 'JJ', 'NN', '.']",16
text_summarization,14,194,"Note that the entailment score is 0.57 for seq2seq model with selective encoding , and we believe that the selective mechanism can filter out secondary information in the input , which will reduce the possibility to generate irrelevant information .","['Note', 'that', 'the', 'entailment', 'score', 'is', '0.57', 'for', 'seq2seq', 'model', 'with', 'selective', 'encoding', ',', 'and', 'we', 'believe', 'that', 'the', 'selective', 'mechanism', 'can', 'filter', 'out', 'secondary', 'information', 'in', 'the', 'input', ',', 'which', 'will', 'reduce', 'the', 'possibility', 'to', 'generate', 'irrelevant', 'information', '.']","['B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'CD', 'IN', 'JJ', 'NN', 'IN', 'JJ', 'NN', ',', 'CC', 'PRP', 'VBP', 'IN', 'DT', 'JJ', 'NN', 'MD', 'VB', 'RP', 'JJ', 'NN', 'IN', 'DT', 'NN', ',', 'WDT', 'MD', 'VB', 'DT', 'NN', 'TO', 'VB', 'JJ', 'NN', '.']",40
text_summarization,14,195,Entailment - aware selective model achieves a high entailment reward of 0.71 .,"['Entailment', '-', 'aware', 'selective', 'model', 'achieves', 'a', 'high', 'entailment', 'reward', 'of', '0.71', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', ':', 'JJ', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', '.']",13
text_summarization,14,196,"In part at least , we can conclude that our model has successfully learned entailment knowledge .","['In', 'part', 'at', 'least', ',', 'we', 'can', 'conclude', 'that', 'our', 'model', 'has', 'successfully', 'learned', 'entailment', 'knowledge', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'NN', 'IN', 'JJS', ',', 'PRP', 'MD', 'VB', 'IN', 'PRP$', 'NN', 'VBZ', 'RB', 'VBN', 'NN', 'NN', '.']",17
text_summarization,14,198,Is it less abstractive for our model ?,"['Is', 'it', 'less', 'abstractive', 'for', 'our', 'model', '?']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['VBZ', 'PRP', 'RBR', 'JJ', 'IN', 'PRP$', 'NN', '.']",8
text_summarization,14,202,"shows that the seq2seq model produces more novel words ( i.e. , words that do not appear in the article ) than our model , indicating a lower degree of abstraction for our model .","['shows', 'that', 'the', 'seq2seq', 'model', 'produces', 'more', 'novel', 'words', '(', 'i.e.', ',', 'words', 'that', 'do', 'not', 'appear', 'in', 'the', 'article', ')', 'than', 'our', 'model', ',', 'indicating', 'a', 'lower', 'degree', 'of', 'abstraction', 'for', 'our', 'model', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NNS', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'JJR', 'JJ', 'NNS', '(', 'FW', ',', 'NNS', 'WDT', 'VBP', 'RB', 'VB', 'IN', 'DT', 'NN', ')', 'IN', 'PRP$', 'NN', ',', 'VBG', 'DT', 'JJR', 'NN', 'IN', 'NN', 'IN', 'PRP$', 'NN', '.']",35
text_summarization,14,203,"However , when we exclude all the words not in the reference ( these words may lead to wrong information ) , our model generates more novel words , suggesting that our model provides a compromise solution for informativeness and correctness .","['However', ',', 'when', 'we', 'exclude', 'all', 'the', 'words', 'not', 'in', 'the', 'reference', '(', 'these', 'words', 'may', 'lead', 'to', 'wrong', 'information', ')', ',', 'our', 'model', 'generates', 'more', 'novel', 'words', ',', 'suggesting', 'that', 'our', 'model', 'provides', 'a', 'compromise', 'solution', 'for', 'informativeness', 'and', 'correctness', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'WRB', 'PRP', 'VBP', 'PDT', 'DT', 'NNS', 'RB', 'IN', 'DT', 'NN', '(', 'DT', 'NNS', 'MD', 'VB', 'TO', 'JJ', 'NN', ')', ',', 'PRP$', 'NN', 'VBZ', 'JJR', 'JJ', 'NNS', ',', 'VBG', 'IN', 'PRP$', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'NN', 'CC', 'NN', '.']",42
text_summarization,14,205,6.6.3 Could the entailment recognition also be improved ?,"['6.6.3', 'Could', 'the', 'entailment', 'recognition', 'also', 'be', 'improved', '?']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['CD', 'MD', 'DT', 'JJ', 'NN', 'RB', 'VB', 'VBN', '.']",9
text_summarization,14,208,shows that our summarization model with MTL outperforms basic seq2seq model .,"['shows', 'that', 'our', 'summarization', 'model', 'with', 'MTL', 'outperforms', 'basic', 'seq2seq', 'model', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNS', 'WDT', 'PRP$', 'NN', 'NN', 'IN', 'NNP', 'NNS', 'JJ', 'JJ', 'NN', '.']",12
text_summarization,14,209,"As ? increases , the accuracy of entailment recognition improves and finally exceeds that of the model without MTL , which reveals the advantage of MTL framework .","['As', '?', 'increases', ',', 'the', 'accuracy', 'of', 'entailment', 'recognition', 'improves', 'and', 'finally', 'exceeds', 'that', 'of', 'the', 'model', 'without', 'MTL', ',', 'which', 'reveals', 'the', 'advantage', 'of', 'MTL', 'framework', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', '.', 'NNS', ',', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NNS', 'CC', 'RB', 'VBZ', 'IN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'NNP', 'NN', '.']",28
text_summarization,7,2,Cutting - off Redundant Repeating Generations for Neural Abstractive Summarization,"['Cutting', '-', 'off', 'Redundant', 'Repeating', 'Generations', 'for', 'Neural', 'Abstractive', 'Summarization']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['VBG', ':', 'RP', 'JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",10
text_summarization,7,8,"The RNN - based encoder - decoder ( EncDec ) approach has recently been providing significant progress in various natural language generation ( NLG ) tasks , i.e. , machine translation ( MT ) and abstractive summarization ( ABS ) .","['The', 'RNN', '-', 'based', 'encoder', '-', 'decoder', '(', 'EncDec', ')', 'approach', 'has', 'recently', 'been', 'providing', 'significant', 'progress', 'in', 'various', 'natural', 'language', 'generation', '(', 'NLG', ')', 'tasks', ',', 'i.e.', ',', 'machine', 'translation', '(', 'MT', ')', 'and', 'abstractive', 'summarization', '(', 'ABS', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', ':', 'VBN', 'SYM', ':', 'NN', '(', 'NNP', ')', 'NN', 'VBZ', 'RB', 'VBN', 'VBG', 'JJ', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'NNS', ',', 'FW', ',', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'JJ', 'NN', '(', 'NNP', ')', '.']",41
text_summarization,7,17,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step .,"['The', 'basic', 'idea', 'of', 'our', 'method', 'is', 'to', 'jointly', 'estimate', 'the', 'upper-bound', 'frequency', 'of', 'each', 'target', 'vocabulary', 'that', 'can', 'occur', 'in', 'a', 'summary', 'during', 'the', 'encoding', 'process', 'and', 'exploit', 'the', 'estimation', 'to', 'control', 'the', 'output', 'words', 'in', 'each', 'decoding', 'step', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'PRP$', 'NN', 'VBZ', 'TO', 'RB', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'MD', 'VB', 'IN', 'DT', 'JJ', 'IN', 'DT', 'VBG', 'NN', 'CC', 'VB', 'DT', 'NN', 'TO', 'VB', 'DT', 'NN', 'NNS', 'IN', 'DT', 'VBG', 'NN', '.']",41
text_summarization,7,18,We refer to our additional component as a wordfrequency estimation ( WFE ) sub-model .,"['We', 'refer', 'to', 'our', 'additional', 'component', 'as', 'a', 'wordfrequency', 'estimation', '(', 'WFE', ')', 'sub-model', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'TO', 'PRP$', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '(', 'NNP', ')', 'NN', '.']",15
text_summarization,7,19,The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process .,"['The', 'WFE', 'sub-model', 'explicitly', 'manages', 'how', 'many', 'times', 'each', 'word', 'has', 'been', 'generated', 'so', 'far', 'and', 'might', 'be', 'generated', 'in', 'the', 'future', 'during', 'the', 'decoding', 'process', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NN', 'RB', 'VBZ', 'WRB', 'JJ', 'NNS', 'DT', 'NN', 'VBZ', 'VBN', 'VBN', 'RB', 'RB', 'CC', 'MD', 'VB', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",27
text_summarization,4,2,Entity Commonsense Representation for Neural Abstractive Summarization,"['Entity', 'Commonsense', 'Representation', 'for', 'Neural', 'Abstractive', 'Summarization']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",7
text_summarization,4,13,Text summarization is a task to generate a shorter and concise version of a text while preserving the meaning of the original text .,"['Text', 'summarization', 'is', 'a', 'task', 'to', 'generate', 'a', 'shorter', 'and', 'concise', 'version', 'of', 'a', 'text', 'while', 'preserving', 'the', 'meaning', 'of', 'the', 'original', 'text', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'VBZ', 'DT', 'NN', 'TO', 'VB', 'DT', 'NN', 'CC', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",24
text_summarization,4,31,"To this end , we present a method to effectively apply linked entities in sequence - tosequence models , called Entity2Topic ( E2T ) .","['To', 'this', 'end', ',', 'we', 'present', 'a', 'method', 'to', 'effectively', 'apply', 'linked', 'entities', 'in', 'sequence', '-', 'tosequence', 'models', ',', 'called', 'Entity2Topic', '(', 'E2T', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'TO', 'RB', 'VB', 'JJ', 'NNS', 'IN', 'NN', ':', 'NN', 'NNS', ',', 'VBN', 'NNP', '(', 'NNP', ')', '.']",25
text_summarization,4,32,E2T is a module that can be easily attached to any sequence - to - sequence based summarization model .,"['E2T', 'is', 'a', 'module', 'that', 'can', 'be', 'easily', 'attached', 'to', 'any', 'sequence', '-', 'to', '-', 'sequence', 'based', 'summarization', 'model', '.']","['B-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'WDT', 'MD', 'VB', 'RB', 'VBN', 'TO', 'DT', 'NN', ':', 'TO', ':', 'NN', 'VBN', 'NN', 'NN', '.']",20
text_summarization,4,33,"The module encodes the entities extracted from the original text by an entity linking system ( ELS ) , constructs a vector representing the topic of the summary to be generated , and informs the decoder about the constructed topic vector .","['The', 'module', 'encodes', 'the', 'entities', 'extracted', 'from', 'the', 'original', 'text', 'by', 'an', 'entity', 'linking', 'system', '(', 'ELS', ')', ',', 'constructs', 'a', 'vector', 'representing', 'the', 'topic', 'of', 'the', 'summary', 'to', 'be', 'generated', ',', 'and', 'informs', 'the', 'decoder', 'about', 'the', 'constructed', 'topic', 'vector', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'NNS', 'VBD', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'VBG', 'NN', '(', 'NNP', ')', ',', 'VBZ', 'DT', 'NN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'TO', 'VB', 'VBN', ',', 'CC', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",42
text_summarization,4,35,We solve this issue by using entity encoders with selective disambiguation and by constructing topic vectors using firm attention .,"['We', 'solve', 'this', 'issue', 'by', 'using', 'entity', 'encoders', 'with', 'selective', 'disambiguation', 'and', 'by', 'constructing', 'topic', 'vectors', 'using', 'firm', 'attention', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'NN', 'NNS', 'IN', 'JJ', 'NN', 'CC', 'IN', 'VBG', 'NN', 'NNS', 'VBG', 'JJ', 'NN', '.']",20
text_summarization,4,177,"For both datasets , we further reduce the size of the input , output , and entity vocabularies to at most 50 K as suggested in and replace less frequent words to "" < unk > "" .","['For', 'both', 'datasets', ',', 'we', 'further', 'reduce', 'the', 'size', 'of', 'the', 'input', ',', 'output', ',', 'and', 'entity', 'vocabularies', 'to', 'at', 'most', '50', 'K', 'as', 'suggested', 'in', 'and', 'replace', 'less', 'frequent', 'words', 'to', '""', '<', 'unk', '>', '""', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'DT', 'NNS', ',', 'PRP', 'VBP', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'NN', ',', 'CC', 'NN', 'NNS', 'TO', 'IN', 'JJS', 'CD', 'NNP', 'IN', 'VBN', 'IN', 'CC', 'VB', 'JJR', 'JJ', 'NNS', 'TO', 'VB', 'NNP', 'JJ', 'NNP', 'NNP', '.']",38
text_summarization,4,178,We use 300D Glove 6 and 1000D wiki2vec 7 pre-trained vectors to initialize our word and entity vectors .,"['We', 'use', '300D', 'Glove', '6', 'and', '1000D', 'wiki2vec', '7', 'pre-trained', 'vectors', 'to', 'initialize', 'our', 'word', 'and', 'entity', 'vectors', '.']","['O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'NNP', 'CD', 'CC', 'CD', 'JJ', 'CD', 'JJ', 'NNS', 'TO', 'VB', 'PRP$', 'NN', 'CC', 'NN', 'NNS', '.']",19
text_summarization,4,179,"For GRUs , we set the state size to 500 .","['For', 'GRUs', ',', 'we', 'set', 'the', 'state', 'size', 'to', '500', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'NNP', ',', 'PRP', 'VBD', 'DT', 'NN', 'NN', 'TO', 'CD', '.']",11
text_summarization,4,180,"For CNN , we set h = 3 , 4 , 5 with 400 , 300 , 300 feature maps , respectively .","['For', 'CNN', ',', 'we', 'set', 'h', '=', '3', ',', '4', ',', '5', 'with', '400', ',', '300', ',', '300', 'feature', 'maps', ',', 'respectively', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['IN', 'NNP', ',', 'PRP', 'VBP', 'JJ', 'JJ', 'CD', ',', 'CD', ',', 'CD', 'IN', 'CD', ',', 'CD', ',', 'CD', 'NN', 'NNS', ',', 'RB', '.']",23
text_summarization,4,181,"For firm attention , k is tuned by calculating the perplexity of the model starting with smaller values ( i.e. k = 1 , 2 , 5 , 10 , 20 , ... ) and stopping when the perplexity of the model becomes worse than the previous model .","['For', 'firm', 'attention', ',', 'k', 'is', 'tuned', 'by', 'calculating', 'the', 'perplexity', 'of', 'the', 'model', 'starting', 'with', 'smaller', 'values', '(', 'i.e.', 'k', '=', '1', ',', '2', ',', '5', ',', '10', ',', '20', ',', '...', ')', 'and', 'stopping', 'when', 'the', 'perplexity', 'of', 'the', 'model', 'becomes', 'worse', 'than', 'the', 'previous', 'model', '.']","['O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'JJ', 'NN', ',', 'NN', 'VBZ', 'VBN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBG', 'IN', 'JJR', 'NNS', '(', 'FW', 'NN', 'NNP', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ',', ':', ')', 'CC', 'VBG', 'WRB', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'JJR', 'IN', 'DT', 'JJ', 'NN', '.']",49
text_summarization,4,183,We use dropout on all non-linear connections with a dropout rate of 0.5 .,"['We', 'use', 'dropout', 'on', 'all', 'non-linear', 'connections', 'with', 'a', 'dropout', 'rate', 'of', '0.5', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",14
text_summarization,4,184,"We set the batch sizes of Gigaword and CNN datasets to 80 and 10 , respectively .","['We', 'set', 'the', 'batch', 'sizes', 'of', 'Gigaword', 'and', 'CNN', 'datasets', 'to', '80', 'and', '10', ',', 'respectively', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'NNS', 'TO', 'CD', 'CC', 'CD', ',', 'RB', '.']",17
text_summarization,4,185,"Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule , with l 2 constraint ( Hinton et al. , 2012 ) of 3 .","['Training', 'is', 'done', 'via', 'stochastic', 'gradient', 'descent', 'over', 'shuffled', 'mini-batches', 'with', 'the', 'Adadelta', 'update', 'rule', ',', 'with', 'l', '2', 'constraint', '(', 'Hinton', 'et', 'al.', ',', '2012', ')', 'of', '3', '.']","['B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'VBZ', 'VBN', 'IN', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'NN', ',', 'IN', 'JJ', 'CD', 'NN', '(', 'NNP', 'VBZ', 'RB', ',', 'CD', ')', 'IN', 'CD', '.']",30
text_summarization,4,186,We perform early stopping using a subset of the given development dataset .,"['We', 'perform', 'early', 'stopping', 'using', 'a', 'subset', 'of', 'the', 'given', 'development', 'dataset', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'VBN', 'NN', 'NN', '.']",13
text_summarization,4,187,We use beam search of size 10 to generate the summary .,"['We', 'use', 'beam', 'search', 'of', 'size', '10', 'to', 'generate', 'the', 'summary', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'NN', 'CD', 'TO', 'VB', 'DT', 'NN', '.']",12
text_summarization,4,189,"For the Gigaword dataset , we compare our models with the following abstractive baselines :","['For', 'the', 'Gigaword', 'dataset', ',', 'we', 'compare', 'our', 'models', 'with', 'the', 'following', 'abstractive', 'baselines', ':']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNP', 'NN', ',', 'PRP', 'VBP', 'PRP$', 'NNS', 'IN', 'DT', 'VBG', 'JJ', 'NNS', ':']",15
text_summarization,4,190,"ABS + is a fine tuned version of ABS which uses an attentive CNN encoder and an NNLM decoder , Feat2s ( Nallapati et al. , 2016 ) is an RNN sequence - to - sequence model with lexical and statistical features in the encoder , Luong - NMT is a two - layer LSTM encoder - decoder model , RAS - Elman uses an attentive CNN encoder and an Elman RNN decoder , and SEASS uses BiGRU encoders and GRU decoders with selective encoding .","['ABS', '+', 'is', 'a', 'fine', 'tuned', 'version', 'of', 'ABS', 'which', 'uses', 'an', 'attentive', 'CNN', 'encoder', 'and', 'an', 'NNLM', 'decoder', ',', 'Feat2s', '(', 'Nallapati', 'et', 'al.', ',', '2016', ')', 'is', 'an', 'RNN', 'sequence', '-', 'to', '-', 'sequence', 'model', 'with', 'lexical', 'and', 'statistical', 'features', 'in', 'the', 'encoder', ',', 'Luong', '-', 'NMT', 'is', 'a', 'two', '-', 'layer', 'LSTM', 'encoder', '-', 'decoder', 'model', ',', 'RAS', '-', 'Elman', 'uses', 'an', 'attentive', 'CNN', 'encoder', 'and', 'an', 'Elman', 'RNN', 'decoder', ',', 'and', 'SEASS', 'uses', 'BiGRU', 'encoders', 'and', 'GRU', 'decoders', 'with', 'selective', 'encoding', '.']","['B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'NNP', 'WDT', 'VBZ', 'DT', 'JJ', 'NNP', 'NN', 'CC', 'DT', 'NNP', 'NN', ',', 'NNP', '(', 'NNP', 'RB', 'RB', ',', 'CD', ')', 'VBZ', 'DT', 'NNP', 'NN', ':', 'TO', ':', 'NN', 'NN', 'IN', 'JJ', 'CC', 'JJ', 'NNS', 'IN', 'DT', 'NN', ',', 'NNP', ':', 'NN', 'VBZ', 'DT', 'CD', ':', 'NN', 'NNP', 'NN', ':', 'NN', 'NN', ',', 'NNP', ':', 'NNP', 'VBZ', 'DT', 'JJ', 'NNP', 'NN', 'CC', 'DT', 'NNP', 'NNP', 'NN', ',', 'CC', 'NNP', 'VBZ', 'NNP', 'NNS', 'CC', 'NNP', 'NNS', 'IN', 'JJ', 'NN', '.']",86
text_summarization,4,191,"For the CNN dataset , we compare our models with the following extractive and abstractive baselines :","['For', 'the', 'CNN', 'dataset', ',', 'we', 'compare', 'our', 'models', 'with', 'the', 'following', 'extractive', 'and', 'abstractive', 'baselines', ':']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNP', 'NN', ',', 'PRP', 'VBP', 'PRP$', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'CC', 'JJ', 'NNS', ':']",17
text_summarization,4,192,"Lead - 3 is a strong baseline that extracts the first three sentences of the document as summary , LexRank extracts texts using LexRank , Bi - GRU is a non-hierarchical one - layer sequence - to - sequence abstractive baseline , Distraction - M3 uses a sequence - to - sequence abstractive model with distraction - based networks , and GBA is a graph - based attentional neural abstractive model .","['Lead', '-', '3', 'is', 'a', 'strong', 'baseline', 'that', 'extracts', 'the', 'first', 'three', 'sentences', 'of', 'the', 'document', 'as', 'summary', ',', 'LexRank', 'extracts', 'texts', 'using', 'LexRank', ',', 'Bi', '-', 'GRU', 'is', 'a', 'non-hierarchical', 'one', '-', 'layer', 'sequence', '-', 'to', '-', 'sequence', 'abstractive', 'baseline', ',', 'Distraction', '-', 'M3', 'uses', 'a', 'sequence', '-', 'to', '-', 'sequence', 'abstractive', 'model', 'with', 'distraction', '-', 'based', 'networks', ',', 'and', 'GBA', 'is', 'a', 'graph', '-', 'based', 'attentional', 'neural', 'abstractive', 'model', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'CD', 'VBZ', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'CD', 'NNS', 'IN', 'DT', 'NN', 'IN', 'JJ', ',', 'NNP', 'VBZ', 'JJ', 'VBG', 'NNP', ',', 'NNP', ':', 'NNP', 'VBZ', 'DT', 'JJ', 'CD', ':', 'NN', 'NN', ':', 'TO', ':', 'NN', 'JJ', 'NN', ',', 'NNP', ':', 'NNP', 'VBZ', 'DT', 'NN', ':', 'TO', ':', 'NN', 'JJ', 'NN', 'IN', 'NN', ':', 'VBN', 'NNS', ',', 'CC', 'NNP', 'VBZ', 'DT', 'NN', ':', 'VBN', 'JJ', 'JJ', 'JJ', 'NN', '.']",72
text_summarization,4,198,"In Gigaword dataset where the texts are short , our best model achieves a comparable performance with the current state - of - the - art .","['In', 'Gigaword', 'dataset', 'where', 'the', 'texts', 'are', 'short', ',', 'our', 'best', 'model', 'achieves', 'a', 'comparable', 'performance', 'with', 'the', 'current', 'state', '-', 'of', '-', 'the', '-', 'art', '.']","['B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', 'NN', 'WRB', 'DT', 'NN', 'VBP', 'JJ', ',', 'PRP$', 'JJS', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', '.']",27
text_summarization,4,199,"In CNN dataset where the texts are longer , our best model outperforms all the previous models .","['In', 'CNN', 'dataset', 'where', 'the', 'texts', 'are', 'longer', ',', 'our', 'best', 'model', 'outperforms', 'all', 'the', 'previous', 'models', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', 'NN', 'WRB', 'DT', 'NN', 'VBP', 'RBR', ',', 'PRP$', 'JJS', 'NN', 'VBZ', 'PDT', 'DT', 'JJ', 'NNS', '.']",18
text_summarization,4,201,"Overall , E2T achieves a significant improvement over the baseline model BASE , with at least 2 ROUGE - 1 points increase in the Gigaword dataset and 6 ROUGE - 1 points increase in the CNN dataset .","['Overall', ',', 'E2T', 'achieves', 'a', 'significant', 'improvement', 'over', 'the', 'baseline', 'model', 'BASE', ',', 'with', 'at', 'least', '2', 'ROUGE', '-', '1', 'points', 'increase', 'in', 'the', 'Gigaword', 'dataset', 'and', '6', 'ROUGE', '-', '1', 'points', 'increase', 'in', 'the', 'CNN', 'dataset', '.']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['JJ', ',', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', 'NNP', ',', 'IN', 'IN', 'JJS', 'CD', 'NNP', ':', 'CD', 'NNS', 'NN', 'IN', 'DT', 'NNP', 'NN', 'CC', 'CD', 'NNP', ':', 'CD', 'NNS', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",38
text_summarization,4,203,"Among the model variants , the CNN - based encoder with selective disambiguation and firm attention performs the best .","['Among', 'the', 'model', 'variants', ',', 'the', 'CNN', '-', 'based', 'encoder', 'with', 'selective', 'disambiguation', 'and', 'firm', 'attention', 'performs', 'the', 'best', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['IN', 'DT', 'NN', 'NNS', ',', 'DT', 'NNP', ':', 'VBN', 'NN', 'IN', 'JJ', 'NN', 'CC', 'NN', 'NN', 'VBZ', 'DT', 'JJS', '.']",20
text_summarization,6,2,Deep Recurrent Generative Decoder for Abstractive Text Summarization,"['Deep', 'Recurrent', 'Generative', 'Decoder', 'for', 'Abstractive', 'Text', 'Summarization']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",8
text_summarization,6,11,Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document .,"['Automatic', 'summarization', 'is', 'the', 'process', 'of', 'automatically', 'generating', 'a', 'summary', 'that', 'retains', 'the', 'most', 'important', 'content', 'of', 'the', 'original', 'text', 'document', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'RB', 'VBG', 'DT', 'JJ', 'WDT', 'VBZ', 'DT', 'RBS', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",22
text_summarization,6,26,"To tackle the above mentioned problems , we design a new framework based on sequence to - sequence oriented encoder - decoder model equipped with a latent structure modeling component .","['To', 'tackle', 'the', 'above', 'mentioned', 'problems', ',', 'we', 'design', 'a', 'new', 'framework', 'based', 'on', 'sequence', 'to', '-', 'sequence', 'oriented', 'encoder', '-', 'decoder', 'model', 'equipped', 'with', 'a', 'latent', 'structure', 'modeling', 'component', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', 'VBD', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBN', 'IN', 'NN', 'TO', ':', 'NN', 'VBD', 'JJR', ':', 'NN', 'NN', 'VBD', 'IN', 'DT', 'JJ', 'NN', 'VBG', 'NN', '.']",31
text_summarization,6,27,We employ Variational Auto - Encoders ( VAEs ) as the base model for our generative framework which can handle the inference problem associated with complex generative modeling .,"['We', 'employ', 'Variational', 'Auto', '-', 'Encoders', '(', 'VAEs', ')', 'as', 'the', 'base', 'model', 'for', 'our', 'generative', 'framework', 'which', 'can', 'handle', 'the', 'inference', 'problem', 'associated', 'with', 'complex', 'generative', 'modeling', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NNP', ':', 'NNS', '(', 'NNP', ')', 'IN', 'DT', 'NN', 'NN', 'IN', 'PRP$', 'NN', 'NN', 'WDT', 'MD', 'VB', 'DT', 'NN', 'NN', 'VBN', 'IN', 'JJ', 'JJ', 'NN', '.']",29
text_summarization,6,29,"Inspired by , we add historical dependencies on the latent variables of VAEs and propose a deep recurrent generative decoder ( DRGD ) for latent structure modeling .","['Inspired', 'by', ',', 'we', 'add', 'historical', 'dependencies', 'on', 'the', 'latent', 'variables', 'of', 'VAEs', 'and', 'propose', 'a', 'deep', 'recurrent', 'generative', 'decoder', '(', 'DRGD', ')', 'for', 'latent', 'structure', 'modeling', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NNP', 'CC', 'VB', 'DT', 'JJ', 'NN', 'JJ', 'NN', '(', 'NNP', ')', 'IN', 'NN', 'NN', 'NN', '.']",28
text_summarization,6,30,Then the standard discriminative deterministic decoder and the recurrent generative decoder are integrated into a unified decoding framework .,"['Then', 'the', 'standard', 'discriminative', 'deterministic', 'decoder', 'and', 'the', 'recurrent', 'generative', 'decoder', 'are', 'integrated', 'into', 'a', 'unified', 'decoding', 'framework', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', 'DT', 'JJ', 'JJ', 'JJ', 'NN', 'CC', 'DT', 'NN', 'JJ', 'NN', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",19
text_summarization,6,31,The target summaries will be decoded based on both the discriminative deterministic variables and the generative latent structural information .,"['The', 'target', 'summaries', 'will', 'be', 'decoded', 'based', 'on', 'both', 'the', 'discriminative', 'deterministic', 'variables', 'and', 'the', 'generative', 'latent', 'structural', 'information', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NNS', 'MD', 'VB', 'VBN', 'VBN', 'IN', 'DT', 'DT', 'JJ', 'JJ', 'NNS', 'CC', 'DT', 'JJ', 'JJ', 'JJ', 'NN', '.']",20
text_summarization,6,39,Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document .,"['Automatic', 'summarization', 'is', 'the', 'process', 'of', 'automatically', 'generating', 'a', 'summary', 'that', 'retains', 'the', 'most', 'important', 'content', 'of', 'the', 'original', 'text', 'document', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'RB', 'VBG', 'DT', 'JJ', 'WDT', 'VBZ', 'DT', 'RBS', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",22
text_summarization,6,192,TOPIARY is the best on DUC2004 Task - 1 for compressive text summarization .,"['TOPIARY', 'is', 'the', 'best', 'on', 'DUC2004', 'Task', '-', '1', 'for', 'compressive', 'text', 'summarization', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'JJS', 'IN', 'NNP', 'NNP', ':', 'CD', 'IN', 'JJ', 'NN', 'NN', '.']",14
text_summarization,6,193,It combines a system using linguistic based transformations and an unsupervised topic detection algorithm for compressive text summarization .,"['It', 'combines', 'a', 'system', 'using', 'linguistic', 'based', 'transformations', 'and', 'an', 'unsupervised', 'topic', 'detection', 'algorithm', 'for', 'compressive', 'text', 'summarization', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBZ', 'DT', 'NN', 'VBG', 'JJ', 'VBN', 'NNS', 'CC', 'DT', 'JJ', 'NN', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",19
text_summarization,6,194,MOSES + uses a phrasebased statistical machine translation system trained on Gigaword to produce summaries .,"['MOSES', '+', 'uses', 'a', 'phrasebased', 'statistical', 'machine', 'translation', 'system', 'trained', 'on', 'Gigaword', 'to', 'produce', 'summaries', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O']","['NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'NN', 'VBD', 'IN', 'NNP', 'TO', 'VB', 'NNS', '.']",16
text_summarization,6,196,ABS and ABS + are both the neural network based models with local attention modeling for abstractive sentence summarization .,"['ABS', 'and', 'ABS', '+', 'are', 'both', 'the', 'neural', 'network', 'based', 'models', 'with', 'local', 'attention', 'modeling', 'for', 'abstractive', 'sentence', 'summarization', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'CC', 'NNP', 'NNP', 'VBP', 'DT', 'DT', 'JJ', 'NN', 'VBN', 'NNS', 'IN', 'JJ', 'NN', 'VBG', 'IN', 'JJ', 'NN', 'NN', '.']",20
text_summarization,6,197,"ABS + is trained on the Gigaword corpus , but combined with an additional log - linear extractive summarization model with handcrafted features .","['ABS', '+', 'is', 'trained', 'on', 'the', 'Gigaword', 'corpus', ',', 'but', 'combined', 'with', 'an', 'additional', 'log', '-', 'linear', 'extractive', 'summarization', 'model', 'with', 'handcrafted', 'features', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'NNP', 'VBZ', 'VBN', 'IN', 'DT', 'NNP', 'NN', ',', 'CC', 'VBD', 'IN', 'DT', 'JJ', 'NN', ':', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'NNS', '.']",24
text_summarization,6,198,RNN and RNN - context are two seq2seq architectures .,"['RNN', 'and', 'RNN', '-', 'context', 'are', 'two', 'seq2seq', 'architectures', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'CC', 'NNP', ':', 'NN', 'VBP', 'CD', 'JJ', 'NNS', '.']",10
text_summarization,6,200,Copy Net integrates a copying mechanism into the sequence - to sequence framework .,"['Copy', 'Net', 'integrates', 'a', 'copying', 'mechanism', 'into', 'the', 'sequence', '-', 'to', 'sequence', 'framework', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'NNP', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', ':', 'TO', 'VB', 'NN', '.']",14
text_summarization,6,201,RNN - distract uses a new attention mechanism by distracting the historical attention in the decoding steps .,"['RNN', '-', 'distract', 'uses', 'a', 'new', 'attention', 'mechanism', 'by', 'distracting', 'the', 'historical', 'attention', 'in', 'the', 'decoding', 'steps', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', ':', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NNS', '.']",18
text_summarization,6,202,RAS - LSTM and RAS - Elman both consider words and word positions as input and use convolutional encoders to handle the source information .,"['RAS', '-', 'LSTM', 'and', 'RAS', '-', 'Elman', 'both', 'consider', 'words', 'and', 'word', 'positions', 'as', 'input', 'and', 'use', 'convolutional', 'encoders', 'to', 'handle', 'the', 'source', 'information', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NNP', 'DT', 'VBP', 'NNS', 'CC', 'NN', 'NNS', 'IN', 'NN', 'CC', 'VB', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'NN', 'NN', '.']",25
text_summarization,6,204,LenEmb uses a mechanism to control the summary length by considering the length embedding vector as the input .,"['LenEmb', 'uses', 'a', 'mechanism', 'to', 'control', 'the', 'summary', 'length', 'by', 'considering', 'the', 'length', 'embedding', 'vector', 'as', 'the', 'input', '.']","['B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'NN', 'VBG', 'NN', 'IN', 'DT', 'NN', '.']",19
text_summarization,6,205,ASC+ FSC 1 ) uses a generative model with attention mechanism to conduct the sentence compression problem .,"['ASC+', 'FSC', '1', ')', 'uses', 'a', 'generative', 'model', 'with', 'attention', 'mechanism', 'to', 'conduct', 'the', 'sentence', 'compression', 'problem', '.']","['B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'CD', ')', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'NN', '.']",18
text_summarization,6,207,lvt2k - 1sent and lvt5k - 1sent utilize a trick to control the vocabulary size to improve the training efficiency .,"['lvt2k', '-', '1sent', 'and', 'lvt5k', '-', '1sent', 'utilize', 'a', 'trick', 'to', 'control', 'the', 'vocabulary', 'size', 'to', 'improve', 'the', 'training', 'efficiency', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['SYM', ':', 'CD', 'CC', 'JJR', ':', 'CD', 'IN', 'DT', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', '.']",21
text_summarization,6,209,"For the experiments on the English dataset Gigawords , we set the dimension of word embeddings to 300 , and the dimension of hidden states and latent variables to 500 .","['For', 'the', 'experiments', 'on', 'the', 'English', 'dataset', 'Gigawords', ',', 'we', 'set', 'the', 'dimension', 'of', 'word', 'embeddings', 'to', '300', ',', 'and', 'the', 'dimension', 'of', 'hidden', 'states', 'and', 'latent', 'variables', 'to', '500', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'NNS', 'TO', 'CD', ',', 'CC', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'CC', 'JJ', 'NNS', 'TO', 'CD', '.']",31
text_summarization,6,210,The maximum length of documents and summaries is 100 and 50 respectively .,"['The', 'maximum', 'length', 'of', 'documents', 'and', 'summaries', 'is', '100', 'and', '50', 'respectively', '.']","['O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O']","['DT', 'JJ', 'NN', 'IN', 'NNS', 'CC', 'NNS', 'VBZ', 'CD', 'CC', 'CD', 'RB', '.']",13
text_summarization,6,211,The batch size of mini-batch training is 256 .,"['The', 'batch', 'size', 'of', 'mini-batch', 'training', 'is', '256', '.']","['O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'IN', 'JJ', 'NN', 'VBZ', 'CD', '.']",9
text_summarization,6,212,"For DUC - 2004 , the maximum length of summaries is 75 bytes .","['For', 'DUC', '-', '2004', ',', 'the', 'maximum', 'length', 'of', 'summaries', 'is', '75', 'bytes', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'NNP', ':', 'CD', ',', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'VBZ', 'CD', 'NNS', '.']",14
text_summarization,6,213,"For the dataset of LCSTS , the dimension of word embeddings is 350 .","['For', 'the', 'dataset', 'of', 'LCSTS', ',', 'the', 'dimension', 'of', 'word', 'embeddings', 'is', '350', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NN', 'IN', 'NNP', ',', 'DT', 'NN', 'IN', 'NN', 'NNS', 'VBZ', 'CD', '.']",14
text_summarization,6,214,We also set the dimension of hidden states and latent variables to 500 .,"['We', 'also', 'set', 'the', 'dimension', 'of', 'hidden', 'states', 'and', 'latent', 'variables', 'to', '500', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'RB', 'VBD', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'CC', 'JJ', 'NNS', 'TO', 'CD', '.']",14
text_summarization,6,215,"The maximum length of documents and summaries is 120 and 25 respectively , and the batch size is also 256 .","['The', 'maximum', 'length', 'of', 'documents', 'and', 'summaries', 'is', '120', 'and', '25', 'respectively', ',', 'and', 'the', 'batch', 'size', 'is', 'also', '256', '.']","['O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'IN', 'NNS', 'CC', 'NNS', 'VBZ', 'CD', 'CC', 'CD', 'RB', ',', 'CC', 'DT', 'NN', 'NN', 'VBZ', 'RB', 'CD', '.']",21
text_summarization,6,216,The beam size of the decoder was set to be 10 .,"['The', 'beam', 'size', 'of', 'the', 'decoder', 'was', 'set', 'to', 'be', '10', '.']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'VBD', 'VBN', 'TO', 'VB', 'CD', '.']",12
text_summarization,6,217,Adadelta with hyperparameter ? = 0.95 and = 1 e ? 6 is used for gradient based optimization .,"['Adadelta', 'with', 'hyperparameter', '?', '=', '0.95', 'and', '=', '1', 'e', '?', '6', 'is', 'used', 'for', 'gradient', 'based', 'optimization', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'IN', 'NN', '.', '$', 'CD', 'CC', '$', 'CD', 'NN', '.', 'CD', 'VBZ', 'VBN', 'IN', 'NN', 'VBN', 'NN', '.']",19
text_summarization,6,218,"Our neural network based framework is implemented using Theano ( Theano Development Team , 2016 ) .","['Our', 'neural', 'network', 'based', 'framework', 'is', 'implemented', 'using', 'Theano', '(', 'Theano', 'Development', 'Team', ',', '2016', ')', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'JJ', 'NN', 'VBN', 'NN', 'VBZ', 'VBN', 'VBG', 'NNP', '(', 'NNP', 'NNP', 'NNP', ',', 'CD', ')', '.']",17
text_summarization,6,229,ROUGE Evaluation,"['ROUGE', 'Evaluation']","['B-n', 'I-n']","['NNP', 'NN']",2
text_summarization,6,230,The results on the Chinese dataset LCSTS are shown in .,"['The', 'results', 'on', 'the', 'Chinese', 'dataset', 'LCSTS', 'are', 'shown', 'in', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'VBP', 'VBN', 'IN', '.']",11
text_summarization,6,231,Our model DRGD also achieves the best performance .,"['Our', 'model', 'DRGD', 'also', 'achieves', 'the', 'best', 'performance', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'NNP', 'RB', 'VBZ', 'DT', 'JJS', 'NN', '.']",9
text_summarization,10,2,Soft Layer - Specific Multi - Task Summarization with Entailment and Question Generation,"['Soft', 'Layer', '-', 'Specific', 'Multi', '-', 'Task', 'Summarization', 'with', 'Entailment', 'and', 'Question', 'Generation']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', ':', 'JJ', 'NNP', ':', 'NN', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'NNP']",13
text_summarization,10,5,"We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation , where the former teaches the summarization model how to look for salient questioning - worthy details , and the latter teaches the model how to rewrite a summary which is a directed - logical subset of the input document .","['We', 'improve', 'these', 'important', 'aspects', 'of', 'abstractive', 'summarization', 'via', 'multi-task', 'learning', 'with', 'the', 'auxiliary', 'tasks', 'of', 'question', 'generation', 'and', 'entailment', 'generation', ',', 'where', 'the', 'former', 'teaches', 'the', 'summarization', 'model', 'how', 'to', 'look', 'for', 'salient', 'questioning', '-', 'worthy', 'details', ',', 'and', 'the', 'latter', 'teaches', 'the', 'model', 'how', 'to', 'rewrite', 'a', 'summary', 'which', 'is', 'a', 'directed', '-', 'logical', 'subset', 'of', 'the', 'input', 'document', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'IN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NN', 'NN', 'CC', 'JJ', 'NN', ',', 'WRB', 'DT', 'JJ', 'NNS', 'DT', 'NN', 'NN', 'WRB', 'TO', 'VB', 'IN', 'NN', 'VBG', ':', 'NN', 'NNS', ',', 'CC', 'DT', 'JJ', 'VBZ', 'DT', 'NN', 'WRB', 'TO', 'VB', 'DT', 'JJ', 'WDT', 'VBZ', 'DT', 'JJ', ':', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",62
text_summarization,10,17,"In this work , we improve abstractive text summarization via soft , high - level ( semantic ) layerspecific multi-task learning with two relevant auxiliary tasks .","['In', 'this', 'work', ',', 'we', 'improve', 'abstractive', 'text', 'summarization', 'via', 'soft', ',', 'high', '-', 'level', '(', 'semantic', ')', 'layerspecific', 'multi-task', 'learning', 'with', 'two', 'relevant', 'auxiliary', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'NN', 'IN', 'JJ', ',', 'JJ', ':', 'NN', '(', 'JJ', ')', 'VBZ', 'JJ', 'VBG', 'IN', 'CD', 'JJ', 'JJ', 'NNS', '.']",27
text_summarization,10,22,"Further , we also present novel multi-task learning architectures based on multi-layered encoder and decoder models , where we empirically show that it is substantially better to share the higherlevel semantic layers between the three aforementioned tasks , while keeping the lower - level ( lexico- syntactic ) layers unshared .","['Further', ',', 'we', 'also', 'present', 'novel', 'multi-task', 'learning', 'architectures', 'based', 'on', 'multi-layered', 'encoder', 'and', 'decoder', 'models', ',', 'where', 'we', 'empirically', 'show', 'that', 'it', 'is', 'substantially', 'better', 'to', 'share', 'the', 'higherlevel', 'semantic', 'layers', 'between', 'the', 'three', 'aforementioned', 'tasks', ',', 'while', 'keeping', 'the', 'lower', '-', 'level', '(', 'lexico-', 'syntactic', ')', 'layers', 'unshared', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'RB', 'VBD', 'JJ', 'JJ', 'NN', 'NNS', 'VBN', 'IN', 'JJ', 'NN', 'CC', 'NN', 'NNS', ',', 'WRB', 'PRP', 'RB', 'VBP', 'IN', 'PRP', 'VBZ', 'RB', 'JJR', 'TO', 'NN', 'DT', 'NN', 'JJ', 'NNS', 'IN', 'DT', 'CD', 'VBD', 'NNS', ',', 'IN', 'VBG', 'DT', 'JJR', ':', 'NN', '(', 'JJ', 'NN', ')', 'NNS', 'VBD', '.']",51
text_summarization,10,144,Pointer + Coverage Baseline,"['Pointer', '+', 'Coverage', 'Baseline']","['B-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'NNP']",4
text_summarization,10,149,"4 On Gigaword dataset , our baseline model ( with pointer only , since coverage not needed for this single - sentence summarization task ) performs better than all previous works , as shown in .","['4', 'On', 'Gigaword', 'dataset', ',', 'our', 'baseline', 'model', '(', 'with', 'pointer', 'only', ',', 'since', 'coverage', 'not', 'needed', 'for', 'this', 'single', '-', 'sentence', 'summarization', 'task', ')', 'performs', 'better', 'than', 'all', 'previous', 'works', ',', 'as', 'shown', 'in', '.']","['O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['CD', 'IN', 'NNP', 'NN', ',', 'PRP$', 'NN', 'NN', '(', 'IN', 'NN', 'RB', ',', 'IN', 'NN', 'RB', 'VBN', 'IN', 'DT', 'JJ', ':', 'NN', 'NN', 'NN', ')', 'VBZ', 'JJR', 'IN', 'DT', 'JJ', 'NNS', ',', 'IN', 'VBN', 'IN', '.']",36
text_summarization,10,150,Multi - Task with Entailment Generation,"['Multi', '-', 'Task', 'with', 'Entailment', 'Generation']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NN', 'IN', 'NNP', 'NNP']",6
text_summarization,10,152,"4 . shows that this multi-task setting is better than our strong baseline models and the improvements are statistically significant on all metrics 5 on both CNN / DailyMail ( p < 0.01 in ROUGE - 1 / ROUGE - L / METEOR and p < 0.05 in ROUGE - 2 ) and Gigaword ( p < 0.01 on all metrics ) datasets , showing that entailment generation task is inducing useful inference skills to the summarization task ( also see analysis examples in Sec. 7 ) .","['4', '.', 'shows', 'that', 'this', 'multi-task', 'setting', 'is', 'better', 'than', 'our', 'strong', 'baseline', 'models', 'and', 'the', 'improvements', 'are', 'statistically', 'significant', 'on', 'all', 'metrics', '5', 'on', 'both', 'CNN', '/', 'DailyMail', '(', 'p', '<', '0.01', 'in', 'ROUGE', '-', '1', '/', 'ROUGE', '-', 'L', '/', 'METEOR', 'and', 'p', '<', '0.05', 'in', 'ROUGE', '-', '2', ')', 'and', 'Gigaword', '(', 'p', '<', '0.01', 'on', 'all', 'metrics', ')', 'datasets', ',', 'showing', 'that', 'entailment', 'generation', 'task', 'is', 'inducing', 'useful', 'inference', 'skills', 'to', 'the', 'summarization', 'task', '(', 'also', 'see', 'analysis', 'examples', 'in', 'Sec.', '7', ')', '.']","['O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['CD', '.', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'RBR', 'IN', 'PRP$', 'JJ', 'NN', 'NNS', 'CC', 'DT', 'NNS', 'VBP', 'RB', 'JJ', 'IN', 'DT', 'NNS', 'CD', 'IN', 'DT', 'NNP', 'NNP', 'NNP', '(', 'JJ', 'NN', 'CD', 'IN', 'NNP', ':', 'CD', 'NN', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'CC', 'VB', 'JJ', 'CD', 'IN', 'NNP', ':', 'CD', ')', 'CC', 'NNP', '(', 'JJ', 'VBP', 'CD', 'IN', 'DT', 'NNS', ')', 'NNS', ',', 'VBG', 'IN', 'JJ', 'NN', 'NN', 'VBZ', 'VBG', 'JJ', 'NN', 'NNS', 'TO', 'DT', 'NN', 'NN', '(', 'RB', 'VBP', 'NN', 'NNS', 'IN', 'NNP', 'CD', ')', '.']",88
text_summarization,10,154,"For multi-task learning with question generation , the improvements are statistically significant in ROUGE - 1 ( p < 0.01 ) , ROUGE - L ( p < 0.05 ) , and METEOR ( p < 0.01 ) for CNN / DailyMail and in all metrics ( p < 0.01 ) for Gigaword , compared to the respective baseline models .","['For', 'multi-task', 'learning', 'with', 'question', 'generation', ',', 'the', 'improvements', 'are', 'statistically', 'significant', 'in', 'ROUGE', '-', '1', '(', 'p', '<', '0.01', ')', ',', 'ROUGE', '-', 'L', '(', 'p', '<', '0.05', ')', ',', 'and', 'METEOR', '(', 'p', '<', '0.01', ')', 'for', 'CNN', '/', 'DailyMail', 'and', 'in', 'all', 'metrics', '(', 'p', '<', '0.01', ')', 'for', 'Gigaword', ',', 'compared', 'to', 'the', 'respective', 'baseline', 'models', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', 'NN', 'IN', 'NN', 'NN', ',', 'DT', 'NNS', 'VBP', 'RB', 'JJ', 'IN', 'NNP', ':', 'CD', '(', 'NN', 'RB', 'CD', ')', ',', 'NNP', ':', 'NNP', '(', 'JJ', 'NNP', 'CD', ')', ',', 'CC', 'NNP', '(', 'JJ', 'NNP', 'CD', ')', 'IN', 'NNP', 'NNP', 'NNP', 'CC', 'IN', 'DT', 'NNS', '(', 'JJ', 'NNP', 'CD', ')', 'IN', 'NNP', ',', 'VBN', 'TO', 'DT', 'JJ', 'NN', 'NNS', '.']",61
text_summarization,10,192,Soft - sharing vs. Hard - sharing,"['Soft', '-', 'sharing', 'vs.', 'Hard', '-', 'sharing']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['JJ', ':', 'VBG', 'FW', 'NNP', ':', 'NN']",7
text_summarization,10,193,"As described in Sec. 4.2 , we choose soft - sharing over hard - sharing because of the more expressive parameter sharing it provides to the model .","['As', 'described', 'in', 'Sec.', '4.2', ',', 'we', 'choose', 'soft', '-', 'sharing', 'over', 'hard', '-', 'sharing', 'because', 'of', 'the', 'more', 'expressive', 'parameter', 'sharing', 'it', 'provides', 'to', 'the', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBN', 'IN', 'NNP', 'CD', ',', 'PRP', 'VBP', 'JJ', ':', 'NN', 'IN', 'JJ', ':', 'NN', 'IN', 'IN', 'DT', 'RBR', 'JJ', 'NN', 'VBG', 'PRP', 'VBZ', 'TO', 'DT', 'NN', '.']",28
text_summarization,10,194,Empirical results in 8 prove that soft - sharing method is statistically significantly better than hard - sharing with p < 0.001 in all metrics .,"['Empirical', 'results', 'in', '8', 'prove', 'that', 'soft', '-', 'sharing', 'method', 'is', 'statistically', 'significantly', 'better', 'than', 'hard', '-', 'sharing', 'with', 'p', '<', '0.001', 'in', 'all', 'metrics', '.']","['O', 'O', 'B-p', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['JJ', 'NNS', 'IN', 'CD', 'NN', 'WDT', 'JJ', ':', 'VBG', 'NN', 'VBZ', 'RB', 'RB', 'JJR', 'IN', 'JJ', ':', 'VBG', 'IN', 'JJ', '$', 'CD', 'IN', 'DT', 'NNS', '.']",26
text_summarization,10,203,Quantitative Improvements in Entailment,"['Quantitative', 'Improvements', 'in', 'Entailment']","['B-n', 'I-n', 'I-n', 'I-n']","['JJ', 'NNS', 'IN', 'NN']",4
text_summarization,10,208,We found that our 2 - way MTL model with entailment generation reduces this extraneous count by 17.2 % w.r.t. the baseline .,"['We', 'found', 'that', 'our', '2', '-', 'way', 'MTL', 'model', 'with', 'entailment', 'generation', 'reduces', 'this', 'extraneous', 'count', 'by', '17.2', '%', 'w.r.t.', 'the', 'baseline', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'IN', 'PRP$', 'CD', ':', 'NN', 'NNP', 'NN', 'IN', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NN', 'IN', 'DT', 'NN', '.']",23
text_summarization,10,210,Quantitative Improvements in Saliency Detection,"['Quantitative', 'Improvements', 'in', 'Saliency', 'Detection']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['JJ', 'NNS', 'IN', 'NNP', 'NNP']",5
text_summarization,10,213,"The results are shown in Table 10 , where the 2 - way - QG MTL model ( with question generation ) versus baseline improvement is stat. significant ( p < 0.01 ) .","['The', 'results', 'are', 'shown', 'in', 'Table', '10', ',', 'where', 'the', '2', '-', 'way', '-', 'QG', 'MTL', 'model', '(', 'with', 'question', 'generation', ')', 'versus', 'baseline', 'improvement', 'is', 'stat.', 'significant', '(', 'p', '<', '0.01', ')', '.']","['O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'JJ', 'CD', ',', 'WRB', 'DT', 'CD', ':', 'NN', ':', 'NNP', 'NNP', 'NN', '(', 'IN', 'NN', 'NN', ')', 'FW', 'NN', 'NN', 'VBZ', 'JJ', 'JJ', '(', 'JJ', 'NNP', 'CD', ')', '.']",34
text_summarization,10,215,"Qualitative Examples on Entailment and Saliency Improvements presents an example of output summaries generated by , our baseline , and our 3 - way multitask model .","['Qualitative', 'Examples', 'on', 'Entailment', 'and', 'Saliency', 'Improvements', 'presents', 'an', 'example', 'of', 'output', 'summaries', 'generated', 'by', ',', 'our', 'baseline', ',', 'and', 'our', '3', '-', 'way', 'multitask', 'model', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNS', 'IN', 'NNP', 'CC', 'NNP', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'NN', 'NNS', 'VBN', 'IN', ',', 'PRP$', 'NN', ',', 'CC', 'PRP$', 'CD', ':', 'NN', 'JJ', 'NN', '.']",27
text_summarization,10,218,"Hence , our 3 - way multi-task model generates summaries that are both better at logical entailment and contain more salient information .","['Hence', ',', 'our', '3', '-', 'way', 'multi-task', 'model', 'generates', 'summaries', 'that', 'are', 'both', 'better', 'at', 'logical', 'entailment', 'and', 'contain', 'more', 'salient', 'information', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'CD', ':', 'NN', 'JJ', 'NN', 'NNS', 'NNS', 'WDT', 'VBP', 'DT', 'JJR', 'IN', 'JJ', 'NN', 'CC', 'VB', 'JJR', 'JJ', 'NN', '.']",23
text_summarization,2,2,Structure - Infused Copy Mechanisms for Abstractive Summarization,"['Structure', '-', 'Infused', 'Copy', 'Mechanisms', 'for', 'Abstractive', 'Summarization']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', ':', 'VBD', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",8
text_summarization,2,4,Seq2seq learning has produced promising results on summarization .,"['Seq2seq', 'learning', 'has', 'produced', 'promising', 'results', 'on', 'summarization', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']","['NNP', 'NN', 'VBZ', 'VBN', 'JJ', 'NNS', 'IN', 'NN', '.']",9
text_summarization,2,25,In this paper we seek to address this problem by incorporating source syntactic structure in neural sentence summarization to help the system identify summary - worthy content and compose summaries that preserve the important meaning of the source texts .,"['In', 'this', 'paper', 'we', 'seek', 'to', 'address', 'this', 'problem', 'by', 'incorporating', 'source', 'syntactic', 'structure', 'in', 'neural', 'sentence', 'summarization', 'to', 'help', 'the', 'system', 'identify', 'summary', '-', 'worthy', 'content', 'and', 'compose', 'summaries', 'that', 'preserve', 'the', 'important', 'meaning', 'of', 'the', 'source', 'texts', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'TO', 'VB', 'DT', 'NN', 'IN', 'VBG', 'NN', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'VB', 'JJ', ':', 'JJ', 'NN', 'CC', 'JJ', 'NNS', 'WDT', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",40
text_summarization,2,26,We present structure - infused copy mechanisms to facilitate copying source words and relations to the summary based on their semantic and structural importance in the source sentences .,"['We', 'present', 'structure', '-', 'infused', 'copy', 'mechanisms', 'to', 'facilitate', 'copying', 'source', 'words', 'and', 'relations', 'to', 'the', 'summary', 'based', 'on', 'their', 'semantic', 'and', 'structural', 'importance', 'in', 'the', 'source', 'sentences', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'JJ', 'NN', ':', 'JJ', 'NN', 'NNS', 'TO', 'VB', 'VBG', 'NN', 'NNS', 'CC', 'NNS', 'TO', 'DT', 'NN', 'VBN', 'IN', 'PRP$', 'JJ', 'CC', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NNS', '.']",29
text_summarization,2,215,We first report results on the Gigaword valid - 2000 dataset in .,"['We', 'first', 'report', 'results', 'on', 'the', 'Gigaword', 'valid', '-', '2000', 'dataset', 'in', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'RB', 'VBP', 'NNS', 'IN', 'DT', 'NNP', 'SYM', ':', 'CD', 'NN', 'IN', '.']",13
text_summarization,2,216,"We present R - 1 , R - 2 , and R - L scores ) that respectively measures the overlapped unigrams , bigrams , and longest common subsequences between the system and reference summaries 3 .","['We', 'present', 'R', '-', '1', ',', 'R', '-', '2', ',', 'and', 'R', '-', 'L', 'scores', ')', 'that', 'respectively', 'measures', 'the', 'overlapped', 'unigrams', ',', 'bigrams', ',', 'and', 'longest', 'common', 'subsequences', 'between', 'the', 'system', 'and', 'reference', 'summaries', '3', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBP', 'NNP', ':', 'CD', ',', 'NNP', ':', 'CD', ',', 'CC', 'NNP', ':', 'NNP', 'NNS', ')', 'WDT', 'RB', 'VBZ', 'DT', 'JJ', 'NNS', ',', 'NNS', ',', 'CC', 'JJS', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NN', 'NNS', 'CD', '.']",37
text_summarization,2,221,"Overall , we observe that models equipped with the structure - infused copy mechanisms are superior to the baseline , suggesting that combining source syntactic structure with the copy mechanism is effective .","['Overall', ',', 'we', 'observe', 'that', 'models', 'equipped', 'with', 'the', 'structure', '-', 'infused', 'copy', 'mechanisms', 'are', 'superior', 'to', 'the', 'baseline', ',', 'suggesting', 'that', 'combining', 'source', 'syntactic', 'structure', 'with', 'the', 'copy', 'mechanism', 'is', 'effective', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', ',', 'PRP', 'VBP', 'DT', 'NNS', 'VBN', 'IN', 'DT', 'NN', ':', 'JJ', 'NN', 'NNS', 'VBP', 'JJ', 'TO', 'DT', 'NN', ',', 'VBG', 'IN', 'VBG', 'NN', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'JJ', '.']",33
text_summarization,2,222,"We found that the "" Struct + Hidden "" architecture , which directly concatenates structural embeddings with the encoder hidden states , outperforms "" Struct + Input "" despite that the latter requires more parameters .","['We', 'found', 'that', 'the', '""', 'Struct', '+', 'Hidden', '""', 'architecture', ',', 'which', 'directly', 'concatenates', 'structural', 'embeddings', 'with', 'the', 'encoder', 'hidden', 'states', ',', 'outperforms', '""', 'Struct', '+', 'Input', '""', 'despite', 'that', 'the', 'latter', 'requires', 'more', 'parameters', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NN', ',', 'WDT', 'RB', 'VBZ', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', 'NNS', ',', 'NNS', 'VBP', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'IN', 'DT', 'JJ', 'VBZ', 'JJR', 'NNS', '.']",36
text_summarization,2,223,""" Struct + 2 Way + Word "" also demonstrates strong performance , achieving 43.21 % , 21. 84 % , and 40.86 % F 1 scores , for R - 1 , R - 2 , and R - L respectively .","['""', 'Struct', '+', '2', 'Way', '+', 'Word', '""', 'also', 'demonstrates', 'strong', 'performance', ',', 'achieving', '43.21', '%', ',', '21.', '84', '%', ',', 'and', '40.86', '%', 'F', '1', 'scores', ',', 'for', 'R', '-', '1', ',', 'R', '-', '2', ',', 'and', 'R', '-', 'L', 'respectively', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['JJ', 'NNP', 'NNP', 'CD', 'NNP', 'NNP', 'NNP', 'NNP', 'RB', 'VBZ', 'JJ', 'NN', ',', 'VBG', 'CD', 'NN', ',', 'CD', 'CD', 'NN', ',', 'CC', 'CD', 'NN', 'NNP', 'CD', 'NNS', ',', 'IN', 'NNP', ':', 'CD', ',', 'NNP', ':', 'CD', ',', 'CC', 'NNP', ':', 'NNP', 'RB', '.']",43
text_summarization,3,2,Concept Pointer Network for Abstractive Summarization,"['Concept', 'Pointer', 'Network', 'for', 'Abstractive', 'Summarization']","['O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",6
text_summarization,3,14,Abstractive summarization ( ABS ) has gained overwhelming success owing to a tremendous development of sequence - to - sequence ( seq2seq ) model and its variants .,"['Abstractive', 'summarization', '(', 'ABS', ')', 'has', 'gained', 'overwhelming', 'success', 'owing', 'to', 'a', 'tremendous', 'development', 'of', 'sequence', '-', 'to', '-', 'sequence', '(', 'seq2seq', ')', 'model', 'and', 'its', 'variants', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', '(', 'NNP', ')', 'VBZ', 'VBN', 'JJ', 'NN', 'VBG', 'TO', 'DT', 'JJ', 'NN', 'IN', 'NN', ':', 'TO', ':', 'NN', '(', 'NN', ')', 'NN', 'CC', 'PRP$', 'NNS', '.']",28
text_summarization,3,26,"Hence , in this paper , we propose a novel model based on a concept pointer generator that encourages the generation of conceptual and abstract words .","['Hence', ',', 'in', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', 'model', 'based', 'on', 'a', 'concept', 'pointer', 'generator', 'that', 'encourages', 'the', 'generation', 'of', 'conceptual', 'and', 'abstract', 'words', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ',', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBN', 'IN', 'DT', 'NN', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'CC', 'JJ', 'NNS', '.']",27
text_summarization,3,27,"As a hidden benefit , the model also alleviates the OOV problems .","['As', 'a', 'hidden', 'benefit', ',', 'the', 'model', 'also', 'alleviates', 'the', 'OOV', 'problems', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'DT', 'NN', 'RB', 'VBZ', 'DT', 'NNP', 'NNS', '.']",13
text_summarization,3,28,"Our model uses pointer network to capture the salient information from a source text , and then employs another pointer to generalize the detailed words according to their upper level of expressions .","['Our', 'model', 'uses', 'pointer', 'network', 'to', 'capture', 'the', 'salient', 'information', 'from', 'a', 'source', 'text', ',', 'and', 'then', 'employs', 'another', 'pointer', 'to', 'generalize', 'the', 'detailed', 'words', 'according', 'to', 'their', 'upper', 'level', 'of', 'expressions', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'CC', 'RB', 'VBZ', 'DT', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'VBG', 'TO', 'PRP$', 'JJ', 'NN', 'IN', 'NNS', '.']",33
text_summarization,3,34,The optimization function is adaptive so as to cater for different datasets with distantly - supervised training .,"['The', 'optimization', 'function', 'is', 'adaptive', 'so', 'as', 'to', 'cater', 'for', 'different', 'datasets', 'with', 'distantly', '-', 'supervised', 'training', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'JJ', 'RB', 'IN', 'TO', 'VB', 'IN', 'JJ', 'NNS', 'IN', 'RB', ':', 'VBD', 'NN', '.']",18
text_summarization,3,35,"The network is then optimized end - to - end using reinforcement learning , with the distant - supervision strategy as a complement to further improve the summary .","['The', 'network', 'is', 'then', 'optimized', 'end', '-', 'to', '-', 'end', 'using', 'reinforcement', 'learning', ',', 'with', 'the', 'distant', '-', 'supervision', 'strategy', 'as', 'a', 'complement', 'to', 'further', 'improve', 'the', 'summary', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'RB', 'JJ', 'NN', ':', 'TO', ':', 'NN', 'VBG', 'JJ', 'NN', ',', 'IN', 'DT', 'JJ', ':', 'NN', 'NN', 'IN', 'DT', 'NN', 'TO', 'RBR', 'VB', 'DT', 'NN', '.']",29
text_summarization,3,159,We initialize word embeddings with 128 - d vectors and fine - tune them during training .,"['We', 'initialize', 'word', 'embeddings', 'with', '128', '-', 'd', 'vectors', 'and', 'fine', '-', 'tune', 'them', 'during', 'training', '.']","['O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'NN', 'NNS', 'IN', 'CD', ':', 'NN', 'NNS', 'CC', 'JJ', ':', 'VB', 'PRP', 'IN', 'NN', '.']",17
text_summarization,3,161,The vocabulary size was set to 150 k for both the source and target text .,"['The', 'vocabulary', 'size', 'was', 'set', 'to', '150', 'k', 'for', 'both', 'the', 'source', 'and', 'target', 'text', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBD', 'VBN', 'TO', 'CD', 'NN', 'IN', 'DT', 'DT', 'NN', 'CC', 'NN', 'NN', '.']",16
text_summarization,3,162,The hidden state size was set to 256 .,"['The', 'hidden', 'state', 'size', 'was', 'set', 'to', '256', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBD', 'VBN', 'TO', 'CD', '.']",9
text_summarization,3,165,"Our code is available on https :// github.com/wprojectsn/codes , and the vocabularies and candidate concepts are also included .","['Our', 'code', 'is', 'available', 'on', 'https', '://', 'github.com/wprojectsn/codes', ',', 'and', 'the', 'vocabularies', 'and', 'candidate', 'concepts', 'are', 'also', 'included', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'VBZ', 'JJ', 'IN', 'NN', 'NNP', 'NNS', ',', 'CC', 'DT', 'NNS', 'CC', 'NN', 'NNS', 'VBP', 'RB', 'VBN', '.']",19
text_summarization,3,166,We trained our models on a single GTX TI - TAN GPU machine .,"['We', 'trained', 'our', 'models', 'on', 'a', 'single', 'GTX', 'TI', '-', 'TAN', 'GPU', 'machine', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'PRP$', 'NNS', 'IN', 'DT', 'JJ', 'NNP', 'NNP', ':', 'NN', 'NNP', 'NN', '.']",14
text_summarization,3,167,We used the Adagrad optimizer with a batch size of 64 to minimize the loss .,"['We', 'used', 'the', 'Adagrad', 'optimizer', 'with', 'a', 'batch', 'size', 'of', '64', 'to', 'minimize', 'the', 'loss', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBD', 'DT', 'NNP', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', 'TO', 'VB', 'DT', 'NN', '.']",16
text_summarization,3,168,"The initial learning rate and the accumulator value were set to 0.15 and 0.1 , respectively .","['The', 'initial', 'learning', 'rate', 'and', 'the', 'accumulator', 'value', 'were', 'set', 'to', '0.15', 'and', '0.1', ',', 'respectively', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NN', 'CC', 'DT', 'NN', 'NN', 'VBD', 'VBN', 'TO', 'CD', 'CC', 'CD', ',', 'RB', '.']",17
text_summarization,3,169,We used gradient clipping with a maximum gradient norm of 2 .,"['We', 'used', 'gradient', 'clipping', 'with', 'a', 'maximum', 'gradient', 'norm', 'of', '2', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBD', 'JJ', 'VBG', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'CD', '.']",12
text_summarization,3,173,"We trained our concept pointer generator for 450 k iterations yielded the best performance , then took the optimization using RL rewards for RG - L at 95 K iterations on DUC - 2004 and at 50 K iterations on Gigaword .","['We', 'trained', 'our', 'concept', 'pointer', 'generator', 'for', '450', 'k', 'iterations', 'yielded', 'the', 'best', 'performance', ',', 'then', 'took', 'the', 'optimization', 'using', 'RL', 'rewards', 'for', 'RG', '-', 'L', 'at', '95', 'K', 'iterations', 'on', 'DUC', '-', '2004', 'and', 'at', '50', 'K', 'iterations', 'on', 'Gigaword', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBD', 'PRP$', 'NN', 'NN', 'NN', 'IN', 'CD', 'NN', 'NNS', 'VBD', 'DT', 'JJS', 'NN', ',', 'RB', 'VBD', 'DT', 'NN', 'VBG', 'NNP', 'NNS', 'IN', 'NNP', ':', 'NN', 'IN', 'CD', 'NNP', 'NNS', 'IN', 'NNP', ':', 'CD', 'CC', 'IN', 'CD', 'NNP', 'NNS', 'IN', 'NNP', '.']",42
text_summarization,3,174,We took the distancesupervised training at 5 K iterations on DUC - 2004 and at 6.5 K iterations on Gigaword .,"['We', 'took', 'the', 'distancesupervised', 'training', 'at', '5', 'K', 'iterations', 'on', 'DUC', '-', '2004', 'and', 'at', '6.5', 'K', 'iterations', 'on', 'Gigaword', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NNP', 'NNS', 'IN', 'NNP', ':', 'CD', 'CC', 'IN', 'CD', 'NNP', 'NNS', 'IN', 'NNP', '.']",21
text_summarization,3,177,ABS + is a tuned ABS model with additional features .,"['ABS', '+', 'is', 'a', 'tuned', 'ABS', 'model', 'with', 'additional', 'features', '.']","['B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NNP', 'NN', 'IN', 'JJ', 'NNS', '.']",11
text_summarization,3,179,RAS - Elman ) is a convolution encoder and an Elman RNN decoder with attention .,"['RAS', '-', 'Elman', ')', 'is', 'a', 'convolution', 'encoder', 'and', 'an', 'Elman', 'RNN', 'decoder', 'with', 'attention', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', ':', 'NN', ')', 'VBZ', 'DT', 'NN', 'NN', 'CC', 'DT', 'NNP', 'NNP', 'NN', 'IN', 'NN', '.']",16
text_summarization,3,180,Seq2seq + att is two - layer BiLSTM encoder and one - layer LSTM decoder equipped with attention .,"['Seq2seq', '+', 'att', 'is', 'two', '-', 'layer', 'BiLSTM', 'encoder', 'and', 'one', '-', 'layer', 'LSTM', 'decoder', 'equipped', 'with', 'attention', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['NNP', 'NNP', 'NN', 'VBZ', 'CD', ':', 'NN', 'NNP', 'NN', 'CC', 'CD', ':', 'NN', 'NNP', 'NN', 'VBD', 'IN', 'NN', '.']",19
text_summarization,3,181,lvt5 k - lsent uses temporal attention to keep track of the past attentive weights of the decoder and restrains the repetition in later sequences .,"['lvt5', 'k', '-', 'lsent', 'uses', 'temporal', 'attention', 'to', 'keep', 'track', 'of', 'the', 'past', 'attentive', 'weights', 'of', 'the', 'decoder', 'and', 'restrains', 'the', 'repetition', 'in', 'later', 'sequences', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['NN', 'SYM', ':', 'NN', 'VBZ', 'JJ', 'NN', 'TO', 'VB', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CC', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NNS', '.']",26
text_summarization,3,182,SEASS includes an additional selective gate to control information flow from the encoder to the decoder .,"['SEASS', 'includes', 'an', 'additional', 'selective', 'gate', 'to', 'control', 'information', 'flow', 'from', 'the', 'encoder', 'to', 'the', 'decoder', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'NN', 'NN', 'IN', 'DT', 'NN', 'TO', 'DT', 'NN', '.']",17
text_summarization,3,183,Pointer - generator is an integrated pointer network and seq2seq model .,"['Pointer', '-', 'generator', 'is', 'an', 'integrated', 'pointer', 'network', 'and', 'seq2seq', 'model', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['NNP', ':', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'CC', 'NN', 'NN', '.']",12
text_summarization,3,186,CGU ) sets a convolutional gated unit and self - attention for global encoding .,"['CGU', ')', 'sets', 'a', 'convolutional', 'gated', 'unit', 'and', 'self', '-', 'attention', 'for', 'global', 'encoding', '.']","['B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ')', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'CC', 'PRP', ':', 'NN', 'IN', 'JJ', 'NN', '.']",15
text_summarization,3,191,We observe that our model outperformed all the strong state of - the - art models on both datasets in all metrics except for RG - 2 on Gigaword .,"['We', 'observe', 'that', 'our', 'model', 'outperformed', 'all', 'the', 'strong', 'state', 'of', '-', 'the', '-', 'art', 'models', 'on', 'both', 'datasets', 'in', 'all', 'metrics', 'except', 'for', 'RG', '-', '2', 'on', 'Gigaword', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'PRP$', 'NN', 'VBD', 'PDT', 'DT', 'JJ', 'NN', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'IN', 'NNP', ':', 'CD', 'IN', 'NNP', '.']",30
text_summarization,3,192,"In terms of the pointer generator performance , the improvements made by our concept pointer are statistically significant ( p < 0.01 ) across all metrics .","['In', 'terms', 'of', 'the', 'pointer', 'generator', 'performance', ',', 'the', 'improvements', 'made', 'by', 'our', 'concept', 'pointer', 'are', 'statistically', 'significant', '(', 'p', '<', '0.01', ')', 'across', 'all', 'metrics', '.']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'NN', ',', 'DT', 'NNS', 'VBN', 'IN', 'PRP$', 'NN', 'NN', 'VBP', 'RB', 'JJ', '(', 'JJ', 'NNP', 'CD', ')', 'IN', 'DT', 'NNS', '.']",27
text_summarization,12,2,Selective Encoding for Abstractive Sentence Summarization,"['Selective', 'Encoding', 'for', 'Abstractive', 'Sentence', 'Summarization']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['JJ', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",6
text_summarization,12,8,"The second level representation is tailored for sentence summarization task , which leads to better performance .","['The', 'second', 'level', 'representation', 'is', 'tailored', 'for', 'sentence', 'summarization', 'task', ',', 'which', 'leads', 'to', 'better', 'performance', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'NN', 'NN', 'NN', ',', 'WDT', 'VBZ', 'TO', 'JJR', 'NN', '.']",17
text_summarization,12,35,In this paper we propose Selective Encoding for Abstractive Sentence Summarization ( SEASS ) .,"['In', 'this', 'paper', 'we', 'propose', 'Selective', 'Encoding', 'for', 'Abstractive', 'Sentence', 'Summarization', '(', 'SEASS', ')', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'JJ', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', '.']",15
text_summarization,12,36,"We treat the sentence summarization as a threephase task : encoding , selection , and decoding .","['We', 'treat', 'the', 'sentence', 'summarization', 'as', 'a', 'threephase', 'task', ':', 'encoding', ',', 'selection', ',', 'and', 'decoding', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', ':', 'NN', ',', 'NN', ',', 'CC', 'NN', '.']",17
text_summarization,12,37,"It consists of a sentence encoder , a selective gate network , and a summary decoder .","['It', 'consists', 'of', 'a', 'sentence', 'encoder', ',', 'a', 'selective', 'gate', 'network', ',', 'and', 'a', 'summary', 'decoder', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBZ', 'IN', 'DT', 'NN', 'NN', ',', 'DT', 'JJ', 'NN', 'NN', ',', 'CC', 'DT', 'JJ', 'NN', '.']",17
text_summarization,12,38,"First , the sentence encoder reads the input words through an RNN unit to construct the first level sentence representation .","['First', ',', 'the', 'sentence', 'encoder', 'reads', 'the', 'input', 'words', 'through', 'an', 'RNN', 'unit', 'to', 'construct', 'the', 'first', 'level', 'sentence', 'representation', '.']","['B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'NN', 'VBZ', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', 'NN', '.']",21
text_summarization,12,39,Then the selective gate network selects the encoded information to construct the second level sentence representation .,"['Then', 'the', 'selective', 'gate', 'network', 'selects', 'the', 'encoded', 'information', 'to', 'construct', 'the', 'second', 'level', 'sentence', 'representation', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', 'NN', '.']",17
text_summarization,12,40,"The selective mechanism controls the information flow from encoder to decoder by applying a gate network according to the sentence information , which helps improve encoding effectiveness and release the burden of the decoder .","['The', 'selective', 'mechanism', 'controls', 'the', 'information', 'flow', 'from', 'encoder', 'to', 'decoder', 'by', 'applying', 'a', 'gate', 'network', 'according', 'to', 'the', 'sentence', 'information', ',', 'which', 'helps', 'improve', 'encoding', 'effectiveness', 'and', 'release', 'the', 'burden', 'of', 'the', 'decoder', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'NN', 'TO', 'VB', 'IN', 'VBG', 'DT', 'NN', 'NN', 'VBG', 'TO', 'DT', 'NN', 'NN', ',', 'WDT', 'VBZ', 'VB', 'VBG', 'NN', 'CC', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",35
text_summarization,12,41,"Finally , the attention - equipped decoder generates the summary using the second level sentence representation .","['Finally', ',', 'the', 'attention', '-', 'equipped', 'decoder', 'generates', 'the', 'summary', 'using', 'the', 'second', 'level', 'sentence', 'representation', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', ':', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'NN', '.']",17
text_summarization,12,164,We initialize model parameters randomly using a Gaussian distribution with Xavier scheme .,"['We', 'initialize', 'model', 'parameters', 'randomly', 'using', 'a', 'Gaussian', 'distribution', 'with', 'Xavier', 'scheme', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NNS', 'RB', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NN', '.']",13
text_summarization,12,165,We use Adam as our optimizing algorithm .,"['We', 'use', 'Adam', 'as', 'our', 'optimizing', 'algorithm', '.']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', 'IN', 'PRP$', 'VBG', 'NN', '.']",8
text_summarization,12,166,"For the hyperparameters of Adam optimizer , we set the learning rate ? = 0.001 , two momentum parameters ? 1 = 0.9 and ? 2 = 0.999 respectively , and = 10 ?8 .","['For', 'the', 'hyperparameters', 'of', 'Adam', 'optimizer', ',', 'we', 'set', 'the', 'learning', 'rate', '?', '=', '0.001', ',', 'two', 'momentum', 'parameters', '?', '1', '=', '0.9', 'and', '?', '2', '=', '0.999', 'respectively', ',', 'and', '=', '10', '?8', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNS', 'IN', 'NNP', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', '.', '$', 'CD', ',', 'CD', 'NN', 'NNS', '.', 'CD', 'JJ', 'CD', 'CC', '.', 'CD', '$', 'CD', 'RB', ',', 'CC', '$', 'CD', 'NN', '.']",35
text_summarization,12,167,"During training , we test the model performance ( ROUGE - 2 F1 ) on development set for every 2,000 batches .","['During', 'training', ',', 'we', 'test', 'the', 'model', 'performance', '(', 'ROUGE', '-', '2', 'F1', ')', 'on', 'development', 'set', 'for', 'every', '2,000', 'batches', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', '(', 'NNP', ':', 'CD', 'NNP', ')', 'IN', 'NN', 'VBN', 'IN', 'DT', 'CD', 'NNS', '.']",22
text_summarization,12,169,"We also apply gradient clipping with range [ ? 5 , 5 ] during training .","['We', 'also', 'apply', 'gradient', 'clipping', 'with', 'range', '[', '?', '5', ',', '5', ']', 'during', 'training', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'RB', 'VBP', 'JJ', 'VBG', 'IN', 'NN', 'NN', '.', 'CD', ',', 'CD', 'NN', 'IN', 'NN', '.']",16
text_summarization,12,170,"To both speedup the training and converge quickly , we use mini-batch size 64 by grid search .","['To', 'both', 'speedup', 'the', 'training', 'and', 'converge', 'quickly', ',', 'we', 'use', 'mini-batch', 'size', '64', 'by', 'grid', 'search', '.']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['TO', 'DT', 'PDT', 'DT', 'NN', 'CC', 'NN', 'RB', ',', 'PRP', 'VBP', 'JJ', 'NN', 'CD', 'IN', 'JJ', 'NN', '.']",18
text_summarization,12,178,"ABS + Based on ABS model , further with two - layer LSTMs for the encoder - decoder with 500 hidden units in each layer implemented in .","['ABS', '+', 'Based', 'on', 'ABS', 'model', ',', 'further', 'with', 'two', '-', 'layer', 'LSTMs', 'for', 'the', 'encoder', '-', 'decoder', 'with', '500', 'hidden', 'units', 'in', 'each', 'layer', 'implemented', 'in', '.']","['B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O']","['NNP', 'NNP', 'VBD', 'IN', 'NNP', 'NN', ',', 'RB', 'IN', 'CD', ':', 'NN', 'NNP', 'IN', 'DT', 'NN', ':', 'NN', 'IN', 'CD', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'VBN', 'IN', '.']",28
text_summarization,12,179,s 2 s+ att,"['s', '2', 's+', 'att']","['B-n', 'I-n', 'I-n', 'I-n']","['RB', 'CD', 'NNS', 'NN']",4
text_summarization,12,180,"We also implement a sequence - to sequence model with attention as our baseline and denote it as "" s2 s + att "" .","['We', 'also', 'implement', 'a', 'sequence', '-', 'to', 'sequence', 'model', 'with', 'attention', 'as', 'our', 'baseline', 'and', 'denote', 'it', 'as', '""', 's2', 's', '+', 'att', '""', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', ':', 'TO', 'VB', 'NN', 'IN', 'NN', 'IN', 'PRP$', 'NN', 'CC', 'VB', 'PRP', 'IN', 'NNP', 'VBD', 'JJ', 'NNP', 'NN', 'NN', '.']",25
text_summarization,12,189,Our SEASS model with beam search outperforms all baseline models by a large margin .,"['Our', 'SEASS', 'model', 'with', 'beam', 'search', 'outperforms', 'all', 'baseline', 'models', 'by', 'a', 'large', 'margin', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NNP', 'NN', 'IN', 'JJ', 'NN', 'VBZ', 'DT', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",15
text_summarization,12,190,"Even for greedy search , our model still performs better than other methods which used beam search .","['Even', 'for', 'greedy', 'search', ',', 'our', 'model', 'still', 'performs', 'better', 'than', 'other', 'methods', 'which', 'used', 'beam', 'search', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['RB', 'IN', 'NN', 'NN', ',', 'PRP$', 'NN', 'RB', 'VBZ', 'JJR', 'IN', 'JJ', 'NNS', 'WDT', 'VBD', 'NN', 'NN', '.']",18
text_summarization,12,191,"For the popular ROUGE - 2 metric , our SEASS model achieves 17.54 F1 score and performs better than the previous works .","['For', 'the', 'popular', 'ROUGE', '-', '2', 'metric', ',', 'our', 'SEASS', 'model', 'achieves', '17.54', 'F1', 'score', 'and', 'performs', 'better', 'than', 'the', 'previous', 'works', '.']","['B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NNP', ':', 'CD', 'JJ', ',', 'PRP$', 'NNP', 'NN', 'VBZ', 'CD', 'NNP', 'NN', 'CC', 'NNS', 'JJR', 'IN', 'DT', 'JJ', 'NNS', '.']",23
text_summarization,12,192,"Compared to the ABS model , our model has a 6.22 ROUGE - 2 F1 relative gain .","['Compared', 'to', 'the', 'ABS', 'model', ',', 'our', 'model', 'has', 'a', '6.22', 'ROUGE', '-', '2', 'F1', 'relative', 'gain', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'TO', 'DT', 'NNP', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'CD', 'NNP', ':', 'CD', 'NNP', 'JJ', 'NN', '.']",18
text_summarization,12,193,"Compared to the highest CAs 2s baseline , our model achieves 1.57 ROUGE - 2 F1 improvement and passes the significant test according to the official ROUGE script .","['Compared', 'to', 'the', 'highest', 'CAs', '2s', 'baseline', ',', 'our', 'model', 'achieves', '1.57', 'ROUGE', '-', '2', 'F1', 'improvement', 'and', 'passes', 'the', 'significant', 'test', 'according', 'to', 'the', 'official', 'ROUGE', 'script', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['VBN', 'TO', 'DT', 'JJS', 'JJ', 'CD', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'CD', 'NNP', ':', 'CD', 'NNP', 'NN', 'CC', 'VBZ', 'DT', 'JJ', 'NN', 'VBG', 'TO', 'DT', 'JJ', 'NNP', 'NN', '.']",29
text_summarization,12,196,DUC 2004,"['DUC', '2004']","['B-n', 'I-n']","['NN', 'CD']",2
text_summarization,12,199,"As summarized in , our SEASS outperforms all the baseline methods and achieves 29.21 , 9.56 and 25.51 for ROUGE 1 , 2 and L recall .","['As', 'summarized', 'in', ',', 'our', 'SEASS', 'outperforms', 'all', 'the', 'baseline', 'methods', 'and', 'achieves', '29.21', ',', '9.56', 'and', '25.51', 'for', 'ROUGE', '1', ',', '2', 'and', 'L', 'recall', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'IN', ',', 'PRP$', 'JJ', 'NNS', 'PDT', 'DT', 'NN', 'NNS', 'CC', 'VBZ', 'CD', ',', 'CD', 'CC', 'CD', 'IN', 'NNP', 'CD', ',', 'CD', 'CC', 'NNP', 'NN', '.']",27
text_summarization,12,200,"Compared to the ABS + model which is tuned using DUC 2003 data , our model performs significantly better by 1.07 ROUGE - 2 recall score and is trained only with English Gigaword sentence - summary data without being tuned using DUC data .","['Compared', 'to', 'the', 'ABS', '+', 'model', 'which', 'is', 'tuned', 'using', 'DUC', '2003', 'data', ',', 'our', 'model', 'performs', 'significantly', 'better', 'by', '1.07', 'ROUGE', '-', '2', 'recall', 'score', 'and', 'is', 'trained', 'only', 'with', 'English', 'Gigaword', 'sentence', '-', 'summary', 'data', 'without', 'being', 'tuned', 'using', 'DUC', 'data', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'TO', 'DT', 'NNP', 'NNP', 'NN', 'WDT', 'VBZ', 'VBN', 'VBG', 'NNP', 'CD', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'RB', 'JJR', 'IN', 'CD', 'NNP', ':', 'CD', 'NN', 'NN', 'CC', 'VBZ', 'VBN', 'RB', 'IN', 'NNP', 'NNP', 'NN', ':', 'NN', 'NNS', 'IN', 'VBG', 'VBN', 'VBG', 'NNP', 'NNS', '.']",44
text_summarization,0,2,Abstractive Text Summarization by Incorporating Reader Comments,"['Abstractive', 'Text', 'Summarization', 'by', 'Incorporating', 'Reader', 'Comments']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', 'IN', 'VBG', 'NNP', 'NNS']",7
text_summarization,0,4,"In neural abstractive summarization field , conventional sequence - to - sequence based models often suffer from summarizing the wrong aspect of the document with respect to the main aspect .","['In', 'neural', 'abstractive', 'summarization', 'field', ',', 'conventional', 'sequence', '-', 'to', '-', 'sequence', 'based', 'models', 'often', 'suffer', 'from', 'summarizing', 'the', 'wrong', 'aspect', 'of', 'the', 'document', 'with', 'respect', 'to', 'the', 'main', 'aspect', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', 'JJ', 'NN', 'NN', ',', 'JJ', 'NN', ':', 'TO', ':', 'NN', 'VBN', 'NNS', 'RB', 'VBP', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'TO', 'DT', 'JJ', 'NN', '.']",31
text_summarization,0,5,"To tackle this problem , we propose the task of reader - aware abstractive summary generation , which utilizes the reader comments to help the model produce better summary about the main aspect .","['To', 'tackle', 'this', 'problem', ',', 'we', 'propose', 'the', 'task', 'of', 'reader', '-', 'aware', 'abstractive', 'summary', 'generation', ',', 'which', 'utilizes', 'the', 'reader', 'comments', 'to', 'help', 'the', 'model', 'produce', 'better', 'summary', 'about', 'the', 'main', 'aspect', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', ':', 'JJ', 'JJ', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'NNS', 'TO', 'VB', 'DT', 'NN', 'VBP', 'JJR', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",34
text_summarization,0,6,"Unlike traditional abstractive summarization task , reader - aware summarization confronts two main challenges :","['Unlike', 'traditional', 'abstractive', 'summarization', 'task', ',', 'reader', '-', 'aware', 'summarization', 'confronts', 'two', 'main', 'challenges', ':']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', 'JJ', 'NN', 'NN', ',', 'VB', ':', 'JJ', 'NN', 'NNS', 'CD', 'JJ', 'NNS', ':']",15
text_summarization,0,51,"In this paper , we propose a summarization framework named reader - aware summary generator ( RASG ) that incorporates reader comments to improve the summarization performance .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'summarization', 'framework', 'named', 'reader', '-', 'aware', 'summary', 'generator', '(', 'RASG', ')', 'that', 'incorporates', 'reader', 'comments', 'to', 'improve', 'the', 'summarization', 'performance', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'VBN', 'NN', ':', 'JJ', 'JJ', 'NN', '(', 'NNP', ')', 'WDT', 'VBZ', 'JJR', 'NNS', 'TO', 'VB', 'DT', 'NN', 'NN', '.']",28
text_summarization,0,52,"Specifically , a seq2seq architecture with attention mechanism is employed as the basic summary generator .","['Specifically', ',', 'a', 'seq2seq', 'architecture', 'with', 'attention', 'mechanism', 'is', 'employed', 'as', 'the', 'basic', 'summary', 'generator', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'JJ', 'NN', '.']",16
text_summarization,0,53,"We first calculate alignment between the reader comments words and document words , and this alignment information is regarded as reader attention representing the "" reader focused aspect "" .","['We', 'first', 'calculate', 'alignment', 'between', 'the', 'reader', 'comments', 'words', 'and', 'document', 'words', ',', 'and', 'this', 'alignment', 'information', 'is', 'regarded', 'as', 'reader', 'attention', 'representing', 'the', '""', 'reader', 'focused', 'aspect', '""', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'RB', 'VB', 'NN', 'IN', 'DT', 'NN', 'NNS', 'NNS', 'CC', 'NN', 'NNS', ',', 'CC', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'NN', 'NN', 'VBG', 'DT', 'JJ', 'NN', 'VBD', 'JJ', 'NNP', '.']",30
text_summarization,0,54,"Then , we treat the decoder attention weights as the focused aspect of the generated summary , a.k.a. , "" decoder focused aspect "" .","['Then', ',', 'we', 'treat', 'the', 'decoder', 'attention', 'weights', 'as', 'the', 'focused', 'aspect', 'of', 'the', 'generated', 'summary', ',', 'a.k.a.', ',', '""', 'decoder', 'focused', 'aspect', '""', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'NN', ',', 'JJ', 'NN', 'VBD', 'JJ', 'NNP', '.']",25
text_summarization,0,55,"After each decoding step , a supervisor is designed to measure the distance between the reader focused aspect and the decoder focused aspect .","['After', 'each', 'decoding', 'step', ',', 'a', 'supervisor', 'is', 'designed', 'to', 'measure', 'the', 'distance', 'between', 'the', 'reader', 'focused', 'aspect', 'and', 'the', 'decoder', 'focused', 'aspect', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'VBG', 'NN', ',', 'DT', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBD', 'NN', 'CC', 'DT', 'NN', 'VBD', 'NN', '.']",24
text_summarization,0,57,The training of our framework RASG is conducted in an adversarial way .,"['The', 'training', 'of', 'our', 'framework', 'RASG', 'is', 'conducted', 'in', 'an', 'adversarial', 'way', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'PRP$', 'NN', 'NNP', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",13
text_summarization,0,224,( 1 ) S2S : Sequence - to - sequence framework has been proposed for language generation task .,"['(', '1', ')', 'S2S', ':', 'Sequence', '-', 'to', '-', 'sequence', 'framework', 'has', 'been', 'proposed', 'for', 'language', 'generation', 'task', '.']","['O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'CD', ')', 'NN', ':', 'NNP', ':', 'TO', ':', 'NN', 'NN', 'VBZ', 'VBN', 'VBN', 'IN', 'NN', 'NN', 'NN', '.']",19
text_summarization,0,225,"( 2 ) S2SR : We simply add the reader attention on attention distribution ? t , in each decoding step .","['(', '2', ')', 'S2SR', ':', 'We', 'simply', 'add', 'the', 'reader', 'attention', 'on', 'attention', 'distribution', '?', 't', ',', 'in', 'each', 'decoding', 'step', '.']","['O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['(', 'CD', ')', 'NN', ':', 'PRP', 'RB', 'VB', 'DT', 'JJR', 'NN', 'IN', 'NN', 'NN', '.', 'NN', ',', 'IN', 'DT', 'VBG', 'NN', '.']",22
text_summarization,0,226,"( 3 ) CGU : propose to use the convolutional gated unit to refine the source representation , which achieves the state - of - the - art performance on social media text summarization dataset .","['(', '3', ')', 'CGU', ':', 'propose', 'to', 'use', 'the', 'convolutional', 'gated', 'unit', 'to', 'refine', 'the', 'source', 'representation', ',', 'which', 'achieves', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'on', 'social', 'media', 'text', 'summarization', 'dataset', '.']","['O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'CD', ')', 'NN', ':', 'NN', 'TO', 'VB', 'DT', 'JJ', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'JJ', 'NNS', 'JJ', 'NN', 'NN', '.']",36
text_summarization,0,227,"( 4 ) LEAD1 : LEAD1 is a commonly used baseline , which selects the first sentence of document as the summary .","['(', '4', ')', 'LEAD1', ':', 'LEAD1', 'is', 'a', 'commonly', 'used', 'baseline', ',', 'which', 'selects', 'the', 'first', 'sentence', 'of', 'document', 'as', 'the', 'summary', '.']","['O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'NN', ':', 'NNP', 'VBZ', 'DT', 'RB', 'VBN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'DT', 'NN', '.']",23
text_summarization,0,228,"( 5 ) TextRank : propose to build a graph , then add each sentence as a vertex and use link to represent semantic similarity .","['(', '5', ')', 'TextRank', ':', 'propose', 'to', 'build', 'a', 'graph', ',', 'then', 'add', 'each', 'sentence', 'as', 'a', 'vertex', 'and', 'use', 'link', 'to', 'represent', 'semantic', 'similarity', '.']","['O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['(', 'CD', ')', 'NN', ':', 'NN', 'TO', 'VB', 'DT', 'NN', ',', 'RB', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'CC', 'NN', 'NN', 'TO', 'VB', 'JJ', 'NN', '.']",26
text_summarization,0,231,We implement our experiments in TensorFlow ) on an NVIDIA P40 GPU .,"['We', 'implement', 'our', 'experiments', 'in', 'TensorFlow', ')', 'on', 'an', 'NVIDIA', 'P40', 'GPU', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'NNS', 'IN', 'NNP', ')', 'IN', 'DT', 'NNP', 'NNP', 'NNP', '.']",13
text_summarization,0,232,The word embedding dimension is set to 256 and the number of hidden units is 512 .,"['The', 'word', 'embedding', 'dimension', 'is', 'set', 'to', '256', 'and', 'the', 'number', 'of', 'hidden', 'units', 'is', '512', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O']","['DT', 'NN', 'VBG', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'CC', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'VBZ', 'CD', '.']",17
text_summarization,0,234,We use Adagrad optimizer as our optimizing algorithm .,"['We', 'use', 'Adagrad', 'optimizer', 'as', 'our', 'optimizing', 'algorithm', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'PRP$', 'VBG', 'NN', '.']",9
text_summarization,0,235,We employ beam search with beam size 5 to generate more fluency summary sentence .,"['We', 'employ', 'beam', 'search', 'with', 'beam', 'size', '5', 'to', 'generate', 'more', 'fluency', 'summary', 'sentence', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'NN', 'NN', 'CD', 'TO', 'VB', 'JJR', 'NN', 'JJ', 'NN', '.']",15
text_summarization,0,240,"We see that RASG achieves a 11.0 % , 9.1 % and 6.6 % increment over the state - of - the - art method CGU in terms of ROUGE - 1 , ROUGE - 2 and ROUGE - L respectively .","['We', 'see', 'that', 'RASG', 'achieves', 'a', '11.0', '%', ',', '9.1', '%', 'and', '6.6', '%', 'increment', 'over', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'method', 'CGU', 'in', 'terms', 'of', 'ROUGE', '-', '1', ',', 'ROUGE', '-', '2', 'and', 'ROUGE', '-', 'L', 'respectively', '.']","['O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBP', 'IN', 'NNP', 'VBZ', 'DT', 'CD', 'NN', ',', 'CD', 'NN', 'CC', 'CD', 'NN', 'NN', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'NNP', 'IN', 'NNS', 'IN', 'NNP', ':', 'CD', ',', 'NNP', ':', 'CD', 'CC', 'NNP', ':', 'NNP', 'RB', '.']",42
text_summarization,0,241,It is worth noticing that the baseline model S2SR achieves better performance than S2S which demonstrates the effectiveness of incorporating reader focused aspect in summary generation .,"['It', 'is', 'worth', 'noticing', 'that', 'the', 'baseline', 'model', 'S2SR', 'achieves', 'better', 'performance', 'than', 'S2S', 'which', 'demonstrates', 'the', 'effectiveness', 'of', 'incorporating', 'reader', 'focused', 'aspect', 'in', 'summary', 'generation', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBZ', 'JJ', 'VBG', 'IN', 'DT', 'NN', 'NN', 'NNP', 'VBZ', 'JJR', 'NN', 'IN', 'NNP', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'NN', 'VBD', 'NN', 'IN', 'JJ', 'NN', '.']",27
text_summarization,0,247,The discriminator provides the scalar training signal L g c for generator training and the feature vector F ( m t ) for goal tracker .,"['The', 'discriminator', 'provides', 'the', 'scalar', 'training', 'signal', 'L', 'g', 'c', 'for', 'generator', 'training', 'and', 'the', 'feature', 'vector', 'F', '(', 'm', 't', ')', 'for', 'goal', 'tracker', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'JJ', 'NNP', 'NN', 'NN', 'IN', 'NN', 'NN', 'CC', 'DT', 'NN', 'NN', 'NNP', '(', 'FW', 'NN', ')', 'IN', 'NN', 'NN', '.']",26
text_summarization,0,248,"Consequently , there is an increment of 17.51 % from RASG w / o GTD to RASG w / o GT in terms of ROUGE - L , which demonstrates the effectiveness of discriminator .","['Consequently', ',', 'there', 'is', 'an', 'increment', 'of', '17.51', '%', 'from', 'RASG', 'w', '/', 'o', 'GTD', 'to', 'RASG', 'w', '/', 'o', 'GT', 'in', 'terms', 'of', 'ROUGE', '-', 'L', ',', 'which', 'demonstrates', 'the', 'effectiveness', 'of', 'discriminator', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'EX', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', 'IN', 'NNP', 'NN', 'NNP', 'VBZ', 'NNP', 'TO', 'NNP', 'NN', 'NNP', 'NN', 'NNP', 'IN', 'NNS', 'IN', 'NNP', ':', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'NN', '.']",35
text_summarization,0,249,"As for the effectiveness of goal tracker , compared with RASG and RASG w / o GT , RASG w/ o GTD offers a decrease of 45. 23 % and 17.88 % in terms of ROUGE - 1 , respectively .","['As', 'for', 'the', 'effectiveness', 'of', 'goal', 'tracker', ',', 'compared', 'with', 'RASG', 'and', 'RASG', 'w', '/', 'o', 'GT', ',', 'RASG', 'w/', 'o', 'GTD', 'offers', 'a', 'decrease', 'of', '45.', '23', '%', 'and', '17.88', '%', 'in', 'terms', 'of', 'ROUGE', '-', '1', ',', 'respectively', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['IN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', ',', 'VBN', 'IN', 'NNP', 'CC', 'NNP', 'VBP', 'NNP', 'NN', 'NNP', ',', 'NNP', 'VBZ', 'JJ', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'NNS', 'IN', 'NNP', ':', 'CD', ',', 'RB', '.']",41
text_summarization,0,252,"Finally , RASG w/o DM offers a decrease of 10 . 22 % compared with RASG in terms of ROUGE - L , which demonstrates the effectiveness of denoising module .","['Finally', ',', 'RASG', 'w/o', 'DM', 'offers', 'a', 'decrease', 'of', '10', '.', '22', '%', 'compared', 'with', 'RASG', 'in', 'terms', 'of', 'ROUGE', '-', 'L', ',', 'which', 'demonstrates', 'the', 'effectiveness', 'of', 'denoising', 'module', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'NNP', 'VBZ', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'CD', '.', 'CD', 'NN', 'VBN', 'IN', 'NNP', 'IN', 'NNS', 'IN', 'NNP', ':', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'NN', '.']",31
prosody_prediction,0,2,Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations,"['Predicting', 'Prosodic', 'Prominence', 'from', 'Text', 'with', 'Pre-trained', 'Contextualized', 'Word', 'Representations']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', 'NN', 'IN', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'NNP']",10
prosody_prediction,0,4,In this paper we introduce a new natural language processing dataset and benchmark for predicting prosodic prominence from written text .,"['In', 'this', 'paper', 'we', 'introduce', 'a', 'new', 'natural', 'language', 'processing', 'dataset', 'and', 'benchmark', 'for', 'predicting', 'prosodic', 'prominence', 'from', 'written', 'text', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'NN', 'CC', 'NN', 'IN', 'VBG', 'JJ', 'NN', 'IN', 'VBN', 'NN', '.']",21
prosody_prediction,0,16,"In this paper we introduce a new NLP dataset and benchmark for predicting prosodic prominence from text which is based on the recently published Libri TTS corpus , containing automatically generated prosodic prominence labels for over 260 hours or 2.8 million words of English audio books , read by 1230 different speakers .","['In', 'this', 'paper', 'we', 'introduce', 'a', 'new', 'NLP', 'dataset', 'and', 'benchmark', 'for', 'predicting', 'prosodic', 'prominence', 'from', 'text', 'which', 'is', 'based', 'on', 'the', 'recently', 'published', 'Libri', 'TTS', 'corpus', ',', 'containing', 'automatically', 'generated', 'prosodic', 'prominence', 'labels', 'for', 'over', '260', 'hours', 'or', '2.8', 'million', 'words', 'of', 'English', 'audio', 'books', ',', 'read', 'by', '1230', 'different', 'speakers', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'DT', 'JJ', 'NNP', 'NN', 'CC', 'NN', 'IN', 'VBG', 'JJ', 'NN', 'IN', 'NN', 'WDT', 'VBZ', 'VBN', 'IN', 'DT', 'RB', 'VBN', 'NNP', 'NNP', 'NN', ',', 'VBG', 'RB', 'VBN', 'JJ', 'NN', 'NNS', 'IN', 'IN', 'CD', 'NNS', 'CC', 'CD', 'CD', 'NNS', 'IN', 'NNP', 'NN', 'NNS', ',', 'VBN', 'IN', 'CD', 'JJ', 'NNS', '.']",53
prosody_prediction,0,17,To our knowledge this will be the largest publicly available dataset with prosodic annotations .,"['To', 'our', 'knowledge', 'this', 'will', 'be', 'the', 'largest', 'publicly', 'available', 'dataset', 'with', 'prosodic', 'annotations', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['TO', 'PRP$', 'NN', 'DT', 'MD', 'VB', 'DT', 'JJS', 'RB', 'JJ', 'NN', 'IN', 'JJ', 'NNS', '.']",15
prosody_prediction,0,20,Prosody prediction can be turned into a sequence labeling task by giving each word in a text a discrete prominence value based on the amount of emphasis the speaker gives to the word when reading the text .,"['Prosody', 'prediction', 'can', 'be', 'turned', 'into', 'a', 'sequence', 'labeling', 'task', 'by', 'giving', 'each', 'word', 'in', 'a', 'text', 'a', 'discrete', 'prominence', 'value', 'based', 'on', 'the', 'amount', 'of', 'emphasis', 'the', 'speaker', 'gives', 'to', 'the', 'word', 'when', 'reading', 'the', 'text', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'MD', 'VB', 'VBN', 'IN', 'DT', 'NN', 'VBG', 'NN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'DT', 'JJ', 'NN', 'NN', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NN', 'DT', 'NN', 'VBZ', 'TO', 'DT', 'NN', 'WRB', 'VBG', 'DT', 'NN', '.']",38
prosody_prediction,0,84,We performed experiments with the following models :,"['We', 'performed', 'experiments', 'with', 'the', 'following', 'models', ':']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']","['PRP', 'VBD', 'NNS', 'IN', 'DT', 'JJ', 'NNS', ':']",8
prosody_prediction,0,85,"BERT - base uncased 3 - layer 600D Bidirectional Long Short - Term Memory ( BiLSTM ) ( Hochreiter and Schmidhuber , 1997 ) Minitagger ( SVM ) ) + GloVe MarMoT ( CRF ) Majority class per word","['BERT', '-', 'base', 'uncased', '3', '-', 'layer', '600D', 'Bidirectional', 'Long', 'Short', '-', 'Term', 'Memory', '(', 'BiLSTM', ')', '(', 'Hochreiter', 'and', 'Schmidhuber', ',', '1997', ')', 'Minitagger', '(', 'SVM', ')', ')', '+', 'GloVe', 'MarMoT', '(', 'CRF', ')', 'Majority', 'class', 'per', 'word']","['B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NN', 'VBD', 'CD', ':', 'NN', 'CD', 'NNP', 'NNP', 'NNP', ':', 'NNP', 'NNP', '(', 'NNP', ')', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'NNP', '(', 'NNP', ')', ')', 'FW', 'NNP', 'NNP', '(', 'NNP', ')', 'NNP', 'NN', 'IN', 'NN']",39
prosody_prediction,0,88,"We use the Huggingface PyTorch implementation of BERT available in the pytorch transformers library , 3 which we further fine - tune during training .","['We', 'use', 'the', 'Huggingface', 'PyTorch', 'implementation', 'of', 'BERT', 'available', 'in', 'the', 'pytorch', 'transformers', 'library', ',', '3', 'which', 'we', 'further', 'fine', '-', 'tune', 'during', 'training', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-p', 'I-p', 'I-p', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NNP', 'NN', 'IN', 'NNP', 'JJ', 'IN', 'DT', 'NN', 'NNS', 'VBP', ',', 'CD', 'WDT', 'PRP', 'VBP', 'JJ', ':', 'NN', 'IN', 'NN', '.']",25
prosody_prediction,0,89,"We take the last hidden layer of BERT and train a single fully - connected classifier layer on top of it , mapping the representation of each word to the labels .","['We', 'take', 'the', 'last', 'hidden', 'layer', 'of', 'BERT', 'and', 'train', 'a', 'single', 'fully', '-', 'connected', 'classifier', 'layer', 'on', 'top', 'of', 'it', ',', 'mapping', 'the', 'representation', 'of', 'each', 'word', 'to', 'the', 'labels', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'NNP', 'CC', 'VB', 'DT', 'JJ', 'RB', ':', 'VBN', 'JJR', 'NN', 'IN', 'NN', 'IN', 'PRP', ',', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'TO', 'DT', 'NNS', '.']",32
prosody_prediction,0,90,For our experiments we use the smaller BERT - base model using the uncased alternative .,"['For', 'our', 'experiments', 'we', 'use', 'the', 'smaller', 'BERT', '-', 'base', 'model', 'using', 'the', 'uncased', 'alternative', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'PRP$', 'NNS', 'PRP', 'VBP', 'DT', 'JJR', 'NNP', ':', 'NN', 'NN', 'VBG', 'DT', 'JJ', 'NN', '.']",16
prosody_prediction,0,91,We use a batch size of 32 and fine - tune the model for 2 epochs .,"['We', 'use', 'a', 'batch', 'size', 'of', '32', 'and', 'fine', '-', 'tune', 'the', 'model', 'for', '2', 'epochs', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CD', 'CC', 'JJ', ':', 'NN', 'DT', 'NN', 'IN', 'CD', 'NNS', '.']",17
prosody_prediction,0,92,For BiLSTM we use pre-trained 300D Glo Ve 840B word embeddings .,"['For', 'BiLSTM', 'we', 'use', 'pre-trained', '300D', 'Glo', 'Ve', '840B', 'word', 'embeddings', '.']","['B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', 'PRP', 'VBP', 'JJ', 'CD', 'NNP', 'NNP', 'CD', 'NN', 'NNS', '.']",12
prosody_prediction,0,94,"As with BERT , we add one fullyconnected classifier layer on top of the BiLSTM , mapping the representation of each word to the labels .","['As', 'with', 'BERT', ',', 'we', 'add', 'one', 'fullyconnected', 'classifier', 'layer', 'on', 'top', 'of', 'the', 'BiLSTM', ',', 'mapping', 'the', 'representation', 'of', 'each', 'word', 'to', 'the', 'labels', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['IN', 'IN', 'NNP', ',', 'PRP', 'VBP', 'CD', 'VBN', 'JJR', 'NN', 'IN', 'NN', 'IN', 'DT', 'NNP', ',', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'TO', 'DT', 'NNS', '.']",26
prosody_prediction,0,95,We use a dropout of 0.2 between the layers of the BiLSTM .,"['We', 'use', 'a', 'dropout', 'of', '0.2', 'between', 'the', 'layers', 'of', 'the', 'BiLSTM', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'CD', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNP', '.']",13
prosody_prediction,0,97,"For the SVM we use Minitagger 4 implementation by using each dimension of the pre-trained 300D Glo Ve 840B word embeddings as features , with context - size 1 , i.e. including the previous and the next word in the context .","['For', 'the', 'SVM', 'we', 'use', 'Minitagger', '4', 'implementation', 'by', 'using', 'each', 'dimension', 'of', 'the', 'pre-trained', '300D', 'Glo', 'Ve', '840B', 'word', 'embeddings', 'as', 'features', ',', 'with', 'context', '-', 'size', '1', ',', 'i.e.', 'including', 'the', 'previous', 'and', 'the', 'next', 'word', 'in', 'the', 'context', '.']","['O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNP', 'PRP', 'VBP', 'JJR', 'CD', 'NN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'JJ', 'CD', 'NNP', 'NNP', 'CD', 'NN', 'NNS', 'IN', 'NNS', ',', 'IN', 'JJ', ':', 'NN', 'CD', ',', 'FW', 'VBG', 'DT', 'JJ', 'CC', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",42
prosody_prediction,0,98,For the conditional random field ( CRF ) model we use MarMot 5 by with the default configuration .,"['For', 'the', 'conditional', 'random', 'field', '(', 'CRF', ')', 'model', 'we', 'use', 'MarMot', '5', 'by', 'with', 'the', 'default', 'configuration', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'NN', 'PRP', 'VBP', 'JJ', 'CD', 'IN', 'IN', 'DT', 'NN', 'NN', '.']",19
prosody_prediction,0,101,All systems except the Minitagger and CRF are our implementations using PyTorch and are made available on GitHub : https://github.com/Helsinki - NLP / prosody .,"['All', 'systems', 'except', 'the', 'Minitagger', 'and', 'CRF', 'are', 'our', 'implementations', 'using', 'PyTorch', 'and', 'are', 'made', 'available', 'on', 'GitHub', ':', 'https://github.com/Helsinki', '-', 'NLP', '/', 'prosody', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'VBP', 'PRP$', 'NNS', 'VBG', 'NNP', 'CC', 'VBP', 'VBN', 'JJ', 'IN', 'NNP', ':', 'NN', ':', 'NNP', 'NNP', 'NN', '.']",25
prosody_prediction,0,106,All models reach over 80 % in the 2 - way classification task while 3 - way classification accuracy stays below 70 % for all of them .,"['All', 'models', 'reach', 'over', '80', '%', 'in', 'the', '2', '-', 'way', 'classification', 'task', 'while', '3', '-', 'way', 'classification', 'accuracy', 'stays', 'below', '70', '%', 'for', 'all', 'of', 'them', '.']","['B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'VBP', 'IN', 'CD', 'NN', 'IN', 'DT', 'CD', ':', 'NN', 'NN', 'NN', 'IN', 'CD', ':', 'NN', 'NN', 'NN', 'VBZ', 'IN', 'CD', 'NN', 'IN', 'DT', 'IN', 'PRP', '.']",28
prosody_prediction,0,107,"The BERTbased model gets the highest accuracy of 83.2 % and 68.6 % in the 2 - way and 3 - way classification tasks , respectively , demonstrating the value of a pytorch - transformers 4 https://github.com/karlstratos/","['The', 'BERTbased', 'model', 'gets', 'the', 'highest', 'accuracy', 'of', '83.2', '%', 'and', '68.6', '%', 'in', 'the', '2', '-', 'way', 'and', '3', '-', 'way', 'classification', 'tasks', ',', 'respectively', ',', 'demonstrating', 'the', 'value', 'of', 'a', 'pytorch', '-', 'transformers', '4', 'https://github.com/karlstratos/']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'DT', 'CD', ':', 'NN', 'CC', 'CD', ':', 'NN', 'NN', 'NNS', ',', 'RB', ',', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', ':', 'NNS', 'CD', 'NN']",37
prosody_prediction,0,111,The 3layer BiLSTM achieves 82.1 % in the 2 - way classification and 66.4 % in the 3 - way classification task .,"['The', '3layer', 'BiLSTM', 'achieves', '82.1', '%', 'in', 'the', '2', '-', 'way', 'classification', 'and', '66.4', '%', 'in', 'the', '3', '-', 'way', 'classification', 'task', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'CD', 'NNP', 'VBZ', 'CD', 'NN', 'IN', 'DT', 'CD', ':', 'NN', 'NN', 'CC', 'CD', 'NN', 'IN', 'DT', 'CD', ':', 'NN', 'NN', 'NN', '.']",23
prosody_prediction,0,112,"The traditional feature - based classifiers perform slightly below the neural network models , with the CRF obtaining 81.8 % and 66.4 % for the two classification tasks , respectively .","['The', 'traditional', 'feature', '-', 'based', 'classifiers', 'perform', 'slightly', 'below', 'the', 'neural', 'network', 'models', ',', 'with', 'the', 'CRF', 'obtaining', '81.8', '%', 'and', '66.4', '%', 'for', 'the', 'two', 'classification', 'tasks', ',', 'respectively', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['DT', 'JJ', 'NN', ':', 'VBN', 'NNS', 'VBP', 'RB', 'IN', 'DT', 'JJ', 'NN', 'NNS', ',', 'IN', 'DT', 'NNP', 'VBG', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'DT', 'CD', 'NN', 'NNS', ',', 'RB', '.']",31
prosody_prediction,0,113,The Minitagger SVM model 's test accuracies are slightly lower than the CRF 's with 80.8 % and 65.4 % test accuracies .,"['The', 'Minitagger', 'SVM', 'model', ""'s"", 'test', 'accuracies', 'are', 'slightly', 'lower', 'than', 'the', 'CRF', ""'s"", 'with', '80.8', '%', 'and', '65.4', '%', 'test', 'accuracies', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'NNP', 'NN', 'POS', 'NN', 'NNS', 'VBP', 'RB', 'JJR', 'IN', 'DT', 'NNP', 'POS', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'NN', 'NNS', '.']",23
prosody_prediction,0,114,Finally taking a simple majority class per word gives 80.2 % for the 2 - way classification task and 62.4 % for the 3 - way classification task .,"['Finally', 'taking', 'a', 'simple', 'majority', 'class', 'per', 'word', 'gives', '80.2', '%', 'for', 'the', '2', '-', 'way', 'classification', 'task', 'and', '62.4', '%', 'for', 'the', '3', '-', 'way', 'classification', 'task', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NN', 'VBZ', 'CD', 'NN', 'IN', 'DT', 'CD', ':', 'NN', 'NN', 'NN', 'CC', 'CD', 'NN', 'IN', 'DT', 'CD', ':', 'NN', 'NN', 'NN', '.']",29
prosody_prediction,0,121,For most of the models the biggest improvement in performance is achieved when moving from 1 % of the training examples to 5 % .,"['For', 'most', 'of', 'the', 'models', 'the', 'biggest', 'improvement', 'in', 'performance', 'is', 'achieved', 'when', 'moving', 'from', '1', '%', 'of', 'the', 'training', 'examples', 'to', '5', '%', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'JJS', 'IN', 'DT', 'NNS', 'DT', 'JJS', 'NN', 'IN', 'NN', 'VBZ', 'VBN', 'WRB', 'VBG', 'IN', 'CD', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'TO', 'CD', 'NN', '.']",25
prosody_prediction,0,122,All models have reached close to their full predictive capacity with only 10 % of the training examples .,"['All', 'models', 'have', 'reached', 'close', 'to', 'their', 'full', 'predictive', 'capacity', 'with', 'only', '10', '%', 'of', 'the', 'training', 'examples', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'RB', 'TO', 'PRP$', 'JJ', 'JJ', 'NN', 'IN', 'RB', 'CD', 'NN', 'IN', 'DT', 'NN', 'NNS', '.']",19
prosody_prediction,0,127,"As the proposed dataset has been automatically generated as described in Section 3 , we also tested the best two models , BERT and BiLSTM , with a manually annotated test set from The Boston University radio news corpus . :","['As', 'the', 'proposed', 'dataset', 'has', 'been', 'automatically', 'generated', 'as', 'described', 'in', 'Section', '3', ',', 'we', 'also', 'tested', 'the', 'best', 'two', 'models', ',', 'BERT', 'and', 'BiLSTM', ',', 'with', 'a', 'manually', 'annotated', 'test', 'set', 'from', 'The', 'Boston', 'University', 'radio', 'news', 'corpus', '.', ':']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'DT', 'VBN', 'NN', 'VBZ', 'VBN', 'RB', 'VBN', 'IN', 'VBN', 'IN', 'NNP', 'CD', ',', 'PRP', 'RB', 'VBD', 'DT', 'JJS', 'CD', 'NNS', ',', 'NNP', 'CC', 'NNP', ',', 'IN', 'DT', 'RB', 'JJ', 'NN', 'VBN', 'IN', 'DT', 'NNP', 'NNP', 'NN', 'NN', 'NN', '.', ':']",41
prosody_prediction,0,131,The good results 6 from this experiment provide further support for the quality of the new dataset .,"['The', 'good', 'results', '6', 'from', 'this', 'experiment', 'provide', 'further', 'support', 'for', 'the', 'quality', 'of', 'the', 'new', 'dataset', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NNS', 'CD', 'IN', 'DT', 'NN', 'VB', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",18
prosody_prediction,0,132,Notice also that the difference between BERT and BiLSTM is much bigger with this test set ( + 3.9 % compared to + 1.1 % ) .,"['Notice', 'also', 'that', 'the', 'difference', 'between', 'BERT', 'and', 'BiLSTM', 'is', 'much', 'bigger', 'with', 'this', 'test', 'set', '(', '+', '3.9', '%', 'compared', 'to', '+', '1.1', '%', ')', '.']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'RB', 'IN', 'DT', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'VBZ', 'RB', 'JJR', 'IN', 'DT', 'NN', 'NN', '(', 'JJ', 'CD', 'NN', 'VBN', 'TO', 'VB', 'CD', 'NN', ')', '.']",27
sentiment_analysis,16,2,Target - Sensitive Memory Networks for Aspect Sentiment Classification,"['Target', '-', 'Sensitive', 'Memory', 'Networks', 'for', 'Aspect', 'Sentiment', 'Classification']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', ':', 'JJ', 'NN', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",9
sentiment_analysis,16,4,Aspect sentiment classification ( ASC ) is a fundamental task in sentiment analysis .,"['Aspect', 'sentiment', 'classification', '(', 'ASC', ')', 'is', 'a', 'fundamental', 'task', 'in', 'sentiment', 'analysis', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NN', '.']",14
sentiment_analysis,16,8,"However , we found an important problem with the current MNs in performing the ASC task .","['However', ',', 'we', 'found', 'an', 'important', 'problem', 'with', 'the', 'current', 'MNs', 'in', 'performing', 'the', 'ASC', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O']","['RB', ',', 'PRP', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'IN', 'VBG', 'DT', 'NNP', 'NN', '.']",17
sentiment_analysis,16,51,"To address this problem , we propose target - sensitive memory networks ( TMNs ) , which can capture the sentiment interaction between targets and contexts .","['To', 'address', 'this', 'problem', ',', 'we', 'propose', 'target', '-', 'sensitive', 'memory', 'networks', '(', 'TMNs', ')', ',', 'which', 'can', 'capture', 'the', 'sentiment', 'interaction', 'between', 'targets', 'and', 'contexts', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'SYM', ':', 'JJ', 'NN', 'NNS', '(', 'NNP', ')', ',', 'WDT', 'MD', 'VB', 'DT', 'NN', 'NN', 'IN', 'NNS', 'CC', 'NN', '.']",27
sentiment_analysis,16,229,AMN : A state - of - the - art memory network used for ASC .,"['AMN', ':', 'A', 'state', '-', 'of', '-', 'the', '-', 'art', 'memory', 'network', 'used', 'for', 'ASC', '.']","['B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['NN', ':', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'NN', 'VBN', 'IN', 'NNP', '.']",16
sentiment_analysis,16,231,"BL - MN : Our basic memory network presented in Section 2 , which does not use the proposed techniques for capturing target - sensitive sentiments .","['BL', '-', 'MN', ':', 'Our', 'basic', 'memory', 'network', 'presented', 'in', 'Section', '2', ',', 'which', 'does', 'not', 'use', 'the', 'proposed', 'techniques', 'for', 'capturing', 'target', '-', 'sensitive', 'sentiments', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', ':', 'PRP$', 'JJ', 'NN', 'NN', 'VBN', 'IN', 'NNP', 'CD', ',', 'WDT', 'VBZ', 'RB', 'VB', 'DT', 'VBN', 'NNS', 'IN', 'VBG', 'NN', ':', 'JJ', 'NNS', '.']",27
sentiment_analysis,16,232,AE - LSTM : RNN / LSTM is another popular attention based neural model .,"['AE', '-', 'LSTM', ':', 'RNN', '/', 'LSTM', 'is', 'another', 'popular', 'attention', 'based', 'neural', 'model', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', ':', 'NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'VBN', 'JJ', 'NN', '.']",15
sentiment_analysis,16,233,"Here we compare with a state - of - the - art attention - based LSTM for ASC , AE - LSTM .","['Here', 'we', 'compare', 'with', 'a', 'state', '-', 'of', '-', 'the', '-', 'art', 'attention', '-', 'based', 'LSTM', 'for', 'ASC', ',', 'AE', '-', 'LSTM', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['RB', 'PRP', 'VBP', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', ':', 'VBN', 'NNP', 'IN', 'NNP', ',', 'NNP', ':', 'NN', '.']",23
sentiment_analysis,16,234,ATAE - LSTM :,"['ATAE', '-', 'LSTM', ':']","['B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', ':']",4
sentiment_analysis,16,235,Another attention - based LSTM for ASC reported in .,"['Another', 'attention', '-', 'based', 'LSTM', 'for', 'ASC', 'reported', 'in', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O']","['DT', 'NN', ':', 'VBN', 'NNP', 'IN', 'NNP', 'VBD', 'IN', '.']",10
sentiment_analysis,16,236,Target - sensitive Memory Networks ( TMNs ) :,"['Target', '-', 'sensitive', 'Memory', 'Networks', '(', 'TMNs', ')', ':']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'JJ', 'NN', 'NNP', '(', 'NNP', ')', ':']",9
sentiment_analysis,16,237,"The six proposed techniques , NP , CNP , IT , CI , JCI , and JPI give six target - sensitive memory networks .","['The', 'six', 'proposed', 'techniques', ',', 'NP', ',', 'CNP', ',', 'IT', ',', 'CI', ',', 'JCI', ',', 'and', 'JPI', 'give', 'six', 'target', '-', 'sensitive', 'memory', 'networks', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'CD', 'VBD', 'NNS', ',', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', 'VBP', 'CD', 'NN', ':', 'JJ', 'NN', 'NNS', '.']",25
sentiment_analysis,16,247,We use the open - domain word embeddings 1 for the initialization of word vectors .,"['We', 'use', 'the', 'open', '-', 'domain', 'word', 'embeddings', '1', 'for', 'the', 'initialization', 'of', 'word', 'vectors', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', ':', 'NN', 'NN', 'VBZ', 'CD', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', '.']",16
sentiment_analysis,16,248,"We initialize other model parameters from a uniform distribution U ( - 0.05 , 0.05 ) .","['We', 'initialize', 'other', 'model', 'parameters', 'from', 'a', 'uniform', 'distribution', 'U', '(', '-', '0.05', ',', '0.05', ')', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NNP', '(', ':', 'CD', ',', 'CD', ')', '.']",17
sentiment_analysis,16,249,The dimension of the word embedding and the size of the hidden layers are 300 .,"['The', 'dimension', 'of', 'the', 'word', 'embedding', 'and', 'the', 'size', 'of', 'the', 'hidden', 'layers', 'are', '300', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'VBP', 'CD', '.']",16
sentiment_analysis,16,250,The learning rate is set to 0.01 and the dropout rate is set to 0.1 .,"['The', 'learning', 'rate', 'is', 'set', 'to', '0.01', 'and', 'the', 'dropout', 'rate', 'is', 'set', 'to', '0.1', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'CC', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",16
sentiment_analysis,16,251,Stochastic gradient descent is used as our optimizer .,"['Stochastic', 'gradient', 'descent', 'is', 'used', 'as', 'our', 'optimizer', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['JJ', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'PRP$', 'NN', '.']",9
sentiment_analysis,16,253,"We also compare the memory networks in their multiple computational layers version ( i.e. , multiple hops ) and the number of hops is set to 3 as used in the mentioned previous studies .","['We', 'also', 'compare', 'the', 'memory', 'networks', 'in', 'their', 'multiple', 'computational', 'layers', 'version', '(', 'i.e.', ',', 'multiple', 'hops', ')', 'and', 'the', 'number', 'of', 'hops', 'is', 'set', 'to', '3', 'as', 'used', 'in', 'the', 'mentioned', 'previous', 'studies', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', 'NNS', 'IN', 'PRP$', 'JJ', 'JJ', 'NNS', 'NN', '(', 'FW', ',', 'JJ', 'NNS', ')', 'CC', 'DT', 'NN', 'IN', 'NNS', 'VBZ', 'VBN', 'TO', 'CD', 'IN', 'VBN', 'IN', 'DT', 'VBN', 'JJ', 'NNS', '.']",35
sentiment_analysis,16,254,"We implemented all models in the TensorFlow environment using same input , embedding size , dropout rate , optimizer , etc.","['We', 'implemented', 'all', 'models', 'in', 'the', 'TensorFlow', 'environment', 'using', 'same', 'input', ',', 'embedding', 'size', ',', 'dropout', 'rate', ',', 'optimizer', ',', 'etc.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['PRP', 'VBD', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'VBG', 'JJ', 'NN', ',', 'VBG', 'NN', ',', 'NN', 'NN', ',', 'NN', ',', 'NN']",21
sentiment_analysis,16,265,"Comparing the 1 - hop memory networks ( first nine rows ) , we see significant performance gains achieved by CNP , CI , JCI , and JPI on both datasets , where each of them has p < 0.01 over the strongest baseline ( BL - MN ) from paired t- test using F1 - Macro .","['Comparing', 'the', '1', '-', 'hop', 'memory', 'networks', '(', 'first', 'nine', 'rows', ')', ',', 'we', 'see', 'significant', 'performance', 'gains', 'achieved', 'by', 'CNP', ',', 'CI', ',', 'JCI', ',', 'and', 'JPI', 'on', 'both', 'datasets', ',', 'where', 'each', 'of', 'them', 'has', 'p', '<', '0.01', 'over', 'the', 'strongest', 'baseline', '(', 'BL', '-', 'MN', ')', 'from', 'paired', 't-', 'test', 'using', 'F1', '-', 'Macro', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['VBG', 'DT', 'CD', ':', 'NN', 'NN', 'NNS', '(', 'JJ', 'CD', 'NNS', ')', ',', 'PRP', 'VBP', 'JJ', 'NN', 'NNS', 'VBN', 'IN', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', 'IN', 'DT', 'NNS', ',', 'WRB', 'DT', 'IN', 'PRP', 'VBZ', 'VBN', 'RB', 'CD', 'IN', 'DT', 'JJS', 'NN', '(', 'NNP', ':', 'NN', ')', 'IN', 'VBN', 'JJ', 'NN', 'VBG', 'NNP', ':', 'NN', '.']",58
sentiment_analysis,16,268,"2 . In the 3 - hop setting , TMNs achieve much better results on Restaurant .","['2', '.', 'In', 'the', '3', '-', 'hop', 'setting', ',', 'TMNs', 'achieve', 'much', 'better', 'results', 'on', 'Restaurant', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['CD', '.', 'IN', 'DT', 'CD', ':', 'NN', 'NN', ',', 'NNP', 'VBP', 'RB', 'JJR', 'NNS', 'IN', 'NNP', '.']",17
sentiment_analysis,16,269,"JCI , IT , and CI achieve the best scores , outperforming the strongest baseline AMN by 2.38 % , 2.18 % , and 2.03 % .","['JCI', ',', 'IT', ',', 'and', 'CI', 'achieve', 'the', 'best', 'scores', ',', 'outperforming', 'the', 'strongest', 'baseline', 'AMN', 'by', '2.38', '%', ',', '2.18', '%', ',', 'and', '2.03', '%', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ',', 'NNP', ',', 'CC', 'NNP', 'VBP', 'DT', 'JJS', 'NNS', ',', 'VBG', 'DT', 'JJS', 'NN', 'NNP', 'IN', 'CD', 'NN', ',', 'CD', 'NN', ',', 'CC', 'CD', 'NN', '.']",27
sentiment_analysis,16,270,"On Laptop , BL - MN and most TMNs ( except CNP and JPI ) perform similarly .","['On', 'Laptop', ',', 'BL', '-', 'MN', 'and', 'most', 'TMNs', '(', 'except', 'CNP', 'and', 'JPI', ')', 'perform', 'similarly', '.']","['B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O']","['IN', 'NNP', ',', 'NNP', ':', 'NNP', 'CC', 'JJS', 'NNP', '(', 'IN', 'NNP', 'CC', 'NNP', ')', 'VBP', 'RB', '.']",18
sentiment_analysis,16,272,"3 . Comparing all TMNs , we see that JCI works the best as it always obtains the top - three scores on two datasets and in two settings .","['3', '.', 'Comparing', 'all', 'TMNs', ',', 'we', 'see', 'that', 'JCI', 'works', 'the', 'best', 'as', 'it', 'always', 'obtains', 'the', 'top', '-', 'three', 'scores', 'on', 'two', 'datasets', 'and', 'in', 'two', 'settings', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['LS', '.', 'VBG', 'DT', 'NNP', ',', 'PRP', 'VBP', 'IN', 'NNP', 'VBZ', 'DT', 'JJS', 'IN', 'PRP', 'RB', 'VBZ', 'DT', 'JJ', ':', 'CD', 'NNS', 'IN', 'CD', 'NNS', 'CC', 'IN', 'CD', 'NNS', '.']",30
sentiment_analysis,16,273,CI and JPI also perform well in most cases .,"['CI', 'and', 'JPI', 'also', 'perform', 'well', 'in', 'most', 'cases', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'CC', 'NNP', 'RB', 'VBP', 'RB', 'IN', 'JJS', 'NNS', '.']",10
sentiment_analysis,16,274,"IT , NP , and CNP can achieve very good scores in some cases but are less stable .","['IT', ',', 'NP', ',', 'and', 'CNP', 'can', 'achieve', 'very', 'good', 'scores', 'in', 'some', 'cases', 'but', 'are', 'less', 'stable', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ',', 'NNP', ',', 'CC', 'NNP', 'MD', 'VB', 'RB', 'JJ', 'NNS', 'IN', 'DT', 'NNS', 'CC', 'VBP', 'RBR', 'JJ', '.']",19
sentiment_analysis,39,2,SentiHood : Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods,"['SentiHood', ':', 'Targeted', 'Aspect', 'Based', 'Sentiment', 'Analysis', 'Dataset', 'for', 'Urban', 'Neighbourhoods']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NN', ':', 'VBN', 'NNP', 'VBD', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",11
sentiment_analysis,39,4,"In this paper , we introduce the task of targeted aspect - based sentiment analysis .","['In', 'this', 'paper', ',', 'we', 'introduce', 'the', 'task', 'of', 'targeted', 'aspect', '-', 'based', 'sentiment', 'analysis', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'NN', ':', 'VBN', 'NN', 'NN', '.']",16
sentiment_analysis,39,16,Sentiment analysis is an important task in natural language processing .,"['Sentiment', 'analysis', 'is', 'an', 'important', 'task', 'in', 'natural', 'language', 'processing', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",11
sentiment_analysis,39,20,"Aspect - based sentiment analysis ( ABSA ) relates to the task of extracting fine - grained information by identifying the polarity towards different aspects of an entity in the same unit of text , and recognizing the polarity associated with each aspect separately .","['Aspect', '-', 'based', 'sentiment', 'analysis', '(', 'ABSA', ')', 'relates', 'to', 'the', 'task', 'of', 'extracting', 'fine', '-', 'grained', 'information', 'by', 'identifying', 'the', 'polarity', 'towards', 'different', 'aspects', 'of', 'an', 'entity', 'in', 'the', 'same', 'unit', 'of', 'text', ',', 'and', 'recognizing', 'the', 'polarity', 'associated', 'with', 'each', 'aspect', 'separately', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'VBN', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'TO', 'DT', 'NN', 'IN', 'VBG', 'JJ', ':', 'VBN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', ',', 'CC', 'VBG', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', 'RB', '.']",45
sentiment_analysis,39,24,Targeted sentiment analysis investigates the classification of opinion polarities towards certain target entity mentions in given sentences ( often a tweet ) .,"['Targeted', 'sentiment', 'analysis', 'investigates', 'the', 'classification', 'of', 'opinion', 'polarities', 'towards', 'certain', 'target', 'entity', 'mentions', 'in', 'given', 'sentences', '(', 'often', 'a', 'tweet', ')', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'NN', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'NN', 'NNS', 'VBP', 'JJ', 'NN', 'NN', 'NNS', 'IN', 'VBN', 'NNS', '(', 'RB', 'DT', 'NN', ')', '.']",23
sentiment_analysis,39,128,SentiHood currently contains annotated sentences containing one or two location entity mentions .,"['SentiHood', 'currently', 'contains', 'annotated', 'sentences', 'containing', 'one', 'or', 'two', 'location', 'entity', 'mentions', '.']","['B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'RB', 'VBZ', 'JJ', 'NNS', 'VBG', 'CD', 'CC', 'CD', 'NN', 'NN', 'NNS', '.']",13
sentiment_analysis,39,129,2 Sen-tiHood contains 5215 sentences with 3862 sentences containing a single location and 1353 sentences containing multiple ( two ) locations .,"['2', 'Sen-tiHood', 'contains', '5215', 'sentences', 'with', '3862', 'sentences', 'containing', 'a', 'single', 'location', 'and', '1353', 'sentences', 'containing', 'multiple', '(', 'two', ')', 'locations', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', 'NN', 'NNS', 'CD', 'NNS', 'IN', 'CD', 'NNS', 'VBG', 'DT', 'JJ', 'NN', 'CC', 'CD', 'NNS', 'VBG', 'NN', '(', 'CD', ')', 'NNS', '.']",22
sentiment_analysis,39,131,""" Positive "" sentiment is dominant for aspects such as dining and shopping .","['""', 'Positive', '""', 'sentiment', 'is', 'dominant', 'for', 'aspects', 'such', 'as', 'dining', 'and', 'shopping', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', 'NNP', 'NNP', 'NN', 'VBZ', 'JJ', 'IN', 'NNS', 'JJ', 'IN', 'NN', 'CC', 'NN', '.']",14
sentiment_analysis,39,133,The general aspect is the most frequent aspect with over 2000 sentences while aspect touristy has occurred in less than 100 sentences .,"['The', 'general', 'aspect', 'is', 'the', 'most', 'frequent', 'aspect', 'with', 'over', '2000', 'sentences', 'while', 'aspect', 'touristy', 'has', 'occurred', 'in', 'less', 'than', '100', 'sentences', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'RBS', 'JJ', 'NN', 'IN', 'IN', 'CD', 'NNS', 'IN', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'JJR', 'IN', 'CD', 'NNS', '.']",23
sentiment_analysis,39,134,"Notice that since each sentence can contain one or more opinions , the total number of opinions ( 5920 ) in the dataset is higher than the number of sentences .","['Notice', 'that', 'since', 'each', 'sentence', 'can', 'contain', 'one', 'or', 'more', 'opinions', ',', 'the', 'total', 'number', 'of', 'opinions', '(', '5920', ')', 'in', 'the', 'dataset', 'is', 'higher', 'than', 'the', 'number', 'of', 'sentences', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'IN', 'IN', 'DT', 'NN', 'MD', 'VB', 'CD', 'CC', 'JJR', 'NNS', ',', 'DT', 'JJ', 'NN', 'IN', 'NNS', '(', 'CD', ')', 'IN', 'DT', 'NN', 'VBZ', 'JJR', 'IN', 'DT', 'NN', 'IN', 'NNS', '.']",31
sentiment_analysis,39,135,"Location entity names are masked by location1 and location 2 in the whole dataset , so the task does not involve identification and segmentation of the named entities .","['Location', 'entity', 'names', 'are', 'masked', 'by', 'location1', 'and', 'location', '2', 'in', 'the', 'whole', 'dataset', ',', 'so', 'the', 'task', 'does', 'not', 'involve', 'identification', 'and', 'segmentation', 'of', 'the', 'named', 'entities', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'NN', 'CC', 'NN', 'CD', 'IN', 'DT', 'JJ', 'NN', ',', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'VB', 'NN', 'CC', 'NN', 'IN', 'DT', 'JJ', 'NNS', '.']",29
sentiment_analysis,39,161,Logistic Regression,"['Logistic', 'Regression']","['B-n', 'I-n']","['JJ', 'NN']",2
sentiment_analysis,39,162,"Many existing works in the aspect - based sentiment analysis task , 3 use a classifier , such as logistic regression or SVM , based on linguistic features such as n-grams , POS information or more hand - engineered features .","['Many', 'existing', 'works', 'in', 'the', 'aspect', '-', 'based', 'sentiment', 'analysis', 'task', ',', '3', 'use', 'a', 'classifier', ',', 'such', 'as', 'logistic', 'regression', 'or', 'SVM', ',', 'based', 'on', 'linguistic', 'features', 'such', 'as', 'n-grams', ',', 'POS', 'information', 'or', 'more', 'hand', '-', 'engineered', 'features', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'VBG', 'NNS', 'IN', 'DT', 'NN', ':', 'VBN', 'NN', 'NN', 'NN', ',', 'CD', 'NN', 'DT', 'NN', ',', 'JJ', 'IN', 'JJ', 'NN', 'CC', 'NNP', ',', 'VBN', 'IN', 'JJ', 'NNS', 'JJ', 'IN', 'NNS', ',', 'NNP', 'NN', 'CC', 'JJR', 'NN', ':', 'VBN', 'NNS', '.']",41
sentiment_analysis,39,164,"More concretely , we define the following sparse representations of locations :","['More', 'concretely', ',', 'we', 'define', 'the', 'following', 'sparse', 'representations', 'of', 'locations', ':']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RBR', 'RB', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NNS', 'IN', 'NNS', ':']",12
sentiment_analysis,39,165,Mask target entity n-grams :,"['Mask', 'target', 'entity', 'n-grams', ':']","['B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NN', 'NN', 'JJ', ':']",5
sentiment_analysis,39,168,Left - right n- grams : we create an n-gram representation for both the right and the left context around each location mention .,"['Left', '-', 'right', 'n-', 'grams', ':', 'we', 'create', 'an', 'n-gram', 'representation', 'for', 'both', 'the', 'right', 'and', 'the', 'left', 'context', 'around', 'each', 'location', 'mention', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'JJ', 'JJ', 'NNS', ':', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'DT', 'NN', 'CC', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",24
sentiment_analysis,39,170,Left right pooling :,"['Left', 'right', 'pooling', ':']","['B-n', 'I-n', 'I-n', 'O']","['NNP', 'JJ', 'NN', ':']",4
sentiment_analysis,39,177,Long Short - Term Memory ( LSTM ),"['Long', 'Short', '-', 'Term', 'Memory', '(', 'LSTM', ')']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', ':', 'NNP', 'NNP', '(', 'NNP', ')']",8
sentiment_analysis,39,178,"Inspired by the recent success of applying deep neural networks on language tasks , we use a bidirectional LSTM to learn a classifier for each of the aspects .","['Inspired', 'by', 'the', 'recent', 'success', 'of', 'applying', 'deep', 'neural', 'networks', 'on', 'language', 'tasks', ',', 'we', 'use', 'a', 'bidirectional', 'LSTM', 'to', 'learn', 'a', 'classifier', 'for', 'each', 'of', 'the', 'aspects', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'JJ', 'JJ', 'NNS', 'IN', 'NN', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNP', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'IN', 'DT', 'NNS', '.']",29
sentiment_analysis,39,179,Representations for a location ( e l ) are obtained using one of the following two approaches :,"['Representations', 'for', 'a', 'location', '(', 'e', 'l', ')', 'are', 'obtained', 'using', 'one', 'of', 'the', 'following', 'two', 'approaches', ':']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNS', 'IN', 'DT', 'NN', '(', 'JJ', 'NN', ')', 'VBP', 'VBN', 'VBG', 'CD', 'IN', 'DT', 'JJ', 'CD', 'NNS', ':']",18
sentiment_analysis,39,180,Final output state ( LSTM - Final ) : e l is the output embedding of the bidirectional LSTM .,"['Final', 'output', 'state', '(', 'LSTM', '-', 'Final', ')', ':', 'e', 'l', 'is', 'the', 'output', 'embedding', 'of', 'the', 'bidirectional', 'LSTM', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'NN', '(', 'NNP', ':', 'NNP', ')', ':', 'NN', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNP', '.']",20
sentiment_analysis,39,181,Location output state ( LSTM - Location ) :,"['Location', 'output', 'state', '(', 'LSTM', '-', 'Location', ')', ':']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'NN', 'NN', '(', 'NNP', ':', 'NN', ')', ':']",9
sentiment_analysis,39,211,"As we can see , the n-gram representation with location masking achieves slightly better results over the left - right context .","['As', 'we', 'can', 'see', ',', 'the', 'n-gram', 'representation', 'with', 'location', 'masking', 'achieves', 'slightly', 'better', 'results', 'over', 'the', 'left', '-', 'right', 'context', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'PRP', 'MD', 'VB', ',', 'DT', 'JJ', 'NN', 'IN', 'NN', 'VBG', 'NNS', 'RB', 'JJR', 'NNS', 'IN', 'DT', 'JJ', ':', 'JJ', 'NN', '.']",22
sentiment_analysis,39,213,"Also , by adding POS information , we gain an increase in the performance .","['Also', ',', 'by', 'adding', 'POS', 'information', ',', 'we', 'gain', 'an', 'increase', 'in', 'the', 'performance', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']","['RB', ',', 'IN', 'VBG', 'NNP', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",15
sentiment_analysis,39,215,"Separating the left and the right context ( LR - Left - Right ) for BoW representation , does not improve the performance .","['Separating', 'the', 'left', 'and', 'the', 'right', 'context', '(', 'LR', '-', 'Left', '-', 'Right', ')', 'for', 'BoW', 'representation', ',', 'does', 'not', 'improve', 'the', 'performance', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['VBG', 'DT', 'NN', 'CC', 'DT', 'JJ', 'NN', '(', 'NNP', ':', 'VBD', ':', 'NN', ')', 'IN', 'NNP', 'NN', ',', 'VBZ', 'RB', 'VB', 'DT', 'NN', '.']",24
sentiment_analysis,39,217,"Amongst the two variations of LSTM , the model with final state embeddings does slightly better than the model where we use the embeddings at the location index , however they are not significantly different ( with a p valueless than 0.01 ) .","['Amongst', 'the', 'two', 'variations', 'of', 'LSTM', ',', 'the', 'model', 'with', 'final', 'state', 'embeddings', 'does', 'slightly', 'better', 'than', 'the', 'model', 'where', 'we', 'use', 'the', 'embeddings', 'at', 'the', 'location', 'index', ',', 'however', 'they', 'are', 'not', 'significantly', 'different', '(', 'with', 'a', 'p', 'valueless', 'than', '0.01', ')', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'CD', 'NNS', 'IN', 'NNP', ',', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NNS', 'VBZ', 'RB', 'JJR', 'IN', 'DT', 'NN', 'WRB', 'PRP', 'VBP', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', ',', 'RB', 'PRP', 'VBP', 'RB', 'RB', 'JJ', '(', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', ')', '.']",44
sentiment_analysis,39,218,"It is interesting to note that the best LSTM model is not superior to logistic regression model , especially in terms of AUC .","['It', 'is', 'interesting', 'to', 'note', 'that', 'the', 'best', 'LSTM', 'model', 'is', 'not', 'superior', 'to', 'logistic', 'regression', 'model', ',', 'especially', 'in', 'terms', 'of', 'AUC', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['PRP', 'VBZ', 'VBG', 'TO', 'VB', 'IN', 'DT', 'JJS', 'JJ', 'NN', 'VBZ', 'RB', 'JJ', 'TO', 'JJ', 'NN', 'NN', ',', 'RB', 'IN', 'NNS', 'IN', 'NNP', '.']",24
sentiment_analysis,39,221,Another interesting observation is that the F 1 measure for logistic regression model with n-grams and POS information is very low while this model 's performance is superior to other models in terms of AUC .,"['Another', 'interesting', 'observation', 'is', 'that', 'the', 'F', '1', 'measure', 'for', 'logistic', 'regression', 'model', 'with', 'n-grams', 'and', 'POS', 'information', 'is', 'very', 'low', 'while', 'this', 'model', ""'s"", 'performance', 'is', 'superior', 'to', 'other', 'models', 'in', 'terms', 'of', 'AUC', '.']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'IN', 'DT', 'NNP', 'CD', 'NN', 'IN', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'CC', 'NNP', 'NN', 'VBZ', 'RB', 'JJ', 'IN', 'DT', 'NN', 'POS', 'NN', 'VBZ', 'JJ', 'TO', 'JJ', 'NNS', 'IN', 'NNS', 'IN', 'NNP', '.']",36
sentiment_analysis,13,2,BERT Post - Training for Review Reading Comprehension and Aspect - based Sentiment Analysis,"['BERT', 'Post', '-', 'Training', 'for', 'Review', 'Reading', 'Comprehension', 'and', 'Aspect', '-', 'based', 'Sentiment', 'Analysis']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', ':', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', ':', 'VBN', 'NN', 'NN']",14
sentiment_analysis,13,10,"To show the generality of the approach , the proposed post - training is also applied to some other review - based tasks such as aspect extraction and aspect sentiment classification in aspect - based sentiment analysis .","['To', 'show', 'the', 'generality', 'of', 'the', 'approach', ',', 'the', 'proposed', 'post', '-', 'training', 'is', 'also', 'applied', 'to', 'some', 'other', 'review', '-', 'based', 'tasks', 'such', 'as', 'aspect', 'extraction', 'and', 'aspect', 'sentiment', 'classification', 'in', 'aspect', '-', 'based', 'sentiment', 'analysis', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'DT', 'VBN', 'NN', ':', 'NN', 'VBZ', 'RB', 'VBN', 'TO', 'DT', 'JJ', 'NN', ':', 'VBN', 'NNS', 'JJ', 'IN', 'JJ', 'NN', 'CC', 'JJ', 'NN', 'NN', 'IN', 'JJ', ':', 'VBN', 'NN', 'NN', '.']",38
sentiment_analysis,13,40,"This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .","['This', 'work', 'first', 'builds', 'an', 'RRC', 'dataset', 'called', 'ReviewRC', ',', 'using', 'reviews', 'from', 'SemEval', '2016', 'Task', '5', '2', ',', 'which', 'is', 'a', 'popular', 'dataset', 'for', 'aspect', '-', 'based', 'sentiment', 'analysis', '(', 'ABSA', ')', 'in', 'the', 'domains', 'of', 'laptop', 'and', 'restaurant', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'RB', 'VBZ', 'DT', 'NNP', 'NN', 'VBD', 'NNP', ',', 'VBG', 'NNS', 'IN', 'JJ', 'CD', 'NNP', 'CD', 'CD', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', ':', 'VBN', 'NN', 'NN', '(', 'NNP', ')', 'IN', 'DT', 'NNS', 'IN', 'NN', 'CC', 'NN', '.']",41
sentiment_analysis,13,43,This work adopts BERT ) as the base model as it achieves the state - of the - art performance on MRC .,"['This', 'work', 'adopts', 'BERT', ')', 'as', 'the', 'base', 'model', 'as', 'it', 'achieves', 'the', 'state', '-', 'of', 'the', '-', 'art', 'performance', 'on', 'MRC', '.']","['O', 'O', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'NNP', ')', 'IN', 'DT', 'NN', 'NN', 'IN', 'PRP', 'VBZ', 'DT', 'NN', ':', 'IN', 'DT', ':', 'NN', 'NN', 'IN', 'NNP', '.']",23
sentiment_analysis,13,48,"To address all the above challenges , we propose a novel joint post - training technique that takes BERT 's pre-trained weights as the initialization 4 for basic language understanding and adapt BERT with both domain knowledge and task ( MRC ) knowledge before fine - tuning using the domain end task annotated data for the domain RRC .","['To', 'address', 'all', 'the', 'above', 'challenges', ',', 'we', 'propose', 'a', 'novel', 'joint', 'post', '-', 'training', 'technique', 'that', 'takes', 'BERT', ""'s"", 'pre-trained', 'weights', 'as', 'the', 'initialization', '4', 'for', 'basic', 'language', 'understanding', 'and', 'adapt', 'BERT', 'with', 'both', 'domain', 'knowledge', 'and', 'task', '(', 'MRC', ')', 'knowledge', 'before', 'fine', '-', 'tuning', 'using', 'the', 'domain', 'end', 'task', 'annotated', 'data', 'for', 'the', 'domain', 'RRC', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['TO', 'VB', 'PDT', 'DT', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', ':', 'NN', 'NN', 'WDT', 'VBZ', 'NNP', 'POS', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CD', 'IN', 'JJ', 'NN', 'NN', 'CC', 'JJ', 'NNP', 'IN', 'DT', 'NN', 'NN', 'CC', 'NN', '(', 'NNP', ')', 'NN', 'IN', 'JJ', ':', 'VBG', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'VBN', 'NNS', 'IN', 'DT', 'NN', 'NNP', '.']",59
sentiment_analysis,13,49,"This technique leverages knowledge from two sources : unsupervised domain reviews and supervised ( yet out - of - domain ) MRC data 5 , where the former enhances domain - awareness and the latter strengthens MRC task - awareness .","['This', 'technique', 'leverages', 'knowledge', 'from', 'two', 'sources', ':', 'unsupervised', 'domain', 'reviews', 'and', 'supervised', '(', 'yet', 'out', '-', 'of', '-', 'domain', ')', 'MRC', 'data', '5', ',', 'where', 'the', 'former', 'enhances', 'domain', '-', 'awareness', 'and', 'the', 'latter', 'strengthens', 'MRC', 'task', '-', 'awareness', '.']","['O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'VBP', 'IN', 'CD', 'NNS', ':', 'JJ', 'NN', 'NNS', 'CC', 'VBN', '(', 'RB', 'IN', ':', 'IN', ':', 'NN', ')', 'NNP', '$', 'CD', ',', 'WRB', 'DT', 'JJ', 'NNS', 'VBP', ':', 'NN', 'CC', 'DT', 'JJ', 'NNS', 'NNP', 'NN', ':', 'NN', '.']",41
sentiment_analysis,13,210,We adopt BERT BASE ( uncased ) as the basis for all experiments,"['We', 'adopt', 'BERT', 'BASE', '(', 'uncased', ')', 'as', 'the', 'basis', 'for', 'all', 'experiments']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n']","['PRP', 'VBP', 'JJ', 'NNP', '(', 'JJ', ')', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNS']",13
sentiment_analysis,13,211,"10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage FP16 computation 11 to reduce the size of both the model and hidden representations of data .","['10', '.', 'Since', 'post', '-', 'training', 'may', 'take', 'a', 'large', 'footprint', 'on', 'GPU', 'memory', '(', 'as', 'BERT', 'pretraining', ')', ',', 'we', 'leverage', 'FP16', 'computation', '11', 'to', 'reduce', 'the', 'size', 'of', 'both', 'the', 'model', 'and', 'hidden', 'representations', 'of', 'data', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['CD', '.', 'IN', 'NN', ':', 'NN', 'MD', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NN', '(', 'IN', 'NNP', 'VBG', ')', ',', 'PRP', 'VBP', 'JJ', 'NN', 'CD', 'TO', 'VB', 'DT', 'NN', 'IN', 'CC', 'DT', 'NN', 'CC', 'JJ', 'NNS', 'IN', 'NNS', '.']",39
sentiment_analysis,13,212,"We set a static loss scale of 2 in FP16 , which can avoid any over / under - flow of floating point computation .","['We', 'set', 'a', 'static', 'loss', 'scale', 'of', '2', 'in', 'FP16', ',', 'which', 'can', 'avoid', 'any', 'over', '/', 'under', '-', 'flow', 'of', 'floating', 'point', 'computation', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'IN', 'NNP', ',', 'WDT', 'MD', 'VB', 'DT', 'IN', 'NN', 'IN', ':', 'NN', 'IN', 'VBG', 'NN', 'NN', '.']",25
sentiment_analysis,13,213,The maximum length of post -training is set to 320 with a batch size of 16 for each type of knowledge .,"['The', 'maximum', 'length', 'of', 'post', '-training', 'is', 'set', 'to', '320', 'with', 'a', 'batch', 'size', 'of', '16', 'for', 'each', 'type', 'of', 'knowledge', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', 'IN', 'NN', '.']",22
sentiment_analysis,13,214,"The number of subbatch u is set to 2 , which is good enough to store each sub - batch iteration into a GPU memory of 11G .","['The', 'number', 'of', 'subbatch', 'u', 'is', 'set', 'to', '2', ',', 'which', 'is', 'good', 'enough', 'to', 'store', 'each', 'sub', '-', 'batch', 'iteration', 'into', 'a', 'GPU', 'memory', 'of', '11G', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', ',', 'WDT', 'VBZ', 'JJ', 'RB', 'TO', 'VB', 'DT', 'NN', ':', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'CD', '.']",28
sentiment_analysis,13,215,We use Adam optimizer and set the learning rate to be 3e - 5 .,"['We', 'use', 'Adam', 'optimizer', 'and', 'set', 'the', 'learning', 'rate', 'to', 'be', '3e', '-', '5', '.']","['O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'CC', 'VBD', 'DT', 'NN', 'NN', 'TO', 'VB', 'CD', ':', 'CD', '.']",15
sentiment_analysis,13,216,"We train 70,000 steps for the laptop domain and 140,000 steps for the restaurant domain , which roughly have one pass over the preprocessed data on the respective domain .","['We', 'train', '70,000', 'steps', 'for', 'the', 'laptop', 'domain', 'and', '140,000', 'steps', 'for', 'the', 'restaurant', 'domain', ',', 'which', 'roughly', 'have', 'one', 'pass', 'over', 'the', 'preprocessed', 'data', 'on', 'the', 'respective', 'domain', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'CD', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'CC', 'CD', 'NNS', 'IN', 'DT', 'NN', 'NN', ',', 'WDT', 'RB', 'VBP', 'CD', 'NN', 'IN', 'DT', 'VBN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",30
sentiment_analysis,13,250,"To answer RQ1 , we observed that the proposed joint post - training ( BERT - PT ) has the best performance over all tasks in all domains , which show the benefits of having two types of knowledge .","['To', 'answer', 'RQ1', ',', 'we', 'observed', 'that', 'the', 'proposed', 'joint', 'post', '-', 'training', '(', 'BERT', '-', 'PT', ')', 'has', 'the', 'best', 'performance', 'over', 'all', 'tasks', 'in', 'all', 'domains', ',', 'which', 'show', 'the', 'benefits', 'of', 'having', 'two', 'types', 'of', 'knowledge', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'NNP', ',', 'PRP', 'VBD', 'IN', 'DT', 'VBN', 'NN', 'NN', ':', 'NN', '(', 'NNP', ':', 'NN', ')', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNS', ',', 'WDT', 'VBP', 'DT', 'NNS', 'IN', 'VBG', 'CD', 'NNS', 'IN', 'NN', '.']",40
sentiment_analysis,13,252,"Rest. Methods EM F1 EM F1 DrQA 38.26 50.99 49.52 63.73 DrQA+MRC 40 To answer RQ2 , to our surprise we found that the vanilla pre-trained weights of BERT do not work well for review - based tasks , although it achieves state - of - the - art results on many other NLP tasks .","['Rest.', 'Methods', 'EM', 'F1', 'EM', 'F1', 'DrQA', '38.26', '50.99', '49.52', '63.73', 'DrQA+MRC', '40', 'To', 'answer', 'RQ2', ',', 'to', 'our', 'surprise', 'we', 'found', 'that', 'the', 'vanilla', 'pre-trained', 'weights', 'of', 'BERT', 'do', 'not', 'work', 'well', 'for', 'review', '-', 'based', 'tasks', ',', 'although', 'it', 'achieves', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'on', 'many', 'other', 'NLP', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'CD', 'CD', 'CD', 'CD', 'NNP', 'CD', 'TO', 'VB', 'NNP', ',', 'TO', 'PRP$', 'NN', 'PRP', 'VBD', 'IN', 'DT', 'NN', 'JJ', 'NNS', 'IN', 'NNP', 'VBP', 'RB', 'VB', 'RB', 'IN', 'NN', ':', 'VBN', 'NNS', ',', 'IN', 'PRP', 'VBZ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'JJ', 'JJ', 'NNP', 'NNS', '.']",56
sentiment_analysis,13,254,"To answer RQ3 , we noticed that the roles of domain knowledge and task knowledge vary for different tasks and domains .","['To', 'answer', 'RQ3', ',', 'we', 'noticed', 'that', 'the', 'roles', 'of', 'domain', 'knowledge', 'and', 'task', 'knowledge', 'vary', 'for', 'different', 'tasks', 'and', 'domains', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'NNP', ',', 'PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', 'NN', 'CC', 'NN', 'NN', 'VBP', 'IN', 'JJ', 'NNS', 'CC', 'NNS', '.']",22
sentiment_analysis,13,255,"For RRC , we found that the performance gain of BERT - PT mostly comes from task - awareness ( MRC ) post -training ( as indicated by BERT - MRC ) .","['For', 'RRC', ',', 'we', 'found', 'that', 'the', 'performance', 'gain', 'of', 'BERT', '-', 'PT', 'mostly', 'comes', 'from', 'task', '-', 'awareness', '(', 'MRC', ')', 'post', '-training', '(', 'as', 'indicated', 'by', 'BERT', '-', 'MRC', ')', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ',', 'PRP', 'VBD', 'IN', 'DT', 'NN', 'NN', 'IN', 'NNP', ':', 'NNP', 'RB', 'VBZ', 'IN', 'NN', ':', 'NN', '(', 'NNP', ')', 'NN', 'VBG', '(', 'IN', 'VBN', 'IN', 'NNP', ':', 'NN', ')', '.']",33
sentiment_analysis,13,258,We further investigated the examples improved by BERT - MRC and found that the boundaries of spans ( especially short spans ) were greatly improved .,"['We', 'further', 'investigated', 'the', 'examples', 'improved', 'by', 'BERT', '-', 'MRC', 'and', 'found', 'that', 'the', 'boundaries', 'of', 'spans', '(', 'especially', 'short', 'spans', ')', 'were', 'greatly', 'improved', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBD', 'DT', 'NNS', 'VBN', 'IN', 'NNP', ':', 'NN', 'CC', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NNS', '(', 'RB', 'RB', 'NNS', ')', 'VBD', 'RB', 'VBN', '.']",26
sentiment_analysis,13,259,"For AE , we found that great performance boost comes mostly from domain knowledge posttraining , which indicates that contextualized representations of domain knowledge are very important for AE .","['For', 'AE', ',', 'we', 'found', 'that', 'great', 'performance', 'boost', 'comes', 'mostly', 'from', 'domain', 'knowledge', 'posttraining', ',', 'which', 'indicates', 'that', 'contextualized', 'representations', 'of', 'domain', 'knowledge', 'are', 'very', 'important', 'for', 'AE', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ',', 'PRP', 'VBD', 'IN', 'JJ', 'NN', 'NN', 'VBZ', 'RB', 'IN', 'NN', 'NN', 'NN', ',', 'WDT', 'VBZ', 'IN', 'JJ', 'NNS', 'IN', 'NN', 'NN', 'VBP', 'RB', 'JJ', 'IN', 'NNP', '.']",30
sentiment_analysis,13,260,"BERT - MRC has almost no improvement on restaurant , which indicates Wikipedia may have no knowledge about aspects of restaurant .","['BERT', '-', 'MRC', 'has', 'almost', 'no', 'improvement', 'on', 'restaurant', ',', 'which', 'indicates', 'Wikipedia', 'may', 'have', 'no', 'knowledge', 'about', 'aspects', 'of', 'restaurant', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', 'VBZ', 'RB', 'DT', 'NN', 'IN', 'NN', ',', 'WDT', 'VBZ', 'NNP', 'MD', 'VB', 'DT', 'NN', 'IN', 'NNS', 'IN', 'NN', '.']",22
sentiment_analysis,13,262,"For ASC , we observed that large - scale annotated MRC data is very useful .","['For', 'ASC', ',', 'we', 'observed', 'that', 'large', '-', 'scale', 'annotated', 'MRC', 'data', 'is', 'very', 'useful', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'NNP', ',', 'PRP', 'VBD', 'IN', 'JJ', ':', 'NN', 'VBD', 'NNP', 'NNS', 'VBZ', 'RB', 'JJ', '.']",16
sentiment_analysis,13,267,The errors on RRC mainly come from boundaries of spans that are not concise enough and incorrect location of spans that may have certain nearby words related to the question .,"['The', 'errors', 'on', 'RRC', 'mainly', 'come', 'from', 'boundaries', 'of', 'spans', 'that', 'are', 'not', 'concise', 'enough', 'and', 'incorrect', 'location', 'of', 'spans', 'that', 'may', 'have', 'certain', 'nearby', 'words', 'related', 'to', 'the', 'question', '.']","['O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['DT', 'NNS', 'IN', 'NNP', 'RB', 'VBP', 'IN', 'NNS', 'IN', 'NNS', 'WDT', 'VBP', 'RB', 'JJ', 'RB', 'CC', 'JJ', 'NN', 'IN', 'NNS', 'WDT', 'MD', 'VB', 'JJ', 'JJ', 'NNS', 'VBN', 'TO', 'DT', 'NN', '.']",31
sentiment_analysis,13,269,"For AE , errors mostly come from annotation inconsistency and boundaries of aspects ( e.g. , apple OS is predicted as OS ) .","['For', 'AE', ',', 'errors', 'mostly', 'come', 'from', 'annotation', 'inconsistency', 'and', 'boundaries', 'of', 'aspects', '(', 'e.g.', ',', 'apple', 'OS', 'is', 'predicted', 'as', 'OS', ')', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ',', 'NNS', 'RB', 'VBP', 'IN', 'NN', 'NN', 'CC', 'NNS', 'IN', 'NNS', '(', 'NN', ',', 'NN', 'NNP', 'VBZ', 'VBN', 'IN', 'NNP', ')', '.']",24
sentiment_analysis,13,271,"ASC tends to have more errors as the decision boundary between the negative and neutral examples is unclear ( e.g. , even annotators may not sure whether the reviewer shows no opinion or slight negative opinion when mentioning an aspect ) .","['ASC', 'tends', 'to', 'have', 'more', 'errors', 'as', 'the', 'decision', 'boundary', 'between', 'the', 'negative', 'and', 'neutral', 'examples', 'is', 'unclear', '(', 'e.g.', ',', 'even', 'annotators', 'may', 'not', 'sure', 'whether', 'the', 'reviewer', 'shows', 'no', 'opinion', 'or', 'slight', 'negative', 'opinion', 'when', 'mentioning', 'an', 'aspect', ')', '.']","['B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'TO', 'VB', 'JJR', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'CC', 'JJ', 'NNS', 'VBZ', 'JJ', '(', 'NN', ',', 'RB', 'NNS', 'MD', 'RB', 'VB', 'IN', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'CC', 'JJ', 'JJ', 'NN', 'WRB', 'VBG', 'DT', 'NN', ')', '.']",42
sentiment_analysis,27,2,Modeling Sentiment Dependencies with Graph Convolutional Networks for Aspect - level Sentiment Classification,"['Modeling', 'Sentiment', 'Dependencies', 'with', 'Graph', 'Convolutional', 'Networks', 'for', 'Aspect', '-', 'level', 'Sentiment', 'Classification']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['VBG', 'NNP', 'NNPS', 'IN', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', ':', 'NN', 'NNP', 'NNP']",13
sentiment_analysis,27,14,"It is a fine - grained task in sentiment analysis , which aims to infer the sentiment polarities of aspects in their context .","['It', 'is', 'a', 'fine', '-', 'grained', 'task', 'in', 'sentiment', 'analysis', ',', 'which', 'aims', 'to', 'infer', 'the', 'sentiment', 'polarities', 'of', 'aspects', 'in', 'their', 'context', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBZ', 'DT', 'JJ', ':', 'VBN', 'NN', 'IN', 'NN', 'NN', ',', 'WDT', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'NNS', 'IN', 'NNS', 'IN', 'PRP$', 'NN', '.']",24
sentiment_analysis,27,39,"In this paper , we propose a novel method to model Sentiment Dependencies with Graph Convolutional Networks ( SDGCN ) for aspect - level sentiment classification .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', 'method', 'to', 'model', 'Sentiment', 'Dependencies', 'with', 'Graph', 'Convolutional', 'Networks', '(', 'SDGCN', ')', 'for', 'aspect', '-', 'level', 'sentiment', 'classification', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'TO', 'VB', 'NNP', 'NNPS', 'IN', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'IN', 'JJ', ':', 'NN', 'NN', 'NN', '.']",27
sentiment_analysis,27,40,"GCN is a simple and effective convolutional neural network operating on graphs , which can catch inter-dependent information from rich relational data .","['GCN', 'is', 'a', 'simple', 'and', 'effective', 'convolutional', 'neural', 'network', 'operating', 'on', 'graphs', ',', 'which', 'can', 'catch', 'inter-dependent', 'information', 'from', 'rich', 'relational', 'data', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'CC', 'JJ', 'JJ', 'JJ', 'NN', 'VBG', 'IN', 'NN', ',', 'WDT', 'MD', 'VB', 'JJ', 'NN', 'IN', 'JJ', 'JJ', 'NNS', '.']",23
sentiment_analysis,27,41,"For every node in graph , GCN encodes relevant information about its neighborhoods as a new feature representation vector .","['For', 'every', 'node', 'in', 'graph', ',', 'GCN', 'encodes', 'relevant', 'information', 'about', 'its', 'neighborhoods', 'as', 'a', 'new', 'feature', 'representation', 'vector', '.']","['B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'IN', 'NN', ',', 'NNP', 'VBZ', 'JJ', 'NN', 'IN', 'PRP$', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', '.']",20
sentiment_analysis,27,42,"In our case , an aspect is treated as a node , and an edge represents the sentiment dependency relation of two nodes .","['In', 'our', 'case', ',', 'an', 'aspect', 'is', 'treated', 'as', 'a', 'node', ',', 'and', 'an', 'edge', 'represents', 'the', 'sentiment', 'dependency', 'relation', 'of', 'two', 'nodes', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'PRP$', 'NN', ',', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', ',', 'CC', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'NN', 'IN', 'CD', 'NNS', '.']",24
sentiment_analysis,27,43,Our model learns the sentiment dependencies of aspects via this graph structure .,"['Our', 'model', 'learns', 'the', 'sentiment', 'dependencies', 'of', 'aspects', 'via', 'this', 'graph', 'structure', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'DT', 'NN', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",13
sentiment_analysis,27,44,"As far as we know , our work is the first to consider the sentiment dependencies between aspects in one sentence for aspect - level sentiment classification task .","['As', 'far', 'as', 'we', 'know', ',', 'our', 'work', 'is', 'the', 'first', 'to', 'consider', 'the', 'sentiment', 'dependencies', 'between', 'aspects', 'in', 'one', 'sentence', 'for', 'aspect', '-', 'level', 'sentiment', 'classification', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', 'RB', 'IN', 'PRP', 'VBP', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'JJ', 'TO', 'VB', 'DT', 'NN', 'NNS', 'IN', 'NNS', 'IN', 'CD', 'NN', 'IN', 'JJ', ':', 'NN', 'NN', 'NN', 'NN', '.']",29
sentiment_analysis,27,45,"Furthermore , in order to capture the aspect - specific representations , our model applies bidirectional attention mechanism with position encoding before GCN .","['Furthermore', ',', 'in', 'order', 'to', 'capture', 'the', 'aspect', '-', 'specific', 'representations', ',', 'our', 'model', 'applies', 'bidirectional', 'attention', 'mechanism', 'with', 'position', 'encoding', 'before', 'GCN', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['RB', ',', 'IN', 'NN', 'TO', 'VB', 'DT', 'NN', ':', 'JJ', 'NNS', ',', 'PRP$', 'NN', 'VBZ', 'JJ', 'NN', 'NN', 'IN', 'NN', 'VBG', 'IN', 'NNP', '.']",24
sentiment_analysis,27,184,"In our implementation , we respectively use the GloVe 3 word vector and the pre-trained language model word representation BERT 4 to initialize the word embeddings .","['In', 'our', 'implementation', ',', 'we', 'respectively', 'use', 'the', 'GloVe', '3', 'word', 'vector', 'and', 'the', 'pre-trained', 'language', 'model', 'word', 'representation', 'BERT', '4', 'to', 'initialize', 'the', 'word', 'embeddings', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'PRP$', 'NN', ',', 'PRP', 'RB', 'VBP', 'DT', 'NNP', 'CD', 'NN', 'NN', 'CC', 'DT', 'JJ', 'NN', 'NN', 'NN', 'NN', 'NNP', 'CD', 'TO', 'VB', 'DT', 'NN', 'NNS', '.']",27
sentiment_analysis,27,185,The dimension of each word vector is 300 for GloVe and 768 for BERT .,"['The', 'dimension', 'of', 'each', 'word', 'vector', 'is', '300', 'for', 'GloVe', 'and', '768', 'for', 'BERT', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'CD', 'IN', 'NNP', 'CC', 'CD', 'IN', 'NNP', '.']",15
sentiment_analysis,27,186,"The number of LSTM hidden units is set to 300 , and the output dimension of GCN layer is set to 600 .","['The', 'number', 'of', 'LSTM', 'hidden', 'units', 'is', 'set', 'to', '300', ',', 'and', 'the', 'output', 'dimension', 'of', 'GCN', 'layer', 'is', 'set', 'to', '600', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NNP', 'JJ', 'NNS', 'VBZ', 'VBN', 'TO', 'CD', ',', 'CC', 'DT', 'NN', 'NN', 'IN', 'NNP', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",23
sentiment_analysis,27,187,"The weight matrix of last fully connect layer is randomly initialized by a normal distribution N ( 0 , 1 ) .","['The', 'weight', 'matrix', 'of', 'last', 'fully', 'connect', 'layer', 'is', 'randomly', 'initialized', 'by', 'a', 'normal', 'distribution', 'N', '(', '0', ',', '1', ')', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'IN', 'JJ', 'RB', 'JJ', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NNP', '(', 'CD', ',', 'CD', ')', '.']",22
sentiment_analysis,27,188,"Besides the last fully connect layer , all the weight matrices are randomly initialized by a uniform distribution U ( ? 0.01 , 0.01 ) .","['Besides', 'the', 'last', 'fully', 'connect', 'layer', ',', 'all', 'the', 'weight', 'matrices', 'are', 'randomly', 'initialized', 'by', 'a', 'uniform', 'distribution', 'U', '(', '?', '0.01', ',', '0.01', ')', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'RB', 'JJ', 'NN', ',', 'PDT', 'DT', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NNP', '(', '.', 'CD', ',', 'CD', ')', '.']",26
sentiment_analysis,27,189,"In addition , we add L2-regularization to the last fully connect layer with a weight of 0.01 .","['In', 'addition', ',', 'we', 'add', 'L2-regularization', 'to', 'the', 'last', 'fully', 'connect', 'layer', 'with', 'a', 'weight', 'of', '0.01', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'JJ', 'TO', 'DT', 'JJ', 'RB', 'VBP', 'NN', 'IN', 'DT', 'NN', 'IN', 'CD', '.']",18
sentiment_analysis,27,190,"During training , we set dropout to 0.5 , the batch size is set to 32 and the optimizer is Adam Optimizer with a learning rate of 0.001 .","['During', 'training', ',', 'we', 'set', 'dropout', 'to', '0.5', ',', 'the', 'batch', 'size', 'is', 'set', 'to', '32', 'and', 'the', 'optimizer', 'is', 'Adam', 'Optimizer', 'with', 'a', 'learning', 'rate', 'of', '0.001', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'RB', 'TO', 'CD', ',', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'CC', 'DT', 'NN', 'VBZ', 'NNP', 'NNP', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', '.']",29
sentiment_analysis,27,191,We implement our proposed model using Tensorflow 5 .,"['We', 'implement', 'our', 'proposed', 'model', 'using', 'Tensorflow', '5', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O']","['PRP', 'VBP', 'PRP$', 'VBN', 'NN', 'VBG', 'NNP', 'CD', '.']",9
sentiment_analysis,27,196,"TD - LSTM constructs aspect-specific representation by the left context with aspect and the right context with aspect , then employs two LSTMs to model them respectively .","['TD', '-', 'LSTM', 'constructs', 'aspect-specific', 'representation', 'by', 'the', 'left', 'context', 'with', 'aspect', 'and', 'the', 'right', 'context', 'with', 'aspect', ',', 'then', 'employs', 'two', 'LSTMs', 'to', 'model', 'them', 'respectively', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'JJ', 'NNS', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'CC', 'DT', 'JJ', 'NN', 'IN', 'NN', ',', 'RB', 'VBZ', 'CD', 'NNP', 'TO', 'VB', 'PRP', 'RB', '.']",28
sentiment_analysis,27,197,The last hidden states of the two LSTMs are finally concatenated for predicting the sentiment polarity of the aspect .,"['The', 'last', 'hidden', 'states', 'of', 'the', 'two', 'LSTMs', 'are', 'finally', 'concatenated', 'for', 'predicting', 'the', 'sentiment', 'polarity', 'of', 'the', 'aspect', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['DT', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'CD', 'NNP', 'VBP', 'RB', 'VBN', 'IN', 'VBG', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', '.']",20
sentiment_analysis,27,198,"ATAE - LSTM first attaches the aspect embedding to each word embedding to capture aspect - dependent information , and then employs attention mechanism to get the sentence representation for final classification .","['ATAE', '-', 'LSTM', 'first', 'attaches', 'the', 'aspect', 'embedding', 'to', 'each', 'word', 'embedding', 'to', 'capture', 'aspect', '-', 'dependent', 'information', ',', 'and', 'then', 'employs', 'attention', 'mechanism', 'to', 'get', 'the', 'sentence', 'representation', 'for', 'final', 'classification', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'RB', 'VBZ', 'DT', 'NN', 'VBG', 'TO', 'DT', 'NN', 'VBG', 'TO', 'VB', 'JJ', ':', 'NN', 'NN', ',', 'CC', 'RB', 'VBZ', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'JJ', 'NN', '.']",33
sentiment_analysis,27,199,Mem Net uses a deep memory network on the context word embeddings for sentence representation to capture the relevance between each context word and the aspect .,"['Mem', 'Net', 'uses', 'a', 'deep', 'memory', 'network', 'on', 'the', 'context', 'word', 'embeddings', 'for', 'sentence', 'representation', 'to', 'capture', 'the', 'relevance', 'between', 'each', 'context', 'word', 'and', 'the', 'aspect', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'JJ', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'NNS', 'IN', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'DT', 'NN', '.']",27
sentiment_analysis,27,201,IAN generates the representations for aspect terms and contexts with two attention - based LSTM network separately .,"['IAN', 'generates', 'the', 'representations', 'for', 'aspect', 'terms', 'and', 'contexts', 'with', 'two', 'attention', '-', 'based', 'LSTM', 'network', 'separately', '.']","['B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['NNP', 'VBZ', 'DT', 'NNS', 'IN', 'NN', 'NNS', 'CC', 'NN', 'IN', 'CD', 'NN', ':', 'VBN', 'NNP', 'NN', 'RB', '.']",18
sentiment_analysis,27,203,"RAM [ 10 ] employs a gated recurrent unit network to model a multiple attention mechanism , and captures the relevance between each context word and the aspect .","['RAM', '[', '10', ']', 'employs', 'a', 'gated', 'recurrent', 'unit', 'network', 'to', 'model', 'a', 'multiple', 'attention', 'mechanism', ',', 'and', 'captures', 'the', 'relevance', 'between', 'each', 'context', 'word', 'and', 'the', 'aspect', '.']","['B-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'CD', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', ',', 'CC', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'DT', 'NN', '.']",29
sentiment_analysis,27,205,PBAN appends the position embedding into each word embedding .,"['PBAN', 'appends', 'the', 'position', 'embedding', 'into', 'each', 'word', 'embedding', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'VBG', 'IN', 'DT', 'NN', 'NN', '.']",10
sentiment_analysis,27,207,TSN is a two - stage framework for aspect - level sentiment analysis .,"['TSN', 'is', 'a', 'two', '-', 'stage', 'framework', 'for', 'aspect', '-', 'level', 'sentiment', 'analysis', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'CD', ':', 'NN', 'NN', 'IN', 'JJ', ':', 'NN', 'NN', 'NN', '.']",14
sentiment_analysis,27,210,"AEN mainly consists of an embedding layer , an attentional encoder layer , an aspect - specific attention layer , and an output layer .","['AEN', 'mainly', 'consists', 'of', 'an', 'embedding', 'layer', ',', 'an', 'attentional', 'encoder', 'layer', ',', 'an', 'aspect', '-', 'specific', 'attention', 'layer', ',', 'and', 'an', 'output', 'layer', '.']","['B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['NNP', 'RB', 'VBZ', 'IN', 'DT', 'VBG', 'NN', ',', 'DT', 'JJ', 'NN', 'NN', ',', 'DT', 'JJ', ':', 'JJ', 'NN', 'NN', ',', 'CC', 'DT', 'NN', 'NN', '.']",25
sentiment_analysis,27,212,AEN - BERT is AEN with BERT embedding .,"['AEN', '-', 'BERT', 'is', 'AEN', 'with', 'BERT', 'embedding', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'VBZ', 'NNP', 'IN', 'NNP', 'VBG', '.']",9
sentiment_analysis,27,218,"Among all the GloVe - based methods , the TD - LSTM approach performs worst because it takes the aspect information into consideration in a very coarse way .","['Among', 'all', 'the', 'GloVe', '-', 'based', 'methods', ',', 'the', 'TD', '-', 'LSTM', 'approach', 'performs', 'worst', 'because', 'it', 'takes', 'the', 'aspect', 'information', 'into', 'consideration', 'in', 'a', 'very', 'coarse', 'way', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PDT', 'DT', 'NNP', ':', 'VBN', 'NNS', ',', 'DT', 'NNP', ':', 'NNP', 'NN', 'NNS', 'VBP', 'IN', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'DT', 'RB', 'JJ', 'NN', '.']",29
sentiment_analysis,27,220,"After taking the importance of the aspect into account with attention mechanism , they achieve a stable improvement comparing to the TD - LSTM .","['After', 'taking', 'the', 'importance', 'of', 'the', 'aspect', 'into', 'account', 'with', 'attention', 'mechanism', ',', 'they', 'achieve', 'a', 'stable', 'improvement', 'comparing', 'to', 'the', 'TD', '-', 'LSTM', '.']","['B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'IN', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBG', 'TO', 'DT', 'NNP', ':', 'NN', '.']",25
sentiment_analysis,27,221,"RAM achieves a better performance than other basic attention - based models , because it combines multiple attentions with a recurrent neural network to capture aspect - specific representations .","['RAM', 'achieves', 'a', 'better', 'performance', 'than', 'other', 'basic', 'attention', '-', 'based', 'models', ',', 'because', 'it', 'combines', 'multiple', 'attentions', 'with', 'a', 'recurrent', 'neural', 'network', 'to', 'capture', 'aspect', '-', 'specific', 'representations', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'DT', 'JJR', 'NN', 'IN', 'JJ', 'JJ', 'NN', ':', 'VBN', 'NNS', ',', 'IN', 'PRP', 'VBZ', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'TO', 'VB', 'JJ', ':', 'JJ', 'NNS', '.']",30
sentiment_analysis,27,222,PBAN achieves a similar performance as RAM by employing a position embedding .,"['PBAN', 'achieves', 'a', 'similar', 'performance', 'as', 'RAM', 'by', 'employing', 'a', 'position', 'embedding', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'IN', 'VBG', 'DT', 'NN', 'NN', '.']",13
sentiment_analysis,27,223,"To be specific , PBAN is better than RAM on Restaurant dataset , but worse than RAN on Laptop dataset .","['To', 'be', 'specific', ',', 'PBAN', 'is', 'better', 'than', 'RAM', 'on', 'Restaurant', 'dataset', ',', 'but', 'worse', 'than', 'RAN', 'on', 'Laptop', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['TO', 'VB', 'JJ', ',', 'NNP', 'VBZ', 'JJR', 'IN', 'NNP', 'IN', 'NNP', 'NN', ',', 'CC', 'JJR', 'IN', 'NNP', 'IN', 'NNP', 'NN', '.']",21
sentiment_analysis,27,224,"Compared with RAM and PBAN , the over all performance of TSN is not perform well on both Restaurant dataset and Laptop dataset , which might because the framework of TSN is too simple to model the representations of context and aspect effectively .","['Compared', 'with', 'RAM', 'and', 'PBAN', ',', 'the', 'over', 'all', 'performance', 'of', 'TSN', 'is', 'not', 'perform', 'well', 'on', 'both', 'Restaurant', 'dataset', 'and', 'Laptop', 'dataset', ',', 'which', 'might', 'because', 'the', 'framework', 'of', 'TSN', 'is', 'too', 'simple', 'to', 'model', 'the', 'representations', 'of', 'context', 'and', 'aspect', 'effectively', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'IN', 'NNP', 'CC', 'NNP', ',', 'DT', 'IN', 'DT', 'NN', 'IN', 'NNP', 'VBZ', 'RB', 'RB', 'RB', 'IN', 'DT', 'NNP', 'NN', 'CC', 'NNP', 'NN', ',', 'WDT', 'MD', 'IN', 'DT', 'NN', 'IN', 'NNP', 'VBZ', 'RB', 'JJ', 'TO', 'VB', 'DT', 'NNS', 'IN', 'NN', 'CC', 'VB', 'RB', '.']",44
sentiment_analysis,27,225,"AEN is slightly better than TSN , but still worse than RAM and PBAN .","['AEN', 'is', 'slightly', 'better', 'than', 'TSN', ',', 'but', 'still', 'worse', 'than', 'RAM', 'and', 'PBAN', '.']","['B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'RB', 'JJR', 'IN', 'NNP', ',', 'CC', 'RB', 'JJR', 'IN', 'NNP', 'CC', 'NNP', '.']",15
sentiment_analysis,27,227,"Comparing the results of SDGCN - A w/o position and SDGCN - G w/o position , SDGCN - A and SDGCN - G , respectively , we observe that the GCN built with global - relation is slightly higher than built with adjacent - relation in both accuracy and Macro - F1 measure .","['Comparing', 'the', 'results', 'of', 'SDGCN', '-', 'A', 'w/o', 'position', 'and', 'SDGCN', '-', 'G', 'w/o', 'position', ',', 'SDGCN', '-', 'A', 'and', 'SDGCN', '-', 'G', ',', 'respectively', ',', 'we', 'observe', 'that', 'the', 'GCN', 'built', 'with', 'global', '-', 'relation', 'is', 'slightly', 'higher', 'than', 'built', 'with', 'adjacent', '-', 'relation', 'in', 'both', 'accuracy', 'and', 'Macro', '-', 'F1', 'measure', '.']","['B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'DT', 'NNS', 'IN', 'NNP', ':', 'DT', 'JJ', 'NN', 'CC', 'NNP', ':', 'NNP', 'JJ', 'NN', ',', 'NNP', ':', 'DT', 'CC', 'NNP', ':', 'NNP', ',', 'RB', ',', 'PRP', 'VBP', 'IN', 'DT', 'NNP', 'VBN', 'IN', 'JJ', ':', 'NN', 'VBZ', 'RB', 'JJR', 'IN', 'VBN', 'IN', 'JJ', ':', 'NN', 'IN', 'DT', 'NN', 'CC', 'NNP', ':', 'NNP', 'NN', '.']",54
sentiment_analysis,27,229,"Moreover , the two models ( SDGCN - A and SDGCN - G ) with position information gain a significant improvement compared to the two models without position information .","['Moreover', ',', 'the', 'two', 'models', '(', 'SDGCN', '-', 'A', 'and', 'SDGCN', '-', 'G', ')', 'with', 'position', 'information', 'gain', 'a', 'significant', 'improvement', 'compared', 'to', 'the', 'two', 'models', 'without', 'position', 'information', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'DT', 'CD', 'NNS', '(', 'NNP', ':', 'DT', 'CC', 'NNP', ':', 'NNP', ')', 'IN', 'NN', 'NN', 'NN', 'DT', 'JJ', 'NN', 'VBN', 'TO', 'DT', 'CD', 'NNS', 'IN', 'NN', 'NN', '.']",30
sentiment_analysis,27,231,"Benefits from the power of pre-trained BERT , BERT - based models have shown huge superiority over GloVe - based models .","['Benefits', 'from', 'the', 'power', 'of', 'pre-trained', 'BERT', ',', 'BERT', '-', 'based', 'models', 'have', 'shown', 'huge', 'superiority', 'over', 'GloVe', '-', 'based', 'models', '.']","['B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNP', ',', 'NNP', ':', 'VBN', 'NNS', 'VBP', 'VBN', 'JJ', 'NN', 'IN', 'NNP', ':', 'VBN', 'NNS', '.']",22
sentiment_analysis,27,232,"Furthermore , compared with AEN - BERT , on the Restaurant dataset , SDGCN - BERT achieves absolute increases of 1.09 % and 1.86 % in accuracy and Macro - F1 measure respectively , and gains absolute increases of 1.42 % and 2.03 % in accuracy and Macro - F1 measure respectively on the Laptop dataset .","['Furthermore', ',', 'compared', 'with', 'AEN', '-', 'BERT', ',', 'on', 'the', 'Restaurant', 'dataset', ',', 'SDGCN', '-', 'BERT', 'achieves', 'absolute', 'increases', 'of', '1.09', '%', 'and', '1.86', '%', 'in', 'accuracy', 'and', 'Macro', '-', 'F1', 'measure', 'respectively', ',', 'and', 'gains', 'absolute', 'increases', 'of', '1.42', '%', 'and', '2.03', '%', 'in', 'accuracy', 'and', 'Macro', '-', 'F1', 'measure', 'respectively', 'on', 'the', 'Laptop', 'dataset', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'VBN', 'IN', 'NNP', ':', 'NNP', ',', 'IN', 'DT', 'NNP', 'NN', ',', 'NNP', ':', 'NNP', 'VBZ', 'JJ', 'NNS', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'NN', 'CC', 'NNP', ':', 'NNP', 'NN', 'RB', ',', 'CC', 'NNS', 'VBP', 'NNS', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'NN', 'CC', 'NNP', ':', 'NNP', 'NN', 'RB', 'IN', 'DT', 'NNP', 'NN', '.']",57
sentiment_analysis,50,2,Exploiting Document Knowledge for Aspect - level Sentiment Classification,"['Exploiting', 'Document', 'Knowledge', 'for', 'Aspect', '-', 'level', 'Sentiment', 'Classification']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['VBG', 'NNP', 'NNP', 'IN', 'NNP', ':', 'NN', 'NNP', 'NNP']",9
sentiment_analysis,50,21,"Specifically , we explore two transfer methods to incorporate this sort of knowledge - pretraining and multi-task learning .","['Specifically', ',', 'we', 'explore', 'two', 'transfer', 'methods', 'to', 'incorporate', 'this', 'sort', 'of', 'knowledge', '-', 'pretraining', 'and', 'multi-task', 'learning', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'CD', 'NN', 'NNS', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', ':', 'NN', 'CC', 'NN', 'NN', '.']",19
sentiment_analysis,50,24,Our source code can be obtained from https://github.com/ruidan/Aspect-level-sentiment.,"['Our', 'source', 'code', 'can', 'be', 'obtained', 'from', 'https://github.com/ruidan/Aspect-level-sentiment.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']","['PRP$', 'NN', 'NN', 'MD', 'VB', 'VBN', 'IN', 'NN']",8
sentiment_analysis,50,87,"In all experiments , 300 - dimension Glo Ve vectors are used to initialize E and E when pretraining is not conducted for weight initialization .","['In', 'all', 'experiments', ',', '300', '-', 'dimension', 'Glo', 'Ve', 'vectors', 'are', 'used', 'to', 'initialize', 'E', 'and', 'E', 'when', 'pretraining', 'is', 'not', 'conducted', 'for', 'weight', 'initialization', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'CD', ':', 'NN', 'NNP', 'NNP', 'NNS', 'VBP', 'VBN', 'TO', 'VB', 'NNP', 'CC', 'NNP', 'WRB', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'JJ', 'NN', '.']",26
sentiment_analysis,50,88,These vectors are also used for initializing E in the pretraining phase .,"['These', 'vectors', 'are', 'also', 'used', 'for', 'initializing', 'E', 'in', 'the', 'pretraining', 'phase', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'VBG', 'NNP', 'IN', 'DT', 'NN', 'NN', '.']",13
sentiment_analysis,50,90,We randomly sample 20 % of the original training data from the aspectlevel dataset as the development set and only use the remaining 80 % for training .,"['We', 'randomly', 'sample', '20', '%', 'of', 'the', 'original', 'training', 'data', 'from', 'the', 'aspectlevel', 'dataset', 'as', 'the', 'development', 'set', 'and', 'only', 'use', 'the', 'remaining', '80', '%', 'for', 'training', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', 'CD', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'RB', 'VB', 'DT', 'VBG', 'CD', 'NN', 'IN', 'NN', '.']",28
sentiment_analysis,50,91,"For all experiments , the dimension of LSTM hidden vectors is set to 300 , ?","['For', 'all', 'experiments', ',', 'the', 'dimension', 'of', 'LSTM', 'hidden', 'vectors', 'is', 'set', 'to', '300', ',', '?']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O']","['IN', 'DT', 'NNS', ',', 'DT', 'NN', 'IN', 'NNP', 'JJ', 'NNS', 'VBZ', 'VBN', 'TO', 'CD', ',', '.']",16
sentiment_analysis,50,92,"is set to 0.1 , and we use dropout with probability 0.5 on sentence / document representations before the output layer .","['is', 'set', 'to', '0.1', ',', 'and', 'we', 'use', 'dropout', 'with', 'probability', '0.5', 'on', 'sentence', '/', 'document', 'representations', 'before', 'the', 'output', 'layer', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['VBZ', 'VBN', 'TO', 'CD', ',', 'CC', 'PRP', 'VBP', 'IN', 'IN', 'NN', 'CD', 'IN', 'NN', 'CD', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",22
sentiment_analysis,50,93,We use RMSProp as the optimizer with the decay rate set to 0.9 and the base learning rate set to 0.001 .,"['We', 'use', 'RMSProp', 'as', 'the', 'optimizer', 'with', 'the', 'decay', 'rate', 'set', 'to', '0.9', 'and', 'the', 'base', 'learning', 'rate', 'set', 'to', '0.001', '.']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'NNP', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBN', 'TO', 'CD', 'CC', 'DT', 'NN', 'VBG', 'NN', 'VBN', 'TO', 'CD', '.']",22
sentiment_analysis,50,94,The mini - batch size is set to 32 .,"['The', 'mini', '-', 'batch', 'size', 'is', 'set', 'to', '32', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', ':', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",10
sentiment_analysis,50,100,"We observe that PRET is very helpful , and consistently gives a 1 - 3 % increase in accuracy over LSTM + ATT across all datasets .","['We', 'observe', 'that', 'PRET', 'is', 'very', 'helpful', ',', 'and', 'consistently', 'gives', 'a', '1', '-', '3', '%', 'increase', 'in', 'accuracy', 'over', 'LSTM', '+', 'ATT', 'across', 'all', 'datasets', '.']","['O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'VBZ', 'RB', 'JJ', ',', 'CC', 'RB', 'VBZ', 'DT', 'CD', ':', 'CD', 'NN', 'NN', 'IN', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'IN', 'DT', 'NNS', '.']",27
sentiment_analysis,50,102,"MULT gives similar performance as LSTM + ATT on D1 and D2 , but improvements can be clearly observed for D3 and D4 .","['MULT', 'gives', 'similar', 'performance', 'as', 'LSTM', '+', 'ATT', 'on', 'D1', 'and', 'D2', ',', 'but', 'improvements', 'can', 'be', 'clearly', 'observed', 'for', 'D3', 'and', 'D4', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'CC', 'NNP', ',', 'CC', 'NNS', 'MD', 'VB', 'RB', 'VBN', 'IN', 'NNP', 'CC', 'NNP', '.']",24
sentiment_analysis,50,103,The combination ( PRET + MULT ) over all yields better results .,"['The', 'combination', '(', 'PRET', '+', 'MULT', ')', 'over', 'all', 'yields', 'better', 'results', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', '(', 'NNP', 'NNP', 'NNP', ')', 'IN', 'DT', 'NNS', 'JJR', 'NNS', '.']",13
sentiment_analysis,50,105,( 2 ) The numbers of neutral examples in the test sets of D3 and D4 are very small .,"['(', '2', ')', 'The', 'numbers', 'of', 'neutral', 'examples', 'in', 'the', 'test', 'sets', 'of', 'D3', 'and', 'D4', 'are', 'very', 'small', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['(', 'CD', ')', 'DT', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NNS', 'IN', 'NNP', 'CC', 'NNP', 'VBP', 'RB', 'JJ', '.']",20
sentiment_analysis,50,115,"( 2 ) Overall , transfers of the LSTM and embedding layer are more useful than the output layer .","['(', '2', ')', 'Overall', ',', 'transfers', 'of', 'the', 'LSTM', 'and', 'embedding', 'layer', 'are', 'more', 'useful', 'than', 'the', 'output', 'layer', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['(', 'CD', ')', 'NNP', ',', 'NNS', 'IN', 'DT', 'NNP', 'CC', 'VBG', 'NN', 'VBP', 'RBR', 'JJ', 'IN', 'DT', 'NN', 'NN', '.']",20
sentiment_analysis,50,117,( 3 ) Transfer of the embedding layer is more helpful on D3 and D4 .,"['(', '3', ')', 'Transfer', 'of', 'the', 'embedding', 'layer', 'is', 'more', 'helpful', 'on', 'D3', 'and', 'D4', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'NN', 'IN', 'DT', 'VBG', 'NN', 'VBZ', 'RBR', 'JJ', 'IN', 'NNP', 'CC', 'NNP', '.']",16
sentiment_analysis,50,119,Sentiment information is not adequately captured by Glo Ve word embeddings .,"['Sentiment', 'information', 'is', 'not', 'adequately', 'captured', 'by', 'Glo', 'Ve', 'word', 'embeddings', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'NN', 'VBZ', 'RB', 'RB', 'VBN', 'IN', 'NNP', 'NNP', 'NN', 'NNS', '.']",12
sentiment_analysis,34,2,Mazajak : An Online Arabic Sentiment Analyser,"['Mazajak', ':', 'An', 'Online', 'Arabic', 'Sentiment', 'Analyser']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NN', ':', 'DT', 'NNP', 'NNP', 'NNP', 'NNP']",7
sentiment_analysis,34,4,Sentiment analysis ( SA ) is one of the most useful natural language processing applications .,"['Sentiment', 'analysis', '(', 'SA', ')', 'is', 'one', 'of', 'the', 'most', 'useful', 'natural', 'language', 'processing', 'applications', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', '(', 'NNP', ')', 'VBZ', 'CD', 'IN', 'DT', 'RBS', 'JJ', 'JJ', 'NN', 'NN', 'NNS', '.']",16
sentiment_analysis,34,14,Sentiment analysis is one of the vital approaches to extract public opinion from large corpora of text .,"['Sentiment', 'analysis', 'is', 'one', 'of', 'the', 'vital', 'approaches', 'to', 'extract', 'public', 'opinion', 'from', 'large', 'corpora', 'of', 'text', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'VBZ', 'CD', 'IN', 'DT', 'JJ', 'NNS', 'TO', 'VB', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NN', '.']",18
sentiment_analysis,34,17,"Work on SA started in early 2000s , particularly with the work of , where they studied the sentiment of movies ' reviews .","['Work', 'on', 'SA', 'started', 'in', 'early', '2000s', ',', 'particularly', 'with', 'the', 'work', 'of', ',', 'where', 'they', 'studied', 'the', 'sentiment', 'of', 'movies', ""'"", 'reviews', '.']","['O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'IN', 'NNP', 'VBD', 'IN', 'JJ', 'CD', ',', 'RB', 'IN', 'DT', 'NN', 'IN', ',', 'WRB', 'PRP', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'POS', 'NNS', '.']",24
sentiment_analysis,34,27,"In this paper , we present Mazajak 2 , an Online Arabic sentiment analysis system that utilises deep learning and massive Arabic word embeddings .","['In', 'this', 'paper', ',', 'we', 'present', 'Mazajak', '2', ',', 'an', 'Online', 'Arabic', 'sentiment', 'analysis', 'system', 'that', 'utilises', 'deep', 'learning', 'and', 'massive', 'Arabic', 'word', 'embeddings', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'CD', ',', 'DT', 'NNP', 'NNP', 'NN', 'NN', 'NN', 'WDT', 'VBZ', 'JJ', 'NN', 'CC', 'JJ', 'NNP', 'NN', 'NNS', '.']",25
sentiment_analysis,34,28,The system is available as an online API that can be used by other researchers .,"['The', 'system', 'is', 'available', 'as', 'an', 'online', 'API', 'that', 'can', 'be', 'used', 'by', 'other', 'researchers', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'JJ', 'IN', 'DT', 'NN', 'NNP', 'WDT', 'MD', 'VB', 'VBN', 'IN', 'JJ', 'NNS', '.']",16
sentiment_analysis,34,113,The best performing system in the SemEval 2017 task is the one described in which achieved an F P N of 0.61 .,"['The', 'best', 'performing', 'system', 'in', 'the', 'SemEval', '2017', 'task', 'is', 'the', 'one', 'described', 'in', 'which', 'achieved', 'an', 'F', 'P', 'N', 'of', '0.61', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJS', 'NN', 'NN', 'IN', 'DT', 'JJ', 'CD', 'NN', 'VBZ', 'DT', 'CD', 'NN', 'IN', 'WDT', 'VBD', 'DT', 'NNP', 'NNP', 'NNP', 'IN', 'CD', '.']",23
sentiment_analysis,34,114,"For the ASTD , the best reported results are by who used an ensemble system combining output of CNN and Bi - LSTM architectures , which achieved an F P N of 0.71 .","['For', 'the', 'ASTD', ',', 'the', 'best', 'reported', 'results', 'are', 'by', 'who', 'used', 'an', 'ensemble', 'system', 'combining', 'output', 'of', 'CNN', 'and', 'Bi', '-', 'LSTM', 'architectures', ',', 'which', 'achieved', 'an', 'F', 'P', 'N', 'of', '0.71', '.']","['B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNP', ',', 'DT', 'JJS', 'JJ', 'NNS', 'VBP', 'IN', 'WP', 'VBD', 'DT', 'JJ', 'NN', 'VBG', 'NN', 'IN', 'NNP', 'CC', 'NNP', ':', 'NNP', 'NNS', ',', 'WDT', 'VBD', 'DT', 'NNP', 'NNP', 'NNP', 'IN', 'CD', '.']",34
sentiment_analysis,34,118,"As shown in the table , Mazajak model outperformed the current state - of - the - art models on the SemEval and ASTD datasets .","['As', 'shown', 'in', 'the', 'table', ',', 'Mazajak', 'model', 'outperformed', 'the', 'current', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', 'on', 'the', 'SemEval', 'and', 'ASTD', 'datasets', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'IN', 'DT', 'NN', ',', 'NNP', 'NN', 'VBD', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NNS', '.']",26
sentiment_analysis,34,119,"In addition , it achieved a high performance on the ArSAS dataset .","['In', 'addition', ',', 'it', 'achieved', 'a', 'high', 'performance', 'on', 'the', 'ArSAS', 'dataset', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",13
sentiment_analysis,34,120,"Our reported scores are higher than current top systems for all the evaluation scores , including average recall , F P N , and accuracy .","['Our', 'reported', 'scores', 'are', 'higher', 'than', 'current', 'top', 'systems', 'for', 'all', 'the', 'evaluation', 'scores', ',', 'including', 'average', 'recall', ',', 'F', 'P', 'N', ',', 'and', 'accuracy', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'JJ', 'NNS', 'VBP', 'JJR', 'IN', 'JJ', 'JJ', 'NNS', 'IN', 'PDT', 'DT', 'NN', 'NNS', ',', 'VBG', 'JJ', 'NN', ',', 'NNP', 'NNP', 'NNP', ',', 'CC', 'NN', '.']",26
sentiment_analysis,34,121,These results confirm that our model choice for our tool represents the current state - of - the - art for Arabic SA .,"['These', 'results', 'confirm', 'that', 'our', 'model', 'choice', 'for', 'our', 'tool', 'represents', 'the', 'current', 'state', '-', 'of', '-', 'the', '-', 'art', 'for', 'Arabic', 'SA', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'IN', 'PRP$', 'NN', 'NN', 'IN', 'PRP$', 'NN', 'VBZ', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'NNP', 'NNP', '.']",24
sentiment_analysis,37,2,IARM : Inter-Aspect Relation Modeling with Memory Networks in Aspect - Based Sentiment Analysis,"['IARM', ':', 'Inter-Aspect', 'Relation', 'Modeling', 'with', 'Memory', 'Networks', 'in', 'Aspect', '-', 'Based', 'Sentiment', 'Analysis']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NN', ':', 'JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'NNP', ':', 'VBD', 'JJ', 'NN']",14
sentiment_analysis,37,4,Sentiment analysis has immense implications in modern businesses through user-feedback mining .,"['Sentiment', 'analysis', 'has', 'immense', 'implications', 'in', 'modern', 'businesses', 'through', 'user-feedback', 'mining', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'VBZ', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NN', '.']",12
sentiment_analysis,37,13,Aspect - based sentiment analysis ( ABSA ) caters to these needs .,"['Aspect', '-', 'based', 'sentiment', 'analysis', '(', 'ABSA', ')', 'caters', 'to', 'these', 'needs', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'VBN', 'NN', 'NN', '(', 'NNP', ')', 'NNS', 'TO', 'DT', 'NNS', '.']",13
sentiment_analysis,37,26,The aim of the ABSA classifier is to learn these connections between the aspects and their sentiment bearing phrases .,"['The', 'aim', 'of', 'the', 'ABSA', 'classifier', 'is', 'to', 'learn', 'these', 'connections', 'between', 'the', 'aspects', 'and', 'their', 'sentiment', 'bearing', 'phrases', '.']","['O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'DT', 'NNP', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NNS', 'IN', 'DT', 'NNS', 'CC', 'PRP$', 'NN', 'NN', 'NNS', '.']",20
sentiment_analysis,37,33,"To model these scenarios , firstly , following , we independently generate aspect - aware sentence representations for all the aspects using gated recurrent unit ( GRU ) and attention mechanism .","['To', 'model', 'these', 'scenarios', ',', 'firstly', ',', 'following', ',', 'we', 'independently', 'generate', 'aspect', '-', 'aware', 'sentence', 'representations', 'for', 'all', 'the', 'aspects', 'using', 'gated', 'recurrent', 'unit', '(', 'GRU', ')', 'and', 'attention', 'mechanism', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NNS', ',', 'RB', ',', 'VBG', ',', 'PRP', 'RB', 'VBP', 'JJ', ':', 'JJ', 'NN', 'NNS', 'IN', 'PDT', 'DT', 'NNS', 'VBG', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'NN', 'NN', '.']",32
sentiment_analysis,37,34,"Then , we employ memory networks to repeatedly match the target aspect representation with the other aspects to generate more accurate representation of the target aspect .","['Then', ',', 'we', 'employ', 'memory', 'networks', 'to', 'repeatedly', 'match', 'the', 'target', 'aspect', 'representation', 'with', 'the', 'other', 'aspects', 'to', 'generate', 'more', 'accurate', 'representation', 'of', 'the', 'target', 'aspect', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'TO', 'RB', 'VB', 'DT', 'NN', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'TO', 'VB', 'JJR', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",27
sentiment_analysis,37,35,This refined representation is fed to a softmax classifier for final classification .,"['This', 'refined', 'representation', 'is', 'fed', 'to', 'a', 'softmax', 'classifier', 'for', 'final', 'classification', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'DT', 'NN', 'NN', 'IN', 'JJ', 'NN', '.']",13
sentiment_analysis,37,162,LSTM,['LSTM'],['B-n'],['NN'],1
sentiment_analysis,37,163,"Following , the sentence is fed to along short - term memory ( LSTM ) network to propagate context among the constituent words .","['Following', ',', 'the', 'sentence', 'is', 'fed', 'to', 'along', 'short', '-', 'term', 'memory', '(', 'LSTM', ')', 'network', 'to', 'propagate', 'context', 'among', 'the', 'constituent', 'words', '.']","['O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['VBG', ',', 'DT', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'JJ', ':', 'NN', 'NN', '(', 'NNP', ')', 'NN', 'TO', 'VB', 'NN', 'IN', 'DT', 'NN', 'NNS', '.']",24
sentiment_analysis,37,166,TD- LSTM,"['TD-', 'LSTM']","['B-n', 'I-n']","['CD', 'NNP']",2
sentiment_analysis,37,167,"Following , sequence of words preceding ( left context ) and succeeding ( right context ) target aspect term are fed to two different LSTMs .","['Following', ',', 'sequence', 'of', 'words', 'preceding', '(', 'left', 'context', ')', 'and', 'succeeding', '(', 'right', 'context', ')', 'target', 'aspect', 'term', 'are', 'fed', 'to', 'two', 'different', 'LSTMs', '.']","['O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['VBG', ',', 'NN', 'IN', 'NNS', 'VBG', '(', 'JJ', 'NN', ')', 'CC', 'VBG', '(', 'JJ', 'NN', ')', 'NN', 'JJ', 'NN', 'VBP', 'VBN', 'TO', 'CD', 'JJ', 'NNP', '.']",26
sentiment_analysis,37,172,"This representation is fed to softmax classifier. , ATAE - LSTM is identical to AE - LSTM , except the LSTM is fed with the concatenation of aspect - term representation and word representation . , target - aspect and its context are sent to two distinct LSTMs and the means of the hidden outputs are taken as intermediate aspect representation and context representation respectively .","['This', 'representation', 'is', 'fed', 'to', 'softmax', 'classifier.', ',', 'ATAE', '-', 'LSTM', 'is', 'identical', 'to', 'AE', '-', 'LSTM', ',', 'except', 'the', 'LSTM', 'is', 'fed', 'with', 'the', 'concatenation', 'of', 'aspect', '-', 'term', 'representation', 'and', 'word', 'representation', '.', ',', 'target', '-', 'aspect', 'and', 'its', 'context', 'are', 'sent', 'to', 'two', 'distinct', 'LSTMs', 'and', 'the', 'means', 'of', 'the', 'hidden', 'outputs', 'are', 'taken', 'as', 'intermediate', 'aspect', 'representation', 'and', 'context', 'representation', 'respectively', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'NN', ',', 'NNP', ':', 'NNP', 'VBZ', 'JJ', 'TO', 'NNP', ':', 'NNP', ',', 'IN', 'DT', 'NNP', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'IN', 'JJ', ':', 'NN', 'NN', 'CC', 'NN', 'NN', '.', ',', 'NN', ':', 'NN', 'CC', 'PRP$', 'NN', 'VBP', 'VBN', 'TO', 'CD', 'JJ', 'NNP', 'CC', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'JJ', 'NN', 'NN', 'CC', 'JJ', 'NN', 'RB', '.']",66
sentiment_analysis,37,196,"It is evident from the results that our IARM model outperforms all the baseline models , including the state of the art , in both of the domains .","['It', 'is', 'evident', 'from', 'the', 'results', 'that', 'our', 'IARM', 'model', 'outperforms', 'all', 'the', 'baseline', 'models', ',', 'including', 'the', 'state', 'of', 'the', 'art', ',', 'in', 'both', 'of', 'the', 'domains', '.']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBZ', 'JJ', 'IN', 'DT', 'NNS', 'IN', 'PRP$', 'NNP', 'NN', 'VBZ', 'PDT', 'DT', 'NN', 'NNS', ',', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'IN', 'DT', 'IN', 'DT', 'NNS', '.']",29
sentiment_analysis,37,197,"We obtained bigger improvement in laptop domain , of 1.7 % , compared to restaurant domain , of 1.4 % .","['We', 'obtained', 'bigger', 'improvement', 'in', 'laptop', 'domain', ',', 'of', '1.7', '%', ',', 'compared', 'to', 'restaurant', 'domain', ',', 'of', '1.4', '%', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBD', 'JJR', 'NN', 'IN', 'JJ', 'NN', ',', 'IN', 'CD', 'NN', ',', 'VBN', 'TO', 'VB', 'NN', ',', 'IN', 'CD', 'NN', '.']",21
sentiment_analysis,49,2,Contextual Inter-modal Attention for Multi-modal Sentiment Analysis,"['Contextual', 'Inter-modal', 'Attention', 'for', 'Multi-modal', 'Sentiment', 'Analysis']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NN']",7
sentiment_analysis,49,12,"Traditionally , sentiment analysis has been applied to a wide variety of texts .","['Traditionally', ',', 'sentiment', 'analysis', 'has', 'been', 'applied', 'to', 'a', 'wide', 'variety', 'of', 'texts', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'NN', 'NN', 'VBZ', 'VBN', 'VBN', 'TO', 'DT', 'JJ', 'NN', 'IN', 'NN', '.']",14
sentiment_analysis,49,26,"In this paper , we propose a novel method that employs a recurrent neural network based multimodal multi-utterance attention framework for sentiment prediction .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', 'method', 'that', 'employs', 'a', 'recurrent', 'neural', 'network', 'based', 'multimodal', 'multi-utterance', 'attention', 'framework', 'for', 'sentiment', 'prediction', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'VBN', 'RB', 'JJ', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",24
sentiment_analysis,49,31,To better address these concerns we propose a novel fusion method by focusing on inter-modality relations computed between the target utterance and its context .,"['To', 'better', 'address', 'these', 'concerns', 'we', 'propose', 'a', 'novel', 'fusion', 'method', 'by', 'focusing', 'on', 'inter-modality', 'relations', 'computed', 'between', 'the', 'target', 'utterance', 'and', 'its', 'context', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'JJR', 'VB', 'DT', 'NNS', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'VBG', 'IN', 'NN', 'NNS', 'VBD', 'IN', 'DT', 'NN', 'NN', 'CC', 'PRP$', 'NN', '.']",25
sentiment_analysis,49,37,The attention mechanism is then used to attend to the important contextual utterances having higher relatedness or similarity ( computed using inter-modality correlations ) with the target utterance .,"['The', 'attention', 'mechanism', 'is', 'then', 'used', 'to', 'attend', 'to', 'the', 'important', 'contextual', 'utterances', 'having', 'higher', 'relatedness', 'or', 'similarity', '(', 'computed', 'using', 'inter-modality', 'correlations', ')', 'with', 'the', 'target', 'utterance', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'RB', 'VBN', 'TO', 'VB', 'TO', 'DT', 'JJ', 'JJ', 'NNS', 'VBG', 'JJR', 'NN', 'CC', 'NN', '(', 'VBN', 'VBG', 'JJ', 'NNS', ')', 'IN', 'DT', 'NN', 'NN', '.']",29
sentiment_analysis,49,38,"Unlike previous approaches that simply apply attentions over the contextual utterance for classification , we attend over the contextual utterances by computing correlations among the modalities of the target utterance and the context utterances .","['Unlike', 'previous', 'approaches', 'that', 'simply', 'apply', 'attentions', 'over', 'the', 'contextual', 'utterance', 'for', 'classification', ',', 'we', 'attend', 'over', 'the', 'contextual', 'utterances', 'by', 'computing', 'correlations', 'among', 'the', 'modalities', 'of', 'the', 'target', 'utterance', 'and', 'the', 'context', 'utterances', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', 'NNS', 'WDT', 'RB', 'VB', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', ',', 'PRP', 'VBP', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'VBG', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', 'CC', 'DT', 'NN', 'NNS', '.']",35
sentiment_analysis,49,40,The model facilitates this modality selection by attending over the contextual utterances and thus generates better multimodal feature representation when these modalities from the context are combined with the modalities of the target utterance .,"['The', 'model', 'facilitates', 'this', 'modality', 'selection', 'by', 'attending', 'over', 'the', 'contextual', 'utterances', 'and', 'thus', 'generates', 'better', 'multimodal', 'feature', 'representation', 'when', 'these', 'modalities', 'from', 'the', 'context', 'are', 'combined', 'with', 'the', 'modalities', 'of', 'the', 'target', 'utterance', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'VBG', 'IN', 'DT', 'JJ', 'NNS', 'CC', 'RB', 'NNS', 'RBR', 'JJ', 'NN', 'NN', 'WRB', 'DT', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'VBN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",35
sentiment_analysis,49,149,"We use Bi-directional GRUs having 300 neurons , each followed by a dense layer consisting of 100 neurons .","['We', 'use', 'Bi-directional', 'GRUs', 'having', '300', 'neurons', ',', 'each', 'followed', 'by', 'a', 'dense', 'layer', 'consisting', 'of', '100', 'neurons', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NNP', 'VBG', 'CD', 'NNS', ',', 'DT', 'VBN', 'IN', 'DT', 'NN', 'NN', 'VBG', 'IN', 'CD', 'NNS', '.']",19
sentiment_analysis,49,150,"Utilizing the dense layer , we project the input features of all the three modalities to the same dimensions .","['Utilizing', 'the', 'dense', 'layer', ',', 'we', 'project', 'the', 'input', 'features', 'of', 'all', 'the', 'three', 'modalities', 'to', 'the', 'same', 'dimensions', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['VBG', 'DT', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NNS', 'IN', 'PDT', 'DT', 'CD', 'NNS', 'TO', 'DT', 'JJ', 'NNS', '.']",20
sentiment_analysis,49,151,We set dropout = 0.5 ( MOSI ) & 0.3 ( MOSEI ) as a measure of regularization .,"['We', 'set', 'dropout', '=', '0.5', '(', 'MOSI', ')', '&', '0.3', '(', 'MOSEI', ')', 'as', 'a', 'measure', 'of', 'regularization', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'RB', 'JJ', 'CD', '(', 'NNP', ')', 'CC', 'CD', '(', 'NNP', ')', 'IN', 'DT', 'NN', 'IN', 'NN', '.']",19
sentiment_analysis,49,152,"In addition , we also use dropout = 0.4 ( MOSI ) & 0.3 ( MOSEI ) for the Bi - GRU layers .","['In', 'addition', ',', 'we', 'also', 'use', 'dropout', '=', '0.4', '(', 'MOSI', ')', '&', '0.3', '(', 'MOSEI', ')', 'for', 'the', 'Bi', '-', 'GRU', 'layers', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'RB', 'VBP', 'NN', 'NN', 'CD', '(', 'NNP', ')', 'CC', 'CD', '(', 'NNP', ')', 'IN', 'DT', 'NNP', ':', 'NN', 'NNS', '.']",24
sentiment_analysis,49,153,"We employ ReLu activation function in the dense layers , and softmax activation in the final classification layer .","['We', 'employ', 'ReLu', 'activation', 'function', 'in', 'the', 'dense', 'layers', ',', 'and', 'softmax', 'activation', 'in', 'the', 'final', 'classification', 'layer', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'NNS', ',', 'CC', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",19
sentiment_analysis,49,154,"For training the network we set the batch size = 32 , use Adam optimizer with cross - entropy loss function and train for 50 epochs .","['For', 'training', 'the', 'network', 'we', 'set', 'the', 'batch', 'size', '=', '32', ',', 'use', 'Adam', 'optimizer', 'with', 'cross', '-', 'entropy', 'loss', 'function', 'and', 'train', 'for', '50', 'epochs', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'VBG', 'DT', 'NN', 'PRP', 'VBD', 'DT', 'NN', 'NN', 'NN', 'CD', ',', 'NN', 'NNP', 'NN', 'IN', 'NN', ':', 'JJ', 'NN', 'NN', 'CC', 'NN', 'IN', 'CD', 'NNS', '.']",27
sentiment_analysis,49,158,"For MOSEI dataset , we obtain better performance with text .","['For', 'MOSEI', 'dataset', ',', 'we', 'obtain', 'better', 'performance', 'with', 'text', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'NNP', 'NN', ',', 'PRP', 'VB', 'JJR', 'NN', 'IN', 'NN', '.']",11
sentiment_analysis,49,160,"For text - acoustic input pairs , we obtain the highest accuracies with 79. 74 % , 79.60 % and 79.32 % for MMMU - BA , MMUU - SA and MU - SA frameworks , respectively .","['For', 'text', '-', 'acoustic', 'input', 'pairs', ',', 'we', 'obtain', 'the', 'highest', 'accuracies', 'with', '79.', '74', '%', ',', '79.60', '%', 'and', '79.32', '%', 'for', 'MMMU', '-', 'BA', ',', 'MMUU', '-', 'SA', 'and', 'MU', '-', 'SA', 'frameworks', ',', 'respectively', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['IN', 'JJ', ':', 'JJ', 'NN', 'NNS', ',', 'PRP', 'VB', 'DT', 'JJS', 'NNS', 'IN', 'CD', 'CD', 'NN', ',', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'NNP', ':', 'NNP', ',', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NNP', 'NNS', ',', 'RB', '.']",38
sentiment_analysis,49,162,"Finally , we experiment with tri-modal inputs and observe an improved performance of 79. 80 % , 79.76 % and 79.63 % for MMMU - BA , MMUU - SA and MU - SA frameworks , respectively .","['Finally', ',', 'we', 'experiment', 'with', 'tri-modal', 'inputs', 'and', 'observe', 'an', 'improved', 'performance', 'of', '79.', '80', '%', ',', '79.76', '%', 'and', '79.63', '%', 'for', 'MMMU', '-', 'BA', ',', 'MMUU', '-', 'SA', 'and', 'MU', '-', 'SA', 'frameworks', ',', 'respectively', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'IN', 'JJ', 'NNS', 'CC', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'CD', 'CD', 'NN', ',', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'NNP', ':', 'NNP', ',', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NNP', 'NNS', ',', 'RB', '.']",38
sentiment_analysis,49,164,The performance improvement was also found to be statistically significant ( T-test ) than the bimodality and uni-modality inputs .,"['The', 'performance', 'improvement', 'was', 'also', 'found', 'to', 'be', 'statistically', 'significant', '(', 'T-test', ')', 'than', 'the', 'bimodality', 'and', 'uni-modality', 'inputs', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBD', 'RB', 'VBN', 'TO', 'VB', 'RB', 'JJ', '(', 'NNP', ')', 'IN', 'DT', 'NN', 'CC', 'JJ', 'NNS', '.']",20
sentiment_analysis,49,165,"Further , we observe that the MMMU - BA framework reports the best accuracy of 79 . 80 % for the MOSEI dataset , thus supporting our claim that multi-modal attention framework ( i.e. MMMU - BA ) captures more information than the self - attention frameworks ( i.e. MMUU - SA & MU - SA ) .","['Further', ',', 'we', 'observe', 'that', 'the', 'MMMU', '-', 'BA', 'framework', 'reports', 'the', 'best', 'accuracy', 'of', '79', '.', '80', '%', 'for', 'the', 'MOSEI', 'dataset', ',', 'thus', 'supporting', 'our', 'claim', 'that', 'multi-modal', 'attention', 'framework', '(', 'i.e.', 'MMMU', '-', 'BA', ')', 'captures', 'more', 'information', 'than', 'the', 'self', '-', 'attention', 'frameworks', '(', 'i.e.', 'MMUU', '-', 'SA', '&', 'MU', '-', 'SA', ')', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'IN', 'DT', 'NNP', ':', 'NNP', 'NN', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'CD', '.', 'CD', 'NN', 'IN', 'DT', 'NNP', 'NN', ',', 'RB', 'VBG', 'PRP$', 'NN', 'IN', 'JJ', 'NN', 'NN', '(', 'JJ', 'NNP', ':', 'NNP', ')', 'VBZ', 'JJR', 'NN', 'IN', 'DT', 'NN', ':', 'NN', 'NNS', '(', 'FW', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NNP', ')', '.']",58
sentiment_analysis,45,2,Utilizing BERT for Aspect - Based Sentiment Analysis via Constructing Auxiliary Sentence,"['Utilizing', 'BERT', 'for', 'Aspect', '-', 'Based', 'Sentiment', 'Analysis', 'via', 'Constructing', 'Auxiliary', 'Sentence']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['VBG', 'NNP', 'IN', 'NNP', ':', 'VBD', 'JJ', 'NNP', 'IN', 'NNP', 'NNP', 'NN']",12
sentiment_analysis,45,4,"Aspect - based sentiment analysis ( ABSA ) , which aims to identify fine - grained opinion polarity towards a specific aspect , is a challenging subtask of sentiment analysis ( SA ) .","['Aspect', '-', 'based', 'sentiment', 'analysis', '(', 'ABSA', ')', ',', 'which', 'aims', 'to', 'identify', 'fine', '-', 'grained', 'opinion', 'polarity', 'towards', 'a', 'specific', 'aspect', ',', 'is', 'a', 'challenging', 'subtask', 'of', 'sentiment', 'analysis', '(', 'SA', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'VBN', 'NN', 'NN', '(', 'NNP', ')', ',', 'WDT', 'VBZ', 'TO', 'VB', 'JJ', ':', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'VBZ', 'DT', 'VBG', 'NN', 'IN', 'NN', 'NN', '(', 'NNP', ')', '.']",34
sentiment_analysis,45,16,"Both SA and ABSA are sentence - level or document - level tasks , but one comment may refer to more than one object , and sentence - level tasks can not handle sentences with multiple targets .","['Both', 'SA', 'and', 'ABSA', 'are', 'sentence', '-', 'level', 'or', 'document', '-', 'level', 'tasks', ',', 'but', 'one', 'comment', 'may', 'refer', 'to', 'more', 'than', 'one', 'object', ',', 'and', 'sentence', '-', 'level', 'tasks', 'can', 'not', 'handle', 'sentences', 'with', 'multiple', 'targets', '.']","['O', 'B-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'CC', 'NNP', 'VBP', 'JJ', ':', 'NN', 'CC', 'NN', ':', 'NN', 'NNS', ',', 'CC', 'CD', 'NN', 'MD', 'VB', 'TO', 'JJR', 'IN', 'CD', 'NN', ',', 'CC', 'NN', ':', 'NN', 'NNS', 'MD', 'RB', 'VB', 'NNS', 'IN', 'JJ', 'NNS', '.']",38
sentiment_analysis,45,17,"Therefore , introduce the task of targeted aspect - based sentiment analysis ( TABSA ) , which aims to identify fine - grained opinion polarity towards a specific aspect associated with a given target .","['Therefore', ',', 'introduce', 'the', 'task', 'of', 'targeted', 'aspect', '-', 'based', 'sentiment', 'analysis', '(', 'TABSA', ')', ',', 'which', 'aims', 'to', 'identify', 'fine', '-', 'grained', 'opinion', 'polarity', 'towards', 'a', 'specific', 'aspect', 'associated', 'with', 'a', 'given', 'target', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NN', ':', 'VBN', 'NN', 'NN', '(', 'NNP', ')', ',', 'WDT', 'VBZ', 'TO', 'VB', 'JJ', ':', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBN', 'IN', 'DT', 'VBN', 'NN', '.']",35
sentiment_analysis,45,27,"In this paper , we investigate several methods of constructing an auxiliary sentence and transform ( T ) ABSA into a sentence - pair classification task .","['In', 'this', 'paper', ',', 'we', 'investigate', 'several', 'methods', 'of', 'constructing', 'an', 'auxiliary', 'sentence', 'and', 'transform', '(', 'T', ')', 'ABSA', 'into', 'a', 'sentence', '-', 'pair', 'classification', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'CC', 'NN', '(', 'NNP', ')', 'NNP', 'IN', 'DT', 'NN', ':', 'NN', 'NN', 'NN', '.']",27
sentiment_analysis,45,28,We fine - tune the pre-trained model from BERT and achieve new state - of - the - art results on ( T ) ABSA task .,"['We', 'fine', '-', 'tune', 'the', 'pre-trained', 'model', 'from', 'BERT', 'and', 'achieve', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'on', '(', 'T', ')', 'ABSA', 'task', '.']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', ':', 'NN', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'CC', 'VB', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', '(', 'NNP', ')', 'NNP', 'NN', '.']",27
sentiment_analysis,45,100,We use the pre-trained uncased BERT - base model 5 for fine - tuning .,"['We', 'use', 'the', 'pre-trained', 'uncased', 'BERT', '-', 'base', 'model', '5', 'for', 'fine', '-', 'tuning', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NNP', ':', 'NN', 'NN', 'CD', 'IN', 'JJ', ':', 'NN', '.']",15
sentiment_analysis,45,101,"The number of Transformer blocks is 12 , the hidden layer size is 768 , the number of self - attention heads is 12 , and the total number of parameters for the pretrained model is 110M .","['The', 'number', 'of', 'Transformer', 'blocks', 'is', '12', ',', 'the', 'hidden', 'layer', 'size', 'is', '768', ',', 'the', 'number', 'of', 'self', '-', 'attention', 'heads', 'is', '12', ',', 'and', 'the', 'total', 'number', 'of', 'parameters', 'for', 'the', 'pretrained', 'model', 'is', '110M', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NNP', 'NNS', 'VBZ', 'CD', ',', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'CD', ',', 'DT', 'NN', 'IN', 'JJ', ':', 'NN', 'NNS', 'VBZ', 'CD', ',', 'CC', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'CD', '.']",38
sentiment_analysis,45,105,"the dropout probability at 0.1 , set the number of epochs to 4 .","['the', 'dropout', 'probability', 'at', '0.1', ',', 'set', 'the', 'number', 'of', 'epochs', 'to', '4', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'IN', 'CD', ',', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'TO', 'CD', '.']",14
sentiment_analysis,45,106,"The initial learning rate is 2 e - 5 , and the batch size is 24 .","['The', 'initial', 'learning', 'rate', 'is', '2', 'e', '-', '5', ',', 'and', 'the', 'batch', 'size', 'is', '24', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'CD', 'NN', ':', 'CD', ',', 'CC', 'DT', 'NN', 'NN', 'VBZ', 'CD', '.']",17
sentiment_analysis,45,109,LR : a logistic regression classifier with n-gram and pos-tag features .,"['LR', ':', 'a', 'logistic', 'regression', 'classifier', 'with', 'n-gram', 'and', 'pos-tag', 'features', '.']","['B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', ':', 'DT', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'CC', 'JJ', 'NNS', '.']",12
sentiment_analysis,45,110,LSTM - Final ) : a biLSTM model with the final state as a representation .,"['LSTM', '-', 'Final', ')', ':', 'a', 'biLSTM', 'model', 'with', 'the', 'final', 'state', 'as', 'a', 'representation', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', ')', ':', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",16
sentiment_analysis,45,111,LSTM - Loc ) : a biLSTM model with the state associated with the target position as a representation .,"['LSTM', '-', 'Loc', ')', ':', 'a', 'biLSTM', 'model', 'with', 'the', 'state', 'associated', 'with', 'the', 'target', 'position', 'as', 'a', 'representation', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', ')', ':', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', '.']",20
sentiment_analysis,45,112,LSTM + TA + SA ) : a biLSTM model which introduces complex target - level and sentence - level attention mechanisms .,"['LSTM', '+', 'TA', '+', 'SA', ')', ':', 'a', 'biLSTM', 'model', 'which', 'introduces', 'complex', 'target', '-', 'level', 'and', 'sentence', '-', 'level', 'attention', 'mechanisms', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NNP', 'NNP', 'NNP', ')', ':', 'DT', 'NN', 'NN', 'WDT', 'VBZ', 'JJ', 'NN', ':', 'NN', 'CC', 'NN', ':', 'NN', 'NN', 'NNS', '.']",23
sentiment_analysis,45,113,SenticLSTM : an upgraded version of the LSTM + TA + SA model which introduces external information from Sentic - Net .,"['SenticLSTM', ':', 'an', 'upgraded', 'version', 'of', 'the', 'LSTM', '+', 'TA', '+', 'SA', 'model', 'which', 'introduces', 'external', 'information', 'from', 'Sentic', '-', 'Net', '.']","['B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', ':', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NN', 'WDT', 'VBZ', 'JJ', 'NN', 'IN', 'NNP', ':', 'NN', '.']",22
sentiment_analysis,45,114,"Dmu - Entnet : a bidirectional EntNet with external "" memory chains "" with a delayed memory update mechanism to track entities .","['Dmu', '-', 'Entnet', ':', 'a', 'bidirectional', 'EntNet', 'with', 'external', '""', 'memory', 'chains', '""', 'with', 'a', 'delayed', 'memory', 'update', 'mechanism', 'to', 'track', 'entities', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['NNP', ':', 'NN', ':', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNP', 'NN', 'NNS', 'VBP', 'IN', 'DT', 'JJ', 'NN', 'JJ', 'NN', 'TO', 'VB', 'NNS', '.']",23
sentiment_analysis,45,120,"We find that BERT - single has achieved better results on these two subtasks , and BERT - pair has achieved further improvements over BERT - single .","['We', 'find', 'that', 'BERT', '-', 'single', 'has', 'achieved', 'better', 'results', 'on', 'these', 'two', 'subtasks', ',', 'and', 'BERT', '-', 'pair', 'has', 'achieved', 'further', 'improvements', 'over', 'BERT', '-', 'single', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'NNP', ':', 'JJ', 'VBZ', 'VBN', 'RB', 'NNS', 'IN', 'DT', 'CD', 'NNS', ',', 'CC', 'NNP', ':', 'NN', 'VBZ', 'VBN', 'JJ', 'NNS', 'IN', 'NNP', ':', 'JJ', '.']",28
sentiment_analysis,45,121,The BERT - pair - NLI - B model achieves the best performance for aspect category detection .,"['The', 'BERT', '-', 'pair', '-', 'NLI', '-', 'B', 'model', 'achieves', 'the', 'best', 'performance', 'for', 'aspect', 'category', 'detection', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', ':', 'NN', ':', 'NNP', ':', 'NNP', 'NN', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",18
sentiment_analysis,45,122,"For aspect category polarity , BERTpair - QA - B performs best on all 4 - way , 3 - way , and binary settings .","['For', 'aspect', 'category', 'polarity', ',', 'BERTpair', '-', 'QA', '-', 'B', 'performs', 'best', 'on', 'all', '4', '-', 'way', ',', '3', '-', 'way', ',', 'and', 'binary', 'settings', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', 'NN', 'NN', ',', 'NNP', ':', 'NNP', ':', 'NN', 'NNS', 'RBS', 'IN', 'DT', 'CD', ':', 'NN', ',', 'CD', ':', 'NN', ',', 'CC', 'JJ', 'NNS', '.']",26
sentiment_analysis,44,2,Exploring Joint Neural Model for Sentence Level Discourse Parsing and Sentiment Analysis,"['Exploring', 'Joint', 'Neural', 'Model', 'for', 'Sentence', 'Level', 'Discourse', 'Parsing', 'and', 'Sentiment', 'Analysis']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['VBG', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', 'NNP']",12
sentiment_analysis,44,107,Our framework consists of three main sub parts .,"['Our', 'framework', 'consists', 'of', 'three', 'main', 'sub', 'parts', '.']","['O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'IN', 'CD', 'JJ', 'NN', 'NNS', '.']",9
sentiment_analysis,44,108,"Given a segmented sentence , the first step is to create meaningful vector representations for all the EDUs .","['Given', 'a', 'segmented', 'sentence', ',', 'the', 'first', 'step', 'is', 'to', 'create', 'meaningful', 'vector', 'representations', 'for', 'all', 'the', 'EDUs', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['VBN', 'DT', 'JJ', 'NN', ',', 'DT', 'JJ', 'NN', 'VBZ', 'TO', 'VB', 'JJ', 'NN', 'NNS', 'IN', 'PDT', 'DT', 'NNP', '.']",19
sentiment_analysis,44,109,"Next , we devise three different Recursive Neural Net models , each designed for one of discourse structure prediction , discourse relation prediction and sentiment analysis .","['Next', ',', 'we', 'devise', 'three', 'different', 'Recursive', 'Neural', 'Net', 'models', ',', 'each', 'designed', 'for', 'one', 'of', 'discourse', 'structure', 'prediction', ',', 'discourse', 'relation', 'prediction', 'and', 'sentiment', 'analysis', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['JJ', ',', 'PRP', 'VBP', 'CD', 'JJ', 'NNP', 'NNP', 'NNP', 'NNS', ',', 'DT', 'VBN', 'IN', 'CD', 'IN', 'NN', 'NN', 'NN', ',', 'JJ', 'NN', 'NN', 'CC', 'NN', 'NN', '.']",27
sentiment_analysis,44,110,"Finally , we join these Neural Nets in two different ways : Multitasking and Pre-training .","['Finally', ',', 'we', 'join', 'these', 'Neural', 'Nets', 'in', 'two', 'different', 'ways', ':', 'Multitasking', 'and', 'Pre-training', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NNP', 'NNPS', 'IN', 'CD', 'JJ', 'NNS', ':', 'NN', 'CC', 'NNP', '.']",16
sentiment_analysis,44,161,All the neural models presented in this paper were implemented using the Tensor Flow python pack - .,"['All', 'the', 'neural', 'models', 'presented', 'in', 'this', 'paper', 'were', 'implemented', 'using', 'the', 'Tensor', 'Flow', 'python', 'pack', '-', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PDT', 'DT', 'JJ', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'VBD', 'VBN', 'VBG', 'DT', 'NNP', 'NNP', 'NN', 'NN', ':', '.']",18
sentiment_analysis,44,162,We minimize the crossentropy error using the Adam optimizer and L2regularization on the set of weights .,"['We', 'minimize', 'the', 'crossentropy', 'error', 'using', 'the', 'Adam', 'optimizer', 'and', 'L2regularization', 'on', 'the', 'set', 'of', 'weights', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'VBG', 'DT', 'NNP', 'NN', 'CC', 'NNP', 'IN', 'DT', 'NN', 'IN', 'NNS', '.']",17
sentiment_analysis,44,163,"For the individual models ( before joining ) , we use 200 training epochs and a batch size of 100 .","['For', 'the', 'individual', 'models', '(', 'before', 'joining', ')', ',', 'we', 'use', '200', 'training', 'epochs', 'and', 'a', 'batch', 'size', 'of', '100', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'JJ', 'NNS', '(', 'IN', 'VBG', ')', ',', 'PRP', 'VBP', 'CD', 'NN', 'NN', 'CC', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",21
sentiment_analysis,44,169,"From the results , we see some improvement on Discourse Structure prediction when we are using a joint model but the improvement is statistically significant only for the Nuclearity and Relation predictions .","['From', 'the', 'results', ',', 'we', 'see', 'some', 'improvement', 'on', 'Discourse', 'Structure', 'prediction', 'when', 'we', 'are', 'using', 'a', 'joint', 'model', 'but', 'the', 'improvement', 'is', 'statistically', 'significant', 'only', 'for', 'the', 'Nuclearity', 'and', 'Relation', 'predictions', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'NN', 'WRB', 'PRP', 'VBP', 'VBG', 'DT', 'JJ', 'NN', 'CC', 'DT', 'NN', 'VBZ', 'RB', 'JJ', 'RB', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NNS', '.']",33
sentiment_analysis,44,170,"The improvements on the Relation predictions were mainly on the Contrastive set , specifically the class of Contrast , Comparison and Cause relations as .","['The', 'improvements', 'on', 'the', 'Relation', 'predictions', 'were', 'mainly', 'on', 'the', 'Contrastive', 'set', ',', 'specifically', 'the', 'class', 'of', 'Contrast', ',', 'Comparison', 'and', 'Cause', 'relations', 'as', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'DT', 'NNP', 'NNS', 'VBD', 'RB', 'IN', 'DT', 'NNP', 'NN', ',', 'RB', 'DT', 'NN', 'IN', 'NNP', ',', 'NNP', 'CC', 'NNP', 'NNS', 'IN', '.']",25
sentiment_analysis,44,178,In the fine grained setting we compute the accuracy of exact match across five classes .,"['In', 'the', 'fine', 'grained', 'setting', 'we', 'compute', 'the', 'accuracy', 'of', 'exact', 'match', 'across', 'five', 'classes', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'JJ', 'NN', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'NN', 'IN', 'CD', 'NNS', '.']",16
sentiment_analysis,51,2,Fine - grained Sentiment Classification using BERT,"['Fine', '-', 'grained', 'Sentiment', 'Classification', 'using', 'BERT']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['NNP', ':', 'VBD', 'NNP', 'NNP', 'VBG', 'NNP']",7
sentiment_analysis,51,4,"Sentiment classification is an important process in understanding people 's perception towards a product , service , or topic .","['Sentiment', 'classification', 'is', 'an', 'important', 'process', 'in', 'understanding', 'people', ""'s"", 'perception', 'towards', 'a', 'product', ',', 'service', ',', 'or', 'topic', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'NNS', 'POS', 'NN', 'IN', 'DT', 'NN', ',', 'NN', ',', 'CC', 'NN', '.']",20
sentiment_analysis,51,22,"In this paper , we use the pretrained BERT model and finetune it for the fine - grained sentiment classification task on the Stanford Sentiment Treebank ( SST ) dataset .","['In', 'this', 'paper', ',', 'we', 'use', 'the', 'pretrained', 'BERT', 'model', 'and', 'finetune', 'it', 'for', 'the', 'fine', '-', 'grained', 'sentiment', 'classification', 'task', 'on', 'the', 'Stanford', 'Sentiment', 'Treebank', '(', 'SST', ')', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNP', 'NN', 'CC', 'VB', 'PRP', 'IN', 'DT', 'JJ', ':', 'JJ', 'NN', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', '.']",31
sentiment_analysis,51,127,1 ) Word embeddings :,"['1', ')', 'Word', 'embeddings', ':']","['O', 'O', 'B-n', 'I-n', 'O']","['CD', ')', 'NNP', 'NNS', ':']",5
sentiment_analysis,51,128,"In this method , the word vectors pretrained on large text corpus such as Wikipedia dump are averaged to get the document vector , which is then fed to the sentiment classifier to compute the sentiment score .","['In', 'this', 'method', ',', 'the', 'word', 'vectors', 'pretrained', 'on', 'large', 'text', 'corpus', 'such', 'as', 'Wikipedia', 'dump', 'are', 'averaged', 'to', 'get', 'the', 'document', 'vector', ',', 'which', 'is', 'then', 'fed', 'to', 'the', 'sentiment', 'classifier', 'to', 'compute', 'the', 'sentiment', 'score', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'DT', 'NN', 'NNS', 'VBD', 'IN', 'JJ', 'JJ', 'NN', 'JJ', 'IN', 'NNP', 'NN', 'VBP', 'VBN', 'TO', 'VB', 'DT', 'NN', 'NN', ',', 'WDT', 'VBZ', 'RB', 'VBN', 'TO', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', '.']",38
sentiment_analysis,51,129,2 ) Recursive networks :,"['2', ')', 'Recursive', 'networks', ':']","['O', 'O', 'B-n', 'I-n', 'O']","['CD', ')', 'JJ', 'NNS', ':']",5
sentiment_analysis,51,130,Various types of recursive neural networks ( RNN ) have been applied on SST .,"['Various', 'types', 'of', 'recursive', 'neural', 'networks', '(', 'RNN', ')', 'have', 'been', 'applied', 'on', 'SST', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['JJ', 'NNS', 'IN', 'JJ', 'JJ', 'NNS', '(', 'NNP', ')', 'VBP', 'VBN', 'VBN', 'IN', 'NNP', '.']",15
sentiment_analysis,51,133,3 ) Recurrent networks :,"['3', ')', 'Recurrent', 'networks', ':']","['O', 'O', 'B-n', 'I-n', 'O']","['LS', ')', 'NN', 'NNS', ':']",5
sentiment_analysis,51,134,Sophisticated recurrent networks such as left - to - right and bidrectional LSTM networks have also been applied on SST .,"['Sophisticated', 'recurrent', 'networks', 'such', 'as', 'left', '-', 'to', '-', 'right', 'and', 'bidrectional', 'LSTM', 'networks', 'have', 'also', 'been', 'applied', 'on', 'SST', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['JJ', 'NN', 'NNS', 'JJ', 'IN', 'JJ', ':', 'TO', ':', 'NN', 'CC', 'JJ', 'NNP', 'NNS', 'VBP', 'RB', 'VBN', 'VBN', 'IN', 'NNP', '.']",21
sentiment_analysis,51,135,4 ) Convolutional networks :,"['4', ')', 'Convolutional', 'networks', ':']","['O', 'O', 'B-n', 'I-n', 'O']","['CD', ')', 'JJ', 'NNS', ':']",5
sentiment_analysis,51,136,"In this approach , the input sequences were passed through a 1 - dimensional convolutional neural network as feature extractors .","['In', 'this', 'approach', ',', 'the', 'input', 'sequences', 'were', 'passed', 'through', 'a', '1', '-', 'dimensional', 'convolutional', 'neural', 'network', 'as', 'feature', 'extractors', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'DT', 'NN', 'NNS', 'VBD', 'VBN', 'IN', 'DT', 'CD', ':', 'JJ', 'JJ', 'JJ', 'NN', 'IN', 'NN', 'NNS', '.']",21
sentiment_analysis,51,145,"We can see that our model , despite being a simple architecture , performs better in terms of accuracy than many popular and sophisticated NLP models .","['We', 'can', 'see', 'that', 'our', 'model', ',', 'despite', 'being', 'a', 'simple', 'architecture', ',', 'performs', 'better', 'in', 'terms', 'of', 'accuracy', 'than', 'many', 'popular', 'and', 'sophisticated', 'NLP', 'models', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'IN', 'PRP$', 'NN', ',', 'IN', 'VBG', 'DT', 'JJ', 'NN', ',', 'NNS', 'RBR', 'IN', 'NNS', 'IN', 'NN', 'IN', 'JJ', 'JJ', 'CC', 'JJ', 'NNP', 'NNS', '.']",27
sentiment_analysis,8,2,Multimodal Speech Emotion Recognition and Ambiguity Resolution,"['Multimodal', 'Speech', 'Emotion', 'Recognition', 'and', 'Ambiguity', 'Resolution']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', 'NNP']",7
sentiment_analysis,8,4,Identifying emotion from speech is a nontrivial task pertaining to the ambiguous definition of emotion itself .,"['Identifying', 'emotion', 'from', 'speech', 'is', 'a', 'nontrivial', 'task', 'pertaining', 'to', 'the', 'ambiguous', 'definition', 'of', 'emotion', 'itself', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NN', 'IN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'VBG', 'TO', 'DT', 'JJ', 'NN', 'IN', 'NN', 'PRP', '.']",17
sentiment_analysis,8,16,"With the rise of deep learning algorithms , there have been multiple attempts to tackle the task of Speech Emotion Recognition ( SER ) as in [ 2 ] and .","['With', 'the', 'rise', 'of', 'deep', 'learning', 'algorithms', ',', 'there', 'have', 'been', 'multiple', 'attempts', 'to', 'tackle', 'the', 'task', 'of', 'Speech', 'Emotion', 'Recognition', '(', 'SER', ')', 'as', 'in', '[', '2', ']', 'and', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', ',', 'EX', 'VBP', 'VBN', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'IN', 'IN', 'JJ', 'CD', 'NN', 'CC', '.']",31
sentiment_analysis,8,18,"In this work , we explore the implication of hand - crafted features for SER and compare the performance of lighter machine learning models with the heavily data - reliant deep learning models .","['In', 'this', 'work', ',', 'we', 'explore', 'the', 'implication', 'of', 'hand', '-', 'crafted', 'features', 'for', 'SER', 'and', 'compare', 'the', 'performance', 'of', 'lighter', 'machine', 'learning', 'models', 'with', 'the', 'heavily', 'data', '-', 'reliant', 'deep', 'learning', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', ':', 'VBN', 'NNS', 'IN', 'NNP', 'CC', 'VB', 'DT', 'NN', 'IN', 'JJR', 'NN', 'VBG', 'NNS', 'IN', 'DT', 'RB', 'VBZ', ':', 'NN', 'JJ', 'NN', 'NNS', '.']",34
sentiment_analysis,8,19,"Furthermore , we also combine features from the textual modality to understand the correlation between different modalities and aid ambiguity resolution .","['Furthermore', ',', 'we', 'also', 'combine', 'features', 'from', 'the', 'textual', 'modality', 'to', 'understand', 'the', 'correlation', 'between', 'different', 'modalities', 'and', 'aid', 'ambiguity', 'resolution', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'RB', 'VBP', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'CC', 'NN', 'NN', 'NN', '.']",22
sentiment_analysis,8,21,"For both the approaches , we first extract handcrafted features from the time domain of the audio signal and train the respective models .","['For', 'both', 'the', 'approaches', ',', 'we', 'first', 'extract', 'handcrafted', 'features', 'from', 'the', 'time', 'domain', 'of', 'the', 'audio', 'signal', 'and', 'train', 'the', 'respective', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'DT', 'NNS', ',', 'PRP', 'RB', 'VB', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'CC', 'VB', 'DT', 'JJ', 'NNS', '.']",24
sentiment_analysis,8,22,"In the first approach , we train traditional machine learning classifiers , namely , Random Forests , Gradient Boosting , Support Vector Machines , Naive - Bayes and Logistic Regression .","['In', 'the', 'first', 'approach', ',', 'we', 'train', 'traditional', 'machine', 'learning', 'classifiers', ',', 'namely', ',', 'Random', 'Forests', ',', 'Gradient', 'Boosting', ',', 'Support', 'Vector', 'Machines', ',', 'Naive', '-', 'Bayes', 'and', 'Logistic', 'Regression', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'NN', 'NNS', ',', 'RB', ',', 'NNP', 'NNP', ',', 'NNP', 'NNP', ',', 'NNP', 'NNP', 'NNPS', ',', 'NNP', ':', 'NNP', 'CC', 'NNP', 'NNP', '.']",31
sentiment_analysis,8,23,"In the second approach , we build a Multi - Layer Perceptron and an LSTM classifier to recognize emotion given a speech signal .","['In', 'the', 'second', 'approach', ',', 'we', 'build', 'a', 'Multi', '-', 'Layer', 'Perceptron', 'and', 'an', 'LSTM', 'classifier', 'to', 'recognize', 'emotion', 'given', 'a', 'speech', 'signal', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', ':', 'NNP', 'NNP', 'CC', 'DT', 'NNP', 'NN', 'TO', 'VB', 'NN', 'VBN', 'DT', 'JJ', 'NN', '.']",24
sentiment_analysis,8,164,"We use librosa , a Python library , to process the audio files and extract features from them .","['We', 'use', 'librosa', ',', 'a', 'Python', 'library', ',', 'to', 'process', 'the', 'audio', 'files', 'and', 'extract', 'features', 'from', 'them', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', ',', 'DT', 'NNP', 'NN', ',', 'TO', 'VB', 'DT', 'NN', 'NNS', 'CC', 'JJ', 'NNS', 'IN', 'PRP', '.']",19
sentiment_analysis,8,165,"We use scikit - learn and xgboost [ 25 ] , the machine learning libraries for Python , to implement all the ML classifiers ( RF , XGB , SVM , MNB , and LR ) and the MLP .","['We', 'use', 'scikit', '-', 'learn', 'and', 'xgboost', '[', '25', ']', ',', 'the', 'machine', 'learning', 'libraries', 'for', 'Python', ',', 'to', 'implement', 'all', 'the', 'ML', 'classifiers', '(', 'RF', ',', 'XGB', ',', 'SVM', ',', 'MNB', ',', 'and', 'LR', ')', 'and', 'the', 'MLP', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', ':', 'NN', 'CC', 'NNP', 'NNP', 'CD', 'NN', ',', 'DT', 'NN', 'NN', 'NNS', 'IN', 'NNP', ',', 'TO', 'VB', 'PDT', 'DT', 'NNP', 'NNS', '(', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', ')', 'CC', 'DT', 'NNP', '.']",40
sentiment_analysis,8,166,We use PyTorch to implement the LSTM classifiers described earlier .,"['We', 'use', 'PyTorch', 'to', 'implement', 'the', 'LSTM', 'classifiers', 'described', 'earlier', '.']","['O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', 'TO', 'VB', 'DT', 'NNP', 'NNS', 'VBD', 'RBR', '.']",11
sentiment_analysis,8,167,"In order to regularize the hidden space of the LSTM classifiers , we use a shut - off mechanism , called dropout , where a fraction of neurons are not used for final prediction .","['In', 'order', 'to', 'regularize', 'the', 'hidden', 'space', 'of', 'the', 'LSTM', 'classifiers', ',', 'we', 'use', 'a', 'shut', '-', 'off', 'mechanism', ',', 'called', 'dropout', ',', 'where', 'a', 'fraction', 'of', 'neurons', 'are', 'not', 'used', 'for', 'final', 'prediction', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', ':', 'IN', 'NN', ',', 'VBN', 'NN', ',', 'WRB', 'DT', 'NN', 'IN', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'JJ', 'NN', '.']",35
sentiment_analysis,8,169,We randomly split our dataset into a train ( 80 % ) and test ( 20 % ) set .,"['We', 'randomly', 'split', 'our', 'dataset', 'into', 'a', 'train', '(', '80', '%', ')', 'and', 'test', '(', '20', '%', ')', 'set', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'VB', 'PRP$', 'NN', 'IN', 'DT', 'NN', '(', 'CD', 'NN', ')', 'CC', 'NN', '(', 'CD', 'NN', ')', 'NN', '.']",20
sentiment_analysis,8,171,The LSTM classifiers were trained on an NVIDIA Titan X GPU for faster processing .,"['The', 'LSTM', 'classifiers', 'were', 'trained', 'on', 'an', 'NVIDIA', 'Titan', 'X', 'GPU', 'for', 'faster', 'processing', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NNS', 'VBD', 'VBN', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NN', 'NN', '.']",15
sentiment_analysis,8,172,We stop the training when we do not see any improvement in validation performance for > 10 epochs .,"['We', 'stop', 'the', 'training', 'when', 'we', 'do', 'not', 'see', 'any', 'improvement', 'in', 'validation', 'performance', 'for', '>', '10', 'epochs', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'WRB', 'PRP', 'VBP', 'RB', 'VB', 'DT', 'NN', 'IN', 'NN', 'NN', 'IN', '$', 'CD', 'NN', '.']",19
sentiment_analysis,8,199,"From , we can see that our simpler and lighter ML models either outperform or are comparable to the much heavier current state - of - the art on this dataset .","['From', ',', 'we', 'can', 'see', 'that', 'our', 'simpler', 'and', 'lighter', 'ML', 'models', 'either', 'outperform', 'or', 'are', 'comparable', 'to', 'the', 'much', 'heavier', 'current', 'state', '-', 'of', '-', 'the', 'art', 'on', 'this', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['IN', ',', 'PRP', 'MD', 'VB', 'IN', 'PRP$', 'NN', 'CC', 'JJR', 'NNP', 'NNS', 'CC', 'NN', 'CC', 'VBP', 'JJ', 'TO', 'DT', 'RB', 'JJR', 'JJ', 'NN', ':', 'IN', ':', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",32
sentiment_analysis,8,201,Audio - only results :,"['Audio', '-', 'only', 'results', ':']","['B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'RB', 'NNS', ':']",5
sentiment_analysis,8,203,Performance of LSTM and ARE reveals that deep models indeed need a lot of information to learn features as the LSTM classifier trained on eight - dimensional features achieves very low accuracy as compared to the end - to - end trained ARE .,"['Performance', 'of', 'LSTM', 'and', 'ARE', 'reveals', 'that', 'deep', 'models', 'indeed', 'need', 'a', 'lot', 'of', 'information', 'to', 'learn', 'features', 'as', 'the', 'LSTM', 'classifier', 'trained', 'on', 'eight', '-', 'dimensional', 'features', 'achieves', 'very', 'low', 'accuracy', 'as', 'compared', 'to', 'the', 'end', '-', 'to', '-', 'end', 'trained', 'ARE', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'IN', 'NNP', 'CC', 'NNP', 'VBZ', 'IN', 'JJ', 'NNS', 'RB', 'VBP', 'DT', 'NN', 'IN', 'NN', 'TO', 'VB', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'VBD', 'IN', 'CD', ':', 'NN', 'NNS', 'VBZ', 'RB', 'JJ', 'NN', 'IN', 'VBN', 'TO', 'DT', 'NN', ':', 'TO', ':', 'NN', 'VBN', 'NNP', '.']",44
sentiment_analysis,8,207,Text - only results :,"['Text', '-', 'only', 'results', ':']","['B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'RB', 'NNS', ':']",5
sentiment_analysis,8,208,We observe that the performance of all the models for this setting is similar .,"['We', 'observe', 'that', 'the', 'performance', 'of', 'all', 'the', 'models', 'for', 'this', 'setting', 'is', 'similar', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NN', 'IN', 'PDT', 'DT', 'NNS', 'IN', 'DT', 'NN', 'VBZ', 'JJ', '.']",15
sentiment_analysis,8,212,c) Audio + Text results :,"['c)', 'Audio', '+', 'Text', 'results', ':']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'NNP', 'NNP', 'NNP', 'NNS', ':']",6
sentiment_analysis,8,213,We see that combining audio and text features gives us a boost of ? 14 % for all the metrics .,"['We', 'see', 'that', 'combining', 'audio', 'and', 'text', 'features', 'gives', 'us', 'a', 'boost', 'of', '?', '14', '%', 'for', 'all', 'the', 'metrics', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'VBG', 'NN', 'CC', 'NN', 'NNS', 'VBZ', 'PRP', 'DT', 'NN', 'IN', '.', 'CD', 'NN', 'IN', 'PDT', 'DT', 'NNS', '.']",21
sentiment_analysis,8,219,"Overall , we can conclude that our simple ML methods are very robust to have achieved comparable performance even though they are modeled to predict six - classes as opposed to four in previous works .","['Overall', ',', 'we', 'can', 'conclude', 'that', 'our', 'simple', 'ML', 'methods', 'are', 'very', 'robust', 'to', 'have', 'achieved', 'comparable', 'performance', 'even', 'though', 'they', 'are', 'modeled', 'to', 'predict', 'six', '-', 'classes', 'as', 'opposed', 'to', 'four', 'in', 'previous', 'works', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', ',', 'PRP', 'MD', 'VB', 'IN', 'PRP$', 'JJ', 'NNP', 'NNS', 'VBP', 'RB', 'JJ', 'TO', 'VB', 'VBN', 'JJ', 'NN', 'RB', 'IN', 'PRP', 'VBP', 'VBN', 'TO', 'VB', 'CD', ':', 'NNS', 'IN', 'VBN', 'TO', 'CD', 'IN', 'JJ', 'NNS', '.']",36
sentiment_analysis,9,2,Graphical Abstract A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction Highlights A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction,"['Graphical', 'Abstract', 'A', 'Multi-task', 'Learning', 'Model', 'for', 'Chinese', '-', 'oriented', 'Aspect', 'Polarity', 'Classification', 'and', 'Aspect', 'Term', 'Extraction', 'Highlights', 'A', 'Multi-task', 'Learning', 'Model', 'for', 'Chinese', '-', 'oriented', 'Aspect', 'Polarity', 'Classification', 'and', 'Aspect', 'Term', 'Extraction', 'A', 'Multi-task', 'Learning', 'Model', 'for', 'Chinese', '-', 'oriented', 'Aspect', 'Polarity', 'Classification', 'and', 'Aspect', 'Term', 'Extraction']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', ':', 'VBD', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NNP', 'NNP', 'IN', 'NNP', ':', 'VBD', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', ':', 'VBD', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', 'NNP', 'NNP']",48
sentiment_analysis,9,8,Aspect - based sentiment analysis ( ABSA ) task is a multi - grained task of natural language processing and consists of two subtasks : aspect term extraction ( ATE ) and aspect polarity classification ( APC ) .,"['Aspect', '-', 'based', 'sentiment', 'analysis', '(', 'ABSA', ')', 'task', 'is', 'a', 'multi', '-', 'grained', 'task', 'of', 'natural', 'language', 'processing', 'and', 'consists', 'of', 'two', 'subtasks', ':', 'aspect', 'term', 'extraction', '(', 'ATE', ')', 'and', 'aspect', 'polarity', 'classification', '(', 'APC', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'VBN', 'NN', 'NN', '(', 'NNP', ')', 'NN', 'VBZ', 'DT', 'JJ', ':', 'VBD', 'NN', 'IN', 'JJ', 'NN', 'NN', 'CC', 'NNS', 'IN', 'CD', 'NNS', ':', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'JJ', 'NN', 'NN', '(', 'NNP', ')', '.']",39
sentiment_analysis,9,9,Most of the existing work focuses on the subtask of aspect term polarity inferring and ignores the significance of aspect term extraction .,"['Most', 'of', 'the', 'existing', 'work', 'focuses', 'on', 'the', 'subtask', 'of', 'aspect', 'term', 'polarity', 'inferring', 'and', 'ignores', 'the', 'significance', 'of', 'aspect', 'term', 'extraction', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['JJS', 'IN', 'DT', 'VBG', 'NN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', 'NN', 'CC', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",23
sentiment_analysis,9,13,"By integrating the domain - adapted BERT model , the LCF - ATEPC model achieved the state - of the - art performance of aspect term extraction and aspect polarity classification in four Chinese review datasets .","['By', 'integrating', 'the', 'domain', '-', 'adapted', 'BERT', 'model', ',', 'the', 'LCF', '-', 'ATEPC', 'model', 'achieved', 'the', 'state', '-', 'of', 'the', '-', 'art', 'performance', 'of', 'aspect', 'term', 'extraction', 'and', 'aspect', 'polarity', 'classification', 'in', 'four', 'Chinese', 'review', 'datasets', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBG', 'DT', 'NN', ':', 'VBN', 'NNP', 'NN', ',', 'DT', 'NNP', ':', 'NNP', 'NN', 'VBD', 'DT', 'NN', ':', 'IN', 'DT', ':', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NN', 'CC', 'JJ', 'NN', 'NN', 'IN', 'CD', 'JJ', 'NN', 'NNS', '.']",37
sentiment_analysis,9,16,"Aspect - based sentiment analysis ; Pontiki , Galanis , Papageorgiou , Androutsopoulos , Manandhar , AL - Smadi , Al - Ayyoub , Zhao , Qin , De Clercq , Hoste , Apidianaki , Tannier , Loukachevitch , Kotelnikov , Bel , Jimnez - Zafra and Eryigit ( 2016 ) ( ABSA ) is a fine - grained task compared with traditional sentiment analysis , which requires the model to be able to automatic extract the aspects and predict the polarities of all the aspects .","['Aspect', '-', 'based', 'sentiment', 'analysis', ';', 'Pontiki', ',', 'Galanis', ',', 'Papageorgiou', ',', 'Androutsopoulos', ',', 'Manandhar', ',', 'AL', '-', 'Smadi', ',', 'Al', '-', 'Ayyoub', ',', 'Zhao', ',', 'Qin', ',', 'De', 'Clercq', ',', 'Hoste', ',', 'Apidianaki', ',', 'Tannier', ',', 'Loukachevitch', ',', 'Kotelnikov', ',', 'Bel', ',', 'Jimnez', '-', 'Zafra', 'and', 'Eryigit', '(', '2016', ')', '(', 'ABSA', ')', 'is', 'a', 'fine', '-', 'grained', 'task', 'compared', 'with', 'traditional', 'sentiment', 'analysis', ',', 'which', 'requires', 'the', 'model', 'to', 'be', 'able', 'to', 'automatic', 'extract', 'the', 'aspects', 'and', 'predict', 'the', 'polarities', 'of', 'all', 'the', 'aspects', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'VBN', 'NN', 'NN', ':', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', ':', 'NNP', ',', 'NNP', ':', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', ':', 'NN', 'CC', 'NNP', '(', 'CD', ')', '(', 'NNP', ')', 'VBZ', 'DT', 'JJ', ':', 'VBN', 'NN', 'VBN', 'IN', 'JJ', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'TO', 'VB', 'JJ', 'TO', 'JJ', 'VB', 'DT', 'NNS', 'CC', 'VBP', 'DT', 'NNS', 'IN', 'PDT', 'DT', 'NNS', '.']",87
sentiment_analysis,9,23,The APC task is a kind of classification problem .,"['The', 'APC', 'task', 'is', 'a', 'kind', 'of', 'classification', 'problem', '.']","['O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'NN', 'NN', '.']",10
sentiment_analysis,9,24,"The researches concerning APC tasks is more abundant than the ATE task , and a large number of deep learning - based models have been proposed to solve APC problems , such as the models ; ; ; based on long short - term memory ( LSTM ) and the methodologies based on transformer .","['The', 'researches', 'concerning', 'APC', 'tasks', 'is', 'more', 'abundant', 'than', 'the', 'ATE', 'task', ',', 'and', 'a', 'large', 'number', 'of', 'deep', 'learning', '-', 'based', 'models', 'have', 'been', 'proposed', 'to', 'solve', 'APC', 'problems', ',', 'such', 'as', 'the', 'models', ';', ';', ';', 'based', 'on', 'long', 'short', '-', 'term', 'memory', '(', 'LSTM', ')', 'and', 'the', 'methodologies', 'based', 'on', 'transformer', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'VBG', 'NNP', 'NNS', 'VBZ', 'RBR', 'JJ', 'IN', 'DT', 'NNP', 'NN', ',', 'CC', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'VBG', ':', 'VBN', 'NNS', 'VBP', 'VBN', 'VBN', 'TO', 'VB', 'NNP', 'NNS', ',', 'JJ', 'IN', 'DT', 'NNS', ':', ':', ':', 'VBN', 'IN', 'JJ', 'JJ', ':', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'DT', 'NNS', 'VBN', 'IN', 'NN', '.']",55
sentiment_analysis,9,32,"Aiming to automatically extract aspects from the text efficiently and analyze the sentiment polarity of aspects simultaneously , this paper proposes a multi-task learning model for aspect - based sentiment analysis .","['Aiming', 'to', 'automatically', 'extract', 'aspects', 'from', 'the', 'text', 'efficiently', 'and', 'analyze', 'the', 'sentiment', 'polarity', 'of', 'aspects', 'simultaneously', ',', 'this', 'paper', 'proposes', 'a', 'multi-task', 'learning', 'model', 'for', 'aspect', '-', 'based', 'sentiment', 'analysis', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'TO', 'RB', 'VB', 'NNS', 'IN', 'DT', 'NN', 'RB', 'CC', 'VB', 'DT', 'NN', 'NN', 'IN', 'NNS', 'RB', ',', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'JJ', ':', 'VBN', 'NN', 'NN', '.']",32
sentiment_analysis,9,34,The LCF - ATEPC 3 model proposed in this paper is a novel multilingual and multi-task - oriented model .,"['The', 'LCF', '-', 'ATEPC', '3', 'model', 'proposed', 'in', 'this', 'paper', 'is', 'a', 'novel', 'multilingual', 'and', 'multi-task', '-', 'oriented', 'model', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', ':', '$', 'CD', 'NN', 'VBN', 'IN', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'JJ', 'CC', 'JJ', ':', 'VBN', 'NN', '.']",20
sentiment_analysis,9,36,"The proposed model is based on multi-head self - attention ( MHSA ) and integrates the pre-trained and the local context focus mechanism , namely LCF - ATEPC .","['The', 'proposed', 'model', 'is', 'based', 'on', 'multi-head', 'self', '-', 'attention', '(', 'MHSA', ')', 'and', 'integrates', 'the', 'pre-trained', 'and', 'the', 'local', 'context', 'focus', 'mechanism', ',', 'namely', 'LCF', '-', 'ATEPC', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'VBN', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', 'NN', ':', 'NN', '(', 'NNP', ')', 'CC', 'VBZ', 'DT', 'JJ', 'CC', 'DT', 'JJ', 'NN', 'NN', 'NN', ',', 'RB', 'NNP', ':', 'NN', '.']",29
sentiment_analysis,9,37,"By training on a small amount of annotated data of aspect and their polarity , the model can be adapted to a large - scale dataset , automatically extracting the aspects and predicting the sentiment polarities .","['By', 'training', 'on', 'a', 'small', 'amount', 'of', 'annotated', 'data', 'of', 'aspect', 'and', 'their', 'polarity', ',', 'the', 'model', 'can', 'be', 'adapted', 'to', 'a', 'large', '-', 'scale', 'dataset', ',', 'automatically', 'extracting', 'the', 'aspects', 'and', 'predicting', 'the', 'sentiment', 'polarities', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'IN', 'VBN', 'NNS', 'IN', 'NN', 'CC', 'PRP$', 'NN', ',', 'DT', 'NN', 'MD', 'VB', 'VBN', 'TO', 'DT', 'JJ', ':', 'NN', 'NN', ',', 'RB', 'VBG', 'DT', 'NNS', 'CC', 'VBG', 'DT', 'NN', 'NNS', '.']",37
sentiment_analysis,9,48,The codes for this paper are available at https://github.com/yangheng95/LCF-ATEPC,"['The', 'codes', 'for', 'this', 'paper', 'are', 'available', 'at', 'https://github.com/yangheng95/LCF-ATEPC']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']","['DT', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'JJ', 'IN', 'NN']",9
sentiment_analysis,9,192,"ATAE - LSTM is a classical LSTM - based network for the APC task , which applies the attention mechanism to focus on the important words in the context .","['ATAE', '-', 'LSTM', 'is', 'a', 'classical', 'LSTM', '-', 'based', 'network', 'for', 'the', 'APC', 'task', ',', 'which', 'applies', 'the', 'attention', 'mechanism', 'to', 'focus', 'on', 'the', 'important', 'words', 'in', 'the', 'context', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['NNP', ':', 'NNP', 'VBZ', 'DT', 'JJ', 'NNP', ':', 'VBN', 'NN', 'IN', 'DT', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'NN', 'TO', 'VB', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']",30
sentiment_analysis,9,195,ATSM -S Peng et al.,"['ATSM', '-S', 'Peng', 'et', 'al.']","['B-n', 'I-n', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', 'FW', 'NN']",5
sentiment_analysis,9,196,is a baseline model of the ATSM variations for Chinese language - oriented ABSA task .,"['is', 'a', 'baseline', 'model', 'of', 'the', 'ATSM', 'variations', 'for', 'Chinese', 'language', '-', 'oriented', 'ABSA', 'task', '.']","['B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NNS', 'IN', 'JJ', 'NN', ':', 'VBN', 'NNP', 'NN', '.']",16
sentiment_analysis,9,198,GANN is novel neural network model for APC task aimed to solve the shortcomings of traditional RNNs and CNNs .,"['GANN', 'is', 'novel', 'neural', 'network', 'model', 'for', 'APC', 'task', 'aimed', 'to', 'solve', 'the', 'shortcomings', 'of', 'traditional', 'RNNs', 'and', 'CNNs', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'NN', 'VBN', 'TO', 'VB', 'DT', 'NNS', 'IN', 'JJ', 'NNP', 'CC', 'NNP', '.']",20
sentiment_analysis,9,201,"AEN - is an attentional encoder network based on the pretrained BERT model , which aims to solve the aspect polarity classification .","['AEN', '-', 'is', 'an', 'attentional', 'encoder', 'network', 'based', 'on', 'the', 'pretrained', 'BERT', 'model', ',', 'which', 'aims', 'to', 'solve', 'the', 'aspect', 'polarity', 'classification', '.']","['B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'VBN', 'IN', 'DT', 'VBN', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', '.']",23
sentiment_analysis,9,202,"BERT - is a BERT - adapted model for Review Reading Comprehension ( RRC ) task , a task inspired by machine reading comprehension ( MRC ) , it could be adapted to aspect - level sentiment classification task .","['BERT', '-', 'is', 'a', 'BERT', '-', 'adapted', 'model', 'for', 'Review', 'Reading', 'Comprehension', '(', 'RRC', ')', 'task', ',', 'a', 'task', 'inspired', 'by', 'machine', 'reading', 'comprehension', '(', 'MRC', ')', ',', 'it', 'could', 'be', 'adapted', 'to', 'aspect', '-', 'level', 'sentiment', 'classification', 'task', '.']","['B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'VBZ', 'DT', 'NNP', ':', 'VBD', 'NN', 'IN', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', ',', 'DT', 'NN', 'VBN', 'IN', 'NN', 'NN', 'NN', '(', 'NNP', ')', ',', 'PRP', 'MD', 'VB', 'VBN', 'TO', 'VB', ':', 'NN', 'NN', 'NN', 'NN', '.']",40
sentiment_analysis,9,203,BERT - BASE is the basic pretrained BERT model .,"['BERT', '-', 'BASE', 'is', 'the', 'basic', 'pretrained', 'BERT', 'model', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', 'VBZ', 'DT', 'JJ', 'VBD', 'NNP', 'NN', '.']",10
sentiment_analysis,9,204,"We adapt it to ABSA multi-task learning , which equips the same ability to automatically extract aspect terms and classify aspects polarity as LCF - ATEPC model .","['We', 'adapt', 'it', 'to', 'ABSA', 'multi-task', 'learning', ',', 'which', 'equips', 'the', 'same', 'ability', 'to', 'automatically', 'extract', 'aspect', 'terms', 'and', 'classify', 'aspects', 'polarity', 'as', 'LCF', '-', 'ATEPC', 'model', '.']","['O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP', 'TO', 'NNP', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'TO', 'RB', 'VB', 'JJ', 'NNS', 'CC', 'NN', 'NNS', 'NN', 'IN', 'NNP', ':', 'NNP', 'NN', '.']",28
sentiment_analysis,9,207,BERT - ADA,"['BERT', '-', 'ADA']","['B-n', 'I-n', 'I-n']","['NNP', ':', 'NN']",3
sentiment_analysis,9,209,"is a domain - adapted BERT - based model proposed for the APC task , which finetuned the BERT - BASE model on task - related corpus .","['is', 'a', 'domain', '-', 'adapted', 'BERT', '-', 'based', 'model', 'proposed', 'for', 'the', 'APC', 'task', ',', 'which', 'finetuned', 'the', 'BERT', '-', 'BASE', 'model', 'on', 'task', '-', 'related', 'corpus', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['VBZ', 'DT', 'NN', ':', 'VBN', 'NNP', ':', 'VBN', 'NN', 'VBN', 'IN', 'DT', 'NNP', 'NN', ',', 'WDT', 'VBD', 'DT', 'NNP', ':', 'NNP', 'NN', 'IN', 'NN', ':', 'JJ', 'NN', '.']",28
sentiment_analysis,9,211,"LCF - ATEPC 5 is the multi -task learning model for the ATE and APC tasks , which is based on the the BERT - SPC model and local context focus mechanism .","['LCF', '-', 'ATEPC', '5', 'is', 'the', 'multi', '-task', 'learning', 'model', 'for', 'the', 'ATE', 'and', 'APC', 'tasks', ',', 'which', 'is', 'based', 'on', 'the', 'the', 'BERT', '-', 'SPC', 'model', 'and', 'local', 'context', 'focus', 'mechanism', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', '$', 'CD', 'VBZ', 'DT', 'NN', 'NNP', 'NN', 'NN', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NNS', ',', 'WDT', 'VBZ', 'VBN', 'IN', 'DT', 'DT', 'NNP', ':', 'NNP', 'NN', 'CC', 'JJ', 'NN', 'NN', 'NN', '.']",33
sentiment_analysis,9,212,LCF - ATE are the variations of the LCF - ATEPC model which only optimize for the ATE task .,"['LCF', '-', 'ATE', 'are', 'the', 'variations', 'of', 'the', 'LCF', '-', 'ATEPC', 'model', 'which', 'only', 'optimize', 'for', 'the', 'ATE', 'task', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['NNP', ':', 'NN', 'VBP', 'DT', 'NNS', 'IN', 'DT', 'NNP', ':', 'NNP', 'NN', 'WDT', 'RB', 'VBP', 'IN', 'DT', 'NNP', 'NN', '.']",20
sentiment_analysis,9,213,LCF - APC are the variations of LCF - ATEPC and it only optimize for the APC task during training process .,"['LCF', '-', 'APC', 'are', 'the', 'variations', 'of', 'LCF', '-', 'ATEPC', 'and', 'it', 'only', 'optimize', 'for', 'the', 'APC', 'task', 'during', 'training', 'process', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ':', 'NN', 'VBP', 'DT', 'NNS', 'IN', 'NNP', ':', 'NN', 'CC', 'PRP', 'RB', 'VB', 'IN', 'DT', 'NNP', 'NN', 'IN', 'VBG', 'NN', '.']",22
sentiment_analysis,9,238,"The CDM layer works better on twitter dataset because there are a lot of non-standard grammar usage and language abbreviations within it , and the local context focus techniques can promote to infer the polarity of terms .","['The', 'CDM', 'layer', 'works', 'better', 'on', 'twitter', 'dataset', 'because', 'there', 'are', 'a', 'lot', 'of', 'non-standard', 'grammar', 'usage', 'and', 'language', 'abbreviations', 'within', 'it', ',', 'and', 'the', 'local', 'context', 'focus', 'techniques', 'can', 'promote', 'to', 'infer', 'the', 'polarity', 'of', 'terms', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NN', 'VBZ', 'RBR', 'IN', 'NN', 'NN', 'IN', 'EX', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', 'CC', 'NN', 'NNS', 'IN', 'PRP', ',', 'CC', 'DT', 'JJ', 'NN', 'NN', 'NNS', 'MD', 'VB', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNS', '.']",38
sentiment_analysis,9,244,"After optimizing the model parameters according to the empirical result , the joint model based on BERT - BASE achieved hopeful performance on all three datasets and even surpassed other proposed BERT based improved models on some datasets , such as BERT - PT , AEN - BERT , SDGCN - BERT , and soon .","['After', 'optimizing', 'the', 'model', 'parameters', 'according', 'to', 'the', 'empirical', 'result', ',', 'the', 'joint', 'model', 'based', 'on', 'BERT', '-', 'BASE', 'achieved', 'hopeful', 'performance', 'on', 'all', 'three', 'datasets', 'and', 'even', 'surpassed', 'other', 'proposed', 'BERT', 'based', 'improved', 'models', 'on', 'some', 'datasets', ',', 'such', 'as', 'BERT', '-', 'PT', ',', 'AEN', '-', 'BERT', ',', 'SDGCN', '-', 'BERT', ',', 'and', 'soon', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBG', 'DT', 'NN', 'NNS', 'VBG', 'TO', 'DT', 'JJ', 'NN', ',', 'DT', 'JJ', 'NN', 'VBN', 'IN', 'NNP', ':', 'NNP', 'VBD', 'JJ', 'NN', 'IN', 'DT', 'CD', 'NNS', 'CC', 'RB', 'VBD', 'JJ', 'VBN', 'NNP', 'VBN', 'JJ', 'NNS', 'IN', 'DT', 'NNS', ',', 'JJ', 'IN', 'NNP', ':', 'NN', ',', 'NNP', ':', 'NNP', ',', 'NNP', ':', 'NNP', ',', 'CC', 'RB', '.']",56
sentiment_analysis,9,246,"Compared with the BERT - BASE model , BERT - SPC significantly improves the accuracy and F 1 score of aspect polarity classification .","['Compared', 'with', 'the', 'BERT', '-', 'BASE', 'model', ',', 'BERT', '-', 'SPC', 'significantly', 'improves', 'the', 'accuracy', 'and', 'F', '1', 'score', 'of', 'aspect', 'polarity', 'classification', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'NNP', ':', 'NNP', 'NN', ',', 'NNP', ':', 'NNP', 'RB', 'VBZ', 'DT', 'NN', 'CC', 'NNP', 'CD', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",24
sentiment_analysis,9,247,"In addition , for the first time , BERT - SPC has increased the F 1 score of ATE subtask on three datasets up to 99 % .","['In', 'addition', ',', 'for', 'the', 'first', 'time', ',', 'BERT', '-', 'SPC', 'has', 'increased', 'the', 'F', '1', 'score', 'of', 'ATE', 'subtask', 'on', 'three', 'datasets', 'up', 'to', '99', '%', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'IN', 'DT', 'JJ', 'NN', ',', 'NNP', ':', 'NNP', 'VBZ', 'VBN', 'DT', 'NNP', 'CD', 'NN', 'IN', 'NNP', 'NN', 'IN', 'CD', 'NNS', 'RB', 'TO', 'CD', 'NN', '.']",28
sentiment_analysis,9,248,"ATEPC - Fusion is a supplementary scheme of LCF mechanism , and it adopts a moderate approach to generate local context features .","['ATEPC', '-', 'Fusion', 'is', 'a', 'supplementary', 'scheme', 'of', 'LCF', 'mechanism', ',', 'and', 'it', 'adopts', 'a', 'moderate', 'approach', 'to', 'generate', 'local', 'context', 'features', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NN', ',', 'CC', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'NN', 'NNS', '.']",23
sentiment_analysis,9,249,The experimental results show that its performance is also better than the existing BERT - based models .,"['The', 'experimental', 'results', 'show', 'that', 'its', 'performance', 'is', 'also', 'better', 'than', 'the', 'existing', 'BERT', '-', 'based', 'models', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NNS', 'VBP', 'IN', 'PRP$', 'NN', 'VBZ', 'RB', 'JJR', 'IN', 'DT', 'VBG', 'NNP', ':', 'VBN', 'NNS', '.']",18
sentiment_analysis,25,2,Aspect Based Sentiment Analysis with Gated Convolutional Networks,"['Aspect', 'Based', 'Sentiment', 'Analysis', 'with', 'Gated', 'Convolutional', 'Networks']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NNP', 'VBD', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",8
sentiment_analysis,25,4,"Aspect based sentiment analysis ( ABSA ) can provide more detailed information than general sentiment analysis , because it aims to predict the sentiment polarities of the given aspects or entities in text .","['Aspect', 'based', 'sentiment', 'analysis', '(', 'ABSA', ')', 'can', 'provide', 'more', 'detailed', 'information', 'than', 'general', 'sentiment', 'analysis', ',', 'because', 'it', 'aims', 'to', 'predict', 'the', 'sentiment', 'polarities', 'of', 'the', 'given', 'aspects', 'or', 'entities', 'in', 'text', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBN', 'NN', 'NN', '(', 'NNP', ')', 'MD', 'VB', 'JJR', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', ',', 'IN', 'PRP', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'NNS', 'IN', 'DT', 'VBN', 'NNS', 'CC', 'NNS', 'IN', 'NN', '.']",34
sentiment_analysis,25,5,We summarize previous approaches into two subtasks : aspect - category sentiment analysis ( ACSA ) and aspect - term sentiment analysis ( ATSA ) .,"['We', 'summarize', 'previous', 'approaches', 'into', 'two', 'subtasks', ':', 'aspect', '-', 'category', 'sentiment', 'analysis', '(', 'ACSA', ')', 'and', 'aspect', '-', 'term', 'sentiment', 'analysis', '(', 'ATSA', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NNS', 'IN', 'CD', 'NNS', ':', 'NN', ':', 'NN', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'JJ', ':', 'NN', 'NN', 'NN', '(', 'NNP', ')', '.']",26
sentiment_analysis,25,18,"A number of models have been developed for ABSA , but there are two different subtasks , namely aspect - category sentiment analysis ( ACSA ) and aspect - term sentiment analysis ( ATSA ) .","['A', 'number', 'of', 'models', 'have', 'been', 'developed', 'for', 'ABSA', ',', 'but', 'there', 'are', 'two', 'different', 'subtasks', ',', 'namely', 'aspect', '-', 'category', 'sentiment', 'analysis', '(', 'ACSA', ')', 'and', 'aspect', '-', 'term', 'sentiment', 'analysis', '(', 'ATSA', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'NNS', 'VBP', 'VBN', 'VBN', 'IN', 'NNP', ',', 'CC', 'EX', 'VBP', 'CD', 'JJ', 'NNS', ',', 'RB', 'JJ', ':', 'NN', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'JJ', ':', 'NN', 'NN', 'NN', '(', 'NNP', ')', '.']",36
sentiment_analysis,25,19,"The goal of ACSA is to predict the sentiment polarity with regard to the given aspect , which is one of a few predefined categories .","['The', 'goal', 'of', 'ACSA', 'is', 'to', 'predict', 'the', 'sentiment', 'polarity', 'with', 'regard', 'to', 'the', 'given', 'aspect', ',', 'which', 'is', 'one', 'of', 'a', 'few', 'predefined', 'categories', '.']","['O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'NNP', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'NN', 'TO', 'DT', 'VBN', 'NN', ',', 'WDT', 'VBZ', 'CD', 'IN', 'DT', 'JJ', 'JJ', 'NNS', '.']",26
sentiment_analysis,25,20,"On the other hand , the goal of ATSA is to identify the sentiment polarity concerning the target entities that appear in the text instead , which could be a multi-word phrase or a single word .","['On', 'the', 'other', 'hand', ',', 'the', 'goal', 'of', 'ATSA', 'is', 'to', 'identify', 'the', 'sentiment', 'polarity', 'concerning', 'the', 'target', 'entities', 'that', 'appear', 'in', 'the', 'text', 'instead', ',', 'which', 'could', 'be', 'a', 'multi-word', 'phrase', 'or', 'a', 'single', 'word', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'DT', 'NN', 'IN', 'NNP', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'NN', 'VBG', 'DT', 'NN', 'NNS', 'WDT', 'VBP', 'IN', 'DT', 'NN', 'RB', ',', 'WDT', 'MD', 'VB', 'DT', 'JJ', 'NN', 'CC', 'DT', 'JJ', 'NN', '.']",37
sentiment_analysis,25,33,"In this paper , we propose a fast and effective neural network for ACSA and ATSA based on convolutions and gating mechanisms , which has much less training time than LSTM based networks , but with better accuracy .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'fast', 'and', 'effective', 'neural', 'network', 'for', 'ACSA', 'and', 'ATSA', 'based', 'on', 'convolutions', 'and', 'gating', 'mechanisms', ',', 'which', 'has', 'much', 'less', 'training', 'time', 'than', 'LSTM', 'based', 'networks', ',', 'but', 'with', 'better', 'accuracy', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'CC', 'JJ', 'JJ', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'VBN', 'IN', 'NNS', 'CC', 'VBG', 'NNS', ',', 'WDT', 'VBZ', 'RB', 'RBR', 'JJ', 'NN', 'IN', 'NNP', 'VBN', 'NNS', ',', 'CC', 'IN', 'JJR', 'NN', '.']",39
sentiment_analysis,25,34,"For ACSA task , our model has two separate convolutional layers on the top of the embedding layer , whose outputs are combined by novel gating units .","['For', 'ACSA', 'task', ',', 'our', 'model', 'has', 'two', 'separate', 'convolutional', 'layers', 'on', 'the', 'top', 'of', 'the', 'embedding', 'layer', ',', 'whose', 'outputs', 'are', 'combined', 'by', 'novel', 'gating', 'units', '.']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'CD', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'VBG', 'NN', ',', 'WP$', 'NNS', 'VBP', 'VBN', 'IN', 'JJ', 'VBG', 'NNS', '.']",28
sentiment_analysis,25,36,"The proposed gating units have two nonlinear gates , each of which is connected to one convolutional layer .","['The', 'proposed', 'gating', 'units', 'have', 'two', 'nonlinear', 'gates', ',', 'each', 'of', 'which', 'is', 'connected', 'to', 'one', 'convolutional', 'layer', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'VBN', 'VBG', 'NNS', 'VBP', 'CD', 'JJ', 'NNS', ',', 'DT', 'IN', 'WDT', 'VBZ', 'VBN', 'TO', 'CD', 'NN', 'NN', '.']",19
sentiment_analysis,25,40,"For ATSA task , where the aspect terms consist of multiple words , we extend our model to include another convolutional layer for the target expressions .","['For', 'ATSA', 'task', ',', 'where', 'the', 'aspect', 'terms', 'consist', 'of', 'multiple', 'words', ',', 'we', 'extend', 'our', 'model', 'to', 'include', 'another', 'convolutional', 'layer', 'for', 'the', 'target', 'expressions', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NNP', 'NN', ',', 'WRB', 'DT', 'NN', 'NNS', 'NN', 'IN', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'PRP$', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NNS', '.']",27
sentiment_analysis,25,148,"In our experiments , word embedding vectors are initialized with 300 - dimension GloVe vectors which are pre-trained on unlabeled data of 840 billion tokens .","['In', 'our', 'experiments', ',', 'word', 'embedding', 'vectors', 'are', 'initialized', 'with', '300', '-', 'dimension', 'GloVe', 'vectors', 'which', 'are', 'pre-trained', 'on', 'unlabeled', 'data', 'of', '840', 'billion', 'tokens', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'PRP$', 'NNS', ',', 'NN', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'CD', ':', 'NN', 'NNP', 'NNS', 'WDT', 'VBP', 'JJ', 'IN', 'JJ', 'NNS', 'IN', 'CD', 'CD', 'NNS', '.']",26
sentiment_analysis,25,149,"Words out of the vocabulary of Glo Ve are randomly initialized with a uniform distribution U ( ? 0.25 , 0.25 ) .","['Words', 'out', 'of', 'the', 'vocabulary', 'of', 'Glo', 'Ve', 'are', 'randomly', 'initialized', 'with', 'a', 'uniform', 'distribution', 'U', '(', '?', '0.25', ',', '0.25', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'IN', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'VBP', 'RB', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NNP', '(', '.', 'CD', ',', 'CD', ')', '.']",23
sentiment_analysis,25,150,"We use Adagrad with a batch size of 32 instances , default learning rate of 1 e ? 2 , and maximal epochs of 30 .","['We', 'use', 'Adagrad', 'with', 'a', 'batch', 'size', 'of', '32', 'instances', ',', 'default', 'learning', 'rate', 'of', '1', 'e', '?', '2', ',', 'and', 'maximal', 'epochs', 'of', '30', '.']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', 'NNS', ',', 'NN', 'VBG', 'NN', 'IN', 'CD', 'NN', '.', 'CD', ',', 'CC', 'JJ', 'NN', 'IN', 'CD', '.']",26
sentiment_analysis,25,151,We only fine tune early stopping with 5 - fold cross validation on training datasets .,"['We', 'only', 'fine', 'tune', 'early', 'stopping', 'with', '5', '-', 'fold', 'cross', 'validation', 'on', 'training', 'datasets', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'RB', 'JJ', 'VBG', 'IN', 'CD', ':', 'NN', 'NN', 'NN', 'IN', 'VBG', 'NNS', '.']",16
sentiment_analysis,25,152,All neural models are implemented in PyTorch .,"['All', 'neural', 'models', 'are', 'implemented', 'in', 'PyTorch', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', '.']",8
sentiment_analysis,25,155,NRC - Canada is the top method in SemEval 2014 Task 4 for ACSA and ATSA task .,"['NRC', '-', 'Canada', 'is', 'the', 'top', 'method', 'in', 'SemEval', '2014', 'Task', '4', 'for', 'ACSA', 'and', 'ATSA', 'task', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'CD', 'NNP', 'CD', 'IN', 'NNP', 'CC', 'NNP', 'NN', '.']",18
sentiment_analysis,25,158,CNN is widely used on text classification task .,"['CNN', 'is', 'widely', 'used', 'on', 'text', 'classification', 'task', '.']","['B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'RB', 'VBN', 'IN', 'JJ', 'NN', 'NN', '.']",9
sentiment_analysis,25,161,TD - LSTM uses two LSTM networks to model the preceding and following contexts of the target to generate target - dependent representation for sentiment prediction .,"['TD', '-', 'LSTM', 'uses', 'two', 'LSTM', 'networks', 'to', 'model', 'the', 'preceding', 'and', 'following', 'contexts', 'of', 'the', 'target', 'to', 'generate', 'target', '-', 'dependent', 'representation', 'for', 'sentiment', 'prediction', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ':', 'JJ', 'NNS', 'CD', 'NNP', 'NNS', 'TO', 'VB', 'DT', 'NN', 'CC', 'VBG', 'NN', 'IN', 'DT', 'NN', 'TO', 'VB', 'NN', ':', 'JJ', 'NN', 'IN', 'NN', 'NN', '.']",27
sentiment_analysis,25,162,ATAE - LSTM is an attention - based LSTM for ACSA task .,"['ATAE', '-', 'LSTM', 'is', 'an', 'attention', '-', 'based', 'LSTM', 'for', 'ACSA', 'task', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'VBZ', 'DT', 'NN', ':', 'VBN', 'NNP', 'IN', 'NNP', 'NN', '.']",13
sentiment_analysis,25,164,"IAN stands for interactive attention network for ATSA task , which is also based on LSTM and attention mechanisms .","['IAN', 'stands', 'for', 'interactive', 'attention', 'network', 'for', 'ATSA', 'task', ',', 'which', 'is', 'also', 'based', 'on', 'LSTM', 'and', 'attention', 'mechanisms', '.']","['B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'IN', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'RB', 'VBN', 'IN', 'NNP', 'CC', 'NN', 'NNS', '.']",20
sentiment_analysis,25,165,"RAM is a recurrent attention network for ATSA task , which uses LSTM and multiple attention mechanisms .","['RAM', 'is', 'a', 'recurrent', 'attention', 'network', 'for', 'ATSA', 'task', ',', 'which', 'uses', 'LSTM', 'and', 'multiple', 'attention', 'mechanisms', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'NNP', 'CC', 'JJ', 'NN', 'NNS', '.']",18
sentiment_analysis,25,166,"GCN stands for gated convolutional neural network , in which GTRU does not have the aspect embedding as an additional input .","['GCN', 'stands', 'for', 'gated', 'convolutional', 'neural', 'network', ',', 'in', 'which', 'GTRU', 'does', 'not', 'have', 'the', 'aspect', 'embedding', 'as', 'an', 'additional', 'input', '.']","['B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'IN', 'JJ', 'JJ', 'JJ', 'NN', ',', 'IN', 'WDT', 'NNP', 'VBZ', 'RB', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",22
sentiment_analysis,25,172,LSTM based model ATAE - LSTM has the worst performance of all neural networks .,"['LSTM', 'based', 'model', 'ATAE', '-', 'LSTM', 'has', 'the', 'worst', 'performance', 'of', 'all', 'neural', 'networks', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBN', 'NN', 'NNP', ':', 'NN', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'DT', 'JJ', 'NNS', '.']",15
sentiment_analysis,25,177,GCAE improves the performance by 1.1 % to 2.5 % compared with ATAE - LSTM .,"['GCAE', 'improves', 'the', 'performance', 'by', '1.1', '%', 'to', '2.5', '%', 'compared', 'with', 'ATAE', '-', 'LSTM', '.']","['B-n', 'B-p', 'O', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', 'TO', 'CD', 'NN', 'VBN', 'IN', 'NNP', ':', 'NN', '.']",16
sentiment_analysis,25,184,"Without the large amount of sentiment lexicons , SVM perform worse than neural methods .","['Without', 'the', 'large', 'amount', 'of', 'sentiment', 'lexicons', ',', 'SVM', 'perform', 'worse', 'than', 'neural', 'methods', '.']","['B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', ',', 'NNP', 'NN', 'JJR', 'IN', 'JJ', 'NNS', '.']",15
sentiment_analysis,25,185,"With multiple sentiment lexicons , the performance is increased by 7.6 % .","['With', 'multiple', 'sentiment', 'lexicons', ',', 'the', 'performance', 'is', 'increased', 'by', '7.6', '%', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'JJ', 'NN', 'NNS', ',', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'CD', 'NN', '.']",13
sentiment_analysis,25,189,GCAE achieves 4 % higher accuracy than ATAE - LSTM on Restaurant - Large and 5 % higher on SemEval - 2014 on ACSA task .,"['GCAE', 'achieves', '4', '%', 'higher', 'accuracy', 'than', 'ATAE', '-', 'LSTM', 'on', 'Restaurant', '-', 'Large', 'and', '5', '%', 'higher', 'on', 'SemEval', '-', '2014', 'on', 'ACSA', 'task', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'CD', 'NN', 'JJR', 'NN', 'IN', 'NNP', ':', 'NN', 'IN', 'NNP', ':', 'NN', 'CC', 'CD', 'NN', 'JJR', 'IN', 'NNP', ':', 'CD', 'IN', 'NNP', 'NN', '.']",26
sentiment_analysis,25,190,"However , GCN , which does not have aspect modeling part , has higher score than GCAE on the original restaurant dataset .","['However', ',', 'GCN', ',', 'which', 'does', 'not', 'have', 'aspect', 'modeling', 'part', ',', 'has', 'higher', 'score', 'than', 'GCAE', 'on', 'the', 'original', 'restaurant', 'dataset', '.']","['O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'NNP', ',', 'WDT', 'VBZ', 'RB', 'VB', 'NN', 'VBG', 'NN', ',', 'VBZ', 'JJR', 'NN', 'IN', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",23
sentiment_analysis,25,192,ATSA,['ATSA'],['B-n'],['NN'],1
sentiment_analysis,25,197,"IAN has better performance than TD - LSTM and ATAE - LSTM , because two attention layers guides the representation learning of the context and the entity interactively .","['IAN', 'has', 'better', 'performance', 'than', 'TD', '-', 'LSTM', 'and', 'ATAE', '-', 'LSTM', ',', 'because', 'two', 'attention', 'layers', 'guides', 'the', 'representation', 'learning', 'of', 'the', 'context', 'and', 'the', 'entity', 'interactively', '.']","['B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'JJR', 'NN', 'IN', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NN', ',', 'IN', 'CD', 'NN', 'NNS', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'CC', 'DT', 'NN', 'RB', '.']",29
sentiment_analysis,25,198,"RAM also achieves good accuracy by combining multiple attentions with a recurrent neural network , but it needs more training time as shown in the following section .","['RAM', 'also', 'achieves', 'good', 'accuracy', 'by', 'combining', 'multiple', 'attentions', 'with', 'a', 'recurrent', 'neural', 'network', ',', 'but', 'it', 'needs', 'more', 'training', 'time', 'as', 'shown', 'in', 'the', 'following', 'section', '.']","['B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'RB', 'VBZ', 'JJ', 'NN', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'JJ', 'NN', ',', 'CC', 'PRP', 'VBZ', 'RBR', 'JJ', 'NN', 'IN', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",28
sentiment_analysis,25,199,"On the hard test dataset , GCAE has 1 % higher accuracy than RAM on restaurant data and 1.7 % higher on laptop data .","['On', 'the', 'hard', 'test', 'dataset', ',', 'GCAE', 'has', '1', '%', 'higher', 'accuracy', 'than', 'RAM', 'on', 'restaurant', 'data', 'and', '1.7', '%', 'higher', 'on', 'laptop', 'data', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'NN', ',', 'NNP', 'VBZ', 'CD', 'NN', 'JJR', 'NN', 'IN', 'NNP', 'IN', 'NN', 'NNS', 'CC', 'CD', 'NN', 'JJR', 'IN', 'JJ', 'NNS', '.']",25
sentiment_analysis,25,201,"Because of the gating mechanisms and the convolutional layer over aspect terms , GCAE outperforms other neural models and basic SVM .","['Because', 'of', 'the', 'gating', 'mechanisms', 'and', 'the', 'convolutional', 'layer', 'over', 'aspect', 'terms', ',', 'GCAE', 'outperforms', 'other', 'neural', 'models', 'and', 'basic', 'SVM', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'IN', 'DT', 'NN', 'NN', 'CC', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', ',', 'NNP', 'VBZ', 'JJ', 'JJ', 'NNS', 'CC', 'JJ', 'NNP', '.']",22
sentiment_analysis,26,2,A Helping Hand : Transfer Learning for Deep Sentiment Analysis,"['A', 'Helping', 'Hand', ':', 'Transfer', 'Learning', 'for', 'Deep', 'Sentiment', 'Analysis']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['DT', 'NNP', 'NNP', ':', 'NN', 'VBG', 'IN', 'NNP', 'NNP', 'NNP']",10
sentiment_analysis,26,9,"Over the past decades , sentiment analysis has grown from an academic endeavour to an essential analytics tool .","['Over', 'the', 'past', 'decades', ',', 'sentiment', 'analysis', 'has', 'grown', 'from', 'an', 'academic', 'endeavour', 'to', 'an', 'essential', 'analytics', 'tool', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NNS', ',', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'TO', 'DT', 'JJ', 'NNS', 'NN', '.']",19
sentiment_analysis,26,12,"In recent years , deep neural architectures based on convolutional or recurrent layers have become established as the preeminent models for supervised sentiment polarity classification .","['In', 'recent', 'years', ',', 'deep', 'neural', 'architectures', 'based', 'on', 'convolutional', 'or', 'recurrent', 'layers', 'have', 'become', 'established', 'as', 'the', 'preeminent', 'models', 'for', 'supervised', 'sentiment', 'polarity', 'classification', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', 'NNS', ',', 'JJ', 'JJ', 'NNS', 'VBN', 'IN', 'JJ', 'CC', 'JJ', 'NNS', 'VBP', 'VBN', 'VBN', 'IN', 'DT', 'NN', 'NNS', 'IN', 'JJ', 'NN', 'NN', 'NN', '.']",26
sentiment_analysis,26,16,"In this paper , we investigate how extrinsic signals can be incorporated into deep neural networks for sentiment analysis .","['In', 'this', 'paper', ',', 'we', 'investigate', 'how', 'extrinsic', 'signals', 'can', 'be', 'incorporated', 'into', 'deep', 'neural', 'networks', 'for', 'sentiment', 'analysis', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'WRB', 'JJ', 'NNS', 'MD', 'VB', 'VBN', 'IN', 'JJ', 'JJ', 'NNS', 'IN', 'NN', 'NN', '.']",20
sentiment_analysis,26,18,"In our paper , we instead consider word embeddings specifically specialized for the task of sentiment analysis , studying how they can lead to stronger and more consistent gains , despite the fact that the embeddings were obtained using out - of - domain data .","['In', 'our', 'paper', ',', 'we', 'instead', 'consider', 'word', 'embeddings', 'specifically', 'specialized', 'for', 'the', 'task', 'of', 'sentiment', 'analysis', ',', 'studying', 'how', 'they', 'can', 'lead', 'to', 'stronger', 'and', 'more', 'consistent', 'gains', ',', 'despite', 'the', 'fact', 'that', 'the', 'embeddings', 'were', 'obtained', 'using', 'out', '-', 'of', '-', 'domain', 'data', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'PRP$', 'NN', ',', 'PRP', 'RB', 'VB', 'NN', 'NNS', 'RB', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', ',', 'VBG', 'WRB', 'PRP', 'MD', 'VB', 'TO', 'JJR', 'CC', 'RBR', 'JJ', 'NNS', ',', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNS', 'VBD', 'VBN', 'VBG', 'RP', ':', 'IN', ':', 'NN', 'NNS', '.']",46
sentiment_analysis,26,20,We instead propose a bespoke convolutional neural network architecture with a separate memory module dedicated to the sentiment embeddings .,"['We', 'instead', 'propose', 'a', 'bespoke', 'convolutional', 'neural', 'network', 'architecture', 'with', 'a', 'separate', 'memory', 'module', 'dedicated', 'to', 'the', 'sentiment', 'embeddings', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBD', 'DT', 'JJ', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'VBN', 'TO', 'DT', 'NN', 'NNS', '.']",20
sentiment_analysis,26,125,Embeddings .,"['Embeddings', '.']","['B-n', 'O']","['NNS', '.']",2
sentiment_analysis,26,126,"The standard pre-trained word vectors used for English are the GloVe ones trained on 840 billion tokens of Common Crawl data 1 , while for other languages , we rely on the Facebook fastText Wikipedia embeddings as input representations .","['The', 'standard', 'pre-trained', 'word', 'vectors', 'used', 'for', 'English', 'are', 'the', 'GloVe', 'ones', 'trained', 'on', '840', 'billion', 'tokens', 'of', 'Common', 'Crawl', 'data', '1', ',', 'while', 'for', 'other', 'languages', ',', 'we', 'rely', 'on', 'the', 'Facebook', 'fastText', 'Wikipedia', 'embeddings', 'as', 'input', 'representations', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'JJ', 'NN', 'NNS', 'VBD', 'IN', 'NNP', 'VBP', 'DT', 'NNP', 'NNS', 'VBD', 'IN', 'CD', 'CD', 'NNS', 'IN', 'NNP', 'NNP', 'VBZ', 'CD', ',', 'IN', 'IN', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'IN', 'DT', 'NNP', 'JJ', 'NNP', 'NNS', 'IN', 'NN', 'NNS', '.']",40
sentiment_analysis,26,127,All of these are 300 - dimensional .,"['All', 'of', 'these', 'are', '300', '-', 'dimensional', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'IN', 'DT', 'VBP', 'CD', ':', 'NN', '.']",8
sentiment_analysis,26,128,"The vectors are either fed to the CNN , or to the convolutional module of the DM - MCNN during initialization , while unknown words are initialized with zeros .","['The', 'vectors', 'are', 'either', 'fed', 'to', 'the', 'CNN', ',', 'or', 'to', 'the', 'convolutional', 'module', 'of', 'the', 'DM', '-', 'MCNN', 'during', 'initialization', ',', 'while', 'unknown', 'words', 'are', 'initialized', 'with', 'zeros', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NNS', 'VBP', 'DT', 'NN', 'TO', 'DT', 'NNP', ',', 'CC', 'TO', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', ':', 'NNP', 'IN', 'NN', ',', 'IN', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'NN', '.']",30
sentiment_analysis,26,129,"All words , including the unknown ones , are fine - tuned during the training process .","['All', 'words', ',', 'including', 'the', 'unknown', 'ones', ',', 'are', 'fine', '-', 'tuned', 'during', 'the', 'training', 'process', '.']","['B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNS', ',', 'VBG', 'DT', 'JJ', 'NNS', ',', 'VBP', 'JJ', ':', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",17
sentiment_analysis,26,130,"For our transfer learning approach , our experiments rely on the multi-domain sentiment dataset by , collected from Amazon customers reviews .","['For', 'our', 'transfer', 'learning', 'approach', ',', 'our', 'experiments', 'rely', 'on', 'the', 'multi-domain', 'sentiment', 'dataset', 'by', ',', 'collected', 'from', 'Amazon', 'customers', 'reviews', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'PRP$', 'NN', 'VBG', 'NN', ',', 'PRP$', 'NNS', 'RB', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', ',', 'VBN', 'IN', 'NNP', 'NNS', 'NNS', '.']",22
sentiment_analysis,26,132,"Specifically , we train linear SVMs using scikit - learn to extract word coefficients in each domain and also for the union of all domains together , yielding a 26 - dimensional sentiment embedding .","['Specifically', ',', 'we', 'train', 'linear', 'SVMs', 'using', 'scikit', '-', 'learn', 'to', 'extract', 'word', 'coefficients', 'in', 'each', 'domain', 'and', 'also', 'for', 'the', 'union', 'of', 'all', 'domains', 'together', ',', 'yielding', 'a', '26', '-', 'dimensional', 'sentiment', 'embedding', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'JJ', 'NNP', 'VBG', 'JJ', ':', 'NN', 'TO', 'VB', 'NN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'RB', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNS', 'RB', ',', 'VBG', 'DT', 'CD', ':', 'JJ', 'NN', 'NN', '.']",35
sentiment_analysis,26,133,"For comparison and analysis , we also consider several alternative forms of infusing external cues .","['For', 'comparison', 'and', 'analysis', ',', 'we', 'also', 'consider', 'several', 'alternative', 'forms', 'of', 'infusing', 'external', 'cues', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'NN', 'CC', 'NN', ',', 'PRP', 'RB', 'VBP', 'JJ', 'JJ', 'NNS', 'IN', 'VBG', 'JJ', 'NNS', '.']",16
sentiment_analysis,26,135,We consider a recent sentiment lexicon called VADER .,"['We', 'consider', 'a', 'recent', 'sentiment', 'lexicon', 'called', 'VADER', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'VBN', 'NNP', '.']",9
sentiment_analysis,26,137,"These contain separate domain - specific scores for 250 different Reddit communities , and hence result in 250 - dimensional embeddings .","['These', 'contain', 'separate', 'domain', '-', 'specific', 'scores', 'for', '250', 'different', 'Reddit', 'communities', ',', 'and', 'hence', 'result', 'in', '250', '-', 'dimensional', 'embeddings', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBP', 'JJ', 'NN', ':', 'JJ', 'NNS', 'IN', 'CD', 'JJ', 'NNP', 'NNS', ',', 'CC', 'RB', 'NN', 'IN', 'CD', ':', 'JJ', 'NNS', '.']",22
sentiment_analysis,26,138,"For cross - lingual projection , we extract links between words from a 2017 dump of the English edition of Wiktionary .","['For', 'cross', '-', 'lingual', 'projection', ',', 'we', 'extract', 'links', 'between', 'words', 'from', 'a', '2017', 'dump', 'of', 'the', 'English', 'edition', 'of', 'Wiktionary', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ':', 'JJ', 'NN', ',', 'PRP', 'VBP', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'CD', 'NN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'NNP', '.']",22
sentiment_analysis,26,139,"We restrict the vocabulary link set to include the languages in , mining corresponding translation , synonymy , derivation , and etymological links from Wiktionary .","['We', 'restrict', 'the', 'vocabulary', 'link', 'set', 'to', 'include', 'the', 'languages', 'in', ',', 'mining', 'corresponding', 'translation', ',', 'synonymy', ',', 'derivation', ',', 'and', 'etymological', 'links', 'from', 'Wiktionary', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBN', 'TO', 'VB', 'DT', 'NNS', 'IN', ',', 'VBG', 'VBG', 'NN', ',', 'NN', ',', 'NN', ',', 'CC', 'JJ', 'NNS', 'IN', 'NNP', '.']",26
sentiment_analysis,26,141,"For CNNs , we make use of the well - known CNN - non-static architecture and hyperparameters proposed by , with a learning rate of 0.0006 , obtained by tuning on the validation data .","['For', 'CNNs', ',', 'we', 'make', 'use', 'of', 'the', 'well', '-', 'known', 'CNN', '-', 'non-static', 'architecture', 'and', 'hyperparameters', 'proposed', 'by', ',', 'with', 'a', 'learning', 'rate', 'of', '0.0006', ',', 'obtained', 'by', 'tuning', 'on', 'the', 'validation', 'data', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NNP', ',', 'PRP', 'VBP', 'NN', 'IN', 'DT', 'NN', ':', 'VBN', 'NNP', ':', 'JJ', 'NN', 'CC', 'NNS', 'VBN', 'IN', ',', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', ',', 'VBN', 'IN', 'VBG', 'IN', 'DT', 'NN', 'NNS', '.']",35
sentiment_analysis,26,142,"For our DM - MCNN models , the configuration of the convolutional module is the same as for CNNs , and the remaining hyperparameter values were as well tuned on the validation sets .","['For', 'our', 'DM', '-', 'MCNN', 'models', ',', 'the', 'configuration', 'of', 'the', 'convolutional', 'module', 'is', 'the', 'same', 'as', 'for', 'CNNs', ',', 'and', 'the', 'remaining', 'hyperparameter', 'values', 'were', 'as', 'well', 'tuned', 'on', 'the', 'validation', 'sets', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'PRP$', 'NNP', ':', 'NNP', 'NNS', ',', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'IN', 'IN', 'NNP', ',', 'CC', 'DT', 'VBG', 'NN', 'NNS', 'VBD', 'RB', 'RB', 'VBN', 'IN', 'DT', 'NN', 'NNS', '.']",34
sentiment_analysis,26,144,"For greater efficiency and better convergence properties , the training relies on mini-batches .","['For', 'greater', 'efficiency', 'and', 'better', 'convergence', 'properties', ',', 'the', 'training', 'relies', 'on', 'mini-batches', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'JJR', 'NN', 'CC', 'JJR', 'NN', 'NNS', ',', 'DT', 'NN', 'NNS', 'IN', 'NNS', '.']",14
sentiment_analysis,26,145,"Our implementation considers the maximal sentence length in each mini-batch and zero - pads all other sentences to this length under convolutional module , thus enabling uniform and fast processing of each mini-batch .","['Our', 'implementation', 'considers', 'the', 'maximal', 'sentence', 'length', 'in', 'each', 'mini-batch', 'and', 'zero', '-', 'pads', 'all', 'other', 'sentences', 'to', 'this', 'length', 'under', 'convolutional', 'module', ',', 'thus', 'enabling', 'uniform', 'and', 'fast', 'processing', 'of', 'each', 'mini-batch', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'CC', 'CD', ':', 'NNS', 'DT', 'JJ', 'NNS', 'TO', 'DT', 'NN', 'IN', 'JJ', 'NN', ',', 'RB', 'VBG', 'JJ', 'CC', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",34
sentiment_analysis,26,146,All neural network architectures are implemented using the PyTorch framework 2 .,"['All', 'neural', 'network', 'architectures', 'are', 'implemented', 'using', 'the', 'PyTorch', 'framework', '2', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NNS', 'VBP', 'VBN', 'VBG', 'DT', 'NNP', 'NN', 'CD', '.']",12
sentiment_analysis,26,152,"Comparing this to CNNs with GloVe / fastText embeddings , where Glo Ve is used for English , and fastText is used for all other languages , we observe substantial improvements across all datasets .","['Comparing', 'this', 'to', 'CNNs', 'with', 'GloVe', '/', 'fastText', 'embeddings', ',', 'where', 'Glo', 'Ve', 'is', 'used', 'for', 'English', ',', 'and', 'fastText', 'is', 'used', 'for', 'all', 'other', 'languages', ',', 'we', 'observe', 'substantial', 'improvements', 'across', 'all', 'datasets', '.']","['B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['VBG', 'DT', 'TO', 'VB', 'IN', 'NNP', 'NNP', 'JJ', 'NNS', ',', 'WRB', 'NNP', 'NNP', 'VBZ', 'VBN', 'IN', 'NNP', ',', 'CC', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'IN', 'DT', 'NNS', '.']",35
sentiment_analysis,26,153,This shows that word vectors do tend to convey pertinent word semantics signals that enable models to generalize better .,"['This', 'shows', 'that', 'word', 'vectors', 'do', 'tend', 'to', 'convey', 'pertinent', 'word', 'semantics', 'signals', 'that', 'enable', 'models', 'to', 'generalize', 'better', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'VBZ', 'IN', 'NN', 'NNS', 'VBP', 'VB', 'TO', 'VB', 'JJ', 'NN', 'NNS', 'NNS', 'WDT', 'JJ', 'NNS', 'TO', 'VB', 'JJR', '.']",20
sentiment_analysis,26,154,Note also that the accuracy using GloVe on the English movies review dataset is consistent with numbers reported in previous work .,"['Note', 'also', 'that', 'the', 'accuracy', 'using', 'GloVe', 'on', 'the', 'English', 'movies', 'review', 'dataset', 'is', 'consistent', 'with', 'numbers', 'reported', 'in', 'previous', 'work', '.']","['B-p', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['NN', 'RB', 'IN', 'DT', 'NN', 'VBG', 'NNP', 'IN', 'DT', 'NNP', 'NNS', 'VBP', 'NN', 'VBZ', 'JJ', 'IN', 'NNS', 'VBN', 'IN', 'JJ', 'NN', '.']",22
sentiment_analysis,26,156,"Next , we consider our DM - MCNNs with their dual - module mechanism to take advantage of transfer learning .","['Next', ',', 'we', 'consider', 'our', 'DM', '-', 'MCNNs', 'with', 'their', 'dual', '-', 'module', 'mechanism', 'to', 'take', 'advantage', 'of', 'transfer', 'learning', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', ',', 'PRP', 'VBP', 'PRP$', 'NNP', ':', 'NN', 'IN', 'PRP$', 'JJ', ':', 'NN', 'NN', 'TO', 'VB', 'NN', 'IN', 'NN', 'NN', '.']",21
sentiment_analysis,26,157,We observe fairly consistent and sometimes quite substan - tial gains over CNNs with just the GloVe / fastText vectors .,"['We', 'observe', 'fairly', 'consistent', 'and', 'sometimes', 'quite', 'substan', '-', 'tial', 'gains', 'over', 'CNNs', 'with', 'just', 'the', 'GloVe', '/', 'fastText', 'vectors', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'RB', 'JJ', 'CC', 'RB', 'RB', 'JJ', ':', 'JJ', 'NNS', 'IN', 'NNP', 'IN', 'RB', 'DT', 'NNP', 'NNP', 'JJ', 'NNS', '.']",21
sentiment_analysis,26,171,"Although the automatically projected cross - lingual embeddings are very noisy and limited in their coverage , particularly with respect to inflected forms , our model succeeds in exploiting them to obtain substantial gains in several different languages and domains .","['Although', 'the', 'automatically', 'projected', 'cross', '-', 'lingual', 'embeddings', 'are', 'very', 'noisy', 'and', 'limited', 'in', 'their', 'coverage', ',', 'particularly', 'with', 'respect', 'to', 'inflected', 'forms', ',', 'our', 'model', 'succeeds', 'in', 'exploiting', 'them', 'to', 'obtain', 'substantial', 'gains', 'in', 'several', 'different', 'languages', 'and', 'domains', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'RB', 'VBN', 'NN', ':', 'JJ', 'NNS', 'VBP', 'RB', 'JJ', 'CC', 'VBN', 'IN', 'PRP$', 'NN', ',', 'RB', 'IN', 'NN', 'TO', 'VBN', 'NNS', ',', 'PRP$', 'NN', 'VBZ', 'IN', 'VBG', 'PRP', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'JJ', 'JJ', 'NNS', 'CC', 'NNS', '.']",41
sentiment_analysis,1,2,EEG - Based Emotion Recognition Using Regularized Graph Neural Networks,"['EEG', '-', 'Based', 'Emotion', 'Recognition', 'Using', 'Regularized', 'Graph', 'Neural', 'Networks']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'VBN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",10
sentiment_analysis,1,15,"E MOTION recognition is an important subarea of affective computing , which focuses on recognizing human emotions based on a variety of modalities , such as audio- visual expressions , body language , physiological signals , etc .","['E', 'MOTION', 'recognition', 'is', 'an', 'important', 'subarea', 'of', 'affective', 'computing', ',', 'which', 'focuses', 'on', 'recognizing', 'human', 'emotions', 'based', 'on', 'a', 'variety', 'of', 'modalities', ',', 'such', 'as', 'audio-', 'visual', 'expressions', ',', 'body', 'language', ',', 'physiological', 'signals', ',', 'etc', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'IN', 'VBG', 'JJ', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NNS', ',', 'JJ', 'IN', 'JJ', 'JJ', 'NNS', ',', 'NN', 'NN', ',', 'JJ', 'NNS', ',', 'FW', '.']",38
sentiment_analysis,1,49,"In this paper , we propose a regularized graph neural network ( RGNN ) aiming to address all three aforementioned challenges .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'regularized', 'graph', 'neural', 'network', '(', 'RGNN', ')', 'aiming', 'to', 'address', 'all', 'three', 'aforementioned', 'challenges', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'JJ', 'NN', '(', 'NNP', ')', 'VBG', 'TO', 'VB', 'DT', 'CD', 'VBD', 'NNS', '.']",22
sentiment_analysis,1,52,"Inspired by , , we consider each channel in EEG signals as a node in our graph .","['Inspired', 'by', ',', ',', 'we', 'consider', 'each', 'channel', 'in', 'EEG', 'signals', 'as', 'a', 'node', 'in', 'our', 'graph', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['VBN', 'IN', ',', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NNP', 'NNS', 'IN', 'DT', 'NN', 'IN', 'PRP$', 'NN', '.']",18
sentiment_analysis,1,53,"Our RGNN model extends the simple graph convolution network ( SGC ) and leverages the topological structure of EEG signals , i.e. , according to the economy of brain network organization , we propose a biologically supported sparse adjacency matrix to capture both local and global inter-channel relations .","['Our', 'RGNN', 'model', 'extends', 'the', 'simple', 'graph', 'convolution', 'network', '(', 'SGC', ')', 'and', 'leverages', 'the', 'topological', 'structure', 'of', 'EEG', 'signals', ',', 'i.e.', ',', 'according', 'to', 'the', 'economy', 'of', 'brain', 'network', 'organization', ',', 'we', 'propose', 'a', 'biologically', 'supported', 'sparse', 'adjacency', 'matrix', 'to', 'capture', 'both', 'local', 'and', 'global', 'inter-channel', 'relations', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NNP', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNS', ',', 'NN', ',', 'VBG', 'TO', 'DT', 'NN', 'IN', 'NN', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'RB', 'VBN', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'CC', 'JJ', 'JJ', 'NNS', '.']",49
sentiment_analysis,1,54,"Local interchannel relations connect nearby groups of neurons and may reveal anatomical connectivity at macroscale , .","['Local', 'interchannel', 'relations', 'connect', 'nearby', 'groups', 'of', 'neurons', 'and', 'may', 'reveal', 'anatomical', 'connectivity', 'at', 'macroscale', ',', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O']","['JJ', 'NN', 'NNS', 'VBP', 'JJ', 'NNS', 'IN', 'NNS', 'CC', 'MD', 'VB', 'JJ', 'NN', 'IN', 'NN', ',', '.']",17
sentiment_analysis,1,55,"Global inter-channel relations connect distant groups of neurons between the left and right hemispheres and may reveal emotion - related functional connectivity , .","['Global', 'inter-channel', 'relations', 'connect', 'distant', 'groups', 'of', 'neurons', 'between', 'the', 'left', 'and', 'right', 'hemispheres', 'and', 'may', 'reveal', 'emotion', '-', 'related', 'functional', 'connectivity', ',', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['JJ', 'JJ', 'NNS', 'VBP', 'JJ', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'JJ', 'NNS', 'CC', 'MD', 'VB', 'NN', ':', 'JJ', 'JJ', 'NN', ',', '.']",24
sentiment_analysis,1,312,"For our RGNN in all experiments , we empirically set the number of convolutional layers L = 2 , dropout rate of 0.7 at the output fully - connected layer , and batch size of 16 .","['For', 'our', 'RGNN', 'in', 'all', 'experiments', ',', 'we', 'empirically', 'set', 'the', 'number', 'of', 'convolutional', 'layers', 'L', '=', '2', ',', 'dropout', 'rate', 'of', '0.7', 'at', 'the', 'output', 'fully', '-', 'connected', 'layer', ',', 'and', 'batch', 'size', 'of', '16', '.']","['O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'PRP$', 'NNP', 'IN', 'DT', 'NNS', ',', 'PRP', 'RB', 'VBD', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'NNP', 'VBD', 'CD', ',', 'NN', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', 'RB', ':', 'VBN', 'NN', ',', 'CC', 'NN', 'NN', 'IN', 'CD', '.']",37
sentiment_analysis,1,313,"We use Adam optimization with default values , i.e. , ? 1 = 0.9 and ? 2 = 0.999 .","['We', 'use', 'Adam', 'optimization', 'with', 'default', 'values', ',', 'i.e.', ',', '?', '1', '=', '0.9', 'and', '?', '2', '=', '0.999', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'NN', 'NNS', ',', 'FW', ',', '.', 'CD', 'JJ', 'CD', 'CC', '.', 'CD', 'JJ', 'CD', '.']",20
sentiment_analysis,1,314,"We only tune the output feature dimension d , label noise level , learning rate ? , L1 regularization factor ? , and L2 regularization for each experiment .","['We', 'only', 'tune', 'the', 'output', 'feature', 'dimension', 'd', ',', 'label', 'noise', 'level', ',', 'learning', 'rate', '?', ',', 'L1', 'regularization', 'factor', '?', ',', 'and', 'L2', 'regularization', 'for', 'each', 'experiment', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBD', 'DT', 'NN', 'NN', 'NN', 'NN', ',', 'JJ', 'NN', 'NN', ',', 'VBG', 'NN', '.', ',', 'NNP', 'NN', 'NN', '.', ',', 'CC', 'NNP', 'NN', 'IN', 'DT', 'NN', '.']",29
sentiment_analysis,1,322,It is encouraging to see that our model achieves superior performance on both datasets as compared to all baselines including the stateof - the - art BiHDM when DE features from all frequency bands are used .,"['It', 'is', 'encouraging', 'to', 'see', 'that', 'our', 'model', 'achieves', 'superior', 'performance', 'on', 'both', 'datasets', 'as', 'compared', 'to', 'all', 'baselines', 'including', 'the', 'stateof', '-', 'the', '-', 'art', 'BiHDM', 'when', 'DE', 'features', 'from', 'all', 'frequency', 'bands', 'are', 'used', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'VBG', 'TO', 'VB', 'IN', 'PRP$', 'NN', 'VBZ', 'JJ', 'NN', 'IN', 'DT', 'NNS', 'IN', 'VBN', 'TO', 'DT', 'NNS', 'VBG', 'DT', 'NN', ':', 'DT', ':', 'NN', 'NNP', 'WRB', 'NNP', 'VBZ', 'IN', 'DT', 'NN', 'NNS', 'VBP', 'VBN', '.']",37
sentiment_analysis,1,323,It is worth noting that our model improves the accuracy of the state - of - the - art model on SEED - IV by around 5 % .,"['It', 'is', 'worth', 'noting', 'that', 'our', 'model', 'improves', 'the', 'accuracy', 'of', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'model', 'on', 'SEED', '-', 'IV', 'by', 'around', '5', '%', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'JJ', 'VBG', 'IN', 'PRP$', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'NNP', ':', 'NN', 'IN', 'IN', 'CD', 'NN', '.']",29
sentiment_analysis,1,324,"In particular , our model performs better than DGCNN , which is another GNN - based model that leverages the topological structure in EEG signals .","['In', 'particular', ',', 'our', 'model', 'performs', 'better', 'than', 'DGCNN', ',', 'which', 'is', 'another', 'GNN', '-', 'based', 'model', 'that', 'leverages', 'the', 'topological', 'structure', 'in', 'EEG', 'signals', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'JJ', ',', 'PRP$', 'NN', 'VBZ', 'JJR', 'IN', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NNP', ':', 'VBN', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNS', '.']",26
sentiment_analysis,1,329,"In subject - dependent experiments on SEED , STRNN achieves the highest accuracy in delta , theta and alpha bands , BiDANN performs best in beta band , and our model performs best in gamma band .","['In', 'subject', '-', 'dependent', 'experiments', 'on', 'SEED', ',', 'STRNN', 'achieves', 'the', 'highest', 'accuracy', 'in', 'delta', ',', 'theta', 'and', 'alpha', 'bands', ',', 'BiDANN', 'performs', 'best', 'in', 'beta', 'band', ',', 'and', 'our', 'model', 'performs', 'best', 'in', 'gamma', 'band', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'JJ', ':', 'NN', 'NNS', 'IN', 'NNP', ',', 'NNP', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'NN', ',', 'NN', 'CC', 'NN', 'NNS', ',', 'NNP', 'VBZ', 'JJS', 'IN', 'NN', 'NN', ',', 'CC', 'PRP$', 'NN', 'NNS', 'JJS', 'IN', 'NN', 'NN', '.']",37
sentiment_analysis,1,330,"In subject - independent experiments on SEED , BiDANN - S achieves the highest accuracy in theta and alpha bands , and our model performs best in delta , beta and gamma bands .","['In', 'subject', '-', 'independent', 'experiments', 'on', 'SEED', ',', 'BiDANN', '-', 'S', 'achieves', 'the', 'highest', 'accuracy', 'in', 'theta', 'and', 'alpha', 'bands', ',', 'and', 'our', 'model', 'performs', 'best', 'in', 'delta', ',', 'beta', 'and', 'gamma', 'bands', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', ':', 'JJ', 'NNS', 'IN', 'NNP', ',', 'NNP', ':', 'NNP', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'NN', 'CC', 'NN', 'NNS', ',', 'CC', 'PRP$', 'NN', 'NNS', 'JJS', 'IN', 'NN', ',', 'NN', 'CC', 'NN', 'NNS', '.']",34
sentiment_analysis,1,332,"For both subject - dependent and subjectindependent settings on SEED , we compare the performance of each model across different frequency bands .","['For', 'both', 'subject', '-', 'dependent', 'and', 'subjectindependent', 'settings', 'on', 'SEED', ',', 'we', 'compare', 'the', 'performance', 'of', 'each', 'model', 'across', 'different', 'frequency', 'bands', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', ':', 'NN', 'CC', 'JJ', 'NNS', 'IN', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NNS', '.']",23
sentiment_analysis,1,333,"In general , most models including our model achieve better performance on beta and gamma bands than delta , theta and alpha bands , with one exception of STRNN , which performs the worst on gamma band .","['In', 'general', ',', 'most', 'models', 'including', 'our', 'model', 'achieve', 'better', 'performance', 'on', 'beta', 'and', 'gamma', 'bands', 'than', 'delta', ',', 'theta', 'and', 'alpha', 'bands', ',', 'with', 'one', 'exception', 'of', 'STRNN', ',', 'which', 'performs', 'the', 'worst', 'on', 'gamma', 'band', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'JJ', ',', 'JJS', 'NNS', 'VBG', 'PRP$', 'NN', 'VBP', 'JJR', 'NN', 'IN', 'NN', 'CC', 'NN', 'NNS', 'IN', 'NN', ',', 'NN', 'CC', 'NN', 'NNS', ',', 'IN', 'CD', 'NN', 'IN', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'JJS', 'IN', 'NN', 'NN', '.']",38
sentiment_analysis,1,335,"One subtle difference between our model and other models is that our model performs consistently better in gamma band than beta band , whereas other models perform comparably in both bands , indicating that gamma band maybe the most discriminative band for our model .","['One', 'subtle', 'difference', 'between', 'our', 'model', 'and', 'other', 'models', 'is', 'that', 'our', 'model', 'performs', 'consistently', 'better', 'in', 'gamma', 'band', 'than', 'beta', 'band', ',', 'whereas', 'other', 'models', 'perform', 'comparably', 'in', 'both', 'bands', ',', 'indicating', 'that', 'gamma', 'band', 'maybe', 'the', 'most', 'discriminative', 'band', 'for', 'our', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['CD', 'JJ', 'NN', 'IN', 'PRP$', 'NN', 'CC', 'JJ', 'NNS', 'VBZ', 'IN', 'PRP$', 'NN', 'NNS', 'RB', 'RBR', 'IN', 'NN', 'NN', 'IN', 'NN', 'NN', ',', 'IN', 'JJ', 'NNS', 'VBP', 'RB', 'IN', 'DT', 'NNS', ',', 'VBG', 'IN', 'NN', 'NN', 'RB', 'DT', 'RBS', 'JJ', 'NN', 'IN', 'PRP$', 'NN', '.']",45
sentiment_analysis,1,349,"The two major designs in our adjacency matrix A , i.e. , global connection and symmetric adjacency matrix designs , are helpful in recognizing emotions .","['The', 'two', 'major', 'designs', 'in', 'our', 'adjacency', 'matrix', 'A', ',', 'i.e.', ',', 'global', 'connection', 'and', 'symmetric', 'adjacency', 'matrix', 'designs', ',', 'are', 'helpful', 'in', 'recognizing', 'emotions', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'CD', 'JJ', 'NNS', 'IN', 'PRP$', 'NN', 'VB', 'NNP', ',', 'NN', ',', 'JJ', 'NN', 'CC', 'JJ', 'NN', 'NN', 'NNS', ',', 'VBP', 'JJ', 'IN', 'VBG', 'NNS', '.']",26
sentiment_analysis,1,350,"The global connection models the asymmetric difference between neuronal activities in the left and right hemispheres and have been shown to reveal certain emotions , , .","['The', 'global', 'connection', 'models', 'the', 'asymmetric', 'difference', 'between', 'neuronal', 'activities', 'in', 'the', 'left', 'and', 'right', 'hemispheres', 'and', 'have', 'been', 'shown', 'to', 'reveal', 'certain', 'emotions', ',', ',', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NNS', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CC', 'JJ', 'NNS', 'CC', 'VBP', 'VBN', 'VBN', 'TO', 'VB', 'JJ', 'NNS', ',', ',', '.']",27
sentiment_analysis,1,352,"Our NodeDAT regularizer has a noticeable positive impact on the performance of our model , which demonstrates that domain adaptation is significantly helpful in crosssubject classification .","['Our', 'NodeDAT', 'regularizer', 'has', 'a', 'noticeable', 'positive', 'impact', 'on', 'the', 'performance', 'of', 'our', 'model', ',', 'which', 'demonstrates', 'that', 'domain', 'adaptation', 'is', 'significantly', 'helpful', 'in', 'crosssubject', 'classification', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NNP', 'NN', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'PRP$', 'NN', ',', 'WDT', 'VBZ', 'IN', 'NN', 'NN', 'VBZ', 'RB', 'JJ', 'IN', 'JJ', 'NN', '.']",27
sentiment_analysis,1,355,"In addition , if NodeDAT is removed , the performance of our model has a greater variance , demonstrating the importance of NodeDAT in improving the robustness of our model against cross - subject variations .","['In', 'addition', ',', 'if', 'NodeDAT', 'is', 'removed', ',', 'the', 'performance', 'of', 'our', 'model', 'has', 'a', 'greater', 'variance', ',', 'demonstrating', 'the', 'importance', 'of', 'NodeDAT', 'in', 'improving', 'the', 'robustness', 'of', 'our', 'model', 'against', 'cross', '-', 'subject', 'variations', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ',', 'IN', 'NNP', 'VBZ', 'VBN', ',', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'VBZ', 'DT', 'JJR', 'NN', ',', 'VBG', 'DT', 'NN', 'IN', 'NNP', 'IN', 'VBG', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'IN', 'NN', ':', 'JJ', 'NNS', '.']",36
sentiment_analysis,1,357,DL regularizer improves performance of our model by around 3 % in accuracy on both datasets .,"['DL', 'regularizer', 'improves', 'performance', 'of', 'our', 'model', 'by', 'around', '3', '%', 'in', 'accuracy', 'on', 'both', 'datasets', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'NN', 'VBZ', 'NN', 'IN', 'PRP$', 'NN', 'IN', 'IN', 'CD', 'NN', 'IN', 'NN', 'IN', 'DT', 'NNS', '.']",17
sentiment_analysis,43,2,Multi - grained Attention Network for Aspect - Level Sentiment Classification,"['Multi', '-', 'grained', 'Attention', 'Network', 'for', 'Aspect', '-', 'Level', 'Sentiment', 'Classification']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'VBD', 'NNP', 'NNP', 'IN', 'NNP', ':', 'NN', 'NNP', 'NNP']",11
sentiment_analysis,43,4,We propose a novel multi-grained attention network ( MGAN ) model for aspect level sentiment classification .,"['We', 'propose', 'a', 'novel', 'multi-grained', 'attention', 'network', '(', 'MGAN', ')', 'model', 'for', 'aspect', 'level', 'sentiment', 'classification', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'NN', 'IN', 'JJ', 'NN', 'NN', 'NN', '.']",17
sentiment_analysis,43,29,"In this paper , we propose a multi -grained attention network to address the above two issues in aspect level sentiment classification .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'multi', '-grained', 'attention', 'network', 'to', 'address', 'the', 'above', 'two', 'issues', 'in', 'aspect', 'level', 'sentiment', 'classification', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBN', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'CD', 'NNS', 'IN', 'JJ', 'NN', 'NN', 'NN', '.']",23
sentiment_analysis,43,30,"Specifically , we propose a fine - grained attention mechanism ( i.e. F- Aspect2Context and F - Context2Aspect ) , which is employed to characterize the word - level interactions between aspect and context words , and relieve the information loss occurred in coarse - grained attention mechanism .","['Specifically', ',', 'we', 'propose', 'a', 'fine', '-', 'grained', 'attention', 'mechanism', '(', 'i.e.', 'F-', 'Aspect2Context', 'and', 'F', '-', 'Context2Aspect', ')', ',', 'which', 'is', 'employed', 'to', 'characterize', 'the', 'word', '-', 'level', 'interactions', 'between', 'aspect', 'and', 'context', 'words', ',', 'and', 'relieve', 'the', 'information', 'loss', 'occurred', 'in', 'coarse', '-', 'grained', 'attention', 'mechanism', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'JJ', ':', 'JJ', 'NN', 'NN', '(', 'JJ', 'NNP', 'NNP', 'CC', 'NNP', ':', 'NN', ')', ',', 'WDT', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NN', ':', 'NN', 'NNS', 'IN', 'NN', 'CC', 'NN', 'NNS', ',', 'CC', 'VB', 'DT', 'NN', 'NN', 'VBD', 'IN', 'NN', ':', 'VBN', 'NN', 'NN', '.']",49
sentiment_analysis,43,31,"In addition , we utilize the bidirectional coarsegrained attention ( i.e. C- Aspect2Context and C - Context2Aspect ) and combine them with finegrained attention vectors to compose the multigrained attention network for the final sentiment polarity prediction , which can leverage the advantages of them .","['In', 'addition', ',', 'we', 'utilize', 'the', 'bidirectional', 'coarsegrained', 'attention', '(', 'i.e.', 'C-', 'Aspect2Context', 'and', 'C', '-', 'Context2Aspect', ')', 'and', 'combine', 'them', 'with', 'finegrained', 'attention', 'vectors', 'to', 'compose', 'the', 'multigrained', 'attention', 'network', 'for', 'the', 'final', 'sentiment', 'polarity', 'prediction', ',', 'which', 'can', 'leverage', 'the', 'advantages', 'of', 'them', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBD', 'NN', '(', 'JJ', 'NNP', 'NNP', 'CC', 'NNP', ':', 'NN', ')', 'CC', 'VB', 'PRP', 'IN', 'VBN', 'NN', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', ',', 'WDT', 'MD', 'VB', 'DT', 'NNS', 'IN', 'PRP', '.']",46
sentiment_analysis,43,32,"More importantly , in order to make use of the valuable aspect - level interaction information , we design an aspect alignment loss in the objective function to enhance the difference of the attention weights towards the aspects which have the same context and different sentiment polarities .","['More', 'importantly', ',', 'in', 'order', 'to', 'make', 'use', 'of', 'the', 'valuable', 'aspect', '-', 'level', 'interaction', 'information', ',', 'we', 'design', 'an', 'aspect', 'alignment', 'loss', 'in', 'the', 'objective', 'function', 'to', 'enhance', 'the', 'difference', 'of', 'the', 'attention', 'weights', 'towards', 'the', 'aspects', 'which', 'have', 'the', 'same', 'context', 'and', 'different', 'sentiment', 'polarities', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RBR', 'RB', ',', 'IN', 'NN', 'TO', 'VB', 'NN', 'IN', 'DT', 'JJ', 'JJ', ':', 'NN', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NNS', 'WDT', 'VBP', 'DT', 'JJ', 'NN', 'CC', 'JJ', 'NN', 'NNS', '.']",48
sentiment_analysis,43,171,"In our experiments , word embeddings for both context and aspect words are initialized by Glove .","['In', 'our', 'experiments', ',', 'word', 'embeddings', 'for', 'both', 'context', 'and', 'aspect', 'words', 'are', 'initialized', 'by', 'Glove', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'PRP$', 'NNS', ',', 'NN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', '.']",17
sentiment_analysis,43,172,The dimension of word embedding d v and hidden stated are 1 The detailed task introduction can be found in http://alt.qcri.org/semeval2014/task4/.,"['The', 'dimension', 'of', 'word', 'embedding', 'd', 'v', 'and', 'hidden', 'stated', 'are', '1', 'The', 'detailed', 'task', 'introduction', 'can', 'be', 'found', 'in', 'http://alt.qcri.org/semeval2014/task4/.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'NN', 'VBG', 'JJ', 'NN', 'CC', 'NN', 'VBN', 'VBP', 'CD', 'DT', 'JJ', 'NN', 'NN', 'MD', 'VB', 'VBN', 'IN', 'NN']",21
sentiment_analysis,43,173,set to 300 .,"['set', 'to', '300', '.']","['B-p', 'I-p', 'B-n', 'O']","['VBN', 'TO', 'CD', '.']",4
sentiment_analysis,43,174,"The weight matrix and bias are initialized by sampling from a uniform distribution U ( 0.01 , 0.01 ) .","['The', 'weight', 'matrix', 'and', 'bias', 'are', 'initialized', 'by', 'sampling', 'from', 'a', 'uniform', 'distribution', 'U', '(', '0.01', ',', '0.01', ')', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'CC', 'NN', 'VBP', 'VBN', 'IN', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NNP', '(', 'CD', ',', 'CD', ')', '.']",20
sentiment_analysis,43,175,"The coefficient ? of L 2 regularization item is 10 ? 5 , the parameter ?","['The', 'coefficient', '?', 'of', 'L', '2', 'regularization', 'item', 'is', '10', '?', '5', ',', 'the', 'parameter', '?']","['O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NN', '.', 'IN', 'NNP', 'CD', 'NN', 'NN', 'VBZ', 'CD', '.', 'CD', ',', 'DT', 'NN', '.']",16
sentiment_analysis,43,176,of aspect alignment loss and dropout rate are set to 0.5 .,"['of', 'aspect', 'alignment', 'loss', 'and', 'dropout', 'rate', 'are', 'set', 'to', '0.5', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'JJ', 'JJ', 'NN', 'CC', 'NN', 'NN', 'VBP', 'VBN', 'TO', 'CD', '.']",12
sentiment_analysis,43,179,"Majority is the basic baseline , which chooses the largest sentiment polarity in the training set to each instance in the test set .","['Majority', 'is', 'the', 'basic', 'baseline', ',', 'which', 'chooses', 'the', 'largest', 'sentiment', 'polarity', 'in', 'the', 'training', 'set', 'to', 'each', 'instance', 'in', 'the', 'test', 'set', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NN', 'VBZ', 'DT', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'DT', 'JJS', 'NN', 'NN', 'IN', 'DT', 'NN', 'VBN', 'TO', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",24
sentiment_analysis,43,180,"MemNet applys multi-hop attentions on the word embeddings , learns the attention weights on context word vectors with respect to the averaged query vector .","['MemNet', 'applys', 'multi-hop', 'attentions', 'on', 'the', 'word', 'embeddings', ',', 'learns', 'the', 'attention', 'weights', 'on', 'context', 'word', 'vectors', 'with', 'respect', 'to', 'the', 'averaged', 'query', 'vector', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBD', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NNS', ',', 'VBZ', 'DT', 'NN', 'NNS', 'IN', 'NN', 'NN', 'NNS', 'IN', 'NN', 'TO', 'DT', 'JJ', 'NN', 'NN', '.']",25
sentiment_analysis,43,181,"IAN interactively learns the coarse - grained attentions between the context and aspect , and concatenate the vectors for prediction .","['IAN', 'interactively', 'learns', 'the', 'coarse', '-', 'grained', 'attentions', 'between', 'the', 'context', 'and', 'aspect', ',', 'and', 'concatenate', 'the', 'vectors', 'for', 'prediction', '.']","['B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['NNP', 'RB', 'VBZ', 'DT', 'JJ', ':', 'VBN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NN', ',', 'CC', 'VB', 'DT', 'NNS', 'IN', 'NN', '.']",21
sentiment_analysis,43,182,"BILSTM - ATT -G ( Liu and Zhang , 2017 ) models left and right context with two attention - based LSTMs and utilizes gates to control the importance of left context , right context and the entire sentence for prediction .","['BILSTM', '-', 'ATT', '-G', '(', 'Liu', 'and', 'Zhang', ',', '2017', ')', 'models', 'left', 'and', 'right', 'context', 'with', 'two', 'attention', '-', 'based', 'LSTMs', 'and', 'utilizes', 'gates', 'to', 'control', 'the', 'importance', 'of', 'left', 'context', ',', 'right', 'context', 'and', 'the', 'entire', 'sentence', 'for', 'prediction', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', ':', 'NNP', 'NNP', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'NNS', 'VBN', 'CC', 'JJ', 'NN', 'IN', 'CD', 'NN', ':', 'VBN', 'NNP', 'CC', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NN', ',', 'JJ', 'NN', 'CC', 'DT', 'JJ', 'NN', 'IN', 'NN', '.']",42
sentiment_analysis,43,183,"RAM learns multi-hop attentions on the hidden states of bidirectional LSTM networks for context words , and proposes to use GRU network to get the aggregated vector from the attentions .","['RAM', 'learns', 'multi-hop', 'attentions', 'on', 'the', 'hidden', 'states', 'of', 'bidirectional', 'LSTM', 'networks', 'for', 'context', 'words', ',', 'and', 'proposes', 'to', 'use', 'GRU', 'network', 'to', 'get', 'the', 'aggregated', 'vector', 'from', 'the', 'attentions', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['NNP', 'VBZ', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'JJ', 'NNP', 'NNS', 'IN', 'NN', 'NNS', ',', 'CC', 'VBZ', 'TO', 'VB', 'NNP', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNS', '.']",31
sentiment_analysis,43,186,"MGAN - C only employs the coarse - grained attentions for prediction , which is similar with IAN .","['MGAN', '-', 'C', 'only', 'employs', 'the', 'coarse', '-', 'grained', 'attentions', 'for', 'prediction', ',', 'which', 'is', 'similar', 'with', 'IAN', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NNP', 'RB', 'VBZ', 'DT', 'JJ', ':', 'JJ', 'NNS', 'IN', 'NN', ',', 'WDT', 'VBZ', 'JJ', 'IN', 'NNP', '.']",19
sentiment_analysis,43,187,MGAN - F only utilizes the proposed fine - grained attentions for prediction .,"['MGAN', '-', 'F', 'only', 'utilizes', 'the', 'proposed', 'fine', '-', 'grained', 'attentions', 'for', 'prediction', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', ':', 'NNP', 'RB', 'VBZ', 'DT', 'VBN', 'JJ', ':', 'JJ', 'NNS', 'IN', 'NN', '.']",14
sentiment_analysis,43,188,"MGAN - CF adopts both the coarse - grained and fine - grained attentions , while without applying the aspect alignment loss .","['MGAN', '-', 'CF', 'adopts', 'both', 'the', 'coarse', '-', 'grained', 'and', 'fine', '-', 'grained', 'attentions', ',', 'while', 'without', 'applying', 'the', 'aspect', 'alignment', 'loss', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', 'VBZ', 'CC', 'DT', 'NN', ':', 'VBN', 'CC', 'JJ', ':', 'JJ', 'NNS', ',', 'IN', 'IN', 'VBG', 'DT', 'JJ', 'JJ', 'NN', '.']",23
sentiment_analysis,43,189,MGAN is the complete multi-grained attention network model .,"['MGAN', 'is', 'the', 'complete', 'multi-grained', 'attention', 'network', 'model', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'NN', '.']",9
sentiment_analysis,43,193,( 1 ) Majority performs worst since it only utilizes the data distribution information .,"['(', '1', ')', 'Majority', 'performs', 'worst', 'since', 'it', 'only', 'utilizes', 'the', 'data', 'distribution', 'information', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'CD', ')', 'NN', 'NNS', 'VBP', 'IN', 'PRP', 'RB', 'VBZ', 'DT', 'NNS', 'NN', 'NN', '.']",15
sentiment_analysis,43,195,Our method MGAN outperforms Majority and Feature + SVM since MGAN could learn the high quality representation for prediction .,"['Our', 'method', 'MGAN', 'outperforms', 'Majority', 'and', 'Feature', '+', 'SVM', 'since', 'MGAN', 'could', 'learn', 'the', 'high', 'quality', 'representation', 'for', 'prediction', '.']","['O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'NNP', 'VBZ', 'NNP', 'CC', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'MD', 'VB', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NN', '.']",20
sentiment_analysis,43,196,( 2 ) ATAE - LSTM is better than LSTM since it employs attention mechanism on the hidden states and combines with attention embedding to generate the final representation .,"['(', '2', ')', 'ATAE', '-', 'LSTM', 'is', 'better', 'than', 'LSTM', 'since', 'it', 'employs', 'attention', 'mechanism', 'on', 'the', 'hidden', 'states', 'and', 'combines', 'with', 'attention', 'embedding', 'to', 'generate', 'the', 'final', 'representation', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'CD', ')', 'NNP', ':', 'NN', 'VBZ', 'JJR', 'IN', 'JJR', 'IN', 'PRP', 'VBZ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'CC', 'NNS', 'IN', 'NN', 'VBG', 'TO', 'VB', 'DT', 'JJ', 'NN', '.']",30
sentiment_analysis,43,197,"TD - LSTM performs slightly better than ATAE - LSTM , and it employs two LSTM networks to capture the left and right context of the aspect .","['TD', '-', 'LSTM', 'performs', 'slightly', 'better', 'than', 'ATAE', '-', 'LSTM', ',', 'and', 'it', 'employs', 'two', 'LSTM', 'networks', 'to', 'capture', 'the', 'left', 'and', 'right', 'context', 'of', 'the', 'aspect', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', 'NNS', 'RB', 'JJR', 'IN', 'NNP', ':', 'NNP', ',', 'CC', 'PRP', 'VBZ', 'CD', 'NNP', 'NNS', 'TO', 'VB', 'DT', 'NN', 'CC', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",28
sentiment_analysis,43,198,TD - LSTM performs worse than our method MGAN since it could not properly pay more attentions on the important parts of the context .,"['TD', '-', 'LSTM', 'performs', 'worse', 'than', 'our', 'method', 'MGAN', 'since', 'it', 'could', 'not', 'properly', 'pay', 'more', 'attentions', 'on', 'the', 'important', 'parts', 'of', 'the', 'context', '.']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', 'NNS', 'JJR', 'IN', 'PRP$', 'NN', 'NNP', 'IN', 'PRP', 'MD', 'RB', 'VB', 'NN', 'JJR', 'NNS', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']",25
sentiment_analysis,43,199,"( 3 ) IAN achieves slightly better results with the previous LSTM - based methods , which interactively learns the attended aspect and context vector as final representation .","['(', '3', ')', 'IAN', 'achieves', 'slightly', 'better', 'results', 'with', 'the', 'previous', 'LSTM', '-', 'based', 'methods', ',', 'which', 'interactively', 'learns', 'the', 'attended', 'aspect', 'and', 'context', 'vector', 'as', 'final', 'representation', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'CD', ')', 'NNP', 'VBZ', 'RB', 'JJR', 'NNS', 'IN', 'DT', 'JJ', 'NNP', ':', 'VBN', 'NNS', ',', 'WDT', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'CC', 'NN', 'NN', 'IN', 'JJ', 'NN', '.']",29
sentiment_analysis,43,200,Our method consistently performs better than IAN since we utilize the finegrained attention vectors to relieve the information loss in IAN .,"['Our', 'method', 'consistently', 'performs', 'better', 'than', 'IAN', 'since', 'we', 'utilize', 'the', 'finegrained', 'attention', 'vectors', 'to', 'relieve', 'the', 'information', 'loss', 'in', 'IAN', '.']","['B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'RB', 'VBZ', 'JJR', 'IN', 'NNP', 'IN', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NNS', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'NNP', '.']",22
sentiment_analysis,43,202,"BILSTM - ATT - G models left context and right context using attention - based LSTMs , which achieves better performance than MemNet .","['BILSTM', '-', 'ATT', '-', 'G', 'models', 'left', 'context', 'and', 'right', 'context', 'using', 'attention', '-', 'based', 'LSTMs', ',', 'which', 'achieves', 'better', 'performance', 'than', 'MemNet', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', ':', 'NNP', ':', 'NNP', 'NNS', 'VBD', 'NN', 'CC', 'JJ', 'NN', 'VBG', 'NN', ':', 'VBN', 'NNP', ',', 'WDT', 'VBZ', 'JJR', 'NN', 'IN', 'NNP', '.']",24
sentiment_analysis,43,203,RAM performs better than other baselines .,"['RAM', 'performs', 'better', 'than', 'other', 'baselines', '.']","['B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['NN', 'NNS', 'RBR', 'IN', 'JJ', 'NNS', '.']",7
sentiment_analysis,43,206,"Our proposed MGAN consistently performs better than MemNet , BILSTM - ATT - G and RAM on all three datasets .","['Our', 'proposed', 'MGAN', 'consistently', 'performs', 'better', 'than', 'MemNet', ',', 'BILSTM', '-', 'ATT', '-', 'G', 'and', 'RAM', 'on', 'all', 'three', 'datasets', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'VBN', 'NNP', 'RB', 'VBZ', 'JJR', 'IN', 'NNP', ',', 'NNP', ':', 'NNP', ':', 'NNP', 'CC', 'NNP', 'IN', 'DT', 'CD', 'NNS', '.']",21
sentiment_analysis,21,2,Sentiment Classification using Document Embeddings trained with Cosine Similarity,"['Sentiment', 'Classification', 'using', 'Document', 'Embeddings', 'trained', 'with', 'Cosine', 'Similarity']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NNP', 'VBG', 'NNP', 'NNS', 'VBD', 'IN', 'NNP', 'NNP']",9
sentiment_analysis,21,4,"In document - level sentiment classification , each document must be mapped to a fixed length vector .","['In', 'document', '-', 'level', 'sentiment', 'classification', ',', 'each', 'document', 'must', 'be', 'mapped', 'to', 'a', 'fixed', 'length', 'vector', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', ':', 'NN', 'NN', 'NN', ',', 'DT', 'NN', 'MD', 'VB', 'VBN', 'TO', 'DT', 'JJ', 'NN', 'NN', '.']",18
sentiment_analysis,21,10,"In document classification tasks such as sentiment classification ( in this paper we focus on binary sentiment classification of long movie reviews , i.e. determining whether each review is positive or negative ) , the choice of document representation is usually more important than the choice of classifier .","['In', 'document', 'classification', 'tasks', 'such', 'as', 'sentiment', 'classification', '(', 'in', 'this', 'paper', 'we', 'focus', 'on', 'binary', 'sentiment', 'classification', 'of', 'long', 'movie', 'reviews', ',', 'i.e.', 'determining', 'whether', 'each', 'review', 'is', 'positive', 'or', 'negative', ')', ',', 'the', 'choice', 'of', 'document', 'representation', 'is', 'usually', 'more', 'important', 'than', 'the', 'choice', 'of', 'classifier', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', 'NN', 'NNS', 'JJ', 'IN', 'NN', 'NN', '(', 'IN', 'DT', 'NN', 'PRP', 'VBP', 'IN', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NNS', ',', 'FW', 'VBG', 'IN', 'DT', 'NN', 'VBZ', 'JJ', 'CC', 'JJ', ')', ',', 'DT', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'RB', 'RBR', 'JJ', 'IN', 'DT', 'NN', 'IN', 'NN', '.']",49
sentiment_analysis,21,13,This paper aims to improve existing document embedding models by training document embeddings using cosine similarity instead of dot product .,"['This', 'paper', 'aims', 'to', 'improve', 'existing', 'document', 'embedding', 'models', 'by', 'training', 'document', 'embeddings', 'using', 'cosine', 'similarity', 'instead', 'of', 'dot', 'product', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'TO', 'VB', 'VBG', 'NN', 'VBG', 'NNS', 'IN', 'VBG', 'JJ', 'NNS', 'VBG', 'JJ', 'NN', 'RB', 'IN', 'JJ', 'NN', '.']",21
sentiment_analysis,21,14,"For example , in the basic model of trying to predict given a document - the words / n - grams in the document , instead of trying to maximize the dot product between a document vector and vectors of the words / n - grams in the document over the training set , we 'll be trying to maximize the cosine similarity instead .","['For', 'example', ',', 'in', 'the', 'basic', 'model', 'of', 'trying', 'to', 'predict', 'given', 'a', 'document', '-', 'the', 'words', '/', 'n', '-', 'grams', 'in', 'the', 'document', ',', 'instead', 'of', 'trying', 'to', 'maximize', 'the', 'dot', 'product', 'between', 'a', 'document', 'vector', 'and', 'vectors', 'of', 'the', 'words', '/', 'n', '-', 'grams', 'in', 'the', 'document', 'over', 'the', 'training', 'set', ',', 'we', ""'ll"", 'be', 'trying', 'to', 'maximize', 'the', 'cosine', 'similarity', 'instead', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O']","['IN', 'NN', ',', 'IN', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'TO', 'VB', 'VBN', 'DT', 'NN', ':', 'DT', 'NNS', 'NNP', 'SYM', ':', 'NNS', 'IN', 'DT', 'NN', ',', 'RB', 'IN', 'VBG', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'NNS', 'IN', 'DT', 'NNS', 'NNP', 'SYM', ':', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'PRP', 'MD', 'VB', 'VBG', 'TO', 'VB', 'DT', 'NN', 'NN', 'RB', '.']",65
sentiment_analysis,21,92,Code to reproduce all experiments is available at https://github.com/tanthongtan/dv-cosine.,"['Code', 'to', 'reproduce', 'all', 'experiments', 'is', 'available', 'at', 'https://github.com/tanthongtan/dv-cosine.']","['B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'TO', 'VB', 'DT', 'NNS', 'VBZ', 'JJ', 'IN', 'NN']",9
sentiment_analysis,21,94,Grid search was performed using 20 % of the training data as a validation set in order to determine the optimal hyperparameters as well as whether to use a constant learning rate or learning rate annealing .,"['Grid', 'search', 'was', 'performed', 'using', '20', '%', 'of', 'the', 'training', 'data', 'as', 'a', 'validation', 'set', 'in', 'order', 'to', 'determine', 'the', 'optimal', 'hyperparameters', 'as', 'well', 'as', 'whether', 'to', 'use', 'a', 'constant', 'learning', 'rate', 'or', 'learning', 'rate', 'annealing', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NN', 'VBD', 'VBN', 'VBG', 'CD', 'NN', 'IN', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'VBN', 'IN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'RB', 'RB', 'IN', 'IN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', 'CC', 'VBG', 'NN', 'NN', '.']",37
sentiment_analysis,21,97,"We did however tune the number of iterations from , learning rate from [ 0.25 , 0.025 , 0.0025 , 0.001 ] and ? from .","['We', 'did', 'however', 'tune', 'the', 'number', 'of', 'iterations', 'from', ',', 'learning', 'rate', 'from', '[', '0.25', ',', '0.025', ',', '0.0025', ',', '0.001', ']', 'and', '?', 'from', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'RB', 'VB', 'DT', 'NN', 'IN', 'NNS', 'IN', ',', 'VBG', 'NN', 'IN', '$', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', 'NN', 'CC', '.', 'IN', '.']",26
sentiment_analysis,21,100,"In the case of using L2 regularized dot product , ? ( regularization strength ) was chosen from [ 1 , 0.1 , 0.01 ] .","['In', 'the', 'case', 'of', 'using', 'L2', 'regularized', 'dot', 'product', ',', '?', '(', 'regularization', 'strength', ')', 'was', 'chosen', 'from', '[', '1', ',', '0.1', ',', '0.01', ']', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'IN', 'VBG', 'NNP', 'VBD', 'JJ', 'NN', ',', '.', '(', 'NN', 'NN', ')', 'VBD', 'VBN', 'IN', 'JJ', 'CD', ',', 'CD', ',', 'CD', 'NN', '.']",26
sentiment_analysis,21,101,"The optimal learning rate in the case of cosine similarity is extremely small , suggesting a chaotic error surface .","['The', 'optimal', 'learning', 'rate', 'in', 'the', 'case', 'of', 'cosine', 'similarity', 'is', 'extremely', 'small', ',', 'suggesting', 'a', 'chaotic', 'error', 'surface', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'RB', 'JJ', ',', 'VBG', 'DT', 'JJ', 'NN', 'NN', '.']",20
sentiment_analysis,21,103,The model in turn requires a larger number of epochs for convergence .,"['The', 'model', 'in', 'turn', 'requires', 'a', 'larger', 'number', 'of', 'epochs', 'for', 'convergence', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NN', 'VBZ', 'DT', 'JJR', 'NN', 'IN', 'NN', 'IN', 'NN', '.']",13
sentiment_analysis,21,104,"For the distribution for sampling negative words , we used the n-gram distribution raised to the 3 / 4 th power in accordance with .","['For', 'the', 'distribution', 'for', 'sampling', 'negative', 'words', ',', 'we', 'used', 'the', 'n-gram', 'distribution', 'raised', 'to', 'the', '3', '/', '4', 'th', 'power', 'in', 'accordance', 'with', '.']","['B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'IN', 'VBG', 'JJ', 'NNS', ',', 'PRP', 'VBD', 'DT', 'JJ', 'NN', 'VBN', 'TO', 'DT', 'CD', '$', 'CD', 'JJ', 'NN', 'IN', 'NN', 'IN', '.']",25
sentiment_analysis,21,105,"The weights of the networks were initialized from a uniform distribution in the range of [ - 0.001 , 0.001 ] .","['The', 'weights', 'of', 'the', 'networks', 'were', 'initialized', 'from', 'a', 'uniform', 'distribution', 'in', 'the', 'range', 'of', '[', '-', '0.001', ',', '0.001', ']', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'DT', 'NNS', 'VBD', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ':', 'CD', ',', 'CD', 'NN', '.']",22
sentiment_analysis,21,109,From here we see that using cosine similarity instead of dot product improves accuracy across the board .,"['From', 'here', 'we', 'see', 'that', 'using', 'cosine', 'similarity', 'instead', 'of', 'dot', 'product', 'improves', 'accuracy', 'across', 'the', 'board', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['IN', 'RB', 'PRP', 'VBP', 'IN', 'VBG', 'JJ', 'NN', 'RB', 'IN', 'JJ', 'NN', 'VBZ', 'JJ', 'IN', 'DT', 'NN', '.']",18
sentiment_analysis,21,111,However it is not to suggest that switching from dot product to cosine similarity alone improves accuracy as other minor ad - justments and hyperparameter tuning as explained was done .,"['However', 'it', 'is', 'not', 'to', 'suggest', 'that', 'switching', 'from', 'dot', 'product', 'to', 'cosine', 'similarity', 'alone', 'improves', 'accuracy', 'as', 'other', 'minor', 'ad', '-', 'justments', 'and', 'hyperparameter', 'tuning', 'as', 'explained', 'was', 'done', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'PRP', 'VBZ', 'RB', 'TO', 'VB', 'IN', 'VBG', 'IN', 'JJ', 'NN', 'TO', 'VB', 'NN', 'RB', 'VBZ', 'NN', 'IN', 'JJ', 'JJ', 'NN', ':', 'NNS', 'CC', 'NN', 'NN', 'IN', 'VBN', 'VBD', 'VBN', '.']",31
sentiment_analysis,21,115,"As seen during grid search , whenever the initial learning rate was 0.25 , accuracy was always poor .","['As', 'seen', 'during', 'grid', 'search', ',', 'whenever', 'the', 'initial', 'learning', 'rate', 'was', '0.25', ',', 'accuracy', 'was', 'always', 'poor', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBN', 'IN', 'JJ', 'NN', ',', 'WP', 'DT', 'JJ', 'NN', 'NN', 'VBD', 'CD', ',', 'NN', 'VBD', 'RB', 'JJ', '.']",19
sentiment_analysis,21,116,"Introducing L2 regularization to dot product improves accuracy for all cases except a depreciation in the case of using unigrams only , lucikily cosine similarity does not suffer from this same depreciation .","['Introducing', 'L2', 'regularization', 'to', 'dot', 'product', 'improves', 'accuracy', 'for', 'all', 'cases', 'except', 'a', 'depreciation', 'in', 'the', 'case', 'of', 'using', 'unigrams', 'only', ',', 'lucikily', 'cosine', 'similarity', 'does', 'not', 'suffer', 'from', 'this', 'same', 'depreciation', '.']","['B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NNP', 'NN', 'TO', 'VB', 'NN', 'VBZ', 'NN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'VBG', 'JJ', 'RB', ',', 'RB', 'JJ', 'NN', 'VBZ', 'RB', 'VB', 'IN', 'DT', 'JJ', 'NN', '.']",33
sentiment_analysis,5,2,A Novel Bi-hemispheric Discrepancy Model for EEG Emotion Recognition,"['A', 'Novel', 'Bi-hemispheric', 'Discrepancy', 'Model', 'for', 'EEG', 'Emotion', 'Recognition']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['DT', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",9
sentiment_analysis,5,5,"Inspired by this study , in this paper , we propose a novel bi-hemispheric discrepancy model ( BiHDM ) to learn the asymmetric differences between two hemispheres for electroencephalograph ( EEG ) emotion recognition .","['Inspired', 'by', 'this', 'study', ',', 'in', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', 'bi-hemispheric', 'discrepancy', 'model', '(', 'BiHDM', ')', 'to', 'learn', 'the', 'asymmetric', 'differences', 'between', 'two', 'hemispheres', 'for', 'electroencephalograph', '(', 'EEG', ')', 'emotion', 'recognition', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'NN', ',', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'IN', 'CD', 'NNS', 'IN', 'NN', '(', 'NNP', ')', 'NN', 'NN', '.']",35
sentiment_analysis,5,13,"As the first step to make machines capture human emotions , emotion recognition has received substantial attention from human - machine - interaction ( HMI ) and pattern recognition research communities in recent years , , .","['As', 'the', 'first', 'step', 'to', 'make', 'machines', 'capture', 'human', 'emotions', ',', 'emotion', 'recognition', 'has', 'received', 'substantial', 'attention', 'from', 'human', '-', 'machine', '-', 'interaction', '(', 'HMI', ')', 'and', 'pattern', 'recognition', 'research', 'communities', 'in', 'recent', 'years', ',', ',', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', 'TO', 'VB', 'NNS', 'VB', 'JJ', 'NNS', ',', 'NN', 'NN', 'VBZ', 'VBN', 'JJ', 'NN', 'IN', 'JJ', ':', 'NN', ':', 'NN', '(', 'NNP', ')', 'CC', 'JJ', 'NN', 'NN', 'NNS', 'IN', 'JJ', 'NNS', ',', ',', '.']",37
sentiment_analysis,5,43,"Thus , in this paper , we propose a novel neural network model BiHDM to learn the bi-hemispheric discrepancy for EEG emotion recognition .","['Thus', ',', 'in', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', 'neural', 'network', 'model', 'BiHDM', 'to', 'learn', 'the', 'bi-hemispheric', 'discrepancy', 'for', 'EEG', 'emotion', 'recognition', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'NNP', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NN', 'NN', '.']",24
sentiment_analysis,5,44,"BiHDM aims to obtain the deep discrepant features between the left and right hemispheres , which is expected to contain more discriminative information to recognize the EEG emotion signals .","['BiHDM', 'aims', 'to', 'obtain', 'the', 'deep', 'discrepant', 'features', 'between', 'the', 'left', 'and', 'right', 'hemispheres', ',', 'which', 'is', 'expected', 'to', 'contain', 'more', 'discriminative', 'information', 'to', 'recognize', 'the', 'EEG', 'emotion', 'signals', '.']","['B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'JJ', 'NNS', ',', 'WDT', 'VBZ', 'VBN', 'TO', 'VB', 'RBR', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NNP', 'NN', 'NNS', '.']",30
sentiment_analysis,5,47,"Hence , to avoid losing this intrinsic graph structural information of EEG data , we can simplify the graph structure learning process by using the horizontal and vertical traversing RNNs , which will construct a complete relationship graph and generate discriminative deep features for all the EEG electrodes .","['Hence', ',', 'to', 'avoid', 'losing', 'this', 'intrinsic', 'graph', 'structural', 'information', 'of', 'EEG', 'data', ',', 'we', 'can', 'simplify', 'the', 'graph', 'structure', 'learning', 'process', 'by', 'using', 'the', 'horizontal', 'and', 'vertical', 'traversing', 'RNNs', ',', 'which', 'will', 'construct', 'a', 'complete', 'relationship', 'graph', 'and', 'generate', 'discriminative', 'deep', 'features', 'for', 'all', 'the', 'EEG', 'electrodes', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'TO', 'VB', 'VBG', 'DT', 'JJ', 'JJ', 'JJ', 'NN', 'IN', 'NNP', 'NNS', ',', 'PRP', 'MD', 'VB', 'DT', 'NN', 'NN', 'VBG', 'NN', 'IN', 'VBG', 'DT', 'NN', 'CC', 'JJ', 'NN', 'NNP', ',', 'WDT', 'MD', 'VB', 'DT', 'JJ', 'NN', 'NN', 'CC', 'VB', 'JJ', 'JJ', 'NNS', 'IN', 'PDT', 'DT', 'NNP', 'NNS', '.']",49
sentiment_analysis,5,48,"After obtaining these deep features of each electrodes , we can extract the asymmetric discrepancy information between two hemispheres by performing specific pairwise operations for any paired symmetric electrodes .","['After', 'obtaining', 'these', 'deep', 'features', 'of', 'each', 'electrodes', ',', 'we', 'can', 'extract', 'the', 'asymmetric', 'discrepancy', 'information', 'between', 'two', 'hemispheres', 'by', 'performing', 'specific', 'pairwise', 'operations', 'for', 'any', 'paired', 'symmetric', 'electrodes', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBG', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NNS', ',', 'PRP', 'MD', 'VB', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'NNS', 'IN', 'VBG', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'JJ', 'NNS', '.']",30
sentiment_analysis,5,158,"We use the released handcrafted features , i.e. , the differential entropy ( DE ) in SEED and SEED - IV , and the Short - Time Fourier Transform ( STFT ) in MPED , as the input to feed our model .","['We', 'use', 'the', 'released', 'handcrafted', 'features', ',', 'i.e.', ',', 'the', 'differential', 'entropy', '(', 'DE', ')', 'in', 'SEED', 'and', 'SEED', '-', 'IV', ',', 'and', 'the', 'Short', '-', 'Time', 'Fourier', 'Transform', '(', 'STFT', ')', 'in', 'MPED', ',', 'as', 'the', 'input', 'to', 'feed', 'our', 'model', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'VBN', 'JJ', 'NNS', ',', 'FW', ',', 'DT', 'JJ', 'NN', '(', 'NNP', ')', 'IN', 'NNP', 'CC', 'NNP', ':', 'NNP', ',', 'CC', 'DT', 'NNP', ':', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'IN', 'NNP', ',', 'IN', 'DT', 'NN', 'TO', 'VB', 'PRP$', 'NN', '.']",43
sentiment_analysis,5,159,"Thus the sizes d N of the input sample X t are 5 62 , 5 62 and 1 62 for these three datasets , respectively .","['Thus', 'the', 'sizes', 'd', 'N', 'of', 'the', 'input', 'sample', 'X', 't', 'are', '5', '62', ',', '5', '62', 'and', '1', '62', 'for', 'these', 'three', 'datasets', ',', 'respectively', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'DT', 'JJ', 'NN', 'NNP', 'IN', 'DT', 'NN', 'NN', 'NNP', 'NN', 'VBP', 'CD', 'CD', ',', 'CD', 'CD', 'CC', 'CD', 'CD', 'IN', 'DT', 'CD', 'NNS', ',', 'RB', '.']",27
sentiment_analysis,5,160,"Moreover , in the experiment , we respectively set the dimension d l of each electrode 's deep representation to 32 ; the parameters d g and K of the global high - level feature to 32 and 6 ; and the dimension do of the output feature to 16 without elaborate traversal .","['Moreover', ',', 'in', 'the', 'experiment', ',', 'we', 'respectively', 'set', 'the', 'dimension', 'd', 'l', 'of', 'each', 'electrode', ""'s"", 'deep', 'representation', 'to', '32', ';', 'the', 'parameters', 'd', 'g', 'and', 'K', 'of', 'the', 'global', 'high', '-', 'level', 'feature', 'to', '32', 'and', '6', ';', 'and', 'the', 'dimension', 'do', 'of', 'the', 'output', 'feature', 'to', '16', 'without', 'elaborate', 'traversal', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'IN', 'DT', 'NN', ',', 'PRP', 'RB', 'VBD', 'DT', 'NN', 'NN', 'NN', 'IN', 'DT', 'NN', 'POS', 'JJ', 'NN', 'TO', 'CD', ':', 'DT', 'NNS', 'VBP', 'NN', 'CC', 'NNP', 'IN', 'DT', 'JJ', 'JJ', ':', 'NN', 'NN', 'TO', 'CD', 'CC', 'CD', ':', 'CC', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'TO', 'CD', 'IN', 'JJ', 'NN', '.']",54
sentiment_analysis,5,161,"Specifically , we implemented BiHDM using Tensor Flow on one Nvidia 1080 Ti GPU .","['Specifically', ',', 'we', 'implemented', 'BiHDM', 'using', 'Tensor', 'Flow', 'on', 'one', 'Nvidia', '1080', 'Ti', 'GPU', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBD', 'NNP', 'VBG', 'NNP', 'NNP', 'IN', 'CD', 'NNP', 'CD', 'NNP', 'NNP', '.']",15
sentiment_analysis,5,162,"The learning rate , momentum and weight decay rate are set as 0.003 , 0.9 and 0.95 respectively .","['The', 'learning', 'rate', ',', 'momentum', 'and', 'weight', 'decay', 'rate', 'are', 'set', 'as', '0.003', ',', '0.9', 'and', '0.95', 'respectively', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['DT', 'NN', 'NN', ',', 'NN', 'CC', 'JJ', 'NN', 'NN', 'VBP', 'VBN', 'IN', 'CD', ',', 'CD', 'CC', 'CD', 'RB', '.']",19
sentiment_analysis,5,163,The network is trained using SGD with batch size of 200 .,"['The', 'network', 'is', 'trained', 'using', 'SGD', 'with', 'batch', 'size', 'of', '200', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'VBG', 'NNP', 'IN', 'NN', 'NN', 'IN', 'CD', '.']",12
sentiment_analysis,5,164,"In addition , we adopt the subtraction as the pairwise operation of the BiHDM model in the experiment section , and discuss the other two types of operations in section III - D.","['In', 'addition', ',', 'we', 'adopt', 'the', 'subtraction', 'as', 'the', 'pairwise', 'operation', 'of', 'the', 'BiHDM', 'model', 'in', 'the', 'experiment', 'section', ',', 'and', 'discuss', 'the', 'other', 'two', 'types', 'of', 'operations', 'in', 'section', 'III', '-', 'D.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'CC', 'VB', 'DT', 'JJ', 'CD', 'NNS', 'IN', 'NNS', 'IN', 'NN', 'NNP', ':', 'NNP']",33
sentiment_analysis,5,167,1 ) The subject - dependent experiment :,"['1', ')', 'The', 'subject', '-', 'dependent', 'experiment', ':']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', ')', 'DT', 'JJ', ':', 'JJ', 'NN', ':']",8
sentiment_analysis,5,171,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .","['To', 'validate', 'the', 'superiority', 'of', 'BiHDM', ',', 'we', 'also', 'conduct', 'the', 'same', 'experiments', 'using', 'twelve', 'methods', ',', 'including', 'linear', 'support', 'vector', 'machine', '(', 'SVM', ')', ',', 'random', 'forest', '(', 'RF', ')', ',', 'canonical', 'correlation', 'analysis', '(', 'CCA', ')', ',', 'group', 'sparse', 'canonical', 'correlation', 'analysis', '(', 'GSCCA', ')', ',', 'deep', 'believe', 'network', '(', 'DBN', ')', ',', 'graph', 'regularization', 'sparse', 'linear', 'regression', '(', 'GRSLR', ')', ',', 'graph', 'convolutional', 'neural', 'network', '(', 'GCNN', ')', ',', 'dynamical', 'graph', 'convolutional', 'neural', 'network', '(', 'DGCNN', ')', ',', 'domain', 'adversarial', 'neural', 'networks', '(', 'DANN', ')', ',', 'bi-hemisphere', 'domain', 'adversarial', 'neural', 'network', '(', 'BiDANN', ')', ',', 'EmotionMeter', ',', 'and', 'attention', '-', 'long', 'short', '-', 'term', 'memory', '(', 'A', '-', 'LSTM', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', 'IN', 'NNP', ',', 'PRP', 'RB', 'VBP', 'DT', 'JJ', 'NNS', 'VBG', 'JJ', 'NNS', ',', 'VBG', 'JJ', 'NN', 'NN', 'NN', '(', 'NNP', ')', ',', 'JJ', 'JJS', '(', 'NNP', ')', ',', 'JJ', 'NN', 'NN', '(', 'NNP', ')', ',', 'NN', 'VBD', 'JJ', 'NN', 'NN', '(', 'NNP', ')', ',', 'JJ', 'VBP', 'NN', '(', 'NNP', ')', ',', 'JJ', 'NN', 'NN', 'JJ', 'NN', '(', 'NNP', ')', ',', 'JJ', 'JJ', 'JJ', 'NN', '(', 'NNP', ')', ',', 'JJ', 'NN', 'JJ', 'JJ', 'NN', '(', 'NNP', ')', ',', 'VBP', 'JJ', 'JJ', 'NNS', '(', 'NNP', ')', ',', 'JJ', 'NN', 'JJ', 'JJ', 'NN', '(', 'NNP', ')', ',', 'NNP', ',', 'CC', 'NN', ':', 'JJ', 'JJ', ':', 'NN', 'NN', '(', 'DT', ':', 'NN', ')', '.']",114
sentiment_analysis,5,175,"From , we can see that the proposed BiHDM model outperforms all the compared methods on all the three public EEG emotional datasets , which verifies the effectiveness of BiHDM .","['From', ',', 'we', 'can', 'see', 'that', 'the', 'proposed', 'BiHDM', 'model', 'outperforms', 'all', 'the', 'compared', 'methods', 'on', 'all', 'the', 'three', 'public', 'EEG', 'emotional', 'datasets', ',', 'which', 'verifies', 'the', 'effectiveness', 'of', 'BiHDM', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', ',', 'PRP', 'MD', 'VB', 'IN', 'DT', 'VBN', 'NNP', 'NN', 'VBZ', 'PDT', 'DT', 'VBN', 'NNS', 'IN', 'PDT', 'DT', 'CD', 'JJ', 'NNP', 'JJ', 'NNS', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'NNP', '.']",31
sentiment_analysis,5,176,"Especially for the result on SEED - IV , the proposed method improves over the state - of - the - art method Emotion - Meter by 4 % .","['Especially', 'for', 'the', 'result', 'on', 'SEED', '-', 'IV', ',', 'the', 'proposed', 'method', 'improves', 'over', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'method', 'Emotion', '-', 'Meter', 'by', '4', '%', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', 'IN', 'DT', 'NN', 'IN', 'NNP', ':', 'NNP', ',', 'DT', 'VBN', 'NN', 'VBZ', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'NNP', ':', 'NN', 'IN', 'CD', 'NN', '.']",30
sentiment_analysis,5,177,"Besides , we can see that the compared method BiDANN , which also considers the bi-hemispheric asymmetry , achieves a comparable performance .","['Besides', ',', 'we', 'can', 'see', 'that', 'the', 'compared', 'method', 'BiDANN', ',', 'which', 'also', 'considers', 'the', 'bi-hemispheric', 'asymmetry', ',', 'achieves', 'a', 'comparable', 'performance', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', ',', 'PRP', 'MD', 'VB', 'IN', 'DT', 'VBN', 'NN', 'NNP', ',', 'WDT', 'RB', 'VBZ', 'DT', 'JJ', 'NN', ',', 'VBZ', 'DT', 'JJ', 'NN', '.']",23
sentiment_analysis,5,183,"shows the t- test statistical analysis results , from which we can see BiHDM is significantly better than the baseline method .","['shows', 'the', 't-', 'test', 'statistical', 'analysis', 'results', ',', 'from', 'which', 'we', 'can', 'see', 'BiHDM', 'is', 'significantly', 'better', 'than', 'the', 'baseline', 'method', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['VBZ', 'DT', 'JJ', 'NN', 'JJ', 'NN', 'NNS', ',', 'IN', 'WDT', 'PRP', 'MD', 'VB', 'NNP', 'VBZ', 'RB', 'JJR', 'IN', 'DT', 'NN', 'NN', '.']",22
sentiment_analysis,5,188,2 ) The subject - independent experiment :,"['2', ')', 'The', 'subject', '-', 'independent', 'experiment', ':']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', ')', 'DT', 'JJ', ':', 'JJ', 'NN', ':']",8
sentiment_analysis,5,193,"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .","['In', 'addition', ',', 'for', 'comparison', 'purpose', ',', 'we', 'use', 'twelve', 'methods', 'including', 'Kullback', '-', 'Leibler', 'importance', 'estimation', 'procedure', '(', 'KLIEP', ')', ',', 'unconstrained', 'least', '-', 'squares', 'importance', 'fitting', '(', 'ULSIF', ')', ',', 'selective', 'transfer', 'machine', '(', 'STM', ')', ',', 'linear', 'SVM', ',', 'transfer', 'component', 'analysis', '(', 'TCA', ')', ',', 'transfer', 'component', 'analysis', '(', 'TCA', ')', ',', 'geodesic', 'flow', 'kernel', '(', 'GFK', ')', ',', 'DANN', ',', 'DGCNN', ',', 'deep', 'adaptation', 'network', '(', 'DAN', ')', ',', 'BiDANN', ',', 'and', 'A', '-', 'LSTM', ',', 'to', 'conduct', 'the', 'same', 'experiments', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ',', 'IN', 'NN', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'VBG', 'NNP', ':', 'NNP', 'NN', 'NN', 'NN', '(', 'NNP', ')', ',', 'VBD', 'JJS', ':', 'NNS', 'VBP', 'NN', '(', 'NNP', ')', ',', 'JJ', 'NN', 'NN', '(', 'NNP', ')', ',', 'JJ', 'NNP', ',', 'VB', 'JJ', 'NN', '(', 'NNP', ')', ',', 'VB', 'JJ', 'NN', '(', 'NNP', ')', ',', 'JJ', 'NN', 'NN', '(', 'NNP', ')', ',', 'NNP', ',', 'NNP', ',', 'JJ', 'NN', 'NN', '(', 'NNP', ')', ',', 'NNP', ',', 'CC', 'NNP', ':', 'NN', ',', 'TO', 'VB', 'DT', 'JJ', 'NNS', '.']",87
sentiment_analysis,5,197,"The results are shown in From , it can be clearly seen that the proposed BiHDM method achieves the best performance in the three public datasets , which verifies the effectiveness of BiHDM in dealing with subject - independent EEG emotion recognition .","['The', 'results', 'are', 'shown', 'in', 'From', ',', 'it', 'can', 'be', 'clearly', 'seen', 'that', 'the', 'proposed', 'BiHDM', 'method', 'achieves', 'the', 'best', 'performance', 'in', 'the', 'three', 'public', 'datasets', ',', 'which', 'verifies', 'the', 'effectiveness', 'of', 'BiHDM', 'in', 'dealing', 'with', 'subject', '-', 'independent', 'EEG', 'emotion', 'recognition', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', ',', 'PRP', 'MD', 'VB', 'RB', 'VBN', 'IN', 'DT', 'VBN', 'NNP', 'NN', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'DT', 'CD', 'JJ', 'NNS', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'NNP', 'IN', 'VBG', 'IN', 'JJ', ':', 'JJ', 'NNP', 'NN', 'NN', '.']",43
sentiment_analysis,5,198,"For the three datasets , the improvements on accuracy are 2.2 % , 3.5 % and 2.4 % , respectively , when compared with the existing state - of - the - art methods .","['For', 'the', 'three', 'datasets', ',', 'the', 'improvements', 'on', 'accuracy', 'are', '2.2', '%', ',', '3.5', '%', 'and', '2.4', '%', ',', 'respectively', ',', 'when', 'compared', 'with', 'the', 'existing', 'state', '-', 'of', '-', 'the', '-', 'art', 'methods', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'CD', 'NNS', ',', 'DT', 'NNS', 'IN', 'NN', 'VBP', 'CD', 'NN', ',', 'CD', 'NN', 'CC', 'CD', 'NN', ',', 'RB', ',', 'WRB', 'VBN', 'IN', 'DT', 'VBG', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', '.']",35
sentiment_analysis,5,200,"shows the t- test statistical analysis results , from which we can see BiHDM is significantly better than the baseline method .","['shows', 'the', 't-', 'test', 'statistical', 'analysis', 'results', ',', 'from', 'which', 'we', 'can', 'see', 'BiHDM', 'is', 'significantly', 'better', 'than', 'the', 'baseline', 'method', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['VBZ', 'DT', 'JJ', 'NN', 'JJ', 'NN', 'NNS', ',', 'IN', 'WDT', 'PRP', 'MD', 'VB', 'NNP', 'VBZ', 'RB', 'JJR', 'IN', 'DT', 'NN', 'NN', '.']",22
sentiment_analysis,11,2,Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos,"['Conversational', 'Memory', 'Network', 'for', 'Emotion', 'Recognition', 'in', 'Dyadic', 'Dialogue', 'Videos']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",10
sentiment_analysis,11,4,Emotion recognition in conversations is crucial for the development of empathetic machines .,"['Emotion', 'recognition', 'in', 'conversations', 'is', 'crucial', 'for', 'the', 'development', 'of', 'empathetic', 'machines', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'IN', 'NNS', 'VBZ', 'JJ', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', '.']",13
sentiment_analysis,11,16,"Emotion detection from such resources can benefit numerous fields like counseling , public opinion mining , financial forecasting , and intelligent systems such as smart homes and chatbots .","['Emotion', 'detection', 'from', 'such', 'resources', 'can', 'benefit', 'numerous', 'fields', 'like', 'counseling', ',', 'public', 'opinion', 'mining', ',', 'financial', 'forecasting', ',', 'and', 'intelligent', 'systems', 'such', 'as', 'smart', 'homes', 'and', 'chatbots', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'IN', 'JJ', 'NNS', 'MD', 'VB', 'JJ', 'NNS', 'IN', 'VBG', ',', 'JJ', 'NN', 'NN', ',', 'JJ', 'NN', ',', 'CC', 'JJ', 'NNS', 'JJ', 'IN', 'JJ', 'NNS', 'CC', 'NNS', '.']",29
sentiment_analysis,11,17,"In this paper , we analyze emotion detection in videos of dyadic conversations .","['In', 'this', 'paper', ',', 'we', 'analyze', 'emotion', 'detection', 'in', 'videos', 'of', 'dyadic', 'conversations', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'IN', 'NN', 'IN', 'JJ', 'NNS', '.']",14
sentiment_analysis,11,19,"We propose a conversational memory network ( CMN ) , which uses a multimodal approach for emotion detection in utterances ( a unit of speech bound by breathes or pauses ) of such conversational videos .","['We', 'propose', 'a', 'conversational', 'memory', 'network', '(', 'CMN', ')', ',', 'which', 'uses', 'a', 'multimodal', 'approach', 'for', 'emotion', 'detection', 'in', 'utterances', '(', 'a', 'unit', 'of', 'speech', 'bound', 'by', 'breathes', 'or', 'pauses', ')', 'of', 'such', 'conversational', 'videos', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', '(', 'NNP', ')', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NN', 'IN', 'NNS', '(', 'DT', 'NN', 'IN', 'NN', 'VBN', 'IN', 'NNS', 'CC', 'NNS', ')', 'IN', 'JJ', 'JJ', 'NNS', '.']",36
sentiment_analysis,11,28,Our proposed CMN incorporates these factors by using emotional context information present in the conversation history .,"['Our', 'proposed', 'CMN', 'incorporates', 'these', 'factors', 'by', 'using', 'emotional', 'context', 'information', 'present', 'in', 'the', 'conversation', 'history', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'VBN', 'NNP', 'VBZ', 'DT', 'NNS', 'IN', 'VBG', 'JJ', 'NN', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",17
sentiment_analysis,11,29,It improves speakerbased emotion modeling by using memory networks which are efficient in capturing long - term dependencies and summarizing task - specific details using attention models .,"['It', 'improves', 'speakerbased', 'emotion', 'modeling', 'by', 'using', 'memory', 'networks', 'which', 'are', 'efficient', 'in', 'capturing', 'long', '-', 'term', 'dependencies', 'and', 'summarizing', 'task', '-', 'specific', 'details', 'using', 'attention', 'models', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'VBZ', 'JJ', 'NN', 'NN', 'IN', 'VBG', 'NN', 'NNS', 'WDT', 'VBP', 'JJ', 'IN', 'VBG', 'JJ', ':', 'NN', 'NNS', 'CC', 'VBG', 'NN', ':', 'NN', 'NNS', 'VBG', 'NN', 'NNS', '.']",28
sentiment_analysis,11,30,"Specifically , the memory cells of CMN are continuous vectors that store the context information found in the utterance histories .","['Specifically', ',', 'the', 'memory', 'cells', 'of', 'CMN', 'are', 'continuous', 'vectors', 'that', 'store', 'the', 'context', 'information', 'found', 'in', 'the', 'utterance', 'histories', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'NNS', 'IN', 'NNP', 'VBP', 'JJ', 'NNS', 'WDT', 'VBP', 'DT', 'NN', 'NN', 'VBN', 'IN', 'DT', 'NN', 'NNS', '.']",21
sentiment_analysis,11,31,CMN also models interplay of these memories to capture interspeaker dependencies .,"['CMN', 'also', 'models', 'interplay', 'of', 'these', 'memories', 'to', 'capture', 'interspeaker', 'dependencies', '.']","['B-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['NNP', 'RB', 'NNS', 'NN', 'IN', 'DT', 'NNS', 'TO', 'VB', 'NN', 'NNS', '.']",12
sentiment_analysis,11,32,"CMN first extracts multimodal features ( audio , visual , and text ) for all utterances in a video .","['CMN', 'first', 'extracts', 'multimodal', 'features', '(', 'audio', ',', 'visual', ',', 'and', 'text', ')', 'for', 'all', 'utterances', 'in', 'a', 'video', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['NNP', 'JJ', 'VBZ', 'JJ', 'NNS', '(', 'NN', ',', 'JJ', ',', 'CC', 'NN', ')', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', '.']",20
sentiment_analysis,11,245,We use 10 % of the training set as a held - out validation set for hyperparameter tuning .,"['We', 'use', '10', '%', 'of', 'the', 'training', 'set', 'as', 'a', 'held', '-', 'out', 'validation', 'set', 'for', 'hyperparameter', 'tuning', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'NN', 'IN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', ':', 'IN', 'NN', 'VBN', 'IN', 'NN', 'NN', '.']",19
sentiment_analysis,11,246,"To optimize the parameters , we use Stochastic Gradient Descent ( SGD ) optimizer , starting with an initial learning Utterances whose history has atleast 3 similar emotion labels in either own history or the history of the other person , is counted in case 1 or 2 , respectively .","['To', 'optimize', 'the', 'parameters', ',', 'we', 'use', 'Stochastic', 'Gradient', 'Descent', '(', 'SGD', ')', 'optimizer', ',', 'starting', 'with', 'an', 'initial', 'learning', 'Utterances', 'whose', 'history', 'has', 'atleast', '3', 'similar', 'emotion', 'labels', 'in', 'either', 'own', 'history', 'or', 'the', 'history', 'of', 'the', 'other', 'person', ',', 'is', 'counted', 'in', 'case', '1', 'or', '2', ',', 'respectively', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'NNS', ',', 'PRP', 'VBP', 'JJ', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', ',', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NNS', 'WP$', 'NN', 'VBZ', 'VBN', 'CD', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'CC', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'VBZ', 'VBN', 'IN', 'NN', 'CD', 'CC', 'CD', ',', 'RB', '.']",51
sentiment_analysis,11,249,An annealing approach halves the lr every 20 epochs and termination is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .,"['An', 'annealing', 'approach', 'halves', 'the', 'lr', 'every', '20', 'epochs', 'and', 'termination', 'is', 'decided', 'using', 'an', 'early', '-', 'stop', 'measure', 'with', 'a', 'patience', 'of', '12', 'by', 'monitoring', 'the', 'validation', 'loss', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'VBG', 'NN', 'VBZ', 'DT', 'NN', 'DT', 'CD', 'NNS', 'CC', 'NN', 'VBZ', 'VBN', 'VBG', 'DT', 'JJ', ':', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'CD', 'IN', 'VBG', 'DT', 'NN', 'NN', '.']",30
sentiment_analysis,11,250,Gradient clipping is used for regularization with a norm set to 40 .,"['Gradient', 'clipping', 'is', 'used', 'for', 'regularization', 'with', 'a', 'norm', 'set', 'to', '40', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'O']","['NNP', 'NN', 'VBZ', 'VBN', 'IN', 'NN', 'IN', 'DT', 'NN', 'VBN', 'TO', 'CD', '.']",13
sentiment_analysis,11,251,Hyperparameters are decided using a Random Search .,"['Hyperparameters', 'are', 'decided', 'using', 'a', 'Random', 'Search', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['NNS', 'VBP', 'VBN', 'VBG', 'DT', 'NNP', 'NNP', '.']",8
sentiment_analysis,11,252,"Based on validation performance , context window length K is set to be 40 and the number of hops R is fixed at 3 hops .","['Based', 'on', 'validation', 'performance', ',', 'context', 'window', 'length', 'K', 'is', 'set', 'to', 'be', '40', 'and', 'the', 'number', 'of', 'hops', 'R', 'is', 'fixed', 'at', '3', 'hops', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['VBN', 'IN', 'NN', 'NN', ',', 'NN', 'NN', 'NN', 'NNP', 'VBZ', 'VBN', 'TO', 'VB', 'CD', 'CC', 'DT', 'NN', 'IN', 'NNS', 'NNP', 'VBZ', 'VBN', 'IN', 'CD', 'NNS', '.']",26
sentiment_analysis,11,254,The dimension size of the memory cells d is set as 50 .,"['The', 'dimension', 'size', 'of', 'the', 'memory', 'cells', 'd', 'is', 'set', 'as', '50', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NNS', 'NN', 'VBZ', 'VBN', 'IN', 'CD', '.']",13
sentiment_analysis,11,257,SVM - ensemble :,"['SVM', '-', 'ensemble', ':']","['B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', ':']",4
sentiment_analysis,11,258,A strong context - free benchmark model which uses similar multimodal approach on an ensemble of trees .,"['A', 'strong', 'context', '-', 'free', 'benchmark', 'model', 'which', 'uses', 'similar', 'multimodal', 'approach', 'on', 'an', 'ensemble', 'of', 'trees', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', ':', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'JJ', 'NNS', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNS', '.']",18
sentiment_analysis,11,260,bc - LSTM :,"['bc', '-', 'LSTM', ':']","['B-n', 'I-n', 'I-n', 'O']","['SYM', ':', 'NN', ':']",4
sentiment_analysis,11,261,"A bi-directional LSTM equipped with hierarchical fusion , proposed by .","['A', 'bi-directional', 'LSTM', 'equipped', 'with', 'hierarchical', 'fusion', ',', 'proposed', 'by', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NNP', 'VBD', 'IN', 'JJ', 'NN', ',', 'VBN', 'IN', '.']",11
sentiment_analysis,11,266,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for each historical utterance using an embedding matrix B as used in equation 7 , without sequential modeling .","['Memn2n', ':', 'The', 'original', 'memory', 'network', 'as', 'proposed', 'by', 'Contrasting', 'to', 'CMN', ',', 'the', 'model', 'generates', 'the', 'memory', 'representations', 'for', 'each', 'historical', 'utterance', 'using', 'an', 'embedding', 'matrix', 'B', 'as', 'used', 'in', 'equation', '7', ',', 'without', 'sequential', 'modeling', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['NN', ':', 'DT', 'JJ', 'NN', 'NN', 'IN', 'VBN', 'IN', 'VBG', 'TO', 'NNP', ',', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'VBG', 'DT', 'VBG', 'NN', 'NNP', 'IN', 'VBN', 'IN', 'NN', 'CD', ',', 'IN', 'JJ', 'NN', '.']",38
sentiment_analysis,11,271,"[ 1 , K ] } for ? ? {a , b}. CMN Self :","['[', '1', ',', 'K', ']', '}', 'for', '?', '?', '{a', ',', 'b}.', 'CMN', 'Self', ':']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['RB', 'CD', ',', 'NNP', 'NNP', ')', 'IN', '.', '.', 'NN', ',', 'NN', 'NNP', 'NNP', ':']",15
sentiment_analysis,11,272,"In this baseline , we use only self history for classifying emotion of utterance u i .","['In', 'this', 'baseline', ',', 'we', 'use', 'only', 'self', 'history', 'for', 'classifying', 'emotion', 'of', 'utterance', 'u', 'i', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'RB', 'JJ', 'NN', 'IN', 'VBG', 'NN', 'IN', 'NN', 'NN', 'NN', '.']",17
sentiment_analysis,11,275,CMN N A : Single layer variant of the CMN with no attention module .,"['CMN', 'N', 'A', ':', 'Single', 'layer', 'variant', 'of', 'the', 'CMN', 'with', 'no', 'attention', 'module', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NNP', ':', 'NNP', 'NN', 'NN', 'IN', 'DT', 'NNP', 'IN', 'DT', 'NN', 'NN', '.']",15
sentiment_analysis,11,289,This suggests that gathering contexts temporally through sequential processing is indeed a superior method over non-temporal memory representations .,"['This', 'suggests', 'that', 'gathering', 'contexts', 'temporally', 'through', 'sequential', 'processing', 'is', 'indeed', 'a', 'superior', 'method', 'over', 'non-temporal', 'memory', 'representations', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'IN', 'VBG', 'NN', 'RB', 'IN', 'JJ', 'NN', 'VBZ', 'RB', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NNS', '.']",19
sentiment_analysis,11,290,CMN self which uses only single history channel also provides lesser performance when compared to CMN .,"['CMN', 'self', 'which', 'uses', 'only', 'single', 'history', 'channel', 'also', 'provides', 'lesser', 'performance', 'when', 'compared', 'to', 'CMN', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['NNP', 'PRP', 'WDT', 'VBZ', 'RB', 'JJ', 'NN', 'NN', 'RB', 'VBZ', 'JJR', 'NN', 'WRB', 'VBN', 'TO', 'NNP', '.']",17
sentiment_analysis,11,292,"Overall , predictions on valence and arousal levels also show similar results which reinforce our hypothesis of CMN 's ability to model emotional dynamics .","['Overall', ',', 'predictions', 'on', 'valence', 'and', 'arousal', 'levels', 'also', 'show', 'similar', 'results', 'which', 'reinforce', 'our', 'hypothesis', 'of', 'CMN', ""'s"", 'ability', 'to', 'model', 'emotional', 'dynamics', '.']","['O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', ',', 'NNS', 'IN', 'NN', 'CC', 'NN', 'NNS', 'RB', 'VBP', 'JJ', 'NNS', 'WDT', 'VBP', 'PRP$', 'NN', 'IN', 'NNP', 'POS', 'NN', 'TO', 'VB', 'JJ', 'NNS', '.']",25
sentiment_analysis,23,2,Discriminative Neural Sentence Modeling by Tree - Based Convolution,"['Discriminative', 'Neural', 'Sentence', 'Modeling', 'by', 'Tree', '-', 'Based', 'Convolution']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', ':', 'VBN', 'NN']",9
sentiment_analysis,23,26,"In this paper , we propose a novel neural architecture for discriminative sentence modeling , called the Tree - Based Convolutional Neural Network ( TBCNN ) .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', 'neural', 'architecture', 'for', 'discriminative', 'sentence', 'modeling', ',', 'called', 'the', 'Tree', '-', 'Based', 'Convolutional', 'Neural', 'Network', '(', 'TBCNN', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', ',', 'VBD', 'DT', 'NNP', ':', 'VBD', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', '.']",27
sentiment_analysis,23,27,"Our models can leverage different sentence parsing trees , e.g. , constituency trees and dependency trees .","['Our', 'models', 'can', 'leverage', 'different', 'sentence', 'parsing', 'trees', ',', 'e.g.', ',', 'constituency', 'trees', 'and', 'dependency', 'trees', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NNS', 'MD', 'VB', 'JJ', 'NN', 'NN', 'NNS', ',', 'NN', ',', 'NN', 'NNS', 'CC', 'NN', 'NNS', '.']",17
sentiment_analysis,23,28,"The model variants are denoted as c- TBCNN and d - TBCNN , respectively .","['The', 'model', 'variants', 'are', 'denoted', 'as', 'c-', 'TBCNN', 'and', 'd', '-', 'TBCNN', ',', 'respectively', '.']","['O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'JJ', 'NNP', 'CC', 'SYM', ':', 'NN', ',', 'RB', '.']",15
sentiment_analysis,23,29,"The idea of tree - based convolution is to apply a set of subtree feature detectors , sliding over the entire parsing tree of a sentence ; then pooling aggregates these extracted feature vectors by taking the maximum value in each dimension .","['The', 'idea', 'of', 'tree', '-', 'based', 'convolution', 'is', 'to', 'apply', 'a', 'set', 'of', 'subtree', 'feature', 'detectors', ',', 'sliding', 'over', 'the', 'entire', 'parsing', 'tree', 'of', 'a', 'sentence', ';', 'then', 'pooling', 'aggregates', 'these', 'extracted', 'feature', 'vectors', 'by', 'taking', 'the', 'maximum', 'value', 'in', 'each', 'dimension', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'JJ', ':', 'VBN', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NNS', ',', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', ':', 'RB', 'VBG', 'NNS', 'DT', 'JJ', 'NN', 'NNS', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",43
sentiment_analysis,23,193,"In our d-TBCNN model , the number of units is 300 for convolution and 200 for the last hidden layer .","['In', 'our', 'd-TBCNN', 'model', ',', 'the', 'number', 'of', 'units', 'is', '300', 'for', 'convolution', 'and', '200', 'for', 'the', 'last', 'hidden', 'layer', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'PRP$', 'JJ', 'NN', ',', 'DT', 'NN', 'IN', 'NNS', 'VBZ', 'CD', 'IN', 'NN', 'CC', 'CD', 'IN', 'DT', 'JJ', 'JJ', 'NN', '.']",21
sentiment_analysis,23,194,"Word embeddings are 300 dimensional , pretrained ourselves using word2vec To train our model , we compute gradient by back - propagation and apply stochastic gradient descent with mini-batch 200 .","['Word', 'embeddings', 'are', '300', 'dimensional', ',', 'pretrained', 'ourselves', 'using', 'word2vec', 'To', 'train', 'our', 'model', ',', 'we', 'compute', 'gradient', 'by', 'back', '-', 'propagation', 'and', 'apply', 'stochastic', 'gradient', 'descent', 'with', 'mini-batch', '200', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'NNS', 'VBP', 'CD', 'JJ', ',', 'JJ', 'PRP', 'VBG', 'NN', 'TO', 'VB', 'PRP$', 'NN', ',', 'PRP', 'VBP', 'JJ', 'IN', 'JJ', ':', 'NN', 'CC', 'VB', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'CD', '.']",31
sentiment_analysis,23,195,We use ReLU as the activation function .,"['We', 'use', 'ReLU', 'as', 'the', 'activation', 'function', '.']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'IN', 'DT', 'NN', 'NN', '.']",8
sentiment_analysis,23,196,"For regularization , we add 2 penalty for weights with a coefficient of 10 ?5 .","['For', 'regularization', ',', 'we', 'add', '2', 'penalty', 'for', 'weights', 'with', 'a', 'coefficient', 'of', '10', '?5', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'CD', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'CD', 'NNS', '.']",16
sentiment_analysis,23,197,Dropout is further applied to both weights and embeddings .,"['Dropout', 'is', 'further', 'applied', 'to', 'both', 'weights', 'and', 'embeddings', '.']","['B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'VBZ', 'RB', 'VBN', 'TO', 'DT', 'NNS', 'CC', 'NNS', '.']",10
sentiment_analysis,23,198,"All hidden layers are dropped out by 50 % , and embeddings 40 % .","['All', 'hidden', 'layers', 'are', 'dropped', 'out', 'by', '50', '%', ',', 'and', 'embeddings', '40', '%', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'IN', 'CD', 'NN', ',', 'CC', 'VBZ', 'CD', 'NN', '.']",15
sentiment_analysis,23,208,"Nonetheless , our d-TBCNN model achieves","['Nonetheless', ',', 'our', 'd-TBCNN', 'model', 'achieves']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p']","['RB', ',', 'PRP$', 'JJ', 'NN', 'NNS']",6
sentiment_analysis,23,209,"87.9 % accuracy , ranking third in the list .","['87.9', '%', 'accuracy', ',', 'ranking', 'third', 'in', 'the', 'list', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['CD', 'NN', 'NN', ',', 'VBG', 'JJ', 'IN', 'DT', 'NN', '.']",10
sentiment_analysis,23,210,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .","['In', 'a', 'more', 'controlled', 'comparison', '-', 'with', 'shallow', 'architectures', 'and', 'the', 'basic', 'interaction', '(', 'linearly', 'transformed', 'and', 'non-linearly', 'squashed', ')', '-', 'TBCNNs', ',', 'of', 'both', 'variants', ',', 'consistently', 'outperform', 'RNNs', 'to', 'a', 'large', 'extent', '(', '50.4', '-', '51.4', '%', 'versus', '43.2', '%', ')', ';', 'they', 'also', 'consistently', 'outperform', '""', 'flat', '""', 'CNNs', 'by', 'more', 'than', '10', '%', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'RBR', 'JJ', 'NN', ':', 'IN', 'JJ', 'NNS', 'CC', 'DT', 'JJ', 'NN', '(', 'RB', 'VBN', 'CC', 'RB', 'VBN', ')', ':', 'NN', ',', 'IN', 'DT', 'NNS', ',', 'RB', 'VBZ', 'NNP', 'TO', 'DT', 'JJ', 'NN', '(', 'CD', ':', 'CD', 'NN', 'IN', 'CD', 'NN', ')', ':', 'PRP', 'RB', 'RB', 'JJ', 'NNP', 'JJ', 'NNP', 'NNP', 'IN', 'JJR', 'IN', 'CD', 'NN', '.']",58
sentiment_analysis,23,212,We also observe d- TBCNN achieves higher performance than c - TBCNN .,"['We', 'also', 'observe', 'd-', 'TBCNN', 'achieves', 'higher', 'performance', 'than', 'c', '-', 'TBCNN', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'JJ', 'NNP', 'VBZ', 'JJR', 'NN', 'IN', 'SYM', ':', 'NN', '.']",13
sentiment_analysis,24,2,An Interactive Multi - Task Learning Network for End - to - End Aspect - Based Sentiment Analysis,"['An', 'Interactive', 'Multi', '-', 'Task', 'Learning', 'Network', 'for', 'End', '-', 'to', '-', 'End', 'Aspect', '-', 'Based', 'Sentiment', 'Analysis']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['DT', 'NNP', 'NNP', ':', 'NN', 'NNP', 'NNP', 'IN', 'NNP', ':', 'TO', ':', 'NN', 'NNP', ':', 'VBD', 'JJ', 'NN']",18
sentiment_analysis,24,11,Aspect - based sentiment analysis ( ABSA ) aims to determine people 's attitude towards specific aspects in a review .,"['Aspect', '-', 'based', 'sentiment', 'analysis', '(', 'ABSA', ')', 'aims', 'to', 'determine', 'people', ""'s"", 'attitude', 'towards', 'specific', 'aspects', 'in', 'a', 'review', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'VBN', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'TO', 'VB', 'NNS', 'POS', 'NN', 'NNS', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']",21
sentiment_analysis,24,12,"This is done by extracting explicit aspect mentions , referred to as aspect term extraction ( AE ) , and detecting the sentiment orientation towards each extracted aspect term , referred to as aspect - level sentiment classification ( AS ) .","['This', 'is', 'done', 'by', 'extracting', 'explicit', 'aspect', 'mentions', ',', 'referred', 'to', 'as', 'aspect', 'term', 'extraction', '(', 'AE', ')', ',', 'and', 'detecting', 'the', 'sentiment', 'orientation', 'towards', 'each', 'extracted', 'aspect', 'term', ',', 'referred', 'to', 'as', 'aspect', '-', 'level', 'sentiment', 'classification', '(', 'AS', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'VBN', 'IN', 'VBG', 'JJ', 'JJ', 'NNS', ',', 'VBD', 'TO', 'IN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', ',', 'CC', 'VBG', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NN', ',', 'VBD', 'TO', 'IN', 'JJ', ':', 'NN', 'NN', 'NN', '(', 'IN', ')', '.']",42
sentiment_analysis,24,20,"In this work , we propose an interactive multitask learning network ( IMN ) , which solves both tasks simultaneously , enabling the interactions between both tasks to be better exploited .","['In', 'this', 'work', ',', 'we', 'propose', 'an', 'interactive', 'multitask', 'learning', 'network', '(', 'IMN', ')', ',', 'which', 'solves', 'both', 'tasks', 'simultaneously', ',', 'enabling', 'the', 'interactions', 'between', 'both', 'tasks', 'to', 'be', 'better', 'exploited', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBG', 'NN', '(', 'NNP', ')', ',', 'WDT', 'VBZ', 'DT', 'NNS', 'RB', ',', 'VBG', 'DT', 'NNS', 'IN', 'DT', 'NNS', 'TO', 'VB', 'RB', 'VBN', '.']",32
sentiment_analysis,24,21,"Furthermore , IMN allows AE and AS to be trained together with related document - level tasks , exploiting the knowledge from larger document - level corpora .","['Furthermore', ',', 'IMN', 'allows', 'AE', 'and', 'AS', 'to', 'be', 'trained', 'together', 'with', 'related', 'document', '-', 'level', 'tasks', ',', 'exploiting', 'the', 'knowledge', 'from', 'larger', 'document', '-', 'level', 'corpora', '.']","['O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'NNP', 'VBZ', 'NNP', 'CC', 'NNP', 'TO', 'VB', 'VBN', 'RB', 'IN', 'JJ', 'NN', ':', 'NN', 'NNS', ',', 'VBG', 'DT', 'NN', 'IN', 'JJR', 'NN', ':', 'NN', 'NN', '.']",28
sentiment_analysis,24,22,IMN introduces a novel message passing mechanism that allows informative interactions between tasks .,"['IMN', 'introduces', 'a', 'novel', 'message', 'passing', 'mechanism', 'that', 'allows', 'informative', 'interactions', 'between', 'tasks', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', 'VBG', 'NN', 'WDT', 'VBZ', 'JJ', 'NNS', 'IN', 'NNS', '.']",14
sentiment_analysis,24,23,"Specifically , it sends useful information from different tasks back to a shared latent representation .","['Specifically', ',', 'it', 'sends', 'useful', 'information', 'from', 'different', 'tasks', 'back', 'to', 'a', 'shared', 'latent', 'representation', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBZ', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'RB', 'TO', 'DT', 'VBN', 'NN', 'NN', '.']",16
sentiment_analysis,24,24,The information is then combined with the shared latent representation and made available to all tasks for further processing .,"['The', 'information', 'is', 'then', 'combined', 'with', 'the', 'shared', 'latent', 'representation', 'and', 'made', 'available', 'to', 'all', 'tasks', 'for', 'further', 'processing', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'DT', 'VBN', 'NN', 'NN', 'CC', 'VBD', 'JJ', 'TO', 'DT', 'NNS', 'IN', 'JJ', 'NN', '.']",20
sentiment_analysis,24,26,"In contrast to most multi-task learning schemes which share information through learning a common feature representation , IMN not only allows shared features , but also explicitly models the interactions between tasks through the message passing mechanism , allowing different tasks to better influence each other .","['In', 'contrast', 'to', 'most', 'multi-task', 'learning', 'schemes', 'which', 'share', 'information', 'through', 'learning', 'a', 'common', 'feature', 'representation', ',', 'IMN', 'not', 'only', 'allows', 'shared', 'features', ',', 'but', 'also', 'explicitly', 'models', 'the', 'interactions', 'between', 'tasks', 'through', 'the', 'message', 'passing', 'mechanism', ',', 'allowing', 'different', 'tasks', 'to', 'better', 'influence', 'each', 'other', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'RBS', 'JJ', 'NN', 'NNS', 'WDT', 'NN', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'NN', ',', 'NNP', 'RB', 'RB', 'VBZ', 'VBN', 'NNS', ',', 'CC', 'RB', 'RB', 'NNS', 'DT', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'NN', ',', 'VBG', 'JJ', 'NNS', 'TO', 'JJR', 'NN', 'DT', 'JJ', '.']",47
sentiment_analysis,24,27,"In addition , IMN allows fined - grained tokenlevel classification tasks to be trained together with document - level classification tasks .","['In', 'addition', ',', 'IMN', 'allows', 'fined', '-', 'grained', 'tokenlevel', 'classification', 'tasks', 'to', 'be', 'trained', 'together', 'with', 'document', '-', 'level', 'classification', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'NNP', 'VBZ', 'VBN', ':', 'VBN', 'JJ', 'NN', 'NNS', 'TO', 'VB', 'VBN', 'RB', 'IN', 'JJ', ':', 'NN', 'NN', 'NNS', '.']",22
sentiment_analysis,24,28,"We incorporated two document - level classification tasks - sentiment classification ( DS ) and domain classification ( DD ) - to be jointly trained with AE and AS , allowing the aspect - level tasks to benefit from document - level information .","['We', 'incorporated', 'two', 'document', '-', 'level', 'classification', 'tasks', '-', 'sentiment', 'classification', '(', 'DS', ')', 'and', 'domain', 'classification', '(', 'DD', ')', '-', 'to', 'be', 'jointly', 'trained', 'with', 'AE', 'and', 'AS', ',', 'allowing', 'the', 'aspect', '-', 'level', 'tasks', 'to', 'benefit', 'from', 'document', '-', 'level', 'information', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'CD', 'NN', ':', 'NN', 'NN', 'NNS', ':', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'JJ', 'NN', '(', 'NNP', ')', ':', 'TO', 'VB', 'RB', 'VBN', 'IN', 'NNP', 'CC', 'NNP', ',', 'VBG', 'DT', 'JJ', ':', 'NN', 'NNS', 'TO', 'VB', 'IN', 'JJ', ':', 'NN', 'NN', '.']",44
sentiment_analysis,24,157,We adopt the multi - layer - CNN structure from as the CNN - based encoders in our proposed network .,"['We', 'adopt', 'the', 'multi', '-', 'layer', '-', 'CNN', 'structure', 'from', 'as', 'the', 'CNN', '-', 'based', 'encoders', 'in', 'our', 'proposed', 'network', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', ':', 'NN', ':', 'NN', 'NN', 'IN', 'IN', 'DT', 'NNP', ':', 'VBN', 'NNS', 'IN', 'PRP$', 'VBN', 'NN', '.']",21
sentiment_analysis,24,160,"For word embedding initialization , we concatenate a general - purpose embedding matrix and a domain - specific embedding matrix 7 following .","['For', 'word', 'embedding', 'initialization', ',', 'we', 'concatenate', 'a', 'general', '-', 'purpose', 'embedding', 'matrix', 'and', 'a', 'domain', '-', 'specific', 'embedding', 'matrix', '7', 'following', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['IN', 'NN', 'VBG', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', ':', 'NN', 'VBG', 'NN', 'CC', 'DT', 'NN', ':', 'JJ', 'VBG', 'JJ', 'CD', 'VBG', '.']",23
sentiment_analysis,24,161,"We adopt their released domainspecific embeddings for restaurant and laptop domains with 100 dimensions , which are trained on a large domain - specific corpus using fast Text .","['We', 'adopt', 'their', 'released', 'domainspecific', 'embeddings', 'for', 'restaurant', 'and', 'laptop', 'domains', 'with', '100', 'dimensions', ',', 'which', 'are', 'trained', 'on', 'a', 'large', 'domain', '-', 'specific', 'corpus', 'using', 'fast', 'Text', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'VBN', 'NN', 'NNS', 'IN', 'NN', 'CC', 'JJ', 'NNS', 'IN', 'CD', 'NNS', ',', 'WDT', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NN', ':', 'JJ', 'NN', 'VBG', 'JJ', 'NNP', '.']",29
sentiment_analysis,24,162,The general - purpose embeddings are pre-trained Glove vectors with 300 dimensions .,"['The', 'general', '-', 'purpose', 'embeddings', 'are', 'pre-trained', 'Glove', 'vectors', 'with', '300', 'dimensions', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', ':', 'JJ', 'NNS', 'VBP', 'JJ', 'NNP', 'NNS', 'IN', 'CD', 'NNS', '.']",13
sentiment_analysis,24,171,We tune the maximum number of iterations T in the message passing mechanism by training IMN ?d via cross validation on D1 .,"['We', 'tune', 'the', 'maximum', 'number', 'of', 'iterations', 'T', 'in', 'the', 'message', 'passing', 'mechanism', 'by', 'training', 'IMN', '?d', 'via', 'cross', 'validation', 'on', 'D1', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'NNP', 'IN', 'DT', 'NN', 'VBG', 'NN', 'IN', 'VBG', 'NNP', 'NNP', 'IN', 'NN', 'NN', 'IN', 'NNP', '.']",23
sentiment_analysis,24,172,It is set to 2 .,"['It', 'is', 'set', 'to', '2', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBZ', 'VBN', 'TO', 'CD', '.']",6
sentiment_analysis,24,175,"We use Adam optimizer with learning rate set to 10 ? 4 , and we set batch size to 32 .","['We', 'use', 'Adam', 'optimizer', 'with', 'learning', 'rate', 'set', 'to', '10', '?', '4', ',', 'and', 'we', 'set', 'batch', 'size', 'to', '32', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'VBG', 'NN', 'VBN', 'TO', 'CD', '.', 'CD', ',', 'CC', 'PRP', 'VBP', 'JJ', 'NN', 'TO', 'CD', '.']",21
sentiment_analysis,24,176,Learning rate and batch size are set to conventional values without specific tuning for our task .,"['Learning', 'rate', 'and', 'batch', 'size', 'are', 'set', 'to', 'conventional', 'values', 'without', 'specific', 'tuning', 'for', 'our', 'task', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['VBG', 'NN', 'CC', 'NN', 'NN', 'VBP', 'VBN', 'TO', 'JJ', 'NNS', 'IN', 'JJ', 'VBG', 'IN', 'PRP$', 'NN', '.']",17
sentiment_analysis,24,177,"At training phase , we randomly sample 20 % of the training data from the aspect - level dataset as the development set and only use the remaining 80 % for training .","['At', 'training', 'phase', ',', 'we', 'randomly', 'sample', '20', '%', 'of', 'the', 'training', 'data', 'from', 'the', 'aspect', '-', 'level', 'dataset', 'as', 'the', 'development', 'set', 'and', 'only', 'use', 'the', 'remaining', '80', '%', 'for', 'training', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'VBG', 'NN', ',', 'PRP', 'VBP', 'JJ', 'CD', 'NN', 'IN', 'DT', 'NN', 'NNS', 'IN', 'DT', 'JJ', ':', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'RB', 'VB', 'DT', 'VBG', 'CD', 'NN', 'IN', 'NN', '.']",33
sentiment_analysis,24,220,"From , we observe that IMN ?d is able to significantly outperform other baselines on F1 - I .","['From', ',', 'we', 'observe', 'that', 'IMN', '?d', 'is', 'able', 'to', 'significantly', 'outperform', 'other', 'baselines', 'on', 'F1', '-', 'I', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O']","['IN', ',', 'PRP', 'VBP', 'DT', 'NNP', 'NNP', 'VBZ', 'JJ', 'TO', 'RB', 'VB', 'JJ', 'NNS', 'IN', 'NNP', ':', 'PRP', '.']",19
sentiment_analysis,24,221,"IMN further boosts the performance and outperforms the best F1 - I results from the baselines by 2.29 % , 1.77 % , and 2.61 % on D1 , D2 , and D3 .","['IMN', 'further', 'boosts', 'the', 'performance', 'and', 'outperforms', 'the', 'best', 'F1', '-', 'I', 'results', 'from', 'the', 'baselines', 'by', '2.29', '%', ',', '1.77', '%', ',', 'and', '2.61', '%', 'on', 'D1', ',', 'D2', ',', 'and', 'D3', '.']","['B-n', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'JJ', 'VBZ', 'DT', 'NN', 'CC', 'VBZ', 'DT', 'JJS', 'NNP', ':', 'PRP', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'CD', 'NN', ',', 'CD', 'NN', ',', 'CC', 'CD', 'NN', 'IN', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', '.']",34
sentiment_analysis,24,222,"Specifically , for AE ( F1 - a and F1 - o ) , IMN ?d performs the best in most cases .","['Specifically', ',', 'for', 'AE', '(', 'F1', '-', 'a', 'and', 'F1', '-', 'o', ')', ',', 'IMN', '?d', 'performs', 'the', 'best', 'in', 'most', 'cases', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'IN', 'NNP', '(', 'NNP', ':', 'DT', 'CC', 'NNP', ':', 'NN', ')', ',', 'NNP', 'NNP', 'VBZ', 'DT', 'JJS', 'IN', 'JJS', 'NNS', '.']",23
sentiment_analysis,24,223,"For AS ( acc - s and F1 - s ) , IMN outperforms other methods by large margins .","['For', 'AS', '(', 'acc', '-', 's', 'and', 'F1', '-', 's', ')', ',', 'IMN', 'outperforms', 'other', 'methods', 'by', 'large', 'margins', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'NNP', '(', 'IN', ':', 'NN', 'CC', 'NNP', ':', 'NN', ')', ',', 'NNP', 'VBZ', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', '.']",20
sentiment_analysis,24,229,IMN wo DE performs only marginally below IMN .,"['IMN', 'wo', 'DE', 'performs', 'only', 'marginally', 'below', 'IMN', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', 'MD', 'VB', 'NNS', 'RB', 'RB', 'IN', 'NNP', '.']",9
sentiment_analysis,24,231,"IMN ?d is more affected without domain - specific embeddings , while it still outperforms all other baselines except DECNN - d Trans .","['IMN', '?d', 'is', 'more', 'affected', 'without', 'domain', '-', 'specific', 'embeddings', ',', 'while', 'it', 'still', 'outperforms', 'all', 'other', 'baselines', 'except', 'DECNN', '-', 'd', 'Trans', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'VBZ', 'RBR', 'JJ', 'IN', 'NN', ':', 'JJ', 'NNS', ',', 'IN', 'PRP', 'RB', 'VBZ', 'DT', 'JJ', 'NNS', 'IN', 'NNP', ':', 'NN', 'NNPS', '.']",24
sentiment_analysis,24,232,DECNN - dTrans is a very strong baseline as it exploits additional knowledge from larger corpora for both tasks .,"['DECNN', '-', 'dTrans', 'is', 'a', 'very', 'strong', 'baseline', 'as', 'it', 'exploits', 'additional', 'knowledge', 'from', 'larger', 'corpora', 'for', 'both', 'tasks', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ':', 'NNS', 'VBZ', 'DT', 'RB', 'JJ', 'NN', 'IN', 'PRP', 'VBZ', 'JJ', 'NN', 'IN', 'JJR', 'NN', 'IN', 'DT', 'NNS', '.']",20
sentiment_analysis,24,233,"IMN ?d wo DE is competitive with DECNN - dTrans even without utilizing additional knowledge , which suggests the effectiveness of the proposed network structure .","['IMN', '?d', 'wo', 'DE', 'is', 'competitive', 'with', 'DECNN', '-', 'dTrans', 'even', 'without', 'utilizing', 'additional', 'knowledge', ',', 'which', 'suggests', 'the', 'effectiveness', 'of', 'the', 'proposed', 'network', 'structure', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'MD', 'NNP', 'VBZ', 'JJ', 'IN', 'NNP', ':', 'NNS', 'RB', 'IN', 'VBG', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'VBN', 'NN', 'NN', '.']",26
sentiment_analysis,24,242,"We observe that + Message passing - a and + Message passing - d contribute to the performance gains the most , which demonstrates the effectiveness of the proposed message passing mechanism .","['We', 'observe', 'that', '+', 'Message', 'passing', '-', 'a', 'and', '+', 'Message', 'passing', '-', 'd', 'contribute', 'to', 'the', 'performance', 'gains', 'the', 'most', ',', 'which', 'demonstrates', 'the', 'effectiveness', 'of', 'the', 'proposed', 'message', 'passing', 'mechanism', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NNP', 'VBG', ':', 'DT', 'CC', 'JJ', 'NNP', 'VBG', ':', 'NN', 'NN', 'TO', 'DT', 'NN', 'NNS', 'DT', 'RBS', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'VBN', 'NN', 'VBG', 'NN', '.']",33
sentiment_analysis,24,243,We also observe that simply adding documentlevel tasks ( + DS / DD ) with parameter sharing only marginally improves the performance of IMN ?d .,"['We', 'also', 'observe', 'that', 'simply', 'adding', 'documentlevel', 'tasks', '(', '+', 'DS', '/', 'DD', ')', 'with', 'parameter', 'sharing', 'only', 'marginally', 'improves', 'the', 'performance', 'of', 'IMN', '?d', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'RB', 'VBG', 'NN', 'NNS', '(', 'JJ', 'NNP', 'NNP', 'NNP', ')', 'IN', 'NN', 'VBG', 'RB', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'NNP', 'NNP', '.']",26
sentiment_analysis,24,245,"However , + Message passing -d is still helpful with considerable performance gains , showing that aspect - level tasks can benefit from knowing predictions of the relevant document - level tasks .","['However', ',', '+', 'Message', 'passing', '-d', 'is', 'still', 'helpful', 'with', 'considerable', 'performance', 'gains', ',', 'showing', 'that', 'aspect', '-', 'level', 'tasks', 'can', 'benefit', 'from', 'knowing', 'predictions', 'of', 'the', 'relevant', 'document', '-', 'level', 'tasks', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'JJ', 'NNP', 'NN', 'NN', 'VBZ', 'RB', 'JJ', 'IN', 'JJ', 'NN', 'NNS', ',', 'VBG', 'IN', 'JJ', ':', 'NN', 'NNS', 'MD', 'VB', 'IN', 'VBG', 'NNS', 'IN', 'DT', 'JJ', 'NN', ':', 'NN', 'NNS', '.']",33
sentiment_analysis,14,2,Emo2 Vec : Learning Generalized Emotion Representation by Multi- task Training,"['Emo2', 'Vec', ':', 'Learning', 'Generalized', 'Emotion', 'Representation', 'by', 'Multi-', 'task', 'Training']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NNP', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NN', 'NN']",11
sentiment_analysis,14,14,This work demonstrates the effectiveness of incorporating sentiment labels in a wordlevel information for sentiment - related tasks compared to other word embeddings .,"['This', 'work', 'demonstrates', 'the', 'effectiveness', 'of', 'incorporating', 'sentiment', 'labels', 'in', 'a', 'wordlevel', 'information', 'for', 'sentiment', '-', 'related', 'tasks', 'compared', 'to', 'other', 'word', 'embeddings', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', ':', 'JJ', 'NNS', 'VBN', 'TO', 'JJ', 'NN', 'NNS', '.']",24
sentiment_analysis,14,24,"1 ) We propose Emo2Vec 1 which are word - level representations that encode emotional semantics into fixed - sized , real - valued vectors .","['1', ')', 'We', 'propose', 'Emo2Vec', '1', 'which', 'are', 'word', '-', 'level', 'representations', 'that', 'encode', 'emotional', 'semantics', 'into', 'fixed', '-', 'sized', ',', 'real', '-', 'valued', 'vectors', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', ')', 'PRP', 'VBP', 'NNP', 'CD', 'WDT', 'VBP', 'NN', ':', 'NN', 'NNS', 'WDT', 'VBP', 'JJ', 'NNS', 'IN', 'VBN', ':', 'VBN', ',', 'JJ', ':', 'VBN', 'NNS', '.']",26
sentiment_analysis,14,25,2 ) We propose to learn Emo2Vec with a multi-task learning framework by including six different emotion - related tasks .,"['2', ')', 'We', 'propose', 'to', 'learn', 'Emo2Vec', 'with', 'a', 'multi-task', 'learning', 'framework', 'by', 'including', 'six', 'different', 'emotion', '-', 'related', 'tasks', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', ')', 'PRP', 'VBP', 'TO', 'VB', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'VBG', 'CD', 'JJ', 'NN', ':', 'JJ', 'NNS', '.']",21
sentiment_analysis,14,87,Pre-training Emo2Vec,"['Pre-training', 'Emo2Vec']","['B-n', 'I-n']","['JJ', 'NNP']",2
sentiment_analysis,14,88,Emo2 Vec embedding matrix and the CNN model are pre-trained using hashtag corpus alone .,"['Emo2', 'Vec', 'embedding', 'matrix', 'and', 'the', 'CNN', 'model', 'are', 'pre-trained', 'using', 'hashtag', 'corpus', 'alone', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O']","['NNP', 'NNP', 'VBG', 'NN', 'CC', 'DT', 'NNP', 'NN', 'VBP', 'JJ', 'VBG', 'JJ', 'NN', 'RB', '.']",15
sentiment_analysis,14,89,Parameters of T and CNN are randomly initialized and Adam is used for optimization .,"['Parameters', 'of', 'T', 'and', 'CNN', 'are', 'randomly', 'initialized', 'and', 'Adam', 'is', 'used', 'for', 'optimization', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['NNS', 'IN', 'NNP', 'CC', 'NNP', 'VBP', 'RB', 'VBN', 'CC', 'NNP', 'VBZ', 'VBN', 'IN', 'NN', '.']",15
sentiment_analysis,14,91,"For the best model , we use the batch size of 16 , embedding size of 100 , 1024 filters and filter sizes are 1 , 3 ,5 and 7 respectively .","['For', 'the', 'best', 'model', ',', 'we', 'use', 'the', 'batch', 'size', 'of', '16', ',', 'embedding', 'size', 'of', '100', ',', '1024', 'filters', 'and', 'filter', 'sizes', 'are', '1', ',', '3', ',5', 'and', '7', 'respectively', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'DT', 'JJS', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CD', ',', 'VBG', 'NN', 'IN', 'CD', ',', 'CD', 'NNS', 'CC', 'NN', 'NNS', 'VBP', 'CD', ',', 'CD', 'NN', 'CC', 'CD', 'RB', '.']",32
sentiment_analysis,14,94,Multi - task training,"['Multi', '-', 'task', 'training']","['B-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NN', 'NN']",4
sentiment_analysis,14,95,"We tune our parameters of learning rate , L2 regularization , whether to pre-train our model and batch size with the average accuracy of the development set of all datasets .","['We', 'tune', 'our', 'parameters', 'of', 'learning', 'rate', ',', 'L2', 'regularization', ',', 'whether', 'to', 'pre-train', 'our', 'model', 'and', 'batch', 'size', 'with', 'the', 'average', 'accuracy', 'of', 'the', 'development', 'set', 'of', 'all', 'datasets', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'PRP$', 'NNS', 'IN', 'VBG', 'NN', ',', 'NNP', 'NN', ',', 'IN', 'TO', 'VB', 'PRP$', 'NN', 'CC', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NNS', '.']",31
sentiment_analysis,14,96,We early stop our model when the averaged dev accuracy stop increasing .,"['We', 'early', 'stop', 'our', 'model', 'when', 'the', 'averaged', 'dev', 'accuracy', 'stop', 'increasing', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'RB', 'VB', 'PRP$', 'NN', 'WRB', 'DT', 'VBN', 'NN', 'NN', 'JJ', 'NN', '.']",13
sentiment_analysis,14,97,"Our best model uses learning rate of 0.001 , L2 regularization of 1.0 , batch size of 32 .","['Our', 'best', 'model', 'uses', 'learning', 'rate', 'of', '0.001', ',', 'L2', 'regularization', 'of', '1.0', ',', 'batch', 'size', 'of', '32', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP$', 'JJS', 'NN', 'VBZ', 'VBG', 'NN', 'IN', 'CD', ',', 'NNP', 'NN', 'IN', 'CD', ',', 'NN', 'NN', 'IN', 'CD', '.']",19
sentiment_analysis,14,98,We save the best model and take the embedding layer as Emo2Vec vectors .,"['We', 'save', 'the', 'best', 'model', 'and', 'take', 'the', 'embedding', 'layer', 'as', 'Emo2Vec', 'vectors', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJS', 'NN', 'CC', 'VB', 'DT', 'VBG', 'NN', 'IN', 'NNP', 'NNS', '.']",14
sentiment_analysis,14,115,"Compared with CNN embedding : Emo2 Vec works better than CNN embedding on 14 / 18 datasets , giving 2.6 % absolute accuracy improvement for the sentiment task and 1.6 % absolute f1score improvement on the other tasks .","['Compared', 'with', 'CNN', 'embedding', ':', 'Emo2', 'Vec', 'works', 'better', 'than', 'CNN', 'embedding', 'on', '14', '/', '18', 'datasets', ',', 'giving', '2.6', '%', 'absolute', 'accuracy', 'improvement', 'for', 'the', 'sentiment', 'task', 'and', '1.6', '%', 'absolute', 'f1score', 'improvement', 'on', 'the', 'other', 'tasks', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['VBN', 'IN', 'NNP', 'VBG', ':', 'NNP', 'NNP', 'VBZ', 'RBR', 'IN', 'NNP', 'VBG', 'IN', 'CD', 'JJ', 'CD', 'NNS', ',', 'VBG', 'CD', 'NN', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'CD', 'NN', 'NN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNS', '.']",39
sentiment_analysis,14,116,It shows multi-task training helps to create better generalized word emotion representations than just using a single task .,"['It', 'shows', 'multi-task', 'training', 'helps', 'to', 'create', 'better', 'generalized', 'word', 'emotion', 'representations', 'than', 'just', 'using', 'a', 'single', 'task', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBZ', 'JJ', 'NN', 'VBZ', 'TO', 'VB', 'JJR', 'VBN', 'NN', 'NN', 'NNS', 'IN', 'RB', 'VBG', 'DT', 'JJ', 'NN', '.']",19
sentiment_analysis,14,117,"Compared with SSWE : Emo2 Vec works much better on all datasets except SS - T datasets , which gives 3.3 % accuracy improvement and 4.7 % f 1 score improvement respectively on sentiment and other tasks .","['Compared', 'with', 'SSWE', ':', 'Emo2', 'Vec', 'works', 'much', 'better', 'on', 'all', 'datasets', 'except', 'SS', '-', 'T', 'datasets', ',', 'which', 'gives', '3.3', '%', 'accuracy', 'improvement', 'and', '4.7', '%', 'f', '1', 'score', 'improvement', 'respectively', 'on', 'sentiment', 'and', 'other', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'NNP', ':', 'NNP', 'NNP', 'VBZ', 'RB', 'JJR', 'IN', 'DT', 'NNS', 'IN', 'NNP', ':', 'NNP', 'NNS', ',', 'WDT', 'VBZ', 'CD', 'NN', 'NN', 'NN', 'CC', 'CD', 'NN', 'JJ', 'CD', 'NN', 'NN', 'RB', 'IN', 'NN', 'CC', 'JJ', 'NNS', '.']",38
sentiment_analysis,14,121,"On average , it gives 1.3 % improvement in accuracy for the sentiment task and 1.1 % improvement of f 1 - score on the other tasks .","['On', 'average', ',', 'it', 'gives', '1.3', '%', 'improvement', 'in', 'accuracy', 'for', 'the', 'sentiment', 'task', 'and', '1.1', '%', 'improvement', 'of', 'f', '1', '-', 'score', 'on', 'the', 'other', 'tasks', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBZ', 'CD', 'NN', 'NN', 'IN', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'CD', 'NN', 'NN', 'IN', '$', 'CD', ':', 'NN', 'IN', 'DT', 'JJ', 'NNS', '.']",28
sentiment_analysis,14,124,"Since Emo2 Vec is not trained by predicting contextual words , it is weak on capturing synthetic and semantic meaning .","['Since', 'Emo2', 'Vec', 'is', 'not', 'trained', 'by', 'predicting', 'contextual', 'words', ',', 'it', 'is', 'weak', 'on', 'capturing', 'synthetic', 'and', 'semantic', 'meaning', '.']","['O', 'O', 'O', 'B-p', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', 'NNP', 'VBZ', 'RB', 'VBN', 'IN', 'VBG', 'JJ', 'NNS', ',', 'PRP', 'VBZ', 'JJ', 'IN', 'VBG', 'JJ', 'CC', 'JJ', 'NN', '.']",21
sentiment_analysis,14,128,"Here , we want to highlight that solely using a simple classifier with good word representation can achieve promising results .","['Here', ',', 'we', 'want', 'to', 'highlight', 'that', 'solely', 'using', 'a', 'simple', 'classifier', 'with', 'good', 'word', 'representation', 'can', 'achieve', 'promising', 'results', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'TO', 'VB', 'DT', 'RB', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', 'MD', 'VB', 'JJ', 'NNS', '.']",21
sentiment_analysis,14,131,"Compared with GloVe+ DeepMoji , GloVe + Emo2 Vec achieves same or better results on 11 / 14 datasets , which on average gives 1.0 % improvement .","['Compared', 'with', 'GloVe+', 'DeepMoji', ',', 'GloVe', '+', 'Emo2', 'Vec', 'achieves', 'same', 'or', 'better', 'results', 'on', '11', '/', '14', 'datasets', ',', 'which', 'on', 'average', 'gives', '1.0', '%', 'improvement', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'NNP', 'NNP', ',', 'NNP', 'NNP', 'NNP', 'NNP', 'VBZ', 'JJ', 'CC', 'JJR', 'NNS', 'IN', 'CD', 'JJ', 'CD', 'NNS', ',', 'WDT', 'IN', 'NN', 'VBZ', 'CD', 'NN', 'NN', '.']",28
sentiment_analysis,14,132,"GloVe + Emo2 Vec achieves better performances on SOTA results on three datasets ( SE0714 , stress and tube tablet ) and comparable result to SOTA on dataset Previous SOTA results","['GloVe', '+', 'Emo2', 'Vec', 'achieves', 'better', 'performances', 'on', 'SOTA', 'results', 'on', 'three', 'datasets', '(', 'SE0714', ',', 'stress', 'and', 'tube', 'tablet', ')', 'and', 'comparable', 'result', 'to', 'SOTA', 'on', 'dataset', 'Previous', 'SOTA', 'results']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', 'NNP', 'VBZ', 'JJR', 'NNS', 'IN', 'NNP', 'NNS', 'IN', 'CD', 'NNS', '(', 'NNP', ',', 'NN', 'CC', 'VB', 'NN', ')', 'CC', 'JJ', 'NN', 'TO', 'NNP', 'IN', 'NN', 'NNP', 'NNP', 'NNS']",31
sentiment_analysis,14,136,"Thus , to detect the corresponding emotion , more attention needs to be paid to words .","['Thus', ',', 'to', 'detect', 'the', 'corresponding', 'emotion', ',', 'more', 'attention', 'needs', 'to', 'be', 'paid', 'to', 'words', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O']","['RB', ',', 'TO', 'VB', 'DT', 'JJ', 'NN', ',', 'JJR', 'NN', 'NNS', 'TO', 'VB', 'VBN', 'TO', 'NNS', '.']",17
sentiment_analysis,7,2,MULTI - MODAL EMOTION RECOGNITION ON IEMOCAP WITH NEURAL NETWORKS,"['MULTI', '-', 'MODAL', 'EMOTION', 'RECOGNITION', 'ON', 'IEMOCAP', 'WITH', 'NEURAL', 'NETWORKS']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",10
sentiment_analysis,7,4,Emotion recognition has become an important field of research in human computer interactions and there is a growing need for automatic emotion recognition systems .,"['Emotion', 'recognition', 'has', 'become', 'an', 'important', 'field', 'of', 'research', 'in', 'human', 'computer', 'interactions', 'and', 'there', 'is', 'a', 'growing', 'need', 'for', 'automatic', 'emotion', 'recognition', 'systems', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'VBZ', 'VBN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'JJ', 'NN', 'NNS', 'CC', 'EX', 'VBZ', 'DT', 'VBG', 'NN', 'IN', 'JJ', 'NN', 'NN', 'NNS', '.']",25
sentiment_analysis,7,15,We explore various deep learning based architectures to first get the best individual detection accuracy from each of the different modes .,"['We', 'explore', 'various', 'deep', 'learning', 'based', 'architectures', 'to', 'first', 'get', 'the', 'best', 'individual', 'detection', 'accuracy', 'from', 'each', 'of', 'the', 'different', 'modes', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', 'JJ', 'NN', 'VBN', 'NNS', 'TO', 'VB', 'VB', 'DT', 'JJS', 'JJ', 'NN', 'NN', 'IN', 'DT', 'IN', 'DT', 'JJ', 'NNS', '.']",22
sentiment_analysis,7,16,We then combine them in an ensemble based architecture to allow for training across the different modalities using the variations of the better individual models .,"['We', 'then', 'combine', 'them', 'in', 'an', 'ensemble', 'based', 'architecture', 'to', 'allow', 'for', 'training', 'across', 'the', 'different', 'modalities', 'using', 'the', 'variations', 'of', 'the', 'better', 'individual', 'models', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VB', 'PRP', 'IN', 'DT', 'NN', 'VBN', 'NN', 'TO', 'VB', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'VBG', 'DT', 'NNS', 'IN', 'DT', 'JJR', 'JJ', 'NNS', '.']",26
sentiment_analysis,7,17,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .","['Our', 'ensemble', 'consists', 'of', 'Long', 'Short', 'Term', 'Memory', 'networks', ',', 'Convolution', 'Neural', 'Networks', ',', 'fully', 'connected', 'Multi', '-', 'Layer', 'Perceptrons', 'and', 'we', 'complement', 'them', 'using', 'techniques', 'such', 'as', 'Dropout', ',', 'adaptive', 'optimizers', 'such', 'as', 'Adam', ',', 'pretrained', 'word', '-', 'embedding', 'models', 'and', 'Attention', 'based', 'RNN', 'decoders', '.']","['B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'JJ', 'NNS', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNS', ',', 'NNP', 'NNP', 'NNP', ',', 'RB', 'VBN', 'NNP', ':', 'NNP', 'NNPS', 'CC', 'PRP', 'VBP', 'PRP', 'VBG', 'NNS', 'JJ', 'IN', 'NNP', ',', 'JJ', 'NNS', 'JJ', 'IN', 'NNP', ',', 'VBD', 'NN', ':', 'NN', 'NNS', 'CC', 'NNP', 'VBN', 'NNP', 'NNS', '.']",47
sentiment_analysis,7,18,This allows us to individually target each modality and only perform feature fusion at the final stage .,"['This', 'allows', 'us', 'to', 'individually', 'target', 'each', 'modality', 'and', 'only', 'perform', 'feature', 'fusion', 'at', 'the', 'final', 'stage', '.']","['O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'VBZ', 'PRP', 'TO', 'RB', 'VB', 'DT', 'NN', 'CC', 'RB', 'VB', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",18
sentiment_analysis,7,64,"For the text transcript of each of the utterance we use pretrained Glove embeddings of dimension 300 , along with the maximum sequence length of 500 to obtain a ( 500,300 ) vector for each utterance .","['For', 'the', 'text', 'transcript', 'of', 'each', 'of', 'the', 'utterance', 'we', 'use', 'pretrained', 'Glove', 'embeddings', 'of', 'dimension', '300', ',', 'along', 'with', 'the', 'maximum', 'sequence', 'length', 'of', '500', 'to', 'obtain', 'a', '(', '500,300', ')', 'vector', 'for', 'each', 'utterance', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'IN', 'DT', 'NN', 'PRP', 'VBP', 'VBN', 'NNP', 'NNS', 'IN', 'NN', 'CD', ',', 'IN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'TO', 'VB', 'DT', '(', 'CD', ')', 'NN', 'IN', 'DT', 'NN', '.']",37
sentiment_analysis,7,65,"For the Mocap data , for each different mode such as face , hand , head rotation we sample all the feature values between the start and finish time values and split them into 200 partitioned arrays .","['For', 'the', 'Mocap', 'data', ',', 'for', 'each', 'different', 'mode', 'such', 'as', 'face', ',', 'hand', ',', 'head', 'rotation', 'we', 'sample', 'all', 'the', 'feature', 'values', 'between', 'the', 'start', 'and', 'finish', 'time', 'values', 'and', 'split', 'them', 'into', '200', 'partitioned', 'arrays', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNP', 'NNS', ',', 'IN', 'DT', 'JJ', 'NN', 'JJ', 'IN', 'NN', ',', 'NN', ',', 'NN', 'NN', 'PRP', 'VBP', 'PDT', 'DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'CC', 'JJ', 'NN', 'NNS', 'CC', 'VB', 'PRP', 'IN', 'CD', 'JJ', 'NNS', '.']",38
sentiment_analysis,7,66,"We then average each of the 200 arrays along the columns ( 165 for faces , 18 for hands , and 6 for rotation ) , and finally concatenate all of them to obtain ( 200,189 ) dimension vector for each utterance .","['We', 'then', 'average', 'each', 'of', 'the', '200', 'arrays', 'along', 'the', 'columns', '(', '165', 'for', 'faces', ',', '18', 'for', 'hands', ',', 'and', '6', 'for', 'rotation', ')', ',', 'and', 'finally', 'concatenate', 'all', 'of', 'them', 'to', 'obtain', '(', '200,189', ')', 'dimension', 'vector', 'for', 'each', 'utterance', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'IN', 'DT', 'CD', 'NNS', 'IN', 'DT', 'NN', '(', 'CD', 'IN', 'VBZ', ',', 'CD', 'IN', 'NNS', ',', 'CC', 'CD', 'IN', 'NN', ')', ',', 'CC', 'RB', 'VB', 'DT', 'IN', 'PRP', 'TO', 'VB', '(', 'CD', ')', 'NN', 'NN', 'IN', 'DT', 'NN', '.']",43
sentiment_analysis,7,97,"Our performance matches the prior state of the art , however the comparison is not fair .","['Our', 'performance', 'matches', 'the', 'prior', 'state', 'of', 'the', 'art', ',', 'however', 'the', 'comparison', 'is', 'not', 'fair', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', ',', 'RB', 'DT', 'NN', 'VBZ', 'RB', 'JJ', '.']",17
sentiment_analysis,28,2,Attentional Encoder Network for Targeted Sentiment Classification,"['Attentional', 'Encoder', 'Network', 'for', 'Targeted', 'Sentiment', 'Classification']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",7
sentiment_analysis,28,12,"Targeted sentiment classification is a fine - grained sentiment analysis task , which aims at determining the sentiment polarities ( e.g. , negative , neutral , or positive ) of a sentence over "" opinion targets "" that explicitly appear in the sentence .","['Targeted', 'sentiment', 'classification', 'is', 'a', 'fine', '-', 'grained', 'sentiment', 'analysis', 'task', ',', 'which', 'aims', 'at', 'determining', 'the', 'sentiment', 'polarities', '(', 'e.g.', ',', 'negative', ',', 'neutral', ',', 'or', 'positive', ')', 'of', 'a', 'sentence', 'over', '""', 'opinion', 'targets', '""', 'that', 'explicitly', 'appear', 'in', 'the', 'sentence', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', ':', 'JJ', 'NN', 'NN', 'NN', ',', 'WDT', 'VBZ', 'IN', 'VBG', 'DT', 'NN', 'NNS', '(', 'NN', ',', 'JJ', ',', 'JJ', ',', 'CC', 'JJ', ')', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NNS', 'VBP', 'IN', 'RB', 'VB', 'IN', 'DT', 'NN', '.']",44
sentiment_analysis,28,16,"However , these neural network models are still in infancy to deal with the fine - grained targeted sentiment classification task .","['However', ',', 'these', 'neural', 'network', 'models', 'are', 'still', 'in', 'infancy', 'to', 'deal', 'with', 'the', 'fine', '-', 'grained', 'targeted', 'sentiment', 'classification', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['RB', ',', 'DT', 'JJ', 'NN', 'NNS', 'VBP', 'RB', 'IN', 'NN', 'TO', 'VB', 'IN', 'DT', 'JJ', ':', 'VBN', 'JJ', 'NN', 'NN', 'NN', '.']",22
sentiment_analysis,28,26,This paper propose an attention based model to solve the problems above .,"['This', 'paper', 'propose', 'an', 'attention', 'based', 'model', 'to', 'solve', 'the', 'problems', 'above', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBD', 'DT', 'NN', 'VBN', 'NN', 'TO', 'VB', 'DT', 'NNS', 'IN', '.']",13
sentiment_analysis,28,27,"Specifically , our model eschews recurrence and employs attention as a competitive alternative to draw the introspective and interactive semantics between target and context words .","['Specifically', ',', 'our', 'model', 'eschews', 'recurrence', 'and', 'employs', 'attention', 'as', 'a', 'competitive', 'alternative', 'to', 'draw', 'the', 'introspective', 'and', 'interactive', 'semantics', 'between', 'target', 'and', 'context', 'words', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'NN', 'VBZ', 'NN', 'CC', 'VBZ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'JJ', 'CC', 'JJ', 'NNS', 'IN', 'NN', 'CC', 'JJ', 'NNS', '.']",26
sentiment_analysis,28,28,"To deal with the label unreliability issue , we employ a label smoothing regularization to encourage the model to be less confident with fuzzy labels .","['To', 'deal', 'with', 'the', 'label', 'unreliability', 'issue', ',', 'we', 'employ', 'a', 'label', 'smoothing', 'regularization', 'to', 'encourage', 'the', 'model', 'to', 'be', 'less', 'confident', 'with', 'fuzzy', 'labels', '.']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['TO', 'VB', 'IN', 'DT', 'NN', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBG', 'NN', 'TO', 'VB', 'DT', 'NN', 'TO', 'VB', 'RBR', 'JJ', 'IN', 'JJ', 'NNS', '.']",26
sentiment_analysis,28,29,We also apply pre-trained BERT to this task and show our model enhances the performance of basic BERT model .,"['We', 'also', 'apply', 'pre-trained', 'BERT', 'to', 'this', 'task', 'and', 'show', 'our', 'model', 'enhances', 'the', 'performance', 'of', 'basic', 'BERT', 'model', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'JJ', 'NNP', 'TO', 'DT', 'NN', 'CC', 'VB', 'PRP$', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NNP', 'NN', '.']",20
sentiment_analysis,28,126,shows the number of training and test instances in each category .,"['shows', 'the', 'number', 'of', 'training', 'and', 'test', 'instances', 'in', 'each', 'category', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['VBZ', 'DT', 'NN', 'IN', 'NN', 'CC', 'NN', 'NNS', 'IN', 'DT', 'NN', '.']",12
sentiment_analysis,28,127,"Word embeddings in AEN - Glo Ve do not get updated in the learning process , but we fine - tune pre-trained BERT 3 in AEN - BERT .","['Word', 'embeddings', 'in', 'AEN', '-', 'Glo', 'Ve', 'do', 'not', 'get', 'updated', 'in', 'the', 'learning', 'process', ',', 'but', 'we', 'fine', '-', 'tune', 'pre-trained', 'BERT', '3', 'in', 'AEN', '-', 'BERT', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNS', 'IN', 'NNP', ':', 'NNP', 'NNP', 'VBP', 'RB', 'VB', 'VBN', 'IN', 'DT', 'NN', 'NN', ',', 'CC', 'PRP', 'VBP', ':', 'NN', 'JJ', 'NNP', 'CD', 'IN', 'NNP', ':', 'NNP', '.']",29
sentiment_analysis,28,128,Embedding dimension d dim is 300 for GloVe and is 768 for pretrained BERT .,"['Embedding', 'dimension', 'd', 'dim', 'is', '300', 'for', 'GloVe', 'and', 'is', '768', 'for', 'pretrained', 'BERT', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['VBG', 'NN', 'NN', 'NN', 'VBZ', 'CD', 'IN', 'NNP', 'CC', 'VBZ', 'CD', 'IN', 'VBN', 'NNP', '.']",15
sentiment_analysis,28,129,Dimension of hidden states d hid is set to 300 .,"['Dimension', 'of', 'hidden', 'states', 'd', 'hid', 'is', 'set', 'to', '300', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['NN', 'IN', 'JJ', 'NNS', 'VBP', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",11
sentiment_analysis,28,130,The weights of our model are initialized with Glorot initialization .,"['The', 'weights', 'of', 'our', 'model', 'are', 'initialized', 'with', 'Glorot', 'initialization', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'PRP$', 'NN', 'VBP', 'VBN', 'IN', 'NNP', 'NN', '.']",11
sentiment_analysis,28,131,"During training , we set label smoothing parameter to 0.2 , the coefficient ? of L 2 regularization item is 10 ? 5 and dropout rate is 0.1 .","['During', 'training', ',', 'we', 'set', 'label', 'smoothing', 'parameter', 'to', '0.2', ',', 'the', 'coefficient', '?', 'of', 'L', '2', 'regularization', 'item', 'is', '10', '?', '5', 'and', 'dropout', 'rate', 'is', '0.1', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'JJ', 'VBG', 'NN', 'TO', 'CD', ',', 'DT', 'NN', '.', 'IN', 'NNP', 'CD', 'NN', 'NN', 'VBZ', 'CD', '.', 'CD', 'CC', 'NN', 'NN', 'VBZ', 'CD', '.']",29
sentiment_analysis,28,132,"Adam optimizer ( Kingma and Ba , 2014 ) is applied to update all the parameters .","['Adam', 'optimizer', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', 'is', 'applied', 'to', 'update', 'all', 'the', 'parameters', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'VBZ', 'VBN', 'TO', 'VB', 'PDT', 'DT', 'NNS', '.']",17
sentiment_analysis,28,136,We also design a basic BERT - based model to evaluate the performance of AEN - BERT .,"['We', 'also', 'design', 'a', 'basic', 'BERT', '-', 'based', 'model', 'to', 'evaluate', 'the', 'performance', 'of', 'AEN', '-', 'BERT', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'JJ', 'NNP', ':', 'VBN', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNP', ':', 'NNP', '.']",18
sentiment_analysis,28,137,Non - RNN based baselines :,"['Non', '-', 'RNN', 'based', 'baselines', ':']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'VBN', 'NNS', ':']",6
sentiment_analysis,28,138,Feature - based SVM is a traditional support vector machine based model with extensive feature engineering .,"['Feature', '-', 'based', 'SVM', 'is', 'a', 'traditional', 'support', 'vector', 'machine', 'based', 'model', 'with', 'extensive', 'feature', 'engineering', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'VBN', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'NN', 'VBN', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",17
sentiment_analysis,28,139,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .","['Rec', '-', 'NN', 'firstly', 'uses', 'rules', 'to', 'transform', 'the', 'dependency', 'tree', 'and', 'put', 'the', 'opinion', 'target', 'at', 'the', 'root', ',', 'and', 'then', 'learns', 'the', 'sentence', 'representation', 'toward', 'target', 'via', 'semantic', 'composition', 'using', 'Recursive', 'NNs', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'RB', 'VBZ', 'NNS', 'TO', 'VB', 'DT', 'NN', 'NN', 'CC', 'VBD', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', ',', 'CC', 'RB', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'NN', 'IN', 'JJ', 'NN', 'VBG', 'NNP', 'NNP', '.']",35
sentiment_analysis,28,140,MemNet uses multi-hops of attention layers on the context word embeddings for sentence representation to explicitly captures the importance of each context word .,"['MemNet', 'uses', 'multi-hops', 'of', 'attention', 'layers', 'on', 'the', 'context', 'word', 'embeddings', 'for', 'sentence', 'representation', 'to', 'explicitly', 'captures', 'the', 'importance', 'of', 'each', 'context', 'word', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['JJ', 'VBZ', 'NNS', 'IN', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'NNS', 'IN', 'NN', 'NN', 'TO', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",24
sentiment_analysis,28,141,RNN based baselines :,"['RNN', 'based', 'baselines', ':']","['B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBN', 'NNS', ':']",4
sentiment_analysis,28,142,TD - LSTM extends LSTM by using two LSTM networks to model the left context with target and the right context with target respectively .,"['TD', '-', 'LSTM', 'extends', 'LSTM', 'by', 'using', 'two', 'LSTM', 'networks', 'to', 'model', 'the', 'left', 'context', 'with', 'target', 'and', 'the', 'right', 'context', 'with', 'target', 'respectively', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O']","['NNP', ':', 'NNP', 'VBZ', 'NNP', 'IN', 'VBG', 'CD', 'NNP', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'CC', 'DT', 'JJ', 'NN', 'IN', 'NN', 'RB', '.']",25
sentiment_analysis,28,144,ATAE - LSTM,"['ATAE', '-', 'LSTM']","['B-n', 'I-n', 'I-n']","['NNP', ':', 'NN']",3
sentiment_analysis,28,145,"( Wang et al. , 2016 ) strengthens the effect of target embeddings , which appends the target embeddings with each word embeddings and use LSTM with attention to get the final representation for classification .","['(', 'Wang', 'et', 'al.', ',', '2016', ')', 'strengthens', 'the', 'effect', 'of', 'target', 'embeddings', ',', 'which', 'appends', 'the', 'target', 'embeddings', 'with', 'each', 'word', 'embeddings', 'and', 'use', 'LSTM', 'with', 'attention', 'to', 'get', 'the', 'final', 'representation', 'for', 'classification', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['(', 'NNP', 'RB', 'RB', ',', 'CD', ')', 'VBZ', 'DT', 'NN', 'IN', 'NN', 'NNS', ',', 'WDT', 'VBZ', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NNS', 'CC', 'VB', 'NNP', 'IN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', '.']",36
sentiment_analysis,28,146,"IAN learns the representations of the target and context with two LSTMs and attentions interactively , which generates the representations for targets and contexts with respect to each other .","['IAN', 'learns', 'the', 'representations', 'of', 'the', 'target', 'and', 'context', 'with', 'two', 'LSTMs', 'and', 'attentions', 'interactively', ',', 'which', 'generates', 'the', 'representations', 'for', 'targets', 'and', 'contexts', 'with', 'respect', 'to', 'each', 'other', '.']","['B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NN', 'IN', 'CD', 'NNP', 'CC', 'NNS', 'RB', ',', 'WDT', 'VBZ', 'DT', 'NNS', 'IN', 'NNS', 'CC', 'NN', 'IN', 'NN', 'TO', 'DT', 'JJ', '.']",30
sentiment_analysis,28,147,RAM strengthens Mem - Net by representing memory with bidirectional LSTM and using a gated recurrent unit network to combine the multiple attention outputs for sentence representation .,"['RAM', 'strengthens', 'Mem', '-', 'Net', 'by', 'representing', 'memory', 'with', 'bidirectional', 'LSTM', 'and', 'using', 'a', 'gated', 'recurrent', 'unit', 'network', 'to', 'combine', 'the', 'multiple', 'attention', 'outputs', 'for', 'sentence', 'representation', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'NNP', ':', 'NN', 'IN', 'VBG', 'NN', 'IN', 'JJ', 'NNP', 'CC', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NNS', 'IN', 'NN', 'NN', '.']",28
sentiment_analysis,28,148,AEN - Glo Ve ablations :,"['AEN', '-', 'Glo', 'Ve', 'ablations', ':']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'NNP', 'NNS', ':']",6
sentiment_analysis,28,149,AEN - GloVe w/ o PCT ablates PCT module .,"['AEN', '-', 'GloVe', 'w/', 'o', 'PCT', 'ablates', 'PCT', 'module', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'NN', 'NN', 'NNP', 'VBZ', 'NNP', 'NN', '.']",10
sentiment_analysis,28,150,AEN - GloVe w/ o MHA ablates MHA module .,"['AEN', '-', 'GloVe', 'w/', 'o', 'MHA', 'ablates', 'MHA', 'module', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'NN', 'NN', 'NNP', 'VBZ', 'NNP', 'NN', '.']",10
sentiment_analysis,28,151,AEN - GloVe w/ o LSR ablates label smoothing regularization .,"['AEN', '-', 'GloVe', 'w/', 'o', 'LSR', 'ablates', 'label', 'smoothing', 'regularization', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'NN', 'NN', 'NNP', 'VBZ', 'JJ', 'VBG', 'NN', '.']",11
sentiment_analysis,28,152,AEN-GloVe-BiLSTM replaces the attentional encoder layer with two bidirectional LSTM .,"['AEN-GloVe-BiLSTM', 'replaces', 'the', 'attentional', 'encoder', 'layer', 'with', 'two', 'bidirectional', 'LSTM', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['JJ', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'JJ', 'NNP', '.']",11
sentiment_analysis,28,153,Basic BERT - based model :,"['Basic', 'BERT', '-', 'based', 'model', ':']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', ':', 'VBN', 'NN', ':']",6
sentiment_analysis,28,154,"BERT - SPC feeds sequence "" [ CLS ] + context + [ SEP ] + target + [ SEP ] "" into the basic BERT model for sentence pair classification task .","['BERT', '-', 'SPC', 'feeds', 'sequence', '""', '[', 'CLS', ']', '+', 'context', '+', '[', 'SEP', ']', '+', 'target', '+', '[', 'SEP', ']', '""', 'into', 'the', 'basic', 'BERT', 'model', 'for', 'sentence', 'pair', 'classification', 'task', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'VBZ', 'NN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'DT', 'JJ', 'NNP', 'NN', 'IN', 'NN', 'NN', 'NN', 'NN', '.']",33
sentiment_analysis,28,160,The over all performance of TD - LSTM is not good since it only makes a rough treatment of the target words .,"['The', 'over', 'all', 'performance', 'of', 'TD', '-', 'LSTM', 'is', 'not', 'good', 'since', 'it', 'only', 'makes', 'a', 'rough', 'treatment', 'of', 'the', 'target', 'words', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'IN', 'DT', 'NN', 'IN', 'NNP', ':', 'NNP', 'VBZ', 'RB', 'JJ', 'IN', 'PRP', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NNS', '.']",23
sentiment_analysis,28,161,"ATAE - LSTM , IAN and RAM are attention based models , they stably exceed the TD - LSTM method on Restaurant and Laptop datasets .","['ATAE', '-', 'LSTM', ',', 'IAN', 'and', 'RAM', 'are', 'attention', 'based', 'models', ',', 'they', 'stably', 'exceed', 'the', 'TD', '-', 'LSTM', 'method', 'on', 'Restaurant', 'and', 'Laptop', 'datasets', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', ',', 'NNP', 'CC', 'NNP', 'VBP', 'NN', 'VBN', 'NNS', ',', 'PRP', 'RB', 'VBP', 'DT', 'NNP', ':', 'NNP', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'NNS', '.']",26
sentiment_analysis,28,162,"RAM is better than other RNN based models , but it does not perform well on Twitter dataset , which might because bidirectional LSTM is not good at modeling small and ungrammatical text .","['RAM', 'is', 'better', 'than', 'other', 'RNN', 'based', 'models', ',', 'but', 'it', 'does', 'not', 'perform', 'well', 'on', 'Twitter', 'dataset', ',', 'which', 'might', 'because', 'bidirectional', 'LSTM', 'is', 'not', 'good', 'at', 'modeling', 'small', 'and', 'ungrammatical', 'text', '.']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'JJR', 'IN', 'JJ', 'NNP', 'VBN', 'NNS', ',', 'CC', 'PRP', 'VBZ', 'RB', 'VB', 'RB', 'IN', 'NNP', 'NN', ',', 'WDT', 'MD', 'IN', 'JJ', 'NNP', 'VBZ', 'RB', 'JJ', 'IN', 'VBG', 'JJ', 'CC', 'JJ', 'NN', '.']",34
sentiment_analysis,28,163,"Feature - based SVM is still a competitive baseline , but relying on manually - designed features .","['Feature', '-', 'based', 'SVM', 'is', 'still', 'a', 'competitive', 'baseline', ',', 'but', 'relying', 'on', 'manually', '-', 'designed', 'features', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'VBN', 'NNP', 'VBZ', 'RB', 'DT', 'JJ', 'NN', ',', 'CC', 'VBG', 'IN', 'RB', ':', 'VBN', 'NNS', '.']",18
sentiment_analysis,28,164,Rec - NN gets the worst performances among all neural network baselines as dependency parsing is not guaranteed to work well on ungrammatical short texts such as tweets and comments .,"['Rec', '-', 'NN', 'gets', 'the', 'worst', 'performances', 'among', 'all', 'neural', 'network', 'baselines', 'as', 'dependency', 'parsing', 'is', 'not', 'guaranteed', 'to', 'work', 'well', 'on', 'ungrammatical', 'short', 'texts', 'such', 'as', 'tweets', 'and', 'comments', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NNP', 'VBZ', 'DT', 'JJS', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NNS', 'IN', 'NN', 'NN', 'VBZ', 'RB', 'VBN', 'TO', 'VB', 'RB', 'IN', 'JJ', 'JJ', 'NN', 'JJ', 'IN', 'NNS', 'CC', 'NNS', '.']",31
sentiment_analysis,28,165,"Like AEN , Mem Net also eschews recurrence , but its over all performance is not good since it does not model the hidden semantic of embeddings , and the result of the last attention is essentially a linear combination of word embeddings .","['Like', 'AEN', ',', 'Mem', 'Net', 'also', 'eschews', 'recurrence', ',', 'but', 'its', 'over', 'all', 'performance', 'is', 'not', 'good', 'since', 'it', 'does', 'not', 'model', 'the', 'hidden', 'semantic', 'of', 'embeddings', ',', 'and', 'the', 'result', 'of', 'the', 'last', 'attention', 'is', 'essentially', 'a', 'linear', 'combination', 'of', 'word', 'embeddings', '.']","['B-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ',', 'NNP', 'NNP', 'RB', 'VBZ', 'NN', ',', 'CC', 'PRP$', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'JJ', 'IN', 'PRP', 'VBZ', 'RB', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NNS', ',', 'CC', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'RB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', '.']",44
sentiment_analysis,28,169,"Comparing the results of AEN - GloVe and AEN - Glo Ve w / o LSR , we observe that the accuracy of AEN - Glo Ve w / o LSR drops significantly on all three datasets .","['Comparing', 'the', 'results', 'of', 'AEN', '-', 'GloVe', 'and', 'AEN', '-', 'Glo', 'Ve', 'w', '/', 'o', 'LSR', ',', 'we', 'observe', 'that', 'the', 'accuracy', 'of', 'AEN', '-', 'Glo', 'Ve', 'w', '/', 'o', 'LSR', 'drops', 'significantly', 'on', 'all', 'three', 'datasets', '.']","['B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['VBG', 'DT', 'NNS', 'IN', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NNP', 'NNP', 'VBD', 'NNP', 'MD', 'NNP', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'IN', 'NNP', ':', 'NNP', 'NNP', 'VBD', 'NNP', 'MD', 'NNP', 'VBZ', 'RB', 'IN', 'DT', 'CD', 'NNS', '.']",38
sentiment_analysis,28,171,"The over all performance of AEN - GloVe and AEN - Glo Ve - BiLSTM is relatively close , AEN - Glo Ve performs better on the Restaurant dataset .","['The', 'over', 'all', 'performance', 'of', 'AEN', '-', 'GloVe', 'and', 'AEN', '-', 'Glo', 'Ve', '-', 'BiLSTM', 'is', 'relatively', 'close', ',', 'AEN', '-', 'Glo', 'Ve', 'performs', 'better', 'on', 'the', 'Restaurant', 'dataset', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'IN', 'DT', 'NN', 'IN', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NNP', 'NNP', ':', 'NNP', 'VBZ', 'RB', 'JJ', ',', 'NNP', ':', 'NNP', 'NNP', 'VBZ', 'RBR', 'IN', 'DT', 'NNP', 'NN', '.']",30
sentiment_analysis,28,172,"More importantly , AEN - Glo Ve has fewer parameters and is easier to parallelize .","['More', 'importantly', ',', 'AEN', '-', 'Glo', 'Ve', 'has', 'fewer', 'parameters', 'and', 'is', 'easier', 'to', 'parallelize', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['RBR', 'RB', ',', 'NNP', ':', 'NNP', 'NNP', 'VBZ', 'JJR', 'NNS', 'CC', 'VBZ', 'JJR', 'TO', 'VB', '.']",16
sentiment_analysis,28,179,"AEN - Glo Ve 's lightweight level ranks second , since it takes some more parameters than MemNet in modeling hidden states of sequences .","['AEN', '-', 'Glo', 'Ve', ""'s"", 'lightweight', 'level', 'ranks', 'second', ',', 'since', 'it', 'takes', 'some', 'more', 'parameters', 'than', 'MemNet', 'in', 'modeling', 'hidden', 'states', 'of', 'sequences', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NNP', 'NNP', 'POS', 'JJ', 'NN', 'NNS', 'JJ', ',', 'IN', 'PRP', 'VBZ', 'DT', 'JJR', 'NNS', 'IN', 'NNP', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'NNS', '.']",25
sentiment_analysis,28,180,"As a comparison , the model size of AEN - Glo Ve - BiLSTM is more than twice that of AEN - GloVe , but does not bring any performance improvements .","['As', 'a', 'comparison', ',', 'the', 'model', 'size', 'of', 'AEN', '-', 'Glo', 'Ve', '-', 'BiLSTM', 'is', 'more', 'than', 'twice', 'that', 'of', 'AEN', '-', 'GloVe', ',', 'but', 'does', 'not', 'bring', 'any', 'performance', 'improvements', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'DT', 'NN', 'NN', 'IN', 'NNP', ':', 'NNP', 'NNP', ':', 'NNP', 'VBZ', 'JJR', 'IN', 'RB', 'IN', 'IN', 'NNP', ':', 'NNP', ',', 'CC', 'VBZ', 'RB', 'VB', 'DT', 'NN', 'NNS', '.']",32
sentiment_analysis,48,2,Variational Semi-supervised Aspect - term Sentiment Analysis via Transformer,"['Variational', 'Semi-supervised', 'Aspect', '-', 'term', 'Sentiment', 'Analysis', 'via', 'Transformer']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['JJ', 'JJ', 'NNP', ':', 'NN', 'NNP', 'NNP', 'IN', 'NNP']",9
sentiment_analysis,48,13,"Aspect based sentiment analysis ( ABSA ) has two sub - tasks , namely aspect - term sentiment analysis ( ATSA ) and aspect - category sentiment analysis ( ACSA ) .","['Aspect', 'based', 'sentiment', 'analysis', '(', 'ABSA', ')', 'has', 'two', 'sub', '-', 'tasks', ',', 'namely', 'aspect', '-', 'term', 'sentiment', 'analysis', '(', 'ATSA', ')', 'and', 'aspect', '-', 'category', 'sentiment', 'analysis', '(', 'ACSA', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBN', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'CD', 'NN', ':', 'NNS', ',', 'RB', 'JJ', ':', 'NN', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'JJ', ':', 'NN', 'NN', 'NN', '(', 'NNP', ')', '.']",32
sentiment_analysis,48,14,"ACSA is to infer the sentiment polarity with regard to the predefined categories , e.g. , the aspect food , price , ambience .","['ACSA', 'is', 'to', 'infer', 'the', 'sentiment', 'polarity', 'with', 'regard', 'to', 'the', 'predefined', 'categories', ',', 'e.g.', ',', 'the', 'aspect', 'food', ',', 'price', ',', 'ambience', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'NN', 'TO', 'DT', 'JJ', 'NNS', ',', 'NN', ',', 'DT', 'NN', 'NN', ',', 'NN', ',', 'NN', '.']",24
sentiment_analysis,48,15,"On the other hand , ATSA aims at classifying the sentiment polarity of a given aspect word or phrase in the text .","['On', 'the', 'other', 'hand', ',', 'ATSA', 'aims', 'at', 'classifying', 'the', 'sentiment', 'polarity', 'of', 'a', 'given', 'aspect', 'word', 'or', 'phrase', 'in', 'the', 'text', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'NNP', 'VBZ', 'IN', 'VBG', 'DT', 'NN', 'NN', 'IN', 'DT', 'VBN', 'NN', 'NN', 'CC', 'NN', 'IN', 'DT', 'NN', '.']",23
sentiment_analysis,48,29,"In this paper , we proposed a classifier - agnostic framework which named Aspect - term Semi-supervised Variational Autoencoder ( Kingma and Welling , 2014 ) based on Transformer ( ASVAET ) .","['In', 'this', 'paper', ',', 'we', 'proposed', 'a', 'classifier', '-', 'agnostic', 'framework', 'which', 'named', 'Aspect', '-', 'term', 'Semi-supervised', 'Variational', 'Autoencoder', '(', 'Kingma', 'and', 'Welling', ',', '2014', ')', 'based', 'on', 'Transformer', '(', 'ASVAET', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBD', 'DT', 'JJR', ':', 'NN', 'NN', 'WDT', 'VBD', 'NNP', ':', 'NN', 'JJ', 'NNP', 'NNP', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'VBN', 'IN', 'NNP', '(', 'NNP', ')', '.']",33
sentiment_analysis,48,30,The variational autoencoder offers the flexibility to customize the model structure .,"['The', 'variational', 'autoencoder', 'offers', 'the', 'flexibility', 'to', 'customize', 'the', 'model', 'structure', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', '.']",12
sentiment_analysis,48,33,"By regarding the aspect sentiment polarity of the unlabeled data as the discrete latent variable , the model implicitly induces the sentiment polarity via the variational inference .","['By', 'regarding', 'the', 'aspect', 'sentiment', 'polarity', 'of', 'the', 'unlabeled', 'data', 'as', 'the', 'discrete', 'latent', 'variable', ',', 'the', 'model', 'implicitly', 'induces', 'the', 'sentiment', 'polarity', 'via', 'the', 'variational', 'inference', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'DT', 'NN', 'RB', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",28
sentiment_analysis,48,34,"Specifically , the representation of the lexical context is extracted by the encoder and the aspect - term sentiment polarity is inferred from the specific ATSA classifier .","['Specifically', ',', 'the', 'representation', 'of', 'the', 'lexical', 'context', 'is', 'extracted', 'by', 'the', 'encoder', 'and', 'the', 'aspect', '-', 'term', 'sentiment', 'polarity', 'is', 'inferred', 'from', 'the', 'specific', 'ATSA', 'classifier', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'CC', 'DT', 'JJ', ':', 'NN', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NNP', 'NN', '.']",28
sentiment_analysis,48,38,"In addition , by separating the representation of the input sentence , the classifier becomes an independent module in our framework , which endows the method with the ability to integrate different classifiers .","['In', 'addition', ',', 'by', 'separating', 'the', 'representation', 'of', 'the', 'input', 'sentence', ',', 'the', 'classifier', 'becomes', 'an', 'independent', 'module', 'in', 'our', 'framework', ',', 'which', 'endows', 'the', 'method', 'with', 'the', 'ability', 'to', 'integrate', 'different', 'classifiers', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ',', 'IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'PRP$', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'TO', 'VB', 'JJ', 'NNS', '.']",34
sentiment_analysis,48,151,"The number of units in the encoder and the decoder is 100 and the latent variable is of size 50 and the number of layers of both Transformer blocks is 2 , the number of selfattention heads is 8 .","['The', 'number', 'of', 'units', 'in', 'the', 'encoder', 'and', 'the', 'decoder', 'is', '100', 'and', 'the', 'latent', 'variable', 'is', 'of', 'size', '50', 'and', 'the', 'number', 'of', 'layers', 'of', 'both', 'Transformer', 'blocks', 'is', '2', ',', 'the', 'number', 'of', 'selfattention', 'heads', 'is', '8', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O']","['DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'DT', 'NN', 'VBZ', 'CD', 'CC', 'DT', 'NN', 'NN', 'VBZ', 'IN', 'NN', 'CD', 'CC', 'DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NNP', 'NNS', 'VBZ', 'CD', ',', 'DT', 'NN', 'IN', 'NN', 'NNS', 'VBZ', 'CD', '.']",40
sentiment_analysis,48,158,"In this work , the KL weight is set to be 1e - 4 .","['In', 'this', 'work', ',', 'the', 'KL', 'weight', 'is', 'set', 'to', 'be', '1e', '-', '4', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'DT', 'NNP', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'CD', ':', 'CD', '.']",15
sentiment_analysis,48,164,"TC - LSTM : Two LSTMs are used to model the left and right context of the target separately , then the concatenation of two representations is used to predict the label .","['TC', '-', 'LSTM', ':', 'Two', 'LSTMs', 'are', 'used', 'to', 'model', 'the', 'left', 'and', 'right', 'context', 'of', 'the', 'target', 'separately', ',', 'then', 'the', 'concatenation', 'of', 'two', 'representations', 'is', 'used', 'to', 'predict', 'the', 'label', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O']","['NNP', ':', 'NN', ':', 'CD', 'NNP', 'VBP', 'VBN', 'TO', 'VB', 'DT', 'NN', 'CC', 'JJ', 'NN', 'IN', 'DT', 'NN', 'RB', ',', 'RB', 'DT', 'NN', 'IN', 'CD', 'NNS', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NN', '.']",33
sentiment_analysis,48,165,"MemNet : It uses the attention mechanism over the word embedding over multiple rounds to aggregate the information in the sentence , the vector of the final round is used for the prediction .","['MemNet', ':', 'It', 'uses', 'the', 'attention', 'mechanism', 'over', 'the', 'word', 'embedding', 'over', 'multiple', 'rounds', 'to', 'aggregate', 'the', 'information', 'in', 'the', 'sentence', ',', 'the', 'vector', 'of', 'the', 'final', 'round', 'is', 'used', 'for', 'the', 'prediction', '.']","['B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', ':', 'PRP', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', '.']",34
sentiment_analysis,48,166,IAN : IAN adopts two LSTMs to derive the representations of the context and the target phrase interactively and the concatenation is fed to the softmax layer .,"['IAN', ':', 'IAN', 'adopts', 'two', 'LSTMs', 'to', 'derive', 'the', 'representations', 'of', 'the', 'context', 'and', 'the', 'target', 'phrase', 'interactively', 'and', 'the', 'concatenation', 'is', 'fed', 'to', 'the', 'softmax', 'layer', '.']","['B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['NN', ':', 'NNP', 'VBZ', 'CD', 'NNP', 'TO', 'VB', 'DT', 'NNS', 'IN', 'DT', 'NN', 'CC', 'DT', 'NN', 'NN', 'RB', 'CC', 'DT', 'NN', 'VBZ', 'VBN', 'TO', 'DT', 'NN', 'NN', '.']",28
sentiment_analysis,48,168,BILSTM - ATT -G : It models left and right contexts using two attention - based LSTMs and makes use of a special gate layer to combine these two representations .,"['BILSTM', '-', 'ATT', '-G', ':', 'It', 'models', 'left', 'and', 'right', 'contexts', 'using', 'two', 'attention', '-', 'based', 'LSTMs', 'and', 'makes', 'use', 'of', 'a', 'special', 'gate', 'layer', 'to', 'combine', 'these', 'two', 'representations', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'NN', ':', 'PRP', 'NNS', 'VBD', 'CC', 'JJ', 'NN', 'VBG', 'CD', 'NN', ':', 'VBN', 'NNP', 'CC', 'VBZ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'CD', 'NNS', '.']",31
sentiment_analysis,48,170,"TNet - AS : Without using an attention module , TNet adopts a convolutional layer to get salient features from the transformed word representations originated from a bidirectional LSTM layer .","['TNet', '-', 'AS', ':', 'Without', 'using', 'an', 'attention', 'module', ',', 'TNet', 'adopts', 'a', 'convolutional', 'layer', 'to', 'get', 'salient', 'features', 'from', 'the', 'transformed', 'word', 'representations', 'originated', 'from', 'a', 'bidirectional', 'LSTM', 'layer', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['SYM', ':', 'IN', ':', 'IN', 'VBG', 'DT', 'NN', 'NN', ',', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NNS', 'VBN', 'IN', 'DT', 'JJ', 'NNP', 'NN', '.']",31
sentiment_analysis,48,197,"From the , the ASVAET is able to improve supervised performance consistently for all classifiers .","['From', 'the', ',', 'the', 'ASVAET', 'is', 'able', 'to', 'improve', 'supervised', 'performance', 'consistently', 'for', 'all', 'classifiers', '.']","['O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', ',', 'DT', 'NNP', 'VBZ', 'JJ', 'TO', 'VB', 'JJ', 'NN', 'RB', 'IN', 'DT', 'NNS', '.']",16
sentiment_analysis,48,198,"For the MemNet , the test accuracy can be improved by about 2 % by the TSSVAE , and so as the Macro - averaged F1 .","['For', 'the', 'MemNet', ',', 'the', 'test', 'accuracy', 'can', 'be', 'improved', 'by', 'about', '2', '%', 'by', 'the', 'TSSVAE', ',', 'and', 'so', 'as', 'the', 'Macro', '-', 'averaged', 'F1', '.']","['B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNP', ',', 'DT', 'NN', 'NN', 'MD', 'VB', 'VBN', 'IN', 'IN', 'CD', 'NN', 'IN', 'DT', 'NNP', ',', 'CC', 'RB', 'IN', 'DT', 'NNP', ':', 'VBD', 'NNP', '.']",27
sentiment_analysis,48,199,The TNet - AS outperforms the other three models .,"['The', 'TNet', '-', 'AS', 'outperforms', 'the', 'other', 'three', 'models', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', ':', 'IN', 'NNS', 'DT', 'JJ', 'CD', 'NNS', '.']",10
sentiment_analysis,48,200,"Compared with the other two semi-supervised methods , the ASVAET also shows better results .","['Compared', 'with', 'the', 'other', 'two', 'semi-supervised', 'methods', ',', 'the', 'ASVAET', 'also', 'shows', 'better', 'results', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'JJ', 'CD', 'JJ', 'NNS', ',', 'DT', 'NNP', 'RB', 'VBZ', 'JJR', 'NNS', '.']",15
sentiment_analysis,48,201,The ASVAET outperforms the compared semisupervised methods evidently .,"['The', 'ASVAET', 'outperforms', 'the', 'compared', 'semisupervised', 'methods', 'evidently', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['DT', 'NNP', 'VBZ', 'DT', 'VBN', 'VBD', 'NNS', 'RB', '.']",9
sentiment_analysis,48,202,The adoption of indomain pre-trained word vectors is beneficial for the performance compared with the Glove vectors .,"['The', 'adoption', 'of', 'indomain', 'pre-trained', 'word', 'vectors', 'is', 'beneficial', 'for', 'the', 'performance', 'compared', 'with', 'the', 'Glove', 'vectors', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'NNS', 'VBZ', 'JJ', 'IN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NNP', 'NNS', '.']",18
sentiment_analysis,19,2,Improved Sentence Modeling using Suffix Bidirectional LSTM,"['Improved', 'Sentence', 'Modeling', 'using', 'Suffix', 'Bidirectional', 'LSTM']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['VBN', 'NN', 'VBG', 'VBG', 'NNP', 'NNP', 'NNP']",7
sentiment_analysis,19,4,"Recurrent neural networks have become ubiquitous in computing representations of sequential data , especially textual data in natural language processing .","['Recurrent', 'neural', 'networks', 'have', 'become', 'ubiquitous', 'in', 'computing', 'representations', 'of', 'sequential', 'data', ',', 'especially', 'textual', 'data', 'in', 'natural', 'language', 'processing', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'JJ', 'NNS', 'VBP', 'VBN', 'JJ', 'IN', 'VBG', 'NNS', 'IN', 'JJ', 'NNS', ',', 'RB', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NN', '.']",21
sentiment_analysis,19,12,Using SuBiLSTM we achieve new state - of - the - art results for fine - grained sentiment classification and question classification .,"['Using', 'SuBiLSTM', 'we', 'achieve', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'for', 'fine', '-', 'grained', 'sentiment', 'classification', 'and', 'question', 'classification', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['VBG', 'NNP', 'PRP', 'VBP', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'JJ', ':', 'JJ', 'NN', 'NN', 'CC', 'NN', 'NN', '.']",23
sentiment_analysis,19,18,Recurrent Neural Networks ( RNN ) ) have emerged as a powerful tool for modeling sequential data .,"['Recurrent', 'Neural', 'Networks', '(', 'RNN', ')', ')', 'have', 'emerged', 'as', 'a', 'powerful', 'tool', 'for', 'modeling', 'sequential', 'data', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['JJ', 'NNP', 'NNP', '(', 'NNP', ')', ')', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'JJ', 'NNS', '.']",18
sentiment_analysis,19,30,"In this paper , we propose a simple , general and effective technique to compute contextual representations that capture long range dependencies .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'simple', ',', 'general', 'and', 'effective', 'technique', 'to', 'compute', 'contextual', 'representations', 'that', 'capture', 'long', 'range', 'dependencies', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', ',', 'JJ', 'CC', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'NN', 'JJ', 'NN', 'NNS', '.']",23
sentiment_analysis,19,31,"For each token t , we encode both its prefix and suffix in both the forward and reverse direction .","['For', 'each', 'token', 't', ',', 'we', 'encode', 'both', 'its', 'prefix', 'and', 'suffix', 'in', 'both', 'the', 'forward', 'and', 'reverse', 'direction', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'PRP$', 'NN', 'CC', 'NN', 'IN', 'CC', 'DT', 'NN', 'CC', 'JJ', 'NN', '.']",20
sentiment_analysis,19,34,"Further , we combine the prefix and suffix representations by a simple max - pooling operation to produce a richer contextual representation of t in both the forward and reverse direction .","['Further', ',', 'we', 'combine', 'the', 'prefix', 'and', 'suffix', 'representations', 'by', 'a', 'simple', 'max', '-', 'pooling', 'operation', 'to', 'produce', 'a', 'richer', 'contextual', 'representation', 'of', 't', 'in', 'both', 'the', 'forward', 'and', 'reverse', 'direction', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'CC', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', ':', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJR', 'JJ', 'NN', 'IN', 'NN', 'IN', 'CC', 'DT', 'NN', 'CC', 'JJ', 'NN', '.']",32
sentiment_analysis,19,35,We call our model Suffix BiLSTM or SuBiLSTM in short .,"['We', 'call', 'our', 'model', 'Suffix', 'BiLSTM', 'or', 'SuBiLSTM', 'in', 'short', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'NNP', 'NNP', 'CC', 'NNP', 'IN', 'JJ', '.']",11
sentiment_analysis,19,115,"For each of the tasks , we compare SuBiLSTM and SuBiLSTM - Tied with a single - layer BiLSTM and a 2 - layer BiLSTM encoder with the same hidden dimension .","['For', 'each', 'of', 'the', 'tasks', ',', 'we', 'compare', 'SuBiLSTM', 'and', 'SuBiLSTM', '-', 'Tied', 'with', 'a', 'single', '-', 'layer', 'BiLSTM', 'and', 'a', '2', '-', 'layer', 'BiLSTM', 'encoder', 'with', 'the', 'same', 'hidden', 'dimension', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'IN', 'DT', 'NNS', ',', 'PRP', 'VBP', 'NNP', 'CC', 'NNP', ':', 'VBN', 'IN', 'DT', 'JJ', ':', 'NN', 'NNP', 'CC', 'DT', 'CD', ':', 'NN', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NN', '.']",32
sentiment_analysis,19,125,"The relative performance of SuBiL - STM and SuBiLSTM - Tied are fairly close to each other , as shown by the relative gains in .","['The', 'relative', 'performance', 'of', 'SuBiL', '-', 'STM', 'and', 'SuBiLSTM', '-', 'Tied', 'are', 'fairly', 'close', 'to', 'each', 'other', ',', 'as', 'shown', 'by', 'the', 'relative', 'gains', 'in', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'IN', 'NNP', ':', 'NN', 'CC', 'NNP', ':', 'VBN', 'VBP', 'RB', 'RB', 'TO', 'DT', 'JJ', ',', 'IN', 'VBN', 'IN', 'DT', 'JJ', 'NNS', 'IN', '.']",26
sentiment_analysis,19,126,"SuBiLSTM - Tied works better on small datasets ( SST and TREC ) , probably owing to the regularizing effect of using the same LSTM to encode both suffixes and prefixes .","['SuBiLSTM', '-', 'Tied', 'works', 'better', 'on', 'small', 'datasets', '(', 'SST', 'and', 'TREC', ')', ',', 'probably', 'owing', 'to', 'the', 'regularizing', 'effect', 'of', 'using', 'the', 'same', 'LSTM', 'to', 'encode', 'both', 'suffixes', 'and', 'prefixes', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'VBD', 'NNS', 'RBR', 'IN', 'JJ', 'NNS', '(', 'NNP', 'CC', 'NNP', ')', ',', 'RB', 'VBG', 'TO', 'DT', 'VBG', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NNP', 'TO', 'VB', 'DT', 'NNS', 'CC', 'NNS', '.']",32
sentiment_analysis,19,127,"For the larger datasets ( SNLI and QUORA ) , SuBILSTM slightly edges out the tied version owing to its larger capacity .","['For', 'the', 'larger', 'datasets', '(', 'SNLI', 'and', 'QUORA', ')', ',', 'SuBILSTM', 'slightly', 'edges', 'out', 'the', 'tied', 'version', 'owing', 'to', 'its', 'larger', 'capacity', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJR', 'NNS', '(', 'NNP', 'CC', 'NNP', ')', ',', 'NNP', 'RB', 'VBZ', 'RP', 'DT', 'JJ', 'NN', 'VBG', 'TO', 'PRP$', 'JJR', 'NN', '.']",23
sentiment_analysis,19,128,"The training complexity for both the models is similar and hence , with half the parameters , SuBILSTM - Tied should be the more favored model for sentence modeling tasks .","['The', 'training', 'complexity', 'for', 'both', 'the', 'models', 'is', 'similar', 'and', 'hence', ',', 'with', 'half', 'the', 'parameters', ',', 'SuBILSTM', '-', 'Tied', 'should', 'be', 'the', 'more', 'favored', 'model', 'for', 'sentence', 'modeling', 'tasks', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'IN', 'DT', 'DT', 'NNS', 'VBZ', 'JJ', 'CC', 'NN', ',', 'IN', 'PDT', 'DT', 'NNS', ',', 'NNP', ':', 'VBN', 'MD', 'VB', 'DT', 'JJR', 'JJ', 'NN', 'IN', 'NN', 'NN', 'NNS', '.']",31
sentiment_analysis,20,2,DataStories at SemEval-2017 Task 4 : Deep LSTM with Attention for Message - level and Topic - based Sentiment Analysis,"['DataStories', 'at', 'SemEval-2017', 'Task', '4', ':', 'Deep', 'LSTM', 'with', 'Attention', 'for', 'Message', '-', 'level', 'and', 'Topic', '-', 'based', 'Sentiment', 'Analysis']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNS', 'IN', 'NNP', 'NNP', 'CD', ':', 'JJ', 'NNP', 'IN', 'NNP', 'IN', 'NNP', ':', 'NN', 'CC', 'NNP', ':', 'VBN', 'NN', 'NN']",20
sentiment_analysis,20,4,"In this paper we present two deep - learning systems that competed at SemEval - 2017 Task 4 "" Sentiment Analysis in Twitter "" .","['In', 'this', 'paper', 'we', 'present', 'two', 'deep', '-', 'learning', 'systems', 'that', 'competed', 'at', 'SemEval', '-', '2017', 'Task', '4', '""', 'Sentiment', 'Analysis', 'in', 'Twitter', '""', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'DT', 'NN', 'PRP', 'JJ', 'CD', 'JJ', ':', 'VBG', 'NNS', 'WDT', 'VBD', 'IN', 'NNP', ':', 'CD', 'NNP', 'CD', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', '.']",25
sentiment_analysis,20,12,"Sentiment analysis is an area in Natural Language Processing ( NLP ) , studying the identification and quantification of the sentiment expressed in text .","['Sentiment', 'analysis', 'is', 'an', 'area', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', ',', 'studying', 'the', 'identification', 'and', 'quantification', 'of', 'the', 'sentiment', 'expressed', 'in', 'text', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'VBG', 'DT', 'NN', 'CC', 'NN', 'IN', 'DT', 'NN', 'VBN', 'IN', 'NN', '.']",25
sentiment_analysis,20,17,"In this paper , we present two deep - learning systems that competed at SemEval - 2017 Task 4 .","['In', 'this', 'paper', ',', 'we', 'present', 'two', 'deep', '-', 'learning', 'systems', 'that', 'competed', 'at', 'SemEval', '-', '2017', 'Task', '4', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'CD', 'JJ', ':', 'VBG', 'NNS', 'WDT', 'VBD', 'IN', 'NNP', ':', 'CD', 'NNP', 'CD', '.']",20
sentiment_analysis,20,18,Our first model is designed for addressing the problem of messagelevel sentiment analysis .,"['Our', 'first', 'model', 'is', 'designed', 'for', 'addressing', 'the', 'problem', 'of', 'messagelevel', 'sentiment', 'analysis', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",14
sentiment_analysis,20,19,"We employ a 2 - layer Bidirectional LSTM , equipped with an attention mechanism .","['We', 'employ', 'a', '2', '-', 'layer', 'Bidirectional', 'LSTM', ',', 'equipped', 'with', 'an', 'attention', 'mechanism', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'CD', ':', 'NN', 'NNP', 'NNP', ',', 'VBD', 'IN', 'DT', 'NN', 'NN', '.']",15
sentiment_analysis,20,20,"For the topic - based sentiment analysis tasks , we propose a Siamese Bidirectional LSTM with a contextaware attention mechanism .","['For', 'the', 'topic', '-', 'based', 'sentiment', 'analysis', 'tasks', ',', 'we', 'propose', 'a', 'Siamese', 'Bidirectional', 'LSTM', 'with', 'a', 'contextaware', 'attention', 'mechanism', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ':', 'VBN', 'NN', 'NN', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNP', 'NNP', 'IN', 'DT', 'NN', 'NN', 'NN', '.']",21
sentiment_analysis,20,79,MSA Model ( message - level ),"['MSA', 'Model', '(', 'message', '-', 'level', ')']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', '(', 'NN', ':', 'NN', ')']",7
sentiment_analysis,20,103,TSA Model ( topic - based ),"['TSA', 'Model', '(', 'topic', '-', 'based', ')']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', '(', 'SYM', ':', 'VBN', ')']",7
sentiment_analysis,20,156,"The size of the embedding layer is 300 , and the LSTM layers 150 ( 300 for BiLSTM ) .","['The', 'size', 'of', 'the', 'embedding', 'layer', 'is', '300', ',', 'and', 'the', 'LSTM', 'layers', '150', '(', '300', 'for', 'BiLSTM', ')', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'DT', 'VBG', 'NN', 'VBZ', 'CD', ',', 'CC', 'DT', 'NNP', 'NNS', 'CD', '(', 'CD', 'IN', 'NNP', ')', '.']",20
sentiment_analysis,20,157,"We add Gaussian noise with ? = 0.2 and dropout of 0.3 at the embedding layer , dropout of 0.5 at the LSTM layers and dropout of 0.25 at the recurrent connections of the LSTM .","['We', 'add', 'Gaussian', 'noise', 'with', '?', '=', '0.2', 'and', 'dropout', 'of', '0.3', 'at', 'the', 'embedding', 'layer', ',', 'dropout', 'of', '0.5', 'at', 'the', 'LSTM', 'layers', 'and', 'dropout', 'of', '0.25', 'at', 'the', 'recurrent', 'connections', 'of', 'the', 'LSTM', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', '.', '$', 'CD', 'CC', 'NN', 'IN', 'CD', 'IN', 'DT', 'VBG', 'NN', ',', 'NN', 'IN', 'CD', 'IN', 'DT', 'NNP', 'NNS', 'CC', 'NN', 'IN', 'CD', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NNP', '.']",36
sentiment_analysis,20,158,"Finally , we add L 2 regularization of 0.0001 at the loss function .","['Finally', ',', 'we', 'add', 'L', '2', 'regularization', 'of', '0.0001', 'at', 'the', 'loss', 'function', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'JJ', 'CD', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', 'NN', '.']",14
sentiment_analysis,20,160,"The size of the embedding layer is 300 , and the LSTM layers 64 ( 128 for BiLSTM ) .","['The', 'size', 'of', 'the', 'embedding', 'layer', 'is', '300', ',', 'and', 'the', 'LSTM', 'layers', '64', '(', '128', 'for', 'BiLSTM', ')', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'DT', 'VBG', 'NN', 'VBZ', 'CD', ',', 'CC', 'DT', 'NNP', 'NNS', 'CD', '(', 'CD', 'IN', 'NNP', ')', '.']",20
sentiment_analysis,20,161,"We insert Gaussian noise with ? = 0.2 at the embedding layer of both inputs and dropout of 0.3 at the embedding layer of the message , dropout of 0.2 at the LSTM layer and the recurrent connection of the LSTM layer and dropout of 0.3 at the attention layer and the Maxout layer .","['We', 'insert', 'Gaussian', 'noise', 'with', '?', '=', '0.2', 'at', 'the', 'embedding', 'layer', 'of', 'both', 'inputs', 'and', 'dropout', 'of', '0.3', 'at', 'the', 'embedding', 'layer', 'of', 'the', 'message', ',', 'dropout', 'of', '0.2', 'at', 'the', 'LSTM', 'layer', 'and', 'the', 'recurrent', 'connection', 'of', 'the', 'LSTM', 'layer', 'and', 'dropout', 'of', '0.3', 'at', 'the', 'attention', 'layer', 'and', 'the', 'Maxout', 'layer', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', '.', '$', 'CD', 'IN', 'DT', 'VBG', 'NN', 'IN', 'DT', 'NNS', 'CC', 'NN', 'IN', 'CD', 'IN', 'DT', 'VBG', 'NN', 'IN', 'DT', 'NN', ',', 'NN', 'IN', 'CD', 'IN', 'DT', 'NNP', 'NN', 'CC', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', 'CC', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', 'NN', 'CC', 'DT', 'NNP', 'NN', '.']",55
sentiment_analysis,20,162,"Finally , we add L 2 regularization of 0.001 at the loss function .","['Finally', ',', 'we', 'add', 'L', '2', 'regularization', 'of', '0.001', 'at', 'the', 'loss', 'function', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'JJ', 'CD', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', 'NN', '.']",14
sentiment_analysis,20,165,"Our official ranking is 1/38 ( tie ) in Subtask A , 2/24 in Subtask B , 2/16 in Subtask C , 2/16 in Subtask D and 11/12 in Subtask E.","['Our', 'official', 'ranking', 'is', '1/38', '(', 'tie', ')', 'in', 'Subtask', 'A', ',', '2/24', 'in', 'Subtask', 'B', ',', '2/16', 'in', 'Subtask', 'C', ',', '2/16', 'in', 'Subtask', 'D', 'and', '11/12', 'in', 'Subtask', 'E.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n']","['PRP$', 'JJ', 'NN', 'VBZ', 'CD', '(', 'NN', ')', 'IN', 'NNP', 'NNP', ',', 'CD', 'IN', 'NNP', 'NNP', ',', 'CD', 'IN', 'NNP', 'NNP', ',', 'CD', 'IN', 'NNP', 'NNP', 'CC', 'CD', 'IN', 'NNP', 'NNP']",31
sentiment_analysis,18,2,Effective Attention Modeling for Aspect - Level Sentiment Classification,"['Effective', 'Attention', 'Modeling', 'for', 'Aspect', '-', 'Level', 'Sentiment', 'Classification']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', ':', 'NN', 'NNP', 'NNP']",9
sentiment_analysis,18,16,Aspect - level sentiment classification is an important task in fine - grained sentiment analysis .,"['Aspect', '-', 'level', 'sentiment', 'classification', 'is', 'an', 'important', 'task', 'in', 'fine', '-', 'grained', 'sentiment', 'analysis', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', ':', 'JJ', 'NN', 'NN', '.']",16
sentiment_analysis,18,27,We propose two novel approaches for improving the effectiveness of attention models .,"['We', 'propose', 'two', 'novel', 'approaches', 'for', 'improving', 'the', 'effectiveness', 'of', 'attention', 'models', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'JJ', 'NNS', 'IN', 'VBG', 'DT', 'NN', 'IN', 'NN', 'NNS', '.']",13
sentiment_analysis,18,28,The first approach is a new way of encoding a target which better captures the aspect semantics of the target expression .,"['The', 'first', 'approach', 'is', 'a', 'new', 'way', 'of', 'encoding', 'a', 'target', 'which', 'better', 'captures', 'the', 'aspect', 'semantics', 'of', 'the', 'target', 'expression', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'NN', 'WDT', 'JJR', 'VBZ', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",22
sentiment_analysis,18,40,"To address this problem , inspired by , we instead model each target as a mixture of K aspect embeddings where we would like each embedded aspect to represent a cluster of closely related targets .","['To', 'address', 'this', 'problem', ',', 'inspired', 'by', ',', 'we', 'instead', 'model', 'each', 'target', 'as', 'a', 'mixture', 'of', 'K', 'aspect', 'embeddings', 'where', 'we', 'would', 'like', 'each', 'embedded', 'aspect', 'to', 'represent', 'a', 'cluster', 'of', 'closely', 'related', 'targets', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'VBN', 'IN', ',', 'PRP', 'RB', 'VBD', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', 'VBP', 'NNS', 'WRB', 'PRP', 'MD', 'VB', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'RB', 'JJ', 'NNS', '.']",36
sentiment_analysis,18,41,We use an autoencoder structure to learn both the aspect embeddings as well as the representation of the target as a weighted combination of the aspect embeddings .,"['We', 'use', 'an', 'autoencoder', 'structure', 'to', 'learn', 'both', 'the', 'aspect', 'embeddings', 'as', 'well', 'as', 'the', 'representation', 'of', 'the', 'target', 'as', 'a', 'weighted', 'combination', 'of', 'the', 'aspect', 'embeddings', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'DT', 'JJ', 'NNS', 'RB', 'RB', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NNS', '.']",28
sentiment_analysis,18,43,The autoencoder structure is jointly trained with a neural attention - based sentiment classifier to provide a good target representation as well as a high accuracy on the predicted sentiment .,"['The', 'autoencoder', 'structure', 'is', 'jointly', 'trained', 'with', 'a', 'neural', 'attention', '-', 'based', 'sentiment', 'classifier', 'to', 'provide', 'a', 'good', 'target', 'representation', 'as', 'well', 'as', 'a', 'high', 'accuracy', 'on', 'the', 'predicted', 'sentiment', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'DT', 'JJ', 'NN', ':', 'VBN', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', 'RB', 'RB', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",31
sentiment_analysis,18,46,Our second approach exploits syntactic information to construct a syntax - based attention model .,"['Our', 'second', 'approach', 'exploits', 'syntactic', 'information', 'to', 'construct', 'a', 'syntax', '-', 'based', 'attention', 'model', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'JJ', 'NN', 'VBZ', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', ':', 'VBN', 'NN', 'NN', '.']",15
sentiment_analysis,18,50,"Instead , our syntax - based attention mechanism selectively focuses on a small subset of context words that are close to the target on the syntactic path which is obtained by applying a dependency parser on the review sentence .","['Instead', ',', 'our', 'syntax', '-', 'based', 'attention', 'mechanism', 'selectively', 'focuses', 'on', 'a', 'small', 'subset', 'of', 'context', 'words', 'that', 'are', 'close', 'to', 'the', 'target', 'on', 'the', 'syntactic', 'path', 'which', 'is', 'obtained', 'by', 'applying', 'a', 'dependency', 'parser', 'on', 'the', 'review', 'sentence', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'NN', ':', 'VBN', 'NN', 'NN', 'RB', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', 'WDT', 'VBP', 'JJ', 'TO', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'VBN', 'IN', 'VBG', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",40
sentiment_analysis,18,165,( 1 ) Feature - based SVM :,"['(', '1', ')', 'Feature', '-', 'based', 'SVM', ':']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'NNP', ':', 'VBN', 'NN', ':']",8
sentiment_analysis,18,166,We compare with the reported results of a top system in SemEval 2014 .,"['We', 'compare', 'with', 'the', 'reported', 'results', 'of', 'a', 'top', 'system', 'in', 'SemEval', '2014', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'VBN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'CD', '.']",14
sentiment_analysis,18,168,( 2 ) LSTM : An LSTM network is built on top of word embeddings .,"['(', '2', ')', 'LSTM', ':', 'An', 'LSTM', 'network', 'is', 'built', 'on', 'top', 'of', 'word', 'embeddings', '.']","['O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'NN', ':', 'DT', 'NNP', 'NN', 'VBZ', 'VBN', 'IN', 'NN', 'IN', 'NN', 'NNS', '.']",16
sentiment_analysis,18,179,"1 ) Feature - based SVM is still a strong baseline , our best model achieves competitive results on D1 and D2 without relying on so many manually - designed features and external resources .","['1', ')', 'Feature', '-', 'based', 'SVM', 'is', 'still', 'a', 'strong', 'baseline', ',', 'our', 'best', 'model', 'achieves', 'competitive', 'results', 'on', 'D1', 'and', 'D2', 'without', 'relying', 'on', 'so', 'many', 'manually', '-', 'designed', 'features', 'and', 'external', 'resources', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', ')', 'NNP', ':', 'VBN', 'NNP', 'VBZ', 'RB', 'DT', 'JJ', 'NN', ',', 'PRP$', 'JJS', 'NN', 'NNS', 'JJ', 'NNS', 'IN', 'NNP', 'CC', 'NNP', 'IN', 'VBG', 'IN', 'RB', 'JJ', 'RB', ':', 'VBN', 'NNS', 'CC', 'JJ', 'NNS', '.']",35
sentiment_analysis,18,180,"2 ) Compared with all other neural baselines , our full model achieves statistically significant improvements ( p < 0.05 ) on both accuracies and macro - F1 scores for D1 , D3 , D4 .","['2', ')', 'Compared', 'with', 'all', 'other', 'neural', 'baselines', ',', 'our', 'full', 'model', 'achieves', 'statistically', 'significant', 'improvements', '(', 'p', '<', '0.05', ')', 'on', 'both', 'accuracies', 'and', 'macro', '-', 'F1', 'scores', 'for', 'D1', ',', 'D3', ',', 'D4', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', ')', 'VBN', 'IN', 'DT', 'JJ', 'JJ', 'NNS', ',', 'PRP$', 'JJ', 'NN', 'NNS', 'RB', 'JJ', 'NNS', '(', 'JJ', 'NNP', 'CD', ')', 'IN', 'DT', 'NNS', 'CC', 'SYM', ':', 'NN', 'NNS', 'IN', 'NNP', ',', 'NNP', ',', 'NNP', '.']",36
sentiment_analysis,18,181,"3 ) Compared with LSTM + ATT , all three settings of our model are able to achieve statistically significant improvements ( p < 0.05 ) on all datasets .","['3', ')', 'Compared', 'with', 'LSTM', '+', 'ATT', ',', 'all', 'three', 'settings', 'of', 'our', 'model', 'are', 'able', 'to', 'achieve', 'statistically', 'significant', 'improvements', '(', 'p', '<', '0.05', ')', 'on', 'all', 'datasets', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['LS', ')', 'VBN', 'IN', 'NNP', 'NNP', 'NNP', ',', 'DT', 'CD', 'NNS', 'IN', 'PRP$', 'NN', 'VBP', 'JJ', 'TO', 'VB', 'RB', 'JJ', 'NNS', '(', 'JJ', 'NNP', 'CD', ')', 'IN', 'DT', 'NNS', '.']",30
sentiment_analysis,18,183,4 ) The integrated full model over all achieves the best performance compared to using only one of the two proposed approaches .,"['4', ')', 'The', 'integrated', 'full', 'model', 'over', 'all', 'achieves', 'the', 'best', 'performance', 'compared', 'to', 'using', 'only', 'one', 'of', 'the', 'two', 'proposed', 'approaches', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', ')', 'DT', 'VBN', 'JJ', 'NN', 'IN', 'DT', 'VBZ', 'DT', 'JJS', 'NN', 'VBN', 'TO', 'VBG', 'RB', 'CD', 'IN', 'DT', 'CD', 'VBD', 'NNS', '.']",23
sentiment_analysis,18,185,"5 ) The proposed target representation is more helpful on restaurant domain ( D1 , D3 , and D4 ) than laptop domain ( D2 ) .","['5', ')', 'The', 'proposed', 'target', 'representation', 'is', 'more', 'helpful', 'on', 'restaurant', 'domain', '(', 'D1', ',', 'D3', ',', 'and', 'D4', ')', 'than', 'laptop', 'domain', '(', 'D2', ')', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', ')', 'DT', 'VBN', 'NN', 'NN', 'VBZ', 'RBR', 'JJ', 'IN', 'NN', 'NN', '(', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', ')', 'IN', 'JJ', 'NN', '(', 'NNP', ')', '.']",27
sentiment_analysis,42,2,Aspect Level Sentiment Classification with Deep Memory Network,"['Aspect', 'Level', 'Sentiment', 'Classification', 'with', 'Deep', 'Memory', 'Network']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",8
sentiment_analysis,42,12,Aspect level sentiment classification is a fundamental task in the field of sentiment analysis .,"['Aspect', 'level', 'sentiment', 'classification', 'is', 'a', 'fundamental', 'task', 'in', 'the', 'field', 'of', 'sentiment', 'analysis', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['NNP', 'NN', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', '.']",15
sentiment_analysis,42,25,"In pursuit of this goal , we develop deep memory network for aspect level sentiment classification , which is inspired by the recent success of computational models with attention mechanism and explicit memory .","['In', 'pursuit', 'of', 'this', 'goal', ',', 'we', 'develop', 'deep', 'memory', 'network', 'for', 'aspect', 'level', 'sentiment', 'classification', ',', 'which', 'is', 'inspired', 'by', 'the', 'recent', 'success', 'of', 'computational', 'models', 'with', 'attention', 'mechanism', 'and', 'explicit', 'memory', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NN', 'NN', ',', 'WDT', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NN', 'NN', 'CC', 'JJ', 'NN', '.']",34
sentiment_analysis,42,26,"Our approach is data - driven , computationally efficient and does not rely on syntactic parser or sentiment lexicon .","['Our', 'approach', 'is', 'data', '-', 'driven', ',', 'computationally', 'efficient', 'and', 'does', 'not', 'rely', 'on', 'syntactic', 'parser', 'or', 'sentiment', 'lexicon', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'VBZ', 'JJ', ':', 'NN', ',', 'RB', 'JJ', 'CC', 'VBZ', 'RB', 'VB', 'IN', 'JJ', 'NN', 'CC', 'JJ', 'NN', '.']",20
sentiment_analysis,42,27,The approach consists of multiple computational layers with shared parameters .,"['The', 'approach', 'consists', 'of', 'multiple', 'computational', 'layers', 'with', 'shared', 'parameters', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'IN', 'JJ', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', '.']",11
sentiment_analysis,42,28,"Each layer is a content - and location - based attention model , which first learns the importance / weight of each context word and then utilizes this information to calculate continuous text representation .","['Each', 'layer', 'is', 'a', 'content', '-', 'and', 'location', '-', 'based', 'attention', 'model', ',', 'which', 'first', 'learns', 'the', 'importance', '/', 'weight', 'of', 'each', 'context', 'word', 'and', 'then', 'utilizes', 'this', 'information', 'to', 'calculate', 'continuous', 'text', 'representation', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', ':', 'CC', 'NN', ':', 'VBN', 'NN', 'NN', ',', 'WDT', 'RB', 'VBZ', 'DT', 'NN', 'NNP', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'RB', 'VB', 'DT', 'NN', 'TO', 'VB', 'JJ', 'JJ', 'NN', '.']",35
sentiment_analysis,42,29,The text representation in the last layer is regarded as the feature for sentiment classification .,"['The', 'text', 'representation', 'in', 'the', 'last', 'layer', 'is', 'regarded', 'as', 'the', 'feature', 'for', 'sentiment', 'classification', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', '.']",16
sentiment_analysis,42,30,"As every component is differentiable , the entire model could be efficiently trained end - toend with gradient descent , where the loss function is the cross - entropy error of sentiment classification .","['As', 'every', 'component', 'is', 'differentiable', ',', 'the', 'entire', 'model', 'could', 'be', 'efficiently', 'trained', 'end', '-', 'toend', 'with', 'gradient', 'descent', ',', 'where', 'the', 'loss', 'function', 'is', 'the', 'cross', '-', 'entropy', 'error', 'of', 'sentiment', 'classification', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'VBZ', 'JJ', ',', 'DT', 'JJ', 'NN', 'MD', 'VB', 'RB', 'JJ', 'NN', ':', 'NN', 'IN', 'JJ', 'NN', ',', 'WRB', 'DT', 'NN', 'NN', 'VBZ', 'DT', 'NN', ':', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",34
sentiment_analysis,42,158,"( 1 ) Majority is a basic baseline method , which assigns the majority sentiment label in training set to each instance in the test set .","['(', '1', ')', 'Majority', 'is', 'a', 'basic', 'baseline', 'method', ',', 'which', 'assigns', 'the', 'majority', 'sentiment', 'label', 'in', 'training', 'set', 'to', 'each', 'instance', 'in', 'the', 'test', 'set', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['(', 'CD', ')', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'NN', 'NN', 'IN', 'NN', 'VBN', 'TO', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",27
sentiment_analysis,42,159,( 2 ) Feature - based SVM performs state - of - the - art on aspect level sentiment classification .,"['(', '2', ')', 'Feature', '-', 'based', 'SVM', 'performs', 'state', '-', 'of', '-', 'the', '-', 'art', 'on', 'aspect', 'level', 'sentiment', 'classification', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'NNP', ':', 'VBN', 'NNP', 'NNS', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'JJ', 'NN', 'NN', 'NN', '.']",21
sentiment_analysis,42,161,"( 3 ) We compare with three LSTM models ( Tang et al. , 2015 a ) ) .","['(', '3', ')', 'We', 'compare', 'with', 'three', 'LSTM', 'models', '(', 'Tang', 'et', 'al.', ',', '2015', 'a', ')', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'CD', ')', 'PRP', 'VBP', 'IN', 'CD', 'NNP', 'NNS', '(', 'NNP', 'RB', 'RB', ',', 'CD', 'DT', ')', ')', '.']",19
sentiment_analysis,42,162,"In LSTM , a LSTM based recurrent model is applied from the start to the end of a sentence , and the last hidden vector is used as the sentence representation .","['In', 'LSTM', ',', 'a', 'LSTM', 'based', 'recurrent', 'model', 'is', 'applied', 'from', 'the', 'start', 'to', 'the', 'end', 'of', 'a', 'sentence', ',', 'and', 'the', 'last', 'hidden', 'vector', 'is', 'used', 'as', 'the', 'sentence', 'representation', '.']","['B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NNP', ',', 'DT', 'NNP', 'VBN', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'TO', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'CC', 'DT', 'JJ', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",32
sentiment_analysis,42,163,"TDLSTM extends LSTM by taking into account of the aspect , and uses two LSTM networks , a forward one and a backward one , towards the aspect .","['TDLSTM', 'extends', 'LSTM', 'by', 'taking', 'into', 'account', 'of', 'the', 'aspect', ',', 'and', 'uses', 'two', 'LSTM', 'networks', ',', 'a', 'forward', 'one', 'and', 'a', 'backward', 'one', ',', 'towards', 'the', 'aspect', '.']","['B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O']","['NNP', 'VBZ', 'NNP', 'IN', 'VBG', 'IN', 'NN', 'IN', 'DT', 'NN', ',', 'CC', 'VBZ', 'CD', 'NNP', 'NNS', ',', 'DT', 'RB', 'NN', 'CC', 'DT', 'NN', 'CD', ',', 'VBZ', 'DT', 'NN', '.']",29
sentiment_analysis,42,164,"TDLSTM + ATT extends TDLSTM by incorporating an attention mechanism ( Bahdanau et al. , 2015 ) over the hidden vectors .","['TDLSTM', '+', 'ATT', 'extends', 'TDLSTM', 'by', 'incorporating', 'an', 'attention', 'mechanism', '(', 'Bahdanau', 'et', 'al.', ',', '2015', ')', 'over', 'the', 'hidden', 'vectors', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', 'NNP', 'NNP', 'VBZ', 'NNP', 'IN', 'VBG', 'DT', 'NN', 'NN', '(', 'NNP', 'RB', 'RB', ',', 'CD', ')', 'IN', 'DT', 'JJ', 'NNS', '.']",22
sentiment_analysis,42,165,We use the same Glove word vectors for fair comparison .,"['We', 'use', 'the', 'same', 'Glove', 'word', 'vectors', 'for', 'fair', 'comparison', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NNP', 'NN', 'NNS', 'IN', 'JJ', 'NN', '.']",11
sentiment_analysis,42,166,"( 4 ) We also implement ContextAVG , a simplistic version of our approach .","['(', '4', ')', 'We', 'also', 'implement', 'ContextAVG', ',', 'a', 'simplistic', 'version', 'of', 'our', 'approach', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['(', 'CD', ')', 'PRP', 'RB', 'VBP', 'NNP', ',', 'DT', 'JJ', 'NN', 'IN', 'PRP$', 'NN', '.']",15
sentiment_analysis,42,171,"We can find that feature - based SVM is an extremely strong performer and substantially outperforms other baseline methods , which demonstrates the importance of a powerful feature representation for aspect level sentiment classification .","['We', 'can', 'find', 'that', 'feature', '-', 'based', 'SVM', 'is', 'an', 'extremely', 'strong', 'performer', 'and', 'substantially', 'outperforms', 'other', 'baseline', 'methods', ',', 'which', 'demonstrates', 'the', 'importance', 'of', 'a', 'powerful', 'feature', 'representation', 'for', 'aspect', 'level', 'sentiment', 'classification', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'MD', 'VB', 'DT', 'NN', ':', 'VBN', 'NNP', 'VBZ', 'DT', 'RB', 'JJ', 'NN', 'CC', 'RB', 'VBZ', 'JJ', 'NN', 'NNS', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NN', 'NN', '.']",35
sentiment_analysis,42,172,"Among three recurrent models , TDLSTM performs better than LSTM , which indicates that taking into account of the aspect information is helpful .","['Among', 'three', 'recurrent', 'models', ',', 'TDLSTM', 'performs', 'better', 'than', 'LSTM', ',', 'which', 'indicates', 'that', 'taking', 'into', 'account', 'of', 'the', 'aspect', 'information', 'is', 'helpful', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'CD', 'NN', 'NNS', ',', 'NNP', 'VBZ', 'JJR', 'IN', 'NNP', ',', 'WDT', 'VBZ', 'IN', 'VBG', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'JJ', '.']",24
sentiment_analysis,42,175,We consider that each hidden vector of TDLSTM encodes the semantics of word sequence until the current position .,"['We', 'consider', 'that', 'each', 'hidden', 'vector', 'of', 'TDLSTM', 'encodes', 'the', 'semantics', 'of', 'word', 'sequence', 'until', 'the', 'current', 'position', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'VBZ', 'DT', 'NNS', 'IN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",19
sentiment_analysis,42,178,"We can also find that the performance of Contex - tAVG is very poor , which means that assigning the same weight / importance to all the context words is not an effective way .","['We', 'can', 'also', 'find', 'that', 'the', 'performance', 'of', 'Contex', '-', 'tAVG', 'is', 'very', 'poor', ',', 'which', 'means', 'that', 'assigning', 'the', 'same', 'weight', '/', 'importance', 'to', 'all', 'the', 'context', 'words', 'is', 'not', 'an', 'effective', 'way', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'MD', 'RB', 'VB', 'IN', 'DT', 'NN', 'IN', 'NNP', ':', 'NN', 'VBZ', 'RB', 'JJ', ',', 'WDT', 'VBZ', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'NN', 'TO', 'PDT', 'DT', 'NN', 'NNS', 'VBZ', 'RB', 'DT', 'JJ', 'NN', '.']",35
sentiment_analysis,42,179,"Among all our models from single hop to nine hops , we can observe that using more computational layers could generally lead to better performance , especially when the number of hops is less than six .","['Among', 'all', 'our', 'models', 'from', 'single', 'hop', 'to', 'nine', 'hops', ',', 'we', 'can', 'observe', 'that', 'using', 'more', 'computational', 'layers', 'could', 'generally', 'lead', 'to', 'better', 'performance', ',', 'especially', 'when', 'the', 'number', 'of', 'hops', 'is', 'less', 'than', 'six', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'PRP$', 'NNS', 'IN', 'JJ', 'NN', 'TO', 'CD', 'NNS', ',', 'PRP', 'MD', 'VB', 'DT', 'VBG', 'JJR', 'JJ', 'NNS', 'MD', 'RB', 'VB', 'TO', 'JJR', 'NN', ',', 'RB', 'WRB', 'DT', 'NN', 'IN', 'NNS', 'VBZ', 'JJR', 'IN', 'CD', '.']",37
sentiment_analysis,42,180,"The best performances are achieved when the model contains seven and nine hops , respectively .","['The', 'best', 'performances', 'are', 'achieved', 'when', 'the', 'model', 'contains', 'seven', 'and', 'nine', 'hops', ',', 'respectively', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['DT', 'JJS', 'NNS', 'VBP', 'VBN', 'WRB', 'DT', 'NN', 'VBZ', 'CD', 'CC', 'CD', 'NNS', ',', 'RB', '.']",16
sentiment_analysis,42,181,"On both datasets , the proposed approach could obtain comparable accuracy compared to the state - of - art feature - based SVM system .","['On', 'both', 'datasets', ',', 'the', 'proposed', 'approach', 'could', 'obtain', 'comparable', 'accuracy', 'compared', 'to', 'the', 'state', '-', 'of', '-', 'art', 'feature', '-', 'based', 'SVM', 'system', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'DT', 'VBN', 'NN', 'MD', 'VB', 'JJ', 'NN', 'VBN', 'TO', 'DT', 'NN', ':', 'IN', ':', 'NN', 'NN', ':', 'VBN', 'NNP', 'NN', '.']",25
sentiment_analysis,42,202,We can find that using multiple computational layers could consistently improve the classification accuracy in all these models .,"['We', 'can', 'find', 'that', 'using', 'multiple', 'computational', 'layers', 'could', 'consistently', 'improve', 'the', 'classification', 'accuracy', 'in', 'all', 'these', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'IN', 'VBG', 'JJ', 'JJ', 'NNS', 'MD', 'RB', 'VB', 'DT', 'NN', 'NN', 'IN', 'PDT', 'DT', 'NNS', '.']",19
sentiment_analysis,42,203,All these models perform comparably when the number of hops is larger than five .,"['All', 'these', 'models', 'perform', 'comparably', 'when', 'the', 'number', 'of', 'hops', 'is', 'larger', 'than', 'five', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PDT', 'DT', 'NNS', 'VBP', 'RB', 'WRB', 'DT', 'NN', 'IN', 'NNS', 'VBZ', 'JJR', 'IN', 'CD', '.']",15
sentiment_analysis,4,2,ICON : Interactive Conversational Memory Network for Multimodal Emotion Detection,"['ICON', ':', 'Interactive', 'Conversational', 'Memory', 'Network', 'for', 'Multimodal', 'Emotion', 'Detection']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NN', ':', 'JJ', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",10
sentiment_analysis,4,4,Emotion recognition in conversations is crucial for building empathetic machines .,"['Emotion', 'recognition', 'in', 'conversations', 'is', 'crucial', 'for', 'building', 'empathetic', 'machines', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'IN', 'NNS', 'VBZ', 'JJ', 'IN', 'VBG', 'JJ', 'NNS', '.']",11
sentiment_analysis,4,16,"Analyzing emotional dynamics in conversations , however , poses complex challenges .","['Analyzing', 'emotional', 'dynamics', 'in', 'conversations', ',', 'however', ',', 'poses', 'complex', 'challenges', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', 'NNS', 'IN', 'NNS', ',', 'RB', ',', 'VBZ', 'JJ', 'NNS', '.']",12
sentiment_analysis,4,20,"We propose Interactive COnversational memory Network ( ICON ) , a multimodal network for identifying emotions in utterance - videos .","['We', 'propose', 'Interactive', 'COnversational', 'memory', 'Network', '(', 'ICON', ')', ',', 'a', 'multimodal', 'network', 'for', 'identifying', 'emotions', 'in', 'utterance', '-', 'videos', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NNP', 'NN', 'NNP', '(', 'NNP', ')', ',', 'DT', 'NN', 'NN', 'IN', 'VBG', 'NNS', 'IN', 'JJ', ':', 'NN', '.']",21
sentiment_analysis,4,28,"First , it extracts multimodal features from all utterancevideos .","['First', ',', 'it', 'extracts', 'multimodal', 'features', 'from', 'all', 'utterancevideos', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBZ', 'JJ', 'NNS', 'IN', 'DT', 'NNS', '.']",10
sentiment_analysis,4,29,"Next , given a test utterance to be classified , ICON considers the preceding utterances of both speakers falling within a context - window and models their self - emotional influences using local gated recurrent units .","['Next', ',', 'given', 'a', 'test', 'utterance', 'to', 'be', 'classified', ',', 'ICON', 'considers', 'the', 'preceding', 'utterances', 'of', 'both', 'speakers', 'falling', 'within', 'a', 'context', '-', 'window', 'and', 'models', 'their', 'self', '-', 'emotional', 'influences', 'using', 'local', 'gated', 'recurrent', 'units', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', ',', 'VBN', 'DT', 'NN', 'NN', 'TO', 'VB', 'VBN', ',', 'NNP', 'VBZ', 'DT', 'VBG', 'NNS', 'IN', 'DT', 'NNS', 'VBG', 'IN', 'DT', 'NN', ':', 'NN', 'CC', 'NNS', 'PRP$', 'JJ', ':', 'JJ', 'NNS', 'VBG', 'JJ', 'VBN', 'JJ', 'NNS', '.']",37
sentiment_analysis,4,30,"Furthermore , to incorporate inter -speaker influences , a global representation is generated using a GRU that intakes output of the local GRUs .","['Furthermore', ',', 'to', 'incorporate', 'inter', '-speaker', 'influences', ',', 'a', 'global', 'representation', 'is', 'generated', 'using', 'a', 'GRU', 'that', 'intakes', 'output', 'of', 'the', 'local', 'GRUs', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'TO', 'VB', 'NN', 'NN', 'NNS', ',', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'VBG', 'DT', 'NNP', 'WDT', 'VBZ', 'NN', 'IN', 'DT', 'JJ', 'NNP', '.']",24
sentiment_analysis,4,31,"For each instance in the context - window , the output of this global GRU is stored as a memory cell .","['For', 'each', 'instance', 'in', 'the', 'context', '-', 'window', ',', 'the', 'output', 'of', 'this', 'global', 'GRU', 'is', 'stored', 'as', 'a', 'memory', 'cell', '.']","['B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'IN', 'DT', 'NN', ':', 'NN', ',', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",22
sentiment_analysis,4,32,These memories are then subjected to multiple read / write cycles that include attention mechanism for generating contextual summaries of the conversational history .,"['These', 'memories', 'are', 'then', 'subjected', 'to', 'multiple', 'read', '/', 'write', 'cycles', 'that', 'include', 'attention', 'mechanism', 'for', 'generating', 'contextual', 'summaries', 'of', 'the', 'conversational', 'history', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'RB', 'VBN', 'TO', 'VB', 'JJ', 'NNP', 'NN', 'NNS', 'WDT', 'VBP', 'NN', 'NN', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",24
sentiment_analysis,4,33,"At each iteration , the representation of the test utterance is improved with this summary representation and finally used for prediction .","['At', 'each', 'iteration', ',', 'the', 'representation', 'of', 'the', 'test', 'utterance', 'is', 'improved', 'with', 'this', 'summary', 'representation', 'and', 'finally', 'used', 'for', 'prediction', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'DT', 'NN', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'CC', 'RB', 'VBN', 'IN', 'NN', '.']",22
sentiment_analysis,4,219,20 % of the training set is used as validation set for hyper - parameter tuning .,"['20', '%', 'of', 'the', 'training', 'set', 'is', 'used', 'as', 'validation', 'set', 'for', 'hyper', '-', 'parameter', 'tuning', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'NN', 'VBN', 'IN', 'JJR', ':', 'NN', 'NN', '.']",17
sentiment_analysis,4,220,"We use the Adam optimizer ( Kingma and Ba , 2014 ) for training the parameters starting with an initial learning rate of 0.001 .","['We', 'use', 'the', 'Adam', 'optimizer', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', 'for', 'training', 'the', 'parameters', 'starting', 'with', 'an', 'initial', 'learning', 'rate', 'of', '0.001', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'IN', 'VBG', 'DT', 'NNS', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', '.']",25
sentiment_analysis,4,221,Termination of the training - phase is decided by early - stopping with a patience of 10 d = 100 dv = 512 dem = 100 K = 40 R = 3 epochs .,"['Termination', 'of', 'the', 'training', '-', 'phase', 'is', 'decided', 'by', 'early', '-', 'stopping', 'with', 'a', 'patience', 'of', '10', 'd', '=', '100', 'dv', '=', '512', 'dem', '=', '100', 'K', '=', '40', 'R', '=', '3', 'epochs', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'IN', 'DT', 'NN', ':', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', ':', 'VBG', 'IN', 'DT', 'NN', 'IN', 'CD', 'NN', 'VBD', 'CD', 'NN', 'VBD', 'CD', 'NN', 'NN', 'CD', 'NNP', 'NNP', 'CD', 'NNP', 'VBD', 'CD', 'NN', '.']",34
sentiment_analysis,4,222,The network is subjected to regularization in the form of Dropout and Gradient - clipping for a norm of 40 .,"['The', 'network', 'is', 'subjected', 'to', 'regularization', 'in', 'the', 'form', 'of', 'Dropout', 'and', 'Gradient', '-', 'clipping', 'for', 'a', 'norm', 'of', '40', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'TO', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', 'CC', 'NNP', ':', 'NN', 'IN', 'DT', 'NN', 'IN', 'CD', '.']",21
sentiment_analysis,4,223,"Finally , the best hyper - parameters are decided using a gridsearch .","['Finally', ',', 'the', 'best', 'hyper', '-', 'parameters', 'are', 'decided', 'using', 'a', 'gridsearch', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['RB', ',', 'DT', 'JJS', 'NN', ':', 'NNS', 'VBP', 'VBN', 'VBG', 'DT', 'NN', '.']",13
sentiment_analysis,4,225,"For multimodal feature extraction , we explore different designs for the employed CNNs .","['For', 'multimodal', 'feature', 'extraction', ',', 'we', 'explore', 'different', 'designs', 'for', 'the', 'employed', 'CNNs', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'JJ', 'NN', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NNP', '.']",14
sentiment_analysis,4,226,"For text , we find the single layer CNN to perform at par with deeper variants .","['For', 'text', ',', 'we', 'find', 'the', 'single', 'layer', 'CNN', 'to', 'perform', 'at', 'par', 'with', 'deeper', 'variants', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NNP', 'TO', 'VB', 'IN', 'NN', 'IN', 'JJR', 'NNS', '.']",17
sentiment_analysis,4,227,"For visual features , however , a deeper CNN provides better representations .","['For', 'visual', 'features', ',', 'however', ',', 'a', 'deeper', 'CNN', 'provides', 'better', 'representations', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'JJ', 'NNS', ',', 'RB', ',', 'DT', 'JJ', 'NNP', 'VBZ', 'JJR', 'NNS', '.']",13
sentiment_analysis,4,228,We also find that contextually conditioned features perform better than context - less features .,"['We', 'also', 'find', 'that', 'contextually', 'conditioned', 'features', 'perform', 'better', 'than', 'context', '-', 'less', 'features', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'IN', 'RB', 'VBN', 'NNS', 'VBP', 'JJR', 'IN', 'JJ', ':', 'JJR', 'NNS', '.']",15
sentiment_analysis,4,233,memnet is an end - toend memory network .,"['memnet', 'is', 'an', 'end', '-', 'toend', 'memory', 'network', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'VBZ', 'DT', 'NN', ':', 'NN', 'NN', 'NN', '.']",9
sentiment_analysis,4,235,cLSTM 4 classifies utterances using neighboring utterances ( of same speaker ) as context .,"['cLSTM', '4', 'classifies', 'utterances', 'using', 'neighboring', 'utterances', '(', 'of', 'same', 'speaker', ')', 'as', 'context', '.']","['B-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['NN', 'CD', 'NNS', 'NNS', 'VBG', 'JJ', 'NNS', '(', 'IN', 'JJ', 'NN', ')', 'IN', 'NN', '.']",15
sentiment_analysis,4,237,"TFN 5 models intra-and intermodality dynamics by explicitly aggregating uni - , bi- and trimodal interactions .","['TFN', '5', 'models', 'intra-and', 'intermodality', 'dynamics', 'by', 'explicitly', 'aggregating', 'uni', '-', ',', 'bi-', 'and', 'trimodal', 'interactions', '.']","['B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['$', 'CD', 'NNS', 'JJ', 'NN', 'NNS', 'IN', 'RB', 'VBG', 'JJ', ':', ',', 'JJ', 'CC', 'JJ', 'NNS', '.']",17
sentiment_analysis,4,239,"MFN performs multi-view learning by using Delta - memory Attention Network , a fusion mechanism to learn cross - view interactions .","['MFN', 'performs', 'multi-view', 'learning', 'by', 'using', 'Delta', '-', 'memory', 'Attention', 'Network', ',', 'a', 'fusion', 'mechanism', 'to', 'learn', 'cross', '-', 'view', 'interactions', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'NNS', 'VBP', 'NN', 'IN', 'VBG', 'NNP', ':', 'NN', 'NNP', 'NNP', ',', 'DT', 'NN', 'NN', 'TO', 'VB', 'JJ', ':', 'NN', 'NNS', '.']",22
sentiment_analysis,4,241,CMN models separate contexts for both speaker and listener to an utterance .,"['CMN', 'models', 'separate', 'contexts', 'for', 'both', 'speaker', 'and', 'listener', 'to', 'an', 'utterance', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['NNP', 'NNS', 'VBP', 'NN', 'IN', 'DT', 'NN', 'CC', 'NN', 'TO', 'DT', 'NN', '.']",13
sentiment_analysis,4,246,ICON performs better than the compared models with significant performance increase in emotions ( ? 2.1 % acc. ) .,"['ICON', 'performs', 'better', 'than', 'the', 'compared', 'models', 'with', 'significant', 'performance', 'increase', 'in', 'emotions', '(', '?', '2.1', '%', 'acc.', ')', '.']","['B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'NNS', 'RBR', 'IN', 'DT', 'VBN', 'NNS', 'IN', 'JJ', 'NN', 'NN', 'IN', 'NNS', '(', '.', 'CD', 'NN', 'NN', ')', '.']",20
sentiment_analysis,4,247,"For each emotion , ICON outperforms all the compared models except for happiness emotion .","['For', 'each', 'emotion', ',', 'ICON', 'outperforms', 'all', 'the', 'compared', 'models', 'except', 'for', 'happiness', 'emotion', '.']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'NNP', 'VBZ', 'PDT', 'DT', 'VBN', 'NNS', 'IN', 'IN', 'JJ', 'NN', '.']",15
sentiment_analysis,4,248,"However , its performance is still at par with c LSTM without a significant gap .","['However', ',', 'its', 'performance', 'is', 'still', 'at', 'par', 'with', 'c', 'LSTM', 'without', 'a', 'significant', 'gap', '.']","['O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'NN', 'VBZ', 'RB', 'IN', 'NN', 'IN', 'JJ', 'NNP', 'IN', 'DT', 'JJ', 'NN', '.']",16
sentiment_analysis,4,249,"Also , ICON manages to correctly identify the relatively similar excitement emotion by a large margin .","['Also', ',', 'ICON', 'manages', 'to', 'correctly', 'identify', 'the', 'relatively', 'similar', 'excitement', 'emotion', 'by', 'a', 'large', 'margin', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'NNP', 'VBZ', 'TO', 'VB', 'VB', 'DT', 'RB', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",17
sentiment_analysis,4,251,"In all the labels , ICON attains improved performance over its counterparts , suggesting the efficacy of its context - modeling scheme .","['In', 'all', 'the', 'labels', ',', 'ICON', 'attains', 'improved', 'performance', 'over', 'its', 'counterparts', ',', 'suggesting', 'the', 'efficacy', 'of', 'its', 'context', '-', 'modeling', 'scheme', '.']","['B-p', 'O', 'O', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PDT', 'DT', 'NNS', ',', 'NNP', 'VBZ', 'JJ', 'NN', 'IN', 'PRP$', 'NNS', ',', 'VBG', 'DT', 'NN', 'IN', 'PRP$', 'NN', ':', 'NN', 'NN', '.']",23
sentiment_analysis,4,263,presents the results for different combinations of modes used by ICON on IEMOCAP .,"['presents', 'the', 'results', 'for', 'different', 'combinations', 'of', 'modes', 'used', 'by', 'ICON', 'on', 'IEMOCAP', '.']","['B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O']","['NNS', 'DT', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'NNS', 'VBN', 'IN', 'NNP', 'IN', 'NNP', '.']",14
sentiment_analysis,4,264,"As seen , the trimodal network provides the best performance which is preceded by the bimodal variants .","['As', 'seen', ',', 'the', 'trimodal', 'network', 'provides', 'the', 'best', 'performance', 'which', 'is', 'preceded', 'by', 'the', 'bimodal', 'variants', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'VBN', ',', 'DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJS', 'NN', 'WDT', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'NNS', '.']",18
sentiment_analysis,4,265,"Among unimodals , language modality performs the best , reaffirming its significance in multimodal systems .","['Among', 'unimodals', ',', 'language', 'modality', 'performs', 'the', 'best', ',', 'reaffirming', 'its', 'significance', 'in', 'multimodal', 'systems', '.']","['B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNS', ',', 'NN', 'NN', 'VBZ', 'DT', 'JJS', ',', 'VBG', 'PRP$', 'NN', 'IN', 'JJ', 'NNS', '.']",16
sentiment_analysis,4,266,"Interestingly , the audio and visual modality , on their own , do not provide good performance , but when used with text , complementary data is shared to improve over all performance .","['Interestingly', ',', 'the', 'audio', 'and', 'visual', 'modality', ',', 'on', 'their', 'own', ',', 'do', 'not', 'provide', 'good', 'performance', ',', 'but', 'when', 'used', 'with', 'text', ',', 'complementary', 'data', 'is', 'shared', 'to', 'improve', 'over', 'all', 'performance', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'CC', 'JJ', 'NN', ',', 'IN', 'PRP$', 'JJ', ',', 'VBP', 'RB', 'VB', 'JJ', 'NN', ',', 'CC', 'WRB', 'VBN', 'IN', 'NN', ',', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'IN', 'DT', 'NN', '.']",34
sentiment_analysis,4,272,Self vs Dual History :,"['Self', 'vs', 'Dual', 'History', ':']","['B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'JJ', 'NNP', 'NN', ':']",5
sentiment_analysis,4,274,"Compared to the dual - history variants ( variants 3 , 5 , and 7 ) , these models provide lesser performance .","['Compared', 'to', 'the', 'dual', '-', 'history', 'variants', '(', 'variants', '3', ',', '5', ',', 'and', '7', ')', ',', 'these', 'models', 'provide', 'lesser', 'performance', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['VBN', 'TO', 'DT', 'JJ', ':', 'NN', 'NNS', '(', 'NNS', 'CD', ',', 'CD', ',', 'CC', 'CD', ')', ',', 'DT', 'NNS', 'VBP', 'JJR', 'NN', '.']",23
sentiment_analysis,4,277,DGIM prevents the storage of dynamic influences between speakers at each historical time step and leads to performance deterioration .,"['DGIM', 'prevents', 'the', 'storage', 'of', 'dynamic', 'influences', 'between', 'speakers', 'at', 'each', 'historical', 'time', 'step', 'and', 'leads', 'to', 'performance', 'deterioration', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', 'CC', 'VBZ', 'TO', 'NN', 'NN', '.']",20
sentiment_analysis,4,278,"Multi - hop vs No - hop : Variants 2 and 3 represent cases where multi-hop is omitted , i.e. , R = 1 . Performance for them are poorer than variants having multi-hop mechanism ( variants 4 - 7 ) .","['Multi', '-', 'hop', 'vs', 'No', '-', 'hop', ':', 'Variants', '2', 'and', '3', 'represent', 'cases', 'where', 'multi-hop', 'is', 'omitted', ',', 'i.e.', ',', 'R', '=', '1', '.', 'Performance', 'for', 'them', 'are', 'poorer', 'than', 'variants', 'having', 'multi-hop', 'mechanism', '(', 'variants', '4', '-', '7', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', 'VBZ', 'DT', ':', 'NN', ':', 'NNS', 'CD', 'CC', 'CD', 'NN', 'NNS', 'WRB', 'NN', 'VBZ', 'VBN', ',', 'FW', ',', 'NNP', 'VBZ', 'CD', '.', 'NN', 'IN', 'PRP', 'VBP', 'JJR', 'IN', 'NNS', 'VBG', 'NN', 'NN', '(', 'NNS', 'CD', ':', 'CD', ')', '.']",42
sentiment_analysis,4,279,"Also , removal of multi-hop leads to worse performance than the removal of DGIM .","['Also', ',', 'removal', 'of', 'multi-hop', 'leads', 'to', 'worse', 'performance', 'than', 'the', 'removal', 'of', 'DGIM', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'NN', 'IN', 'JJ', 'NNS', 'TO', 'JJR', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', '.']",15
sentiment_analysis,4,281,"However , best performance is achieved by variant 6 which contains all the proposed modules in its pipeline .","['However', ',', 'best', 'performance', 'is', 'achieved', 'by', 'variant', '6', 'which', 'contains', 'all', 'the', 'proposed', 'modules', 'in', 'its', 'pipeline', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'JJS', 'NN', 'VBZ', 'VBN', 'IN', 'NN', 'CD', 'WDT', 'VBZ', 'PDT', 'DT', 'VBN', 'NNS', 'IN', 'PRP$', 'NN', '.']",19
sentiment_analysis,6,2,Context - Dependent Sentiment Analysis in User- Generated Videos,"['Context', '-', 'Dependent', 'Sentiment', 'Analysis', 'in', 'User-', 'Generated', 'Videos']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NNP', ':', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'NNP', 'NNP']",9
sentiment_analysis,6,4,"Multimodal sentiment analysis is a developing area of research , which involves the identification of sentiments in videos .","['Multimodal', 'sentiment', 'analysis', 'is', 'a', 'developing', 'area', 'of', 'research', ',', 'which', 'involves', 'the', 'identification', 'of', 'sentiments', 'in', 'videos', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NN', 'NN', 'VBZ', 'DT', 'VBG', 'NN', 'IN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'NNS', 'IN', 'NN', '.']",19
sentiment_analysis,6,9,"Sentiment analysis is a ' suitcase ' research problem that requires tackling many NLP sub - tasks , e.g. , aspect extraction , named entity recognition , concept extraction , sarcasm detection , personality recognition , and more .","['Sentiment', 'analysis', 'is', 'a', ""'"", 'suitcase', ""'"", 'research', 'problem', 'that', 'requires', 'tackling', 'many', 'NLP', 'sub', '-', 'tasks', ',', 'e.g.', ',', 'aspect', 'extraction', ',', 'named', 'entity', 'recognition', ',', 'concept', 'extraction', ',', 'sarcasm', 'detection', ',', 'personality', 'recognition', ',', 'and', 'more', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'VBZ', 'DT', ""''"", 'NN', ""''"", 'NN', 'NN', 'WDT', 'VBZ', 'VBG', 'JJ', 'NNP', 'SYM', ':', 'NNS', ',', 'NN', ',', 'JJ', 'NN', ',', 'VBN', 'NN', 'NN', ',', 'NN', 'NN', ',', 'JJ', 'NN', ',', 'NN', 'NN', ',', 'CC', 'JJR', '.']",39
sentiment_analysis,6,11,"Emotion recognition further breaks down the inferred polarity into a set of emotions conveyed by the subjective data , e.g. , positive sentiment can be caused by joy or anticipation , while negative sentiment can be caused by fear or disgust .","['Emotion', 'recognition', 'further', 'breaks', 'down', 'the', 'inferred', 'polarity', 'into', 'a', 'set', 'of', 'emotions', 'conveyed', 'by', 'the', 'subjective', 'data', ',', 'e.g.', ',', 'positive', 'sentiment', 'can', 'be', 'caused', 'by', 'joy', 'or', 'anticipation', ',', 'while', 'negative', 'sentiment', 'can', 'be', 'caused', 'by', 'fear', 'or', 'disgust', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'RB', 'VBZ', 'RP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'JJ', 'NNS', ',', 'NN', ',', 'JJ', 'NN', 'MD', 'VB', 'VBN', 'IN', 'NN', 'CC', 'NN', ',', 'IN', 'JJ', 'NN', 'MD', 'VB', 'VBN', 'IN', 'NN', 'CC', 'NN', '.']",42
sentiment_analysis,6,22,"Recently , a number of approaches to multimodal sentiment analysis , producing interesting results , have been proposed .","['Recently', ',', 'a', 'number', 'of', 'approaches', 'to', 'multimodal', 'sentiment', 'analysis', ',', 'producing', 'interesting', 'results', ',', 'have', 'been', 'proposed', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'DT', 'NN', 'IN', 'NNS', 'TO', 'VB', 'NN', 'NN', ',', 'VBG', 'JJ', 'NNS', ',', 'VBP', 'VBN', 'VBN', '.']",19
sentiment_analysis,6,36,"In this paper , we discard such an oversimplifying hypothesis and develop a framework based on long shortterm memory ( LSTM ) that takes a sequence of utterances as input and extracts contextual utterancelevel features .","['In', 'this', 'paper', ',', 'we', 'discard', 'such', 'an', 'oversimplifying', 'hypothesis', 'and', 'develop', 'a', 'framework', 'based', 'on', 'long', 'shortterm', 'memory', '(', 'LSTM', ')', 'that', 'takes', 'a', 'sequence', 'of', 'utterances', 'as', 'input', 'and', 'extracts', 'contextual', 'utterancelevel', 'features', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'PDT', 'DT', 'VBG', 'NN', 'CC', 'VB', 'DT', 'NN', 'VBN', 'IN', 'JJ', 'JJ', 'NN', '(', 'NNP', ')', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'NNS', 'IN', 'NN', 'CC', 'VBZ', 'JJ', 'JJ', 'NNS', '.']",36
sentiment_analysis,6,40,"Our model preserves the sequential order of utterances and enables consecutive utterances to share information , thus providing contextual information to the utterance - level sentiment classification process .","['Our', 'model', 'preserves', 'the', 'sequential', 'order', 'of', 'utterances', 'and', 'enables', 'consecutive', 'utterances', 'to', 'share', 'information', ',', 'thus', 'providing', 'contextual', 'information', 'to', 'the', 'utterance', '-', 'level', 'sentiment', 'classification', 'process', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'CC', 'NNS', 'JJ', 'NNS', 'TO', 'NN', 'NN', ',', 'RB', 'VBG', 'JJ', 'NN', 'TO', 'DT', 'NN', ':', 'NN', 'NN', 'NN', 'NN', '.']",29
sentiment_analysis,6,237,"As expected , trained contextual unimodal features help the hierarchical fusion framework to outperform the non-hierarchical framework .","['As', 'expected', ',', 'trained', 'contextual', 'unimodal', 'features', 'help', 'the', 'hierarchical', 'fusion', 'framework', 'to', 'outperform', 'the', 'non-hierarchical', 'framework', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'VBN', ',', 'VBD', 'JJ', 'JJ', 'NNS', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', '.']",18
sentiment_analysis,6,240,"The non-hierarchical model outperforms the baseline uni - SVM , which confirms that it is the contextsensitive learning paradigm that plays the key role in improving performance over the baseline .","['The', 'non-hierarchical', 'model', 'outperforms', 'the', 'baseline', 'uni', '-', 'SVM', ',', 'which', 'confirms', 'that', 'it', 'is', 'the', 'contextsensitive', 'learning', 'paradigm', 'that', 'plays', 'the', 'key', 'role', 'in', 'improving', 'performance', 'over', 'the', 'baseline', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'NN', 'JJ', ':', 'NN', ',', 'WDT', 'VBZ', 'IN', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'NN', 'IN', 'DT', 'NN', '.']",31
sentiment_analysis,6,242,It is to be noted that both sc - LSTM and bc - LSTM perform quite well on the multimodal emotion recognition and sentiment analysis datasets .,"['It', 'is', 'to', 'be', 'noted', 'that', 'both', 'sc', '-', 'LSTM', 'and', 'bc', '-', 'LSTM', 'perform', 'quite', 'well', 'on', 'the', 'multimodal', 'emotion', 'recognition', 'and', 'sentiment', 'analysis', 'datasets', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'TO', 'VB', 'VBN', 'IN', 'DT', 'SYM', ':', 'NN', 'CC', 'SYM', ':', 'NNP', 'VB', 'RB', 'RB', 'IN', 'DT', 'JJ', 'NN', 'NN', 'CC', 'NN', 'NN', 'NNS', '.']",27
sentiment_analysis,6,243,"Since bc - LSTM has access to both the preceding and following information of the utterance sequence , it performs consistently better on all the datasets over sc - LSTM .","['Since', 'bc', '-', 'LSTM', 'has', 'access', 'to', 'both', 'the', 'preceding', 'and', 'following', 'information', 'of', 'the', 'utterance', 'sequence', ',', 'it', 'performs', 'consistently', 'better', 'on', 'all', 'the', 'datasets', 'over', 'sc', '-', 'LSTM', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'SYM', ':', 'NN', 'VBZ', 'NN', 'TO', 'DT', 'DT', 'NN', 'CC', 'VBG', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'PRP', 'VBZ', 'RB', 'JJR', 'IN', 'PDT', 'DT', 'NNS', 'IN', 'JJ', ':', 'NN', '.']",31
sentiment_analysis,6,245,The performance improvement is in the range of 0.3 % to 1.5 % on MOSI and MOUD datasets .,"['The', 'performance', 'improvement', 'is', 'in', 'the', 'range', 'of', '0.3', '%', 'to', '1.5', '%', 'on', 'MOSI', 'and', 'MOUD', 'datasets', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'CD', 'NN', 'TO', 'CD', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'NNS', '.']",19
sentiment_analysis,6,246,"On the IEMOCAP dataset , the performance improvement of bc - LSTM and sc - LSTM over h- LSTM is in the range of 1 % to 5 % .","['On', 'the', 'IEMOCAP', 'dataset', ',', 'the', 'performance', 'improvement', 'of', 'bc', '-', 'LSTM', 'and', 'sc', '-', 'LSTM', 'over', 'h-', 'LSTM', 'is', 'in', 'the', 'range', 'of', '1', '%', 'to', '5', '%', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNP', 'NN', ',', 'DT', 'NN', 'NN', 'IN', 'NN', ':', 'NNP', 'CC', 'SYM', ':', 'NN', 'IN', 'JJ', 'NNP', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'CD', 'NN', 'TO', 'CD', 'NN', '.']",30
sentiment_analysis,6,248,Every LSTM network variant has outperformed the baseline uni - SVM on all the datasets by the margin of 2 % to 5 % ( see ) .,"['Every', 'LSTM', 'network', 'variant', 'has', 'outperformed', 'the', 'baseline', 'uni', '-', 'SVM', 'on', 'all', 'the', 'datasets', 'by', 'the', 'margin', 'of', '2', '%', 'to', '5', '%', '(', 'see', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NN', 'NN', 'VBZ', 'VBN', 'DT', 'NN', 'JJ', ':', 'NN', 'IN', 'PDT', 'DT', 'NNS', 'IN', 'DT', 'NN', 'IN', 'CD', 'NN', 'TO', 'CD', 'NN', '(', 'NN', ')', '.']",28
sentiment_analysis,6,255,Experimental results in show that the proposed method outperformes by a significant margin .,"['Experimental', 'results', 'in', 'show', 'that', 'the', 'proposed', 'method', 'outperformes', 'by', 'a', 'significant', 'margin', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['JJ', 'NNS', 'IN', 'NN', 'IN', 'DT', 'VBN', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",14
sentiment_analysis,10,2,DialogueRNN : An Attentive RNN for Emotion Detection in Conversations,"['DialogueRNN', ':', 'An', 'Attentive', 'RNN', 'for', 'Emotion', 'Detection', 'in', 'Conversations']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']","['NN', ':', 'DT', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'NNP']",10
sentiment_analysis,10,18,Our proposed DialogueRNN system employs three gated recurrent units ( GRU ) to model these aspects .,"['Our', 'proposed', 'DialogueRNN', 'system', 'employs', 'three', 'gated', 'recurrent', 'units', '(', 'GRU', ')', 'to', 'model', 'these', 'aspects', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'VBN', 'NNP', 'NN', 'VBZ', 'CD', 'JJ', 'NN', 'NNS', '(', 'NNP', ')', 'TO', 'VB', 'DT', 'NNS', '.']",17
sentiment_analysis,10,19,"The incoming utterance is fed into two GRUs called global GRU and party GRU to update the context and party Copyright 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .","['The', 'incoming', 'utterance', 'is', 'fed', 'into', 'two', 'GRUs', 'called', 'global', 'GRU', 'and', 'party', 'GRU', 'to', 'update', 'the', 'context', 'and', 'party', 'Copyright', '2019', ',', 'Association', 'for', 'the', 'Advancement', 'of', 'Artificial', 'Intelligence', '(', 'www.aaai.org', ')', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', 'NNP', 'VBD', 'JJ', 'NNP', 'CC', 'NN', 'NNP', 'TO', 'VB', 'DT', 'NN', 'CC', 'NN', 'NNP', 'CD', ',', 'NNP', 'IN', 'DT', 'NNP', 'IN', 'NNP', 'NNP', '(', 'NN', ')', '.']",34
sentiment_analysis,10,22,The global GRU encodes corresponding party information while encoding an utterance .,"['The', 'global', 'GRU', 'encodes', 'corresponding', 'party', 'information', 'while', 'encoding', 'an', 'utterance', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['DT', 'JJ', 'NNP', 'VBZ', 'VBG', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', '.']",12
sentiment_analysis,10,23,Attending over this GRU gives contextual representation that has information of all preceding utterances by different parties in the conversation .,"['Attending', 'over', 'this', 'GRU', 'gives', 'contextual', 'representation', 'that', 'has', 'information', 'of', 'all', 'preceding', 'utterances', 'by', 'different', 'parties', 'in', 'the', 'conversation', '.']","['B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['VBG', 'IN', 'DT', 'NNP', 'VBZ', 'JJ', 'NN', 'WDT', 'VBZ', 'NN', 'IN', 'DT', 'VBG', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']",21
sentiment_analysis,10,24,The speaker state depends on this context through attention and the speaker 's previous state .,"['The', 'speaker', 'state', 'depends', 'on', 'this', 'context', 'through', 'attention', 'and', 'the', 'speaker', ""'s"", 'previous', 'state', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'NN', 'CC', 'DT', 'NN', 'POS', 'JJ', 'NN', '.']",16
sentiment_analysis,10,25,"This ensures that at time t , the speaker state directly gets information from the speaker 's previous state and global GRU which has information on the preceding parties .","['This', 'ensures', 'that', 'at', 'time', 't', ',', 'the', 'speaker', 'state', 'directly', 'gets', 'information', 'from', 'the', 'speaker', ""'s"", 'previous', 'state', 'and', 'global', 'GRU', 'which', 'has', 'information', 'on', 'the', 'preceding', 'parties', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'VBZ', 'IN', 'IN', 'NN', 'NN', ',', 'DT', 'NN', 'NN', 'RB', 'VBZ', 'NN', 'IN', 'DT', 'NN', 'POS', 'JJ', 'NN', 'CC', 'JJ', 'NNP', 'WDT', 'VBZ', 'NN', 'IN', 'DT', 'VBG', 'NNS', '.']",30
sentiment_analysis,10,26,"Finally , the updated speaker state is fed into the emotion GRU to decode the emotion representation of the given utterance , which is used for emotion classification .","['Finally', ',', 'the', 'updated', 'speaker', 'state', 'is', 'fed', 'into', 'the', 'emotion', 'GRU', 'to', 'decode', 'the', 'emotion', 'representation', 'of', 'the', 'given', 'utterance', ',', 'which', 'is', 'used', 'for', 'emotion', 'classification', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['RB', ',', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'NNP', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'DT', 'VBN', 'NN', ',', 'WDT', 'VBZ', 'VBN', 'IN', 'NN', 'NN', '.']",29
sentiment_analysis,10,27,"At time t , the emotion GRU cell gets the emotion representation of t ? 1 and the speaker state of t .","['At', 'time', 't', ',', 'the', 'emotion', 'GRU', 'cell', 'gets', 'the', 'emotion', 'representation', 'of', 't', '?', '1', 'and', 'the', 'speaker', 'state', 'of', 't', '.']","['B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'NN', ',', 'DT', 'NN', 'NNP', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'NN', '.', 'CD', 'CC', 'DT', 'NN', 'NN', 'IN', 'NN', '.']",23
sentiment_analysis,10,149,c - LSTM : Biredectional LSTM is used to capture the context from the surrounding utterances to generate contextaware utterance representation .,"['c', '-', 'LSTM', ':', 'Biredectional', 'LSTM', 'is', 'used', 'to', 'capture', 'the', 'context', 'from', 'the', 'surrounding', 'utterances', 'to', 'generate', 'contextaware', 'utterance', 'representation', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['SYM', ':', 'NN', ':', 'JJ', 'NNP', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'VBG', 'NNS', 'TO', 'VB', 'NN', 'JJ', 'NN', '.']",22
sentiment_analysis,10,151,c- LSTM+ Att :,"['c-', 'LSTM+', 'Att', ':']","['B-n', 'I-n', 'I-n', 'O']","['JJ', 'NNP', 'NNP', ':']",4
sentiment_analysis,10,152,In this variant attention is applied applied to the c - LSTM output at each timestamp by following Eqs. and .,"['In', 'this', 'variant', 'attention', 'is', 'applied', 'applied', 'to', 'the', 'c', '-', 'LSTM', 'output', 'at', 'each', 'timestamp', 'by', 'following', 'Eqs.', 'and', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'JJ', 'TO', 'DT', 'NN', ':', 'NNP', 'NN', 'IN', 'DT', 'NN', 'IN', 'VBG', 'NNP', 'CC', '.']",21
sentiment_analysis,10,154,TFN :,"['TFN', ':']","['B-n', 'O']","['NN', ':']",2
sentiment_analysis,10,155,This is specific to multimodal scenario .,"['This', 'is', 'specific', 'to', 'multimodal', 'scenario', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'VBZ', 'JJ', 'TO', 'VB', 'NN', '.']",7
sentiment_analysis,10,156,Tensor outer product is used to capture intermodality and intra-modality interactions .,"['Tensor', 'outer', 'product', 'is', 'used', 'to', 'capture', 'intermodality', 'and', 'intra-modality', 'interactions', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'NN', 'CC', 'NN', 'NNS', '.']",12
sentiment_analysis,10,158,"MFN ) : Specific to multimodal scenario , this model utilizes multi-view learning by modeling view - specific and cross - view interactions .","['MFN', ')', ':', 'Specific', 'to', 'multimodal', 'scenario', ',', 'this', 'model', 'utilizes', 'multi-view', 'learning', 'by', 'modeling', 'view', '-', 'specific', 'and', 'cross', '-', 'view', 'interactions', '.']","['B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ')', ':', 'JJ', 'TO', 'VB', 'NN', ',', 'DT', 'NN', 'VBZ', 'NN', 'NN', 'IN', 'VBG', 'NN', ':', 'NN', 'CC', 'NN', ':', 'NN', 'NNS', '.']",24
sentiment_analysis,10,160,CNN : This is identical to our textual feature extractor network ( Section 3.2 ) and it does not use contextual information from the surrounding utterances .,"['CNN', ':', 'This', 'is', 'identical', 'to', 'our', 'textual', 'feature', 'extractor', 'network', '(', 'Section', '3.2', ')', 'and', 'it', 'does', 'not', 'use', 'contextual', 'information', 'from', 'the', 'surrounding', 'utterances', '.']","['B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NN', ':', 'DT', 'VBZ', 'JJ', 'TO', 'PRP$', 'JJ', 'NN', 'NN', 'NN', '(', 'NNP', 'CD', ')', 'CC', 'PRP', 'VBZ', 'RB', 'VB', 'JJ', 'NN', 'IN', 'DT', 'VBG', 'NNS', '.']",27
sentiment_analysis,10,161,"Memnet : As described in , the current utterance is fed to a memory network , where the memories correspond to preceding utterances .","['Memnet', ':', 'As', 'described', 'in', ',', 'the', 'current', 'utterance', 'is', 'fed', 'to', 'a', 'memory', 'network', ',', 'where', 'the', 'memories', 'correspond', 'to', 'preceding', 'utterances', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['NN', ':', 'IN', 'NN', 'IN', ',', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'DT', 'NN', 'NN', ',', 'WRB', 'DT', 'NNS', 'VBP', 'TO', 'VBG', 'NNS', '.']",24
sentiment_analysis,10,162,The output from the memory network is used as the final utterance representation for emotion classification .,"['The', 'output', 'from', 'the', 'memory', 'network', 'is', 'used', 'as', 'the', 'final', 'utterance', 'representation', 'for', 'emotion', 'classification', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",17
sentiment_analysis,10,163,CMN : This state - of - the - art method models utterance context from dialogue history using two distinct GRUs for two speakers .,"['CMN', ':', 'This', 'state', '-', 'of', '-', 'the', '-', 'art', 'method', 'models', 'utterance', 'context', 'from', 'dialogue', 'history', 'using', 'two', 'distinct', 'GRUs', 'for', 'two', 'speakers', '.']","['B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NN', ':', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'NNS', 'VBP', 'NN', 'IN', 'NN', 'NN', 'VBG', 'CD', 'JJ', 'NNP', 'IN', 'CD', 'NNS', '.']",25
sentiment_analysis,10,170,"As expected , on average Di - alogue RNN outperforms all the baseline methods , including the state - of - the - art CMN , on both of the datasets .","['As', 'expected', ',', 'on', 'average', 'Di', '-', 'alogue', 'RNN', 'outperforms', 'all', 'the', 'baseline', 'methods', ',', 'including', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'CMN', ',', 'on', 'both', 'of', 'the', 'datasets', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBN', ',', 'IN', 'JJ', 'NNP', ':', 'NN', 'NNP', 'VBZ', 'PDT', 'DT', 'NN', 'NNS', ',', 'VBG', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNP', ',', 'IN', 'DT', 'IN', 'DT', 'NNS', '.']",32
sentiment_analysis,10,174,"As evidenced by , for IEMOCAP dataset , our model surpasses the state - of - the - art method CMN by 2.77 % accuracy and 3.76 % f 1 - score on average .","['As', 'evidenced', 'by', ',', 'for', 'IEMOCAP', 'dataset', ',', 'our', 'model', 'surpasses', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'method', 'CMN', 'by', '2.77', '%', 'accuracy', 'and', '3.76', '%', 'f', '1', '-', 'score', 'on', 'average', '.']","['O', 'O', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['IN', 'VBN', 'IN', ',', 'IN', 'NNP', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'NNP', 'IN', 'CD', 'NN', 'NN', 'CC', 'CD', 'NN', 'NN', 'CD', ':', 'NN', 'IN', 'NN', '.']",35
sentiment_analysis,10,182,"AVEC DialogueRNN outperforms CMN for valence , arousal , expectancy , and power attributes ; see .","['AVEC', 'DialogueRNN', 'outperforms', 'CMN', 'for', 'valence', ',', 'arousal', ',', 'expectancy', ',', 'and', 'power', 'attributes', ';', 'see', '.']","['B-n', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['NNP', 'NNP', 'VBZ', 'NNP', 'IN', 'NN', ',', 'NN', ',', 'NN', ',', 'CC', 'NN', 'NNS', ':', 'VB', '.']",17
sentiment_analysis,10,185,DialogueRNN vs. DialogueRNN Variants,"['DialogueRNN', 'vs.', 'DialogueRNN', 'Variants']","['B-n', 'I-n', 'I-n', 'I-n']","['NNP', 'FW', 'NNP', 'NNS']",4
sentiment_analysis,10,187,DialogueRNN l :,"['DialogueRNN', 'l', ':']","['B-n', 'I-n', 'O']","['NNP', 'NN', ':']",3
sentiment_analysis,10,188,"Following , using explicit listener state update yields slightly worse performance than regular DialogueRNN .","['Following', ',', 'using', 'explicit', 'listener', 'state', 'update', 'yields', 'slightly', 'worse', 'performance', 'than', 'regular', 'DialogueRNN', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['VBG', ',', 'VBG', 'JJ', 'NN', 'NN', 'JJ', 'NNS', 'RB', 'JJR', 'NN', 'IN', 'JJ', 'NNP', '.']",15
sentiment_analysis,10,193,"BiDialogueRNN : Since BiDialogueRNN captures context from the future utterances , we expect improved performance from it over DialogueRNN .","['BiDialogueRNN', ':', 'Since', 'BiDialogueRNN', 'captures', 'context', 'from', 'the', 'future', 'utterances', ',', 'we', 'expect', 'improved', 'performance', 'from', 'it', 'over', 'DialogueRNN', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', ':', 'IN', 'NNP', 'NNS', 'VBP', 'IN', 'DT', 'NN', 'NNS', ',', 'PRP', 'VBP', 'VBN', 'NN', 'IN', 'PRP', 'IN', 'NNP', '.']",20
sentiment_analysis,10,194,"This is confirmed in , where BiDialogueRNN outperforms Dialogue RNN on average on both datasets .","['This', 'is', 'confirmed', 'in', ',', 'where', 'BiDialogueRNN', 'outperforms', 'Dialogue', 'RNN', 'on', 'average', 'on', 'both', 'datasets', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O']","['DT', 'VBZ', 'VBN', 'IN', ',', 'WRB', 'NNP', 'VBZ', 'NNP', 'NNP', 'IN', 'NN', 'IN', 'DT', 'NNS', '.']",16
sentiment_analysis,33,2,Convolutional Neural Networks with Recurrent Neural Filters,"['Convolutional', 'Neural', 'Networks', 'with', 'Recurrent', 'Neural', 'Filters']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",7
sentiment_analysis,33,7,"In this work , we model convolution filters with RNNs that naturally capture compositionality and long - term dependencies in language .","['In', 'this', 'work', ',', 'we', 'model', 'convolution', 'filters', 'with', 'RNNs', 'that', 'naturally', 'capture', 'compositionality', 'and', 'long', '-', 'term', 'dependencies', 'in', 'language', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'IN', 'NNP', 'WDT', 'RB', 'VBP', 'NN', 'CC', 'JJ', ':', 'NN', 'NNS', 'IN', 'NN', '.']",22
sentiment_analysis,33,18,"To overcome this , we propose to employ recurrent neural networks ( RNNs ) as convolution filters of CNN systems for various NLP tasks .","['To', 'overcome', 'this', ',', 'we', 'propose', 'to', 'employ', 'recurrent', 'neural', 'networks', '(', 'RNNs', ')', 'as', 'convolution', 'filters', 'of', 'CNN', 'systems', 'for', 'various', 'NLP', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'DT', ',', 'PRP', 'VBP', 'TO', 'VB', 'JJ', 'JJ', 'NNS', '(', 'NNP', ')', 'IN', 'NN', 'NNS', 'IN', 'NNP', 'NNS', 'IN', 'JJ', 'NNP', 'NNS', '.']",25
sentiment_analysis,33,19,"Our recurrent neural filters ( RNFs ) can naturally deal with language compositionality with a recurrent function that models word relations , and they are also able to implicitly model long - term dependencies .","['Our', 'recurrent', 'neural', 'filters', '(', 'RNFs', ')', 'can', 'naturally', 'deal', 'with', 'language', 'compositionality', 'with', 'a', 'recurrent', 'function', 'that', 'models', 'word', 'relations', ',', 'and', 'they', 'are', 'also', 'able', 'to', 'implicitly', 'model', 'long', '-', 'term', 'dependencies', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'JJ', 'JJ', 'NNS', '(', 'NNP', ')', 'MD', 'RB', 'VB', 'IN', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'NNS', 'NN', 'NNS', ',', 'CC', 'PRP', 'VBP', 'RB', 'JJ', 'TO', 'VB', 'NN', 'JJ', ':', 'NN', 'NNS', '.']",35
sentiment_analysis,33,20,"RNFs are typically applied to word sequences of moderate lengths , which alleviates some well - known drawbacks of RNNs , including their vulnerability to the gradient vanishing and exploding problems .","['RNFs', 'are', 'typically', 'applied', 'to', 'word', 'sequences', 'of', 'moderate', 'lengths', ',', 'which', 'alleviates', 'some', 'well', '-', 'known', 'drawbacks', 'of', 'RNNs', ',', 'including', 'their', 'vulnerability', 'to', 'the', 'gradient', 'vanishing', 'and', 'exploding', 'problems', '.']","['B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBP', 'RB', 'VBN', 'TO', 'NN', 'NNS', 'IN', 'JJ', 'NNS', ',', 'WDT', 'VBZ', 'DT', 'RB', ':', 'VBN', 'NNS', 'IN', 'NNP', ',', 'VBG', 'PRP$', 'NN', 'TO', 'DT', 'NN', 'NN', 'CC', 'VBG', 'NNS', '.']",32
sentiment_analysis,33,21,"As in conventional CNNs , the computation of the convolution operation with RNFs can be easily parallelized .","['As', 'in', 'conventional', 'CNNs', ',', 'the', 'computation', 'of', 'the', 'convolution', 'operation', 'with', 'RNFs', 'can', 'be', 'easily', 'parallelized', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['IN', 'IN', 'JJ', 'NNP', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'NNP', 'MD', 'VB', 'RB', 'VBN', '.']",18
sentiment_analysis,33,22,"As a result , RNF - based CNN models can be 3 - 8 x faster than their RNN counterparts .","['As', 'a', 'result', ',', 'RNF', '-', 'based', 'CNN', 'models', 'can', 'be', '3', '-', '8', 'x', 'faster', 'than', 'their', 'RNN', 'counterparts', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'NNP', ':', 'VBN', 'NNP', 'NNS', 'MD', 'VB', 'CD', ':', 'CD', 'NN', 'RBR', 'IN', 'PRP$', 'NNP', 'NNS', '.']",21
sentiment_analysis,33,23,We present two RNF - based CNN architectures for sentence classification and answer sentence selection problems .,"['We', 'present', 'two', 'RNF', '-', 'based', 'CNN', 'architectures', 'for', 'sentence', 'classification', 'and', 'answer', 'sentence', 'selection', 'problems', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'NNP', ':', 'VBN', 'NNP', 'NNS', 'IN', 'NN', 'NN', 'CC', 'VB', 'NN', 'NN', 'NNS', '.']",17
sentiment_analysis,33,75,We consider CNN variants with linear filters and RNFs.,"['We', 'consider', 'CNN', 'variants', 'with', 'linear', 'filters', 'and', 'RNFs.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n']","['PRP', 'VBP', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', 'CC', 'NNP']",9
sentiment_analysis,33,76,"For RNFs , we adopt two implementations based on GRUs and LSTMs respectively .","['For', 'RNFs', ',', 'we', 'adopt', 'two', 'implementations', 'based', 'on', 'GRUs', 'and', 'LSTMs', 'respectively', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'NNP', ',', 'PRP', 'VBP', 'CD', 'NNS', 'VBN', 'IN', 'NNP', 'CC', 'NNP', 'RB', '.']",14
sentiment_analysis,33,77,"We also compare against the following RNN variants : GRU , LSTM , GRU with max pooling , and LSTM with max pooling .","['We', 'also', 'compare', 'against', 'the', 'following', 'RNN', 'variants', ':', 'GRU', ',', 'LSTM', ',', 'GRU', 'with', 'max', 'pooling', ',', 'and', 'LSTM', 'with', 'max', 'pooling', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'IN', 'DT', 'JJ', 'JJ', 'NNS', ':', 'NNP', ',', 'NNP', ',', 'NNP', 'IN', 'NN', 'NN', ',', 'CC', 'NNP', 'IN', 'NN', 'NN', '.']",24
sentiment_analysis,33,81,"In particular , CNN - RNF - LSTM achieves 53.4 % and 90.0 % accuracies on the fine - grained and binary sentiment classification tasks respectively , which match the state - of the - art results on the Stanford Sentiment Treebank .","['In', 'particular', ',', 'CNN', '-', 'RNF', '-', 'LSTM', 'achieves', '53.4', '%', 'and', '90.0', '%', 'accuracies', 'on', 'the', 'fine', '-', 'grained', 'and', 'binary', 'sentiment', 'classification', 'tasks', 'respectively', ',', 'which', 'match', 'the', 'state', '-', 'of', 'the', '-', 'art', 'results', 'on', 'the', 'Stanford', 'Sentiment', 'Treebank', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', ',', 'NNP', ':', 'NNP', ':', 'NNP', 'VBZ', 'CD', 'NN', 'CC', 'CD', 'NN', 'NNS', 'IN', 'DT', 'JJ', ':', 'VBN', 'CC', 'JJ', 'NN', 'NN', 'NNS', 'RB', ',', 'WDT', 'VBP', 'DT', 'NN', ':', 'IN', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'NNP', 'NNP', '.']",43
sentiment_analysis,33,82,"CNN - RNF - LSTM also obtains competitive results on answer sentence selection datasets , despite the simple model architecture compared to state - of - the - art systems .","['CNN', '-', 'RNF', '-', 'LSTM', 'also', 'obtains', 'competitive', 'results', 'on', 'answer', 'sentence', 'selection', 'datasets', ',', 'despite', 'the', 'simple', 'model', 'architecture', 'compared', 'to', 'state', '-', 'of', '-', 'the', '-', 'art', 'systems', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', ':', 'NNP', 'RB', 'VBZ', 'JJ', 'NNS', 'IN', 'JJR', 'NN', 'NN', 'NNS', ',', 'IN', 'DT', 'JJ', 'NN', 'NN', 'VBN', 'TO', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', '.']",31
sentiment_analysis,33,83,"Conventional RNN models clearly benefit from max pooling , especially on the task of answer sentence selection .","['Conventional', 'RNN', 'models', 'clearly', 'benefit', 'from', 'max', 'pooling', ',', 'especially', 'on', 'the', 'task', 'of', 'answer', 'sentence', 'selection', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['JJ', 'NNP', 'NNS', 'RB', 'VBP', 'IN', 'NN', 'NN', ',', 'RB', 'IN', 'DT', 'NN', 'IN', 'JJR', 'NN', 'NN', '.']",18
sentiment_analysis,33,87,"As a result , RNF - based CNN models perform consistently better than max - pooled RNN models .","['As', 'a', 'result', ',', 'RNF', '-', 'based', 'CNN', 'models', 'perform', 'consistently', 'better', 'than', 'max', '-', 'pooled', 'RNN', 'models', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'NNP', ':', 'VBN', 'NNP', 'NNS', 'VBP', 'RB', 'JJR', 'IN', 'SYM', ':', 'VBN', 'NNP', 'NNS', '.']",19
sentiment_analysis,29,2,Aspect Level Sentiment Classification with Attention - over - Attention Neural Networks,"['Aspect', 'Level', 'Sentiment', 'Classification', 'with', 'Attention', '-', 'over', '-', 'Attention', 'Neural', 'Networks']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', ':', 'IN', ':', 'NNP', 'NNP', 'NNP']",12
sentiment_analysis,29,4,Aspect - level sentiment classification aims to identify the sentiment expressed towards some aspects given context sentences .,"['Aspect', '-', 'level', 'sentiment', 'classification', 'aims', 'to', 'identify', 'the', 'sentiment', 'expressed', 'towards', 'some', 'aspects', 'given', 'context', 'sentences', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', 'NN', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'VBD', 'NNS', 'DT', 'NNS', 'VBN', 'JJ', 'NNS', '.']",18
sentiment_analysis,29,24,"Because of advantages of neural networks , we approach this aspect level sentiment classification problem based on long short - term memory ( LSTM ) neural networks .","['Because', 'of', 'advantages', 'of', 'neural', 'networks', ',', 'we', 'approach', 'this', 'aspect', 'level', 'sentiment', 'classification', 'problem', 'based', 'on', 'long', 'short', '-', 'term', 'memory', '(', 'LSTM', ')', 'neural', 'networks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'IN', 'NNS', 'IN', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'NN', 'NN', 'VBN', 'IN', 'JJ', 'JJ', ':', 'NN', 'NN', '(', 'NNP', ')', 'JJ', 'NNS', '.']",28
sentiment_analysis,29,25,"Previous LSTM - based methods mainly focus on modeling texts separately , while our approach models aspects and texts simultaneously using LSTMs .","['Previous', 'LSTM', '-', 'based', 'methods', 'mainly', 'focus', 'on', 'modeling', 'texts', 'separately', ',', 'while', 'our', 'approach', 'models', 'aspects', 'and', 'texts', 'simultaneously', 'using', 'LSTMs', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['JJ', 'NNP', ':', 'VBN', 'NNS', 'RB', 'VBP', 'IN', 'VBG', 'NN', 'RB', ',', 'IN', 'PRP$', 'NN', 'NNS', 'NNS', 'CC', 'NN', 'RB', 'VBG', 'NNP', '.']",23
sentiment_analysis,29,26,"Furthermore , the target representation and text representation generated from LSTMs interact with each other by an attention - over - attention ( AOA ) module .","['Furthermore', ',', 'the', 'target', 'representation', 'and', 'text', 'representation', 'generated', 'from', 'LSTMs', 'interact', 'with', 'each', 'other', 'by', 'an', 'attention', '-', 'over', '-', 'attention', '(', 'AOA', ')', 'module', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'NN', 'CC', 'JJ', 'NN', 'VBD', 'IN', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'IN', 'DT', 'NN', ':', 'IN', ':', 'NN', '(', 'NNP', ')', 'NN', '.']",27
sentiment_analysis,29,27,AOA automatically generates mutual attentions not only from aspect - to - text but also text - to - aspect .,"['AOA', 'automatically', 'generates', 'mutual', 'attentions', 'not', 'only', 'from', 'aspect', '-', 'to', '-', 'text', 'but', 'also', 'text', '-', 'to', '-', 'aspect', '.']","['B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'RB', 'VBZ', 'JJ', 'NNS', 'RB', 'RB', 'IN', 'JJ', ':', 'TO', ':', 'NN', 'CC', 'RB', 'JJ', ':', 'TO', ':', 'NN', '.']",21
sentiment_analysis,29,33,That is why we choose AOA to attend to the most important parts in both aspect and sentence .,"['That', 'is', 'why', 'we', 'choose', 'AOA', 'to', 'attend', 'to', 'the', 'most', 'important', 'parts', 'in', 'both', 'aspect', 'and', 'sentence', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'WRB', 'PRP', 'VBP', 'NNP', 'TO', 'VB', 'TO', 'DT', 'RBS', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'CC', 'NN', '.']",19
sentiment_analysis,29,120,"In experiments , we first randomly select 20 % of training data as validation set to tune the hyperparameters .","['In', 'experiments', ',', 'we', 'first', 'randomly', 'select', '20', '%', 'of', 'training', 'data', 'as', 'validation', 'set', 'to', 'tune', 'the', 'hyperparameters', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['IN', 'NNS', ',', 'PRP', 'RB', 'VBD', 'JJ', 'CD', 'NN', 'IN', 'VBG', 'NNS', 'IN', 'NN', 'VBN', 'TO', 'VB', 'DT', 'NNS', '.']",20
sentiment_analysis,29,121,"All weight matrices are randomly initialized from uniform distribution U ( ?10 ?4 , 10 ?4 ) and all bias terms are set to zero .","['All', 'weight', 'matrices', 'are', 'randomly', 'initialized', 'from', 'uniform', 'distribution', 'U', '(', '?10', '?4', ',', '10', '?4', ')', 'and', 'all', 'bias', 'terms', 'are', 'set', 'to', 'zero', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'JJ', 'NN', 'NNP', '(', 'NNP', 'NNP', ',', 'CD', 'NN', ')', 'CC', 'DT', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'CD', '.']",26
sentiment_analysis,29,122,The L 2 regularization coefficient is set to 10 ? 4 and the dropout keep rate is set to 0.2 .,"['The', 'L', '2', 'regularization', 'coefficient', 'is', 'set', 'to', '10', '?', '4', 'and', 'the', 'dropout', 'keep', 'rate', 'is', 'set', 'to', '0.2', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NNP', 'CD', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.', 'CD', 'CC', 'DT', 'NN', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",21
sentiment_analysis,29,123,The word embeddings are initialized with 300 - dimensional Glove vectors and are fixed during training .,"['The', 'word', 'embeddings', 'are', 'initialized', 'with', '300', '-', 'dimensional', 'Glove', 'vectors', 'and', 'are', 'fixed', 'during', 'training', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'CD', ':', 'JJ', 'NNP', 'NNS', 'CC', 'VBP', 'VBN', 'IN', 'NN', '.']",17
sentiment_analysis,29,124,"For the out of vocabulary words we initialize them randomly from uniform distribution U ( ? 0.01 , 0.01 ) .","['For', 'the', 'out', 'of', 'vocabulary', 'words', 'we', 'initialize', 'them', 'randomly', 'from', 'uniform', 'distribution', 'U', '(', '?', '0.01', ',', '0.01', ')', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'PRP', 'VBP', 'PRP', 'RB', 'IN', 'JJ', 'NN', 'NNP', '(', '.', 'CD', ',', 'CD', ')', '.']",21
sentiment_analysis,29,125,The dimension of LSTM hidden states is set to 150 .,"['The', 'dimension', 'of', 'LSTM', 'hidden', 'states', 'is', 'set', 'to', '150', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NNP', 'JJ', 'NNS', 'VBZ', 'VBN', 'TO', 'CD', '.']",11
sentiment_analysis,29,126,The initial learning rate is 0.01 for the Adam optimizer .,"['The', 'initial', 'learning', 'rate', 'is', '0.01', 'for', 'the', 'Adam', 'optimizer', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'CD', 'IN', 'DT', 'NNP', 'NN', '.']",11
sentiment_analysis,29,127,"If the training loss does not drop after every three epochs , we decrease the learning rate by half .","['If', 'the', 'training', 'loss', 'does', 'not', 'drop', 'after', 'every', 'three', 'epochs', ',', 'we', 'decrease', 'the', 'learning', 'rate', 'by', 'half', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NN', 'NN', 'VBZ', 'RB', 'VB', 'IN', 'DT', 'CD', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'NN', '.']",20
sentiment_analysis,29,128,The batch size is set as 25 .,"['The', 'batch', 'size', 'is', 'set', 'as', '25', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', '.']",8
sentiment_analysis,29,134,"Majority is a basic baseline method , which assigns the largest sentiment polarity in the training set to each sample in the test set .","['Majority', 'is', 'a', 'basic', 'baseline', 'method', ',', 'which', 'assigns', 'the', 'largest', 'sentiment', 'polarity', 'in', 'the', 'training', 'set', 'to', 'each', 'sample', 'in', 'the', 'test', 'set', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'JJS', 'NN', 'NN', 'IN', 'DT', 'NN', 'VBN', 'TO', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",25
sentiment_analysis,29,135,"LSTM uses one LSTM network to model the sentence , and the last hidden state is used as the sentence representation for the final classification .","['LSTM', 'uses', 'one', 'LSTM', 'network', 'to', 'model', 'the', 'sentence', ',', 'and', 'the', 'last', 'hidden', 'state', 'is', 'used', 'as', 'the', 'sentence', 'representation', 'for', 'the', 'final', 'classification', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'CD', 'NNP', 'NN', 'TO', 'VB', 'DT', 'NN', ',', 'CC', 'DT', 'JJ', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",26
sentiment_analysis,29,136,TD - LSTM uses two LSTM networks to model the preceding and following contexts surrounding the aspect term .,"['TD', '-', 'LSTM', 'uses', 'two', 'LSTM', 'networks', 'to', 'model', 'the', 'preceding', 'and', 'following', 'contexts', 'surrounding', 'the', 'aspect', 'term', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', ':', 'JJ', 'NNS', 'CD', 'NNP', 'NNS', 'TO', 'VB', 'DT', 'NN', 'CC', 'VBG', 'NN', 'VBG', 'DT', 'JJ', 'NN', '.']",19
sentiment_analysis,29,138,AT - LSTM first models the sentence via a LSTM model .,"['AT', '-', 'LSTM', 'first', 'models', 'the', 'sentence', 'via', 'a', 'LSTM', 'model', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'RB', 'NNS', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",12
sentiment_analysis,29,141,ATAE - LSTM further extends AT - LSTM by appending the aspect embedding into each word vector .,"['ATAE', '-', 'LSTM', 'further', 'extends', 'AT', '-', 'LSTM', 'by', 'appending', 'the', 'aspect', 'embedding', 'into', 'each', 'word', 'vector', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'JJ', 'NNS', 'NNP', ':', 'NN', 'IN', 'VBG', 'DT', 'NN', 'VBG', 'IN', 'DT', 'NN', 'NN', '.']",18
sentiment_analysis,29,142,IAN uses two LSTM networks to model the sentence and aspect term respectively .,"['IAN', 'uses', 'two', 'LSTM', 'networks', 'to', 'model', 'the', 'sentence', 'and', 'aspect', 'term', 'respectively', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['JJ', 'VBZ', 'CD', 'NNP', 'NNS', 'TO', 'VB', 'DT', 'NN', 'CC', 'JJ', 'NN', 'RB', '.']",14
sentiment_analysis,29,150,"In our implementation , we found that the performance fluctuates with different random initialization , which is a well - known issue in training neural networks .","['In', 'our', 'implementation', ',', 'we', 'found', 'that', 'the', 'performance', 'fluctuates', 'with', 'different', 'random', 'initialization', ',', 'which', 'is', 'a', 'well', '-', 'known', 'issue', 'in', 'training', 'neural', 'networks', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PRP$', 'NN', ',', 'PRP', 'VBD', 'IN', 'DT', 'NN', 'VBZ', 'IN', 'JJ', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'DT', 'RB', ':', 'VBN', 'NN', 'IN', 'VBG', 'JJ', 'NNS', '.']",27
sentiment_analysis,29,153,"On average , our algorithm is better than these baseline methods and our best trained model outperforms them in a large margin .","['On', 'average', ',', 'our', 'algorithm', 'is', 'better', 'than', 'these', 'baseline', 'methods', 'and', 'our', 'best', 'trained', 'model', 'outperforms', 'them', 'in', 'a', 'large', 'margin', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'JJR', 'IN', 'DT', 'JJ', 'NNS', 'CC', 'PRP$', 'JJS', 'JJ', 'NN', 'VBZ', 'PRP', 'IN', 'DT', 'JJ', 'NN', '.']",23
sentiment_analysis,17,2,Improved Semantic Representations From Tree - Structured Long Short - Term Memory Networks,"['Improved', 'Semantic', 'Representations', 'From', 'Tree', '-', 'Structured', 'Long', 'Short', '-', 'Term', 'Memory', 'Networks']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'JJ', 'NNS', 'IN', 'NNP', ':', 'VBD', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'NNP']",13
sentiment_analysis,17,8,"Tree - LSTMs outperform all existing systems and strong LSTM baselines on two tasks : predicting the semantic relatedness of two sentences ( Sem Eval 2014 , Task 1 ) and sentiment classification ( Stanford Sentiment Treebank ) .","['Tree', '-', 'LSTMs', 'outperform', 'all', 'existing', 'systems', 'and', 'strong', 'LSTM', 'baselines', 'on', 'two', 'tasks', ':', 'predicting', 'the', 'semantic', 'relatedness', 'of', 'two', 'sentences', '(', 'Sem', 'Eval', '2014', ',', 'Task', '1', ')', 'and', 'sentiment', 'classification', '(', 'Stanford', 'Sentiment', 'Treebank', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NNP', 'IN', 'DT', 'VBG', 'NNS', 'CC', 'JJ', 'NNP', 'NNS', 'IN', 'CD', 'NNS', ':', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NNS', '(', 'NNP', 'NNP', 'CD', ',', 'NNP', 'CD', ')', 'CC', 'JJ', 'NN', '(', 'NNP', 'NNP', 'NNP', ')', '.']",39
sentiment_analysis,17,33,"In this paper , we introduce a generalization of the standard LSTM architecture to tree - structured network topologies and show its superiority for representing sentence meaning over a sequential LSTM .","['In', 'this', 'paper', ',', 'we', 'introduce', 'a', 'generalization', 'of', 'the', 'standard', 'LSTM', 'architecture', 'to', 'tree', '-', 'structured', 'network', 'topologies', 'and', 'show', 'its', 'superiority', 'for', 'representing', 'sentence', 'meaning', 'over', 'a', 'sequential', 'LSTM', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'NN', 'TO', 'VB', ':', 'VBN', 'NN', 'NNS', 'CC', 'VB', 'PRP$', 'NN', 'IN', 'VBG', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNP', '.']",32
sentiment_analysis,17,34,"While the standard LSTM composes its hidden state from the input at the current time step and the hidden state of the LSTM unit in the previous time step , the tree - structured LSTM , or Tree - LSTM , composes its state from an input vector and the hidden states of arbitrarily many child units .","['While', 'the', 'standard', 'LSTM', 'composes', 'its', 'hidden', 'state', 'from', 'the', 'input', 'at', 'the', 'current', 'time', 'step', 'and', 'the', 'hidden', 'state', 'of', 'the', 'LSTM', 'unit', 'in', 'the', 'previous', 'time', 'step', ',', 'the', 'tree', '-', 'structured', 'LSTM', ',', 'or', 'Tree', '-', 'LSTM', ',', 'composes', 'its', 'state', 'from', 'an', 'input', 'vector', 'and', 'the', 'hidden', 'states', 'of', 'arbitrarily', 'many', 'child', 'units', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NNP', 'VBZ', 'PRP$', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'CC', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'DT', 'JJ', ':', 'JJ', 'NNP', ',', 'CC', 'NNP', ':', 'NNP', ',', 'VBZ', 'PRP$', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'DT', 'JJ', 'NNS', 'IN', 'RB', 'JJ', 'JJ', 'NNS', '.']",58
sentiment_analysis,17,39,Implementations of our models and experiments are available at https :// github.com/stanfordnlp/treelstm.,"['Implementations', 'of', 'our', 'models', 'and', 'experiments', 'are', 'available', 'at', 'https', '://', 'github.com/stanfordnlp/treelstm.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNS', 'IN', 'PRP$', 'NNS', 'CC', 'NNS', 'VBP', 'JJ', 'IN', 'JJ', 'CD', 'NN']",12
sentiment_analysis,17,182,The hyperparameters for our models were tuned on the development set for each task .,"['The', 'hyperparameters', 'for', 'our', 'models', 'were', 'tuned', 'on', 'the', 'development', 'set', 'for', 'each', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'PRP$', 'NNS', 'VBD', 'VBN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', '.']",15
sentiment_analysis,17,183,We initialized our word representations using publicly available 300 - dimensional Glove vectors,"['We', 'initialized', 'our', 'word', 'representations', 'using', 'publicly', 'available', '300', '-', 'dimensional', 'Glove', 'vectors']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['PRP', 'VBD', 'PRP$', 'NN', 'NNS', 'VBG', 'RB', 'JJ', 'CD', ':', 'JJ', 'NNP', 'NNS']",13
sentiment_analysis,17,185,"For the sentiment classification task , word representations were updated during training with a learning rate of 0.1 .","['For', 'the', 'sentiment', 'classification', 'task', ',', 'word', 'representations', 'were', 'updated', 'during', 'training', 'with', 'a', 'learning', 'rate', 'of', '0.1', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NN', 'NN', 'NN', ',', 'NN', 'NNS', 'VBD', 'VBN', 'IN', 'VBG', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', '.']",19
sentiment_analysis,17,186,"For the semantic relatedness task , word representations were held fixed as we did not observe any significant improvement when the representations were tuned .","['For', 'the', 'semantic', 'relatedness', 'task', ',', 'word', 'representations', 'were', 'held', 'fixed', 'as', 'we', 'did', 'not', 'observe', 'any', 'significant', 'improvement', 'when', 'the', 'representations', 'were', 'tuned', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', 'NN', ',', 'NN', 'NNS', 'VBD', 'VBN', 'VBN', 'IN', 'PRP', 'VBD', 'RB', 'VB', 'DT', 'JJ', 'NN', 'WRB', 'DT', 'NNS', 'VBD', 'VBN', '.']",25
sentiment_analysis,17,187,Our models were trained using AdaGrad with a learning rate of 0.05 and a minibatch size of 25 .,"['Our', 'models', 'were', 'trained', 'using', 'AdaGrad', 'with', 'a', 'learning', 'rate', 'of', '0.05', 'and', 'a', 'minibatch', 'size', 'of', '25', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP$', 'NNS', 'VBD', 'VBN', 'VBG', 'NNP', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', 'CC', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",19
sentiment_analysis,17,188,The model parameters were regularized with a per-minibatch L2 regularization strength of 10 ?4 .,"['The', 'model', 'parameters', 'were', 'regularized', 'with', 'a', 'per-minibatch', 'L2', 'regularization', 'strength', 'of', '10', '?4', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NNS', 'VBD', 'VBN', 'IN', 'DT', 'JJ', 'NNP', 'NN', 'NN', 'IN', 'CD', 'NNS', '.']",15
sentiment_analysis,17,189,The sentiment classifier was additionally regularized using dropout with a dropout rate of 0.5 .,"['The', 'sentiment', 'classifier', 'was', 'additionally', 'regularized', 'using', 'dropout', 'with', 'a', 'dropout', 'rate', 'of', '0.5', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBD', 'RB', 'VBN', 'VBG', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",15
sentiment_analysis,2,2,A Position - aware Bidirectional Attention Network for Aspect - level Sentiment Analysis,"['A', 'Position', '-', 'aware', 'Bidirectional', 'Attention', 'Network', 'for', 'Aspect', '-', 'level', 'Sentiment', 'Analysis']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['DT', 'NNP', ':', 'JJ', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', ':', 'NN', 'NN', 'NN']",13
sentiment_analysis,2,18,"Sentiment analysis , also known as opinion mining , is a vital task in Natural Language Processing ( NLP ) .","['Sentiment', 'analysis', ',', 'also', 'known', 'as', 'opinion', 'mining', ',', 'is', 'a', 'vital', 'task', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', ',', 'RB', 'VBN', 'IN', 'NN', 'NN', ',', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', '.']",21
sentiment_analysis,2,39,"Inspired by this , we go one step further and propose a position - aware bidirectional attention network ( PBAN ) based on bidirectional Gated Recurrent Units ( Bi - GRU ) .","['Inspired', 'by', 'this', ',', 'we', 'go', 'one', 'step', 'further', 'and', 'propose', 'a', 'position', '-', 'aware', 'bidirectional', 'attention', 'network', '(', 'PBAN', ')', 'based', 'on', 'bidirectional', 'Gated', 'Recurrent', 'Units', '(', 'Bi', '-', 'GRU', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'DT', ',', 'PRP', 'VBP', 'CD', 'NN', 'RBR', 'CC', 'VB', 'DT', 'NN', ':', 'JJ', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBN', 'IN', 'JJ', 'NNP', 'NNP', 'NNP', '(', 'NNP', ':', 'NNP', ')', '.']",33
sentiment_analysis,2,40,"In addition to utilizing the position information , PBAN also mutually models the relationship between the sentence and different words in the aspect term by adopting a bidirectional attention mechanism .","['In', 'addition', 'to', 'utilizing', 'the', 'position', 'information', ',', 'PBAN', 'also', 'mutually', 'models', 'the', 'relationship', 'between', 'the', 'sentence', 'and', 'different', 'words', 'in', 'the', 'aspect', 'term', 'by', 'adopting', 'a', 'bidirectional', 'attention', 'mechanism', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'VBG', 'DT', 'NN', 'NN', ',', 'NNP', 'RB', 'RB', 'NNS', 'DT', 'NN', 'IN', 'DT', 'NN', 'CC', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'NN', '.']",31
sentiment_analysis,2,42,"1 ) Obtaining position information of each word in corresponding sentence based on the current aspect term , then converting the position information into position embedding .","['1', ')', 'Obtaining', 'position', 'information', 'of', 'each', 'word', 'in', 'corresponding', 'sentence', 'based', 'on', 'the', 'current', 'aspect', 'term', ',', 'then', 'converting', 'the', 'position', 'information', 'into', 'position', 'embedding', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['CD', ')', 'VBG', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'VBG', 'NN', 'VBN', 'IN', 'DT', 'JJ', 'JJ', 'NN', ',', 'RB', 'VBG', 'DT', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",27
sentiment_analysis,2,43,2 ) The PBAN composes of two Bi - GRU networks focusing on extracting the aspectlevel features and sentence - level features respectively .,"['2', ')', 'The', 'PBAN', 'composes', 'of', 'two', 'Bi', '-', 'GRU', 'networks', 'focusing', 'on', 'extracting', 'the', 'aspectlevel', 'features', 'and', 'sentence', '-', 'level', 'features', 'respectively', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['CD', ')', 'DT', 'NNP', 'NNS', 'IN', 'CD', 'NNP', ':', 'NNP', 'NNS', 'VBG', 'IN', 'VBG', 'DT', 'NN', 'NNS', 'CC', 'NN', ':', 'NN', 'NNS', 'RB', '.']",24
sentiment_analysis,2,44,3 ) Using the bidirectional attention mechanism to model the mutual relation between aspect term and its corresponding sentence .,"['3', ')', 'Using', 'the', 'bidirectional', 'attention', 'mechanism', 'to', 'model', 'the', 'mutual', 'relation', 'between', 'aspect', 'term', 'and', 'its', 'corresponding', 'sentence', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['LS', ')', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'CC', 'PRP$', 'JJ', 'NN', '.']",20
sentiment_analysis,2,120,"In our experiments , all word embedding are initialized by the pre-trained Glove vector 2 .","['In', 'our', 'experiments', ',', 'all', 'word', 'embedding', 'are', 'initialized', 'by', 'the', 'pre-trained', 'Glove', 'vector', '2', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'PRP$', 'NNS', ',', 'DT', 'NN', 'VBG', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NNP', 'NN', 'CD', '.']",16
sentiment_analysis,2,121,"All the weight matrices are given the initial value by sampling from the uniform distribution U ( ?0.1 , 0.1 ) , and all the biases are set to zero .","['All', 'the', 'weight', 'matrices', 'are', 'given', 'the', 'initial', 'value', 'by', 'sampling', 'from', 'the', 'uniform', 'distribution', 'U', '(', '?0.1', ',', '0.1', ')', ',', 'and', 'all', 'the', 'biases', 'are', 'set', 'to', 'zero', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['PDT', 'DT', 'NN', 'NNS', 'VBP', 'VBN', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NNP', '(', 'NNP', ',', 'CD', ')', ',', 'CC', 'PDT', 'DT', 'NNS', 'VBP', 'VBN', 'TO', 'CD', '.']",31
sentiment_analysis,2,122,"The dimension of the word embedding and aspect term embedding are set to 300 , and the number of the hidden units are set to 200 .","['The', 'dimension', 'of', 'the', 'word', 'embedding', 'and', 'aspect', 'term', 'embedding', 'are', 'set', 'to', '300', ',', 'and', 'the', 'number', 'of', 'the', 'hidden', 'units', 'are', 'set', 'to', '200', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'JJ', 'NN', 'VBG', 'VBP', 'VBN', 'TO', 'CD', ',', 'CC', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'VBP', 'VBN', 'TO', 'CD', '.']",27
sentiment_analysis,2,123,"The dimension of position embedding is set to 100 , which is randomly initialized and updated during the training process .","['The', 'dimension', 'of', 'position', 'embedding', 'is', 'set', 'to', '100', ',', 'which', 'is', 'randomly', 'initialized', 'and', 'updated', 'during', 'the', 'training', 'process', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', ',', 'WDT', 'VBZ', 'RB', 'VBN', 'CC', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",21
sentiment_analysis,2,124,"We use Tensorflow to implement our proposed model and employ the Momentum as the training method , whose momentum parameter ? is set to 0.9 , ? is set to 10 ? 6 , and the initial learning rate is set to 0.01 .","['We', 'use', 'Tensorflow', 'to', 'implement', 'our', 'proposed', 'model', 'and', 'employ', 'the', 'Momentum', 'as', 'the', 'training', 'method', ',', 'whose', 'momentum', 'parameter', '?', 'is', 'set', 'to', '0.9', ',', '?', 'is', 'set', 'to', '10', '?', '6', ',', 'and', 'the', 'initial', 'learning', 'rate', 'is', 'set', 'to', '0.01', '.']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'TO', 'VB', 'PRP$', 'VBN', 'NN', 'CC', 'VB', 'DT', 'NNP', 'IN', 'DT', 'NN', 'NN', ',', 'WP$', 'NN', 'NN', '.', 'VBZ', 'VBN', 'TO', 'CD', ',', '.', 'VBZ', 'VBN', 'TO', 'CD', '.', 'CD', ',', 'CC', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",44
sentiment_analysis,2,134,LSTM : LSTM takes the sentence as input so as to get the hidden representation of each word .,"['LSTM', ':', 'LSTM', 'takes', 'the', 'sentence', 'as', 'input', 'so', 'as', 'to', 'get', 'the', 'hidden', 'representation', 'of', 'each', 'word', '.']","['B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NN', ':', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'NN', 'RB', 'IN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",19
sentiment_analysis,2,135,"Then it regards the average value of all hidden states as the representation of sentence , and puts it into softmax layer to predict the probability of each sentiment polarity .","['Then', 'it', 'regards', 'the', 'average', 'value', 'of', 'all', 'hidden', 'states', 'as', 'the', 'representation', 'of', 'sentence', ',', 'and', 'puts', 'it', 'into', 'softmax', 'layer', 'to', 'predict', 'the', 'probability', 'of', 'each', 'sentiment', 'polarity', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NN', ',', 'CC', 'VBZ', 'PRP', 'IN', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",31
sentiment_analysis,2,137,"AE - LSTM : AE - LSTM first models the words in sentence via LSTM network and concatenate the aspect embedding to the hidden contextual representation for calculating the attention weights , which are employed to produce the final representation for the input sentence to judge the sentiment polarity .","['AE', '-', 'LSTM', ':', 'AE', '-', 'LSTM', 'first', 'models', 'the', 'words', 'in', 'sentence', 'via', 'LSTM', 'network', 'and', 'concatenate', 'the', 'aspect', 'embedding', 'to', 'the', 'hidden', 'contextual', 'representation', 'for', 'calculating', 'the', 'attention', 'weights', ',', 'which', 'are', 'employed', 'to', 'produce', 'the', 'final', 'representation', 'for', 'the', 'input', 'sentence', 'to', 'judge', 'the', 'sentiment', 'polarity', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['NNP', ':', 'NN', ':', 'NNP', ':', 'NNP', 'RB', 'NNS', 'DT', 'NNS', 'IN', 'NN', 'IN', 'NNP', 'NN', 'CC', 'VB', 'DT', 'NN', 'VBG', 'TO', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'NN', 'NNS', ',', 'WDT', 'VBP', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', '.']",50
sentiment_analysis,2,138,"ATAE - LSTM : ATAE - LSTM extended AE - LSTM by appending the aspect embedding to each word embedding so as to represent the input sentence , which highlights the role of aspect embedding .","['ATAE', '-', 'LSTM', ':', 'ATAE', '-', 'LSTM', 'extended', 'AE', '-', 'LSTM', 'by', 'appending', 'the', 'aspect', 'embedding', 'to', 'each', 'word', 'embedding', 'so', 'as', 'to', 'represent', 'the', 'input', 'sentence', ',', 'which', 'highlights', 'the', 'role', 'of', 'aspect', 'embedding', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', ':', 'NNP', ':', 'NNP', 'VBD', 'NNP', ':', 'NN', 'IN', 'VBG', 'DT', 'NN', 'VBG', 'TO', 'DT', 'NN', 'VBG', 'RB', 'IN', 'TO', 'VB', 'DT', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NN', '.']",36
sentiment_analysis,2,140,IAN : IAN considers the separate modeling of aspect terms and sentences respectively .,"['IAN', ':', 'IAN', 'considers', 'the', 'separate', 'modeling', 'of', 'aspect', 'terms', 'and', 'sentences', 'respectively', '.']","['B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['NN', ':', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', 'CC', 'NNS', 'RB', '.']",14
sentiment_analysis,2,143,"MemNet : MemNet applies attention multiple times on the word embedding , so that more abstractive evidences could be selected from the external memory .","['MemNet', ':', 'MemNet', 'applies', 'attention', 'multiple', 'times', 'on', 'the', 'word', 'embedding', ',', 'so', 'that', 'more', 'abstractive', 'evidences', 'could', 'be', 'selected', 'from', 'the', 'external', 'memory', '.']","['B-n', 'O', 'O', 'B-p', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['NN', ':', 'NNP', 'NNS', 'NN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', ',', 'IN', 'IN', 'RBR', 'JJ', 'NNS', 'MD', 'VB', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",25
sentiment_analysis,2,145,shows the performance of our model and other baseline models on datasets Restaurant and Laptop respectively .,"['shows', 'the', 'performance', 'of', 'our', 'model', 'and', 'other', 'baseline', 'models', 'on', 'datasets', 'Restaurant', 'and', 'Laptop', 'respectively', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['VBZ', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'CC', 'JJ', 'NN', 'NNS', 'IN', 'NNS', 'NNP', 'CC', 'NNP', 'RB', '.']",17
sentiment_analysis,2,146,We can observe that our proposed PBAN model achieves the best performance among all methods .,"['We', 'can', 'observe', 'that', 'our', 'proposed', 'PBAN', 'model', 'achieves', 'the', 'best', 'performance', 'among', 'all', 'methods', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['PRP', 'MD', 'VB', 'DT', 'PRP$', 'VBN', 'NNP', 'NN', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NNS', '.']",16
sentiment_analysis,2,160,"Generally speaking , by integrating the position information and the bidirectional attention mechanism , PBAN achieves the state - of - the - art performances , and it can effectively judge the sentiment polarity of different aspect term in its corresponding sentence so as to improve the classification accuracy .","['Generally', 'speaking', ',', 'by', 'integrating', 'the', 'position', 'information', 'and', 'the', 'bidirectional', 'attention', 'mechanism', ',', 'PBAN', 'achieves', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'performances', ',', 'and', 'it', 'can', 'effectively', 'judge', 'the', 'sentiment', 'polarity', 'of', 'different', 'aspect', 'term', 'in', 'its', 'corresponding', 'sentence', 'so', 'as', 'to', 'improve', 'the', 'classification', 'accuracy', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['RB', 'VBG', ',', 'IN', 'VBG', 'DT', 'NN', 'NN', 'CC', 'DT', 'JJ', 'NN', 'NN', ',', 'NNP', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', ',', 'CC', 'PRP', 'MD', 'RB', 'VB', 'DT', 'NN', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'IN', 'PRP$', 'JJ', 'NN', 'RB', 'IN', 'TO', 'VB', 'DT', 'NN', 'NN', '.']",50
sentiment_analysis,32,2,Interactive Attention Networks for Aspect - Level Sentiment Classification,"['Interactive', 'Attention', 'Networks', 'for', 'Aspect', '-', 'Level', 'Sentiment', 'Classification']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', ':', 'NN', 'NNP', 'NNP']",9
sentiment_analysis,32,5,Previous approaches have realized the importance of targets in sentiment classification and developed various methods with the goal of precisely modeling their contexts via generating target - specific representations .,"['Previous', 'approaches', 'have', 'realized', 'the', 'importance', 'of', 'targets', 'in', 'sentiment', 'classification', 'and', 'developed', 'various', 'methods', 'with', 'the', 'goal', 'of', 'precisely', 'modeling', 'their', 'contexts', 'via', 'generating', 'target', '-', 'specific', 'representations', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNS', 'VBP', 'VBN', 'DT', 'NN', 'IN', 'NNS', 'IN', 'NN', 'NN', 'CC', 'VBD', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'RB', 'VBG', 'PRP$', 'NN', 'IN', 'VBG', 'NN', ':', 'JJ', 'NNS', '.']",30
sentiment_analysis,32,42,"Based on the two points analyzed above , we propose an interactive attention network ( IAN ) model which is based on long - short term memory networks ( LSTM ) and attention mechanism .","['Based', 'on', 'the', 'two', 'points', 'analyzed', 'above', ',', 'we', 'propose', 'an', 'interactive', 'attention', 'network', '(', 'IAN', ')', 'model', 'which', 'is', 'based', 'on', 'long', '-', 'short', 'term', 'memory', 'networks', '(', 'LSTM', ')', 'and', 'attention', 'mechanism', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'CD', 'NNS', 'VBN', 'RB', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'NN', 'WDT', 'VBZ', 'VBN', 'IN', 'JJ', ':', 'JJ', 'NN', 'NN', 'NNS', '(', 'NNP', ')', 'CC', 'NN', 'NN', '.']",35
sentiment_analysis,32,43,IAN utilizes the attention mechanism associated with a target to get important information from the context and compute context representation for sentiment classification .,"['IAN', 'utilizes', 'the', 'attention', 'mechanism', 'associated', 'with', 'a', 'target', 'to', 'get', 'important', 'information', 'from', 'the', 'context', 'and', 'compute', 'context', 'representation', 'for', 'sentiment', 'classification', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'NN', 'VBN', 'IN', 'DT', 'NN', 'TO', 'VB', 'JJ', 'NN', 'IN', 'DT', 'NN', 'CC', 'NN', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",24
sentiment_analysis,32,44,"Further , IAN makes use of the interactive information from context to supervise the modeling of the target which is helpful to judging sentiment .","['Further', ',', 'IAN', 'makes', 'use', 'of', 'the', 'interactive', 'information', 'from', 'context', 'to', 'supervise', 'the', 'modeling', 'of', 'the', 'target', 'which', 'is', 'helpful', 'to', 'judging', 'sentiment', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'NNP', 'VBZ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'WDT', 'VBZ', 'JJ', 'TO', 'VBG', 'NN', '.']",25
sentiment_analysis,32,45,"Finally , with both target representation and context representation concatenated , IAN predicts the sentiment polarity for the target within its context .","['Finally', ',', 'with', 'both', 'target', 'representation', 'and', 'context', 'representation', 'concatenated', ',', 'IAN', 'predicts', 'the', 'sentiment', 'polarity', 'for', 'the', 'target', 'within', 'its', 'context', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'IN', 'DT', 'NN', 'NN', 'CC', 'JJ', 'NN', 'VBD', ',', 'NNP', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'PRP$', 'NN', '.']",23
sentiment_analysis,32,125,"In our experiments , all word embeddings from context and target are initialized by GloVe 2 , and all out - of - vocabulary words are initialized by sampling from the uniform distribution U ( ?0.1 , 0.1 ) .","['In', 'our', 'experiments', ',', 'all', 'word', 'embeddings', 'from', 'context', 'and', 'target', 'are', 'initialized', 'by', 'GloVe', '2', ',', 'and', 'all', 'out', '-', 'of', '-', 'vocabulary', 'words', 'are', 'initialized', 'by', 'sampling', 'from', 'the', 'uniform', 'distribution', 'U', '(', '?0.1', ',', '0.1', ')', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'PRP$', 'NNS', ',', 'DT', 'NN', 'NNS', 'IN', 'NN', 'CC', 'NN', 'VBP', 'VBN', 'IN', 'NNP', 'CD', ',', 'CC', 'DT', 'IN', ':', 'IN', ':', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NNP', '(', 'NNP', ',', 'CD', ')', '.']",40
sentiment_analysis,32,126,"All weight matrices are given their initial values by sampling from uniform distribution U ( ?0.1 , 0.1 ) , and all biases are set to zeros .","['All', 'weight', 'matrices', 'are', 'given', 'their', 'initial', 'values', 'by', 'sampling', 'from', 'uniform', 'distribution', 'U', '(', '?0.1', ',', '0.1', ')', ',', 'and', 'all', 'biases', 'are', 'set', 'to', 'zeros', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'VBN', 'PRP$', 'JJ', 'NNS', 'IN', 'VBG', 'IN', 'JJ', 'NN', 'NNP', '(', 'NNP', ',', 'CD', ')', ',', 'CC', 'DT', 'NNS', 'VBP', 'VBN', 'TO', 'NNS', '.']",28
sentiment_analysis,32,127,"The dimensions of word embeddings , attention vectors and LSTM hidden states are set to 300 as in .","['The', 'dimensions', 'of', 'word', 'embeddings', ',', 'attention', 'vectors', 'and', 'LSTM', 'hidden', 'states', 'are', 'set', 'to', '300', 'as', 'in', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'NN', 'NNS', ',', 'NN', 'NNS', 'CC', 'NNP', 'VBP', 'NNS', 'VBP', 'VBN', 'TO', 'CD', 'IN', 'IN', '.']",19
sentiment_analysis,32,128,"To train the parameters of IAN , we employ the Momentum , which adds a fraction ? of the update vector in the prior step to the current update vector .","['To', 'train', 'the', 'parameters', 'of', 'IAN', ',', 'we', 'employ', 'the', 'Momentum', ',', 'which', 'adds', 'a', 'fraction', '?', 'of', 'the', 'update', 'vector', 'in', 'the', 'prior', 'step', 'to', 'the', 'current', 'update', 'vector', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NNS', 'IN', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NN', '.', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'TO', 'DT', 'JJ', 'JJ', 'NN', '.']",31
sentiment_analysis,32,129,"The coefficient of L 2 normalization in the objective function is set to 10 ?5 , and the dropout rate is set to 0.5 .","['The', 'coefficient', 'of', 'L', '2', 'normalization', 'in', 'the', 'objective', 'function', 'is', 'set', 'to', '10', '?5', ',', 'and', 'the', 'dropout', 'rate', 'is', 'set', 'to', '0.5', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NNP', 'CD', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'NN', ',', 'CC', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",25
sentiment_analysis,32,133,"Majority is a basic baseline method , which assigns the largest sentiment polarity in the training set to each sample in the test set .","['Majority', 'is', 'a', 'basic', 'baseline', 'method', ',', 'which', 'assigns', 'the', 'largest', 'sentiment', 'polarity', 'in', 'the', 'training', 'set', 'to', 'each', 'sample', 'in', 'the', 'test', 'set', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'JJS', 'NN', 'NN', 'IN', 'DT', 'NN', 'VBN', 'TO', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",25
sentiment_analysis,32,134,LSTM only uses one LSTM network to model the context and get the hidden state of each word .,"['LSTM', 'only', 'uses', 'one', 'LSTM', 'network', 'to', 'model', 'the', 'context', 'and', 'get', 'the', 'hidden', 'state', 'of', 'each', 'word', '.']","['B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'RB', 'VBZ', 'CD', 'NNP', 'NN', 'TO', 'VB', 'DT', 'NN', 'CC', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",19
sentiment_analysis,32,136,TD - LSTM adopts two long short - term memory ( LSTM ) networks to model the left context with target and the right context with target respectively .,"['TD', '-', 'LSTM', 'adopts', 'two', 'long', 'short', '-', 'term', 'memory', '(', 'LSTM', ')', 'networks', 'to', 'model', 'the', 'left', 'context', 'with', 'target', 'and', 'the', 'right', 'context', 'with', 'target', 'respectively', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O']","['NNP', ':', 'NNP', 'NNS', 'CD', 'JJ', 'JJ', ':', 'NN', 'NN', '(', 'NNP', ')', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'CC', 'DT', 'JJ', 'NN', 'IN', 'NN', 'RB', '.']",29
sentiment_analysis,32,138,AE - LSTM represents targets with aspect embeddings .,"['AE', '-', 'LSTM', 'represents', 'targets', 'with', 'aspect', 'embeddings', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ':', 'JJ', 'VBZ', 'NNS', 'IN', 'JJ', 'NNS', '.']",9
sentiment_analysis,32,140,ATAE - LSTM is developed based on AE - LSTM .,"['ATAE', '-', 'LSTM', 'is', 'developed', 'based', 'on', 'AE', '-', 'LSTM', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'VBZ', 'VBN', 'VBN', 'IN', 'NNP', ':', 'NN', '.']",11
sentiment_analysis,32,143,"All the other methods are based on LSTM models and better than the Majority method , showing that LSTM has potentials in automatically generating representations and can all bring performance improvement for sentiment classification .","['All', 'the', 'other', 'methods', 'are', 'based', 'on', 'LSTM', 'models', 'and', 'better', 'than', 'the', 'Majority', 'method', ',', 'showing', 'that', 'LSTM', 'has', 'potentials', 'in', 'automatically', 'generating', 'representations', 'and', 'can', 'all', 'bring', 'performance', 'improvement', 'for', 'sentiment', 'classification', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PDT', 'DT', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'NNS', 'CC', 'JJR', 'IN', 'DT', 'NNP', 'NN', ',', 'VBG', 'IN', 'NNP', 'VBZ', 'NNS', 'IN', 'RB', 'VBG', 'NNS', 'CC', 'MD', 'DT', 'VB', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",35
sentiment_analysis,32,144,"The LSTM method gets the worst performance of all the neural network baseline methods , because it treats targets equally with other context words and does not make full use of the target information .","['The', 'LSTM', 'method', 'gets', 'the', 'worst', 'performance', 'of', 'all', 'the', 'neural', 'network', 'baseline', 'methods', ',', 'because', 'it', 'treats', 'targets', 'equally', 'with', 'other', 'context', 'words', 'and', 'does', 'not', 'make', 'full', 'use', 'of', 'the', 'target', 'information', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NN', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'PDT', 'DT', 'JJ', 'NN', 'NN', 'NNS', ',', 'IN', 'PRP', 'VBZ', 'NNS', 'RB', 'IN', 'JJ', 'NN', 'NNS', 'CC', 'VBZ', 'RB', 'VB', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",35
sentiment_analysis,32,146,"TD - LSTM outperforms LSTM over 1 percent and 2 percent on the Restaurant and Laptop category respectively , since it develops from the standard LSTM and processes the left and right contexts with targets .","['TD', '-', 'LSTM', 'outperforms', 'LSTM', 'over', '1', 'percent', 'and', '2', 'percent', 'on', 'the', 'Restaurant', 'and', 'Laptop', 'category', 'respectively', ',', 'since', 'it', 'develops', 'from', 'the', 'standard', 'LSTM', 'and', 'processes', 'the', 'left', 'and', 'right', 'contexts', 'with', 'targets', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', 'NNS', 'NNP', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NN', 'RB', ',', 'IN', 'PRP', 'VBZ', 'IN', 'DT', 'JJ', 'NNP', 'CC', 'VBZ', 'DT', 'NN', 'CC', 'JJ', 'NN', 'IN', 'NNS', '.']",36
sentiment_analysis,32,148,"Further , both AE - LSTM and ATAE - LSTM stably exceed the TD - LSTM method because of the introduction of attention mechanism .","['Further', ',', 'both', 'AE', '-', 'LSTM', 'and', 'ATAE', '-', 'LSTM', 'stably', 'exceed', 'the', 'TD', '-', 'LSTM', 'method', 'because', 'of', 'the', 'introduction', 'of', 'attention', 'mechanism', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'DT', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NNP', 'RB', 'VBP', 'DT', 'NNP', ':', 'NN', 'NN', 'IN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', '.']",25
sentiment_analysis,32,149,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the supervision of target and generate more reasonable representations for aspect - level sentiment classification .","['For', 'AE', '-', 'LSTM', 'and', 'ATAE', '-', 'LSTM', ',', 'they', 'capture', 'important', 'information', 'in', 'the', 'context', 'with', 'the', 'supervision', 'of', 'target', 'and', 'generate', 'more', 'reasonable', 'representations', 'for', 'aspect', '-', 'level', 'sentiment', 'classification', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'CC', 'VB', 'RBR', 'JJ', 'NNS', 'IN', 'JJ', ':', 'NN', 'NN', 'NN', '.']",33
sentiment_analysis,32,150,"We can also see that AE - LSTM and ATAE - LSTM further emphasize the modeling of targets via the addition of the aspect embedding , which is also the reason of performance improvement .","['We', 'can', 'also', 'see', 'that', 'AE', '-', 'LSTM', 'and', 'ATAE', '-', 'LSTM', 'further', 'emphasize', 'the', 'modeling', 'of', 'targets', 'via', 'the', 'addition', 'of', 'the', 'aspect', 'embedding', ',', 'which', 'is', 'also', 'the', 'reason', 'of', 'performance', 'improvement', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'MD', 'RB', 'VB', 'IN', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NNP', 'RB', 'VB', 'DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'WDT', 'VBZ', 'RB', 'DT', 'NN', 'IN', 'NN', 'NN', '.']",35
sentiment_analysis,32,151,"Compared with AE - LSTM , ATAE - LSTM especially enhance the interaction between the context words and target and thus has a better performance than AE - LSTM .","['Compared', 'with', 'AE', '-', 'LSTM', ',', 'ATAE', '-', 'LSTM', 'especially', 'enhance', 'the', 'interaction', 'between', 'the', 'context', 'words', 'and', 'target', 'and', 'thus', 'has', 'a', 'better', 'performance', 'than', 'AE', '-', 'LSTM', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'NNP', ':', 'NNP', ',', 'NNP', ':', 'NNP', 'RB', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'NNS', 'CC', 'NN', 'CC', 'RB', 'VBZ', 'DT', 'JJR', 'NN', 'IN', 'NNP', ':', 'NN', '.']",30
sentiment_analysis,32,153,We can see that IAN achieves the best performance among all baselines .,"['We', 'can', 'see', 'that', 'IAN', 'achieves', 'the', 'best', 'performance', 'among', 'all', 'baselines', '.']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'DT', 'NNP', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NNS', '.']",13
sentiment_analysis,32,154,"Compared with ATAE - LSTM model , IAN improves the performance about 1.4 % and 3.2 % on the Restaurant and Laptop categories respectively .","['Compared', 'with', 'ATAE', '-', 'LSTM', 'model', ',', 'IAN', 'improves', 'the', 'performance', 'about', '1.4', '%', 'and', '3.2', '%', 'on', 'the', 'Restaurant', 'and', 'Laptop', 'categories', 'respectively', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['VBN', 'IN', 'NNP', ':', 'NNP', 'NN', ',', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NNS', 'RB', '.']",25
sentiment_analysis,32,159,"The more attentions are paid to targets , the higher accuracy the system achieves .","['The', 'more', 'attentions', 'are', 'paid', 'to', 'targets', ',', 'the', 'higher', 'accuracy', 'the', 'system', 'achieves', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O']","['DT', 'JJR', 'NNS', 'VBP', 'VBN', 'TO', 'NNS', ',', 'DT', 'JJR', 'NN', 'DT', 'NN', 'VBZ', '.']",15
sentiment_analysis,41,2,Attention - based LSTM for Aspect - level Sentiment Classification,"['Attention', '-', 'based', 'LSTM', 'for', 'Aspect', '-', 'level', 'Sentiment', 'Classification']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'VBN', 'NNP', 'IN', 'NNP', ':', 'NN', 'NNP', 'NNP']",10
sentiment_analysis,41,4,Aspect - level sentiment classification is a finegrained task in sentiment analysis .,"['Aspect', '-', 'level', 'sentiment', 'classification', 'is', 'a', 'finegrained', 'task', 'in', 'sentiment', 'analysis', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['NNP', ':', 'NN', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NN', '.']",13
sentiment_analysis,41,15,"In this paper , we deal with aspect - level sentiment classification and we find that the sentiment polarity of a sentence is highly dependent on both content and aspect .","['In', 'this', 'paper', ',', 'we', 'deal', 'with', 'aspect', '-', 'level', 'sentiment', 'classification', 'and', 'we', 'find', 'that', 'the', 'sentiment', 'polarity', 'of', 'a', 'sentence', 'is', 'highly', 'dependent', 'on', 'both', 'content', 'and', 'aspect', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'IN', 'JJ', ':', 'NN', 'NN', 'NN', 'CC', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'JJ', 'IN', 'DT', 'NN', 'CC', 'NN', '.']",31
sentiment_analysis,41,24,"In this paper , we propose an attention mechanism to enforce the model to attend to the important part of a sentence , in response to a specific aspect .","['In', 'this', 'paper', ',', 'we', 'propose', 'an', 'attention', 'mechanism', 'to', 'enforce', 'the', 'model', 'to', 'attend', 'to', 'the', 'important', 'part', 'of', 'a', 'sentence', ',', 'in', 'response', 'to', 'a', 'specific', 'aspect', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'TO', 'VB', 'TO', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', ',', 'IN', 'NN', 'TO', 'DT', 'JJ', 'NN', '.']",30
sentiment_analysis,41,25,We design an aspect - tosentence attention mechanism that can concentrate on the key part of a sentence given the aspect .,"['We', 'design', 'an', 'aspect', '-', 'tosentence', 'attention', 'mechanism', 'that', 'can', 'concentrate', 'on', 'the', 'key', 'part', 'of', 'a', 'sentence', 'given', 'the', 'aspect', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', ':', 'NN', 'NN', 'NN', 'WDT', 'MD', 'VB', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'VBN', 'DT', 'NN', '.']",22
sentiment_analysis,41,26,We explore the potential correlation of aspect and sentiment polarity in aspect - level sentiment classification .,"['We', 'explore', 'the', 'potential', 'correlation', 'of', 'aspect', 'and', 'sentiment', 'polarity', 'in', 'aspect', '-', 'level', 'sentiment', 'classification', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NN', 'CC', 'NN', 'NN', 'IN', 'JJ', ':', 'NN', 'NN', 'NN', '.']",17
sentiment_analysis,41,27,"In order to capture important information in response to a given aspect , we design an attentionbased LSTM .","['In', 'order', 'to', 'capture', 'important', 'information', 'in', 'response', 'to', 'a', 'given', 'aspect', ',', 'we', 'design', 'an', 'attentionbased', 'LSTM', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'VB', 'JJ', 'NN', 'IN', 'NN', 'TO', 'DT', 'VBN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNP', '.']",19
sentiment_analysis,41,146,We apply the proposed model to aspect - level sentiment classification .,"['We', 'apply', 'the', 'proposed', 'model', 'to', 'aspect', '-', 'level', 'sentiment', 'classification', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'VBN', 'NN', 'TO', 'VB', ':', 'NN', 'NN', 'NN', '.']",12
sentiment_analysis,41,147,"In our experiments , all word vectors are initialized by Glove 1 .","['In', 'our', 'experiments', ',', 'all', 'word', 'vectors', 'are', 'initialized', 'by', 'Glove', '1', '.']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O']","['IN', 'PRP$', 'NNS', ',', 'DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'CD', '.']",13
sentiment_analysis,41,148,The word embedding vectors are pre-trained on an unlabeled corpus whose size is about 840 billion .,"['The', 'word', 'embedding', 'vectors', 'are', 'pre-trained', 'on', 'an', 'unlabeled', 'corpus', 'whose', 'size', 'is', 'about', '840', 'billion', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'NNS', 'VBP', 'JJ', 'IN', 'DT', 'JJ', 'NN', 'WP$', 'NN', 'VBZ', 'IN', 'CD', 'CD', '.']",17
sentiment_analysis,41,149,"The other parameters are initialized by sampling from a uniform distribution U (?? , ? ) .","['The', 'other', 'parameters', 'are', 'initialized', 'by', 'sampling', 'from', 'a', 'uniform', 'distribution', 'U', '(??', ',', '?', ')', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'NNP', ',', '.', ')', '.']",17
sentiment_analysis,41,150,"The dimension of word vectors , aspect embeddings and the size of hidden layer are 300 .","['The', 'dimension', 'of', 'word', 'vectors', ',', 'aspect', 'embeddings', 'and', 'the', 'size', 'of', 'hidden', 'layer', 'are', '300', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NN', 'NNS', ',', 'JJ', 'NNS', 'CC', 'DT', 'NN', 'IN', 'JJ', 'NN', 'VBP', 'CD', '.']",17
sentiment_analysis,41,151,The length of attention weights is the same as the length of sentence .,"['The', 'length', 'of', 'attention', 'weights', 'is', 'the', 'same', 'as', 'the', 'length', 'of', 'sentence', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NN', 'NNS', 'VBZ', 'DT', 'JJ', 'IN', 'DT', 'NN', 'IN', 'NN', '.']",14
sentiment_analysis,41,152,Theano is used for implementing our neural network models .,"['Theano', 'is', 'used', 'for', 'implementing', 'our', 'neural', 'network', 'models', '.']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NN', 'VBZ', 'VBN', 'IN', 'VBG', 'PRP$', 'JJ', 'NN', 'NNS', '.']",10
sentiment_analysis,41,153,"We trained all models with a batch size of 25 examples , and a momentum of 0.9 , L 2 - regularization weight of 0.001 and initial learning rate of 0.01 for AdaGrad .","['We', 'trained', 'all', 'models', 'with', 'a', 'batch', 'size', 'of', '25', 'examples', ',', 'and', 'a', 'momentum', 'of', '0.9', ',', 'L', '2', '-', 'regularization', 'weight', 'of', '0.001', 'and', 'initial', 'learning', 'rate', 'of', '0.01', 'for', 'AdaGrad', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'VBD', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', 'NNS', ',', 'CC', 'DT', 'NN', 'IN', 'CD', ',', 'NNP', 'CD', ':', 'NN', 'NN', 'IN', 'CD', 'CC', 'JJ', 'VBG', 'NN', 'IN', 'CD', 'IN', 'NNP', '.']",34
sentiment_analysis,41,176,"LSTM : Standard LSTM can not capture any aspect information in sentence , so it must get the same ( a ) the aspect of this sentence : service ( b ) the aspect of this sentence : food : Attention Visualizations .","['LSTM', ':', 'Standard', 'LSTM', 'can', 'not', 'capture', 'any', 'aspect', 'information', 'in', 'sentence', ',', 'so', 'it', 'must', 'get', 'the', 'same', '(', 'a', ')', 'the', 'aspect', 'of', 'this', 'sentence', ':', 'service', '(', 'b', ')', 'the', 'aspect', 'of', 'this', 'sentence', ':', 'food', ':', 'Attention', 'Visualizations', '.']","['B-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', ':', 'JJ', 'NNP', 'MD', 'RB', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', ',', 'IN', 'PRP', 'MD', 'VB', 'DT', 'JJ', '(', 'DT', ')', 'DT', 'NN', 'IN', 'DT', 'NN', ':', 'NN', '(', 'NN', ')', 'DT', 'NN', 'IN', 'DT', 'NN', ':', 'NN', ':', 'NN', 'NNS', '.']",43
sentiment_analysis,41,182,"Since it can not take advantage of the aspect information , not surprisingly the model has worst performance .","['Since', 'it', 'can', 'not', 'take', 'advantage', 'of', 'the', 'aspect', 'information', ',', 'not', 'surprisingly', 'the', 'model', 'has', 'worst', 'performance', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['IN', 'PRP', 'MD', 'RB', 'VB', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'RB', 'RB', 'DT', 'NN', 'VBZ', 'VBN', 'NN', '.']",19
sentiment_analysis,41,183,TD - LSTM : TD - LSTM can improve the performance of sentiment classifier by treating an aspect as a target .,"['TD', '-', 'LSTM', ':', 'TD', '-', 'LSTM', 'can', 'improve', 'the', 'performance', 'of', 'sentiment', 'classifier', 'by', 'treating', 'an', 'aspect', 'as', 'a', 'target', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']","['NNP', ':', 'NN', ':', 'NNP', ':', 'NN', 'MD', 'VB', 'DT', 'NN', 'IN', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",22
sentiment_analysis,41,184,"Since there is no attention mechanism in TD - LSTM , it can not "" know "" which words are important for a given aspect .","['Since', 'there', 'is', 'no', 'attention', 'mechanism', 'in', 'TD', '-', 'LSTM', ',', 'it', 'can', 'not', '""', 'know', '""', 'which', 'words', 'are', 'important', 'for', 'a', 'given', 'aspect', '.']","['O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'EX', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'NNP', ':', 'NN', ',', 'PRP', 'MD', 'RB', 'VB', 'NNS', 'NNP', 'WDT', 'NNS', 'VBP', 'JJ', 'IN', 'DT', 'VBN', 'NN', '.']",26
sentiment_analysis,41,186,It is worth noting that TC - LSTM performs worse than LSTM and TD - LSTM in .,"['It', 'is', 'worth', 'noting', 'that', 'TC', '-', 'LSTM', 'performs', 'worse', 'than', 'LSTM', 'and', 'TD', '-', 'LSTM', 'in', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBZ', 'JJ', 'VBG', 'IN', 'NNP', ':', 'NN', 'NNS', 'JJR', 'IN', 'NNP', 'CC', 'NNP', ':', 'NN', 'IN', '.']",18
sentiment_analysis,41,190,"ATAE - LSTM not only addresses the shortcoming of the unconformity between word vectors and aspect embeddings , but also can capture the most important information in response to a given aspect .","['ATAE', '-', 'LSTM', 'not', 'only', 'addresses', 'the', 'shortcoming', 'of', 'the', 'unconformity', 'between', 'word', 'vectors', 'and', 'aspect', 'embeddings', ',', 'but', 'also', 'can', 'capture', 'the', 'most', 'important', 'information', 'in', 'response', 'to', 'a', 'given', 'aspect', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'RB', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', 'CC', 'JJ', 'NNS', ',', 'CC', 'RB', 'MD', 'VB', 'DT', 'RBS', 'JJ', 'NN', 'IN', 'NN', 'TO', 'DT', 'VBN', 'NN', '.']",33
sentiment_analysis,3,2,Knowledge - Enriched Transformer for Emotion Detection in Textual Conversations,"['Knowledge', '-', 'Enriched', 'Transformer', 'for', 'Emotion', 'Detection', 'in', 'Textual', 'Conversations']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'VBD', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",10
sentiment_analysis,3,5,The task of detecting emotions in textual conversations leads to a wide range of applications such as opinion mining in social networks .,"['The', 'task', 'of', 'detecting', 'emotions', 'in', 'textual', 'conversations', 'leads', 'to', 'a', 'wide', 'range', 'of', 'applications', 'such', 'as', 'opinion', 'mining', 'in', 'social', 'networks', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'VBG', 'NNS', 'IN', 'JJ', 'NNS', 'VBZ', 'TO', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'JJ', 'IN', 'NN', 'NN', 'IN', 'JJ', 'NNS', '.']",23
sentiment_analysis,3,14,"This work addresses the task of detecting emotions ( e.g. , happy , sad , angry , etc. ) in textual conversations , where the emotion of an utterance is detected in the conversational context .","['This', 'work', 'addresses', 'the', 'task', 'of', 'detecting', 'emotions', '(', 'e.g.', ',', 'happy', ',', 'sad', ',', 'angry', ',', 'etc.', ')', 'in', 'textual', 'conversations', ',', 'where', 'the', 'emotion', 'of', 'an', 'utterance', 'is', 'detected', 'in', 'the', 'conversational', 'context', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'NNS', '(', 'NN', ',', 'JJ', ',', 'JJ', ',', 'JJ', ',', 'NN', ')', 'IN', 'JJ', 'NNS', ',', 'WRB', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",36
sentiment_analysis,3,29,"To this end , we propose a Knowledge - Enriched Transformer ( KET ) to effectively incorporate contextual information and external knowledge bases to address the aforementioned challenges .","['To', 'this', 'end', ',', 'we', 'propose', 'a', 'Knowledge', '-', 'Enriched', 'Transformer', '(', 'KET', ')', 'to', 'effectively', 'incorporate', 'contextual', 'information', 'and', 'external', 'knowledge', 'bases', 'to', 'address', 'the', 'aforementioned', 'challenges', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', ':', 'VBD', 'NNP', '(', 'NNP', ')', 'TO', 'RB', 'VB', 'JJ', 'NN', 'CC', 'JJ', 'NN', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NNS', '.']",29
sentiment_analysis,3,31,"The self - attention and cross-attention modules in the Transformer capture the intra-sentence and inter-sentence correlations , respectively .","['The', 'self', '-', 'attention', 'and', 'cross-attention', 'modules', 'in', 'the', 'Transformer', 'capture', 'the', 'intra-sentence', 'and', 'inter-sentence', 'correlations', ',', 'respectively', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['DT', 'NN', ':', 'NN', 'CC', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'DT', 'NN', 'CC', 'NN', 'NNS', ',', 'RB', '.']",19
sentiment_analysis,3,32,The shorter path of information flow in these two modules compared to gated RNNs and CNNs allows KET to model contextual information more efficiently .,"['The', 'shorter', 'path', 'of', 'information', 'flow', 'in', 'these', 'two', 'modules', 'compared', 'to', 'gated', 'RNNs', 'and', 'CNNs', 'allows', 'KET', 'to', 'model', 'contextual', 'information', 'more', 'efficiently', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJR', 'NN', 'IN', 'NN', 'NN', 'IN', 'DT', 'CD', 'NNS', 'VBN', 'TO', 'VBN', 'NNP', 'CC', 'NNP', 'VBZ', 'NNP', 'TO', 'VB', 'JJ', 'NN', 'RBR', 'RB', '.']",25
sentiment_analysis,3,33,"In addition , we propose a hierarchical self - attention mechanism allowing KET to model the hierarchical structure of conversations .","['In', 'addition', ',', 'we', 'propose', 'a', 'hierarchical', 'self', '-', 'attention', 'mechanism', 'allowing', 'KET', 'to', 'model', 'the', 'hierarchical', 'structure', 'of', 'conversations', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', ':', 'NN', 'NN', 'VBG', 'NNP', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NNS', '.']",21
sentiment_analysis,3,34,"Our model separates context and response into the encoder and decoder , respectively , which is different from other Transformer - based models , e.g. , BERT , which directly concatenate context and response , and then train language models using only the encoder part .","['Our', 'model', 'separates', 'context', 'and', 'response', 'into', 'the', 'encoder', 'and', 'decoder', ',', 'respectively', ',', 'which', 'is', 'different', 'from', 'other', 'Transformer', '-', 'based', 'models', ',', 'e.g.', ',', 'BERT', ',', 'which', 'directly', 'concatenate', 'context', 'and', 'response', ',', 'and', 'then', 'train', 'language', 'models', 'using', 'only', 'the', 'encoder', 'part', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'VBZ', 'JJ', 'CC', 'NN', 'IN', 'DT', 'NN', 'CC', 'NN', ',', 'RB', ',', 'WDT', 'VBZ', 'JJ', 'IN', 'JJ', 'NNP', ':', 'VBN', 'NNS', ',', 'NN', ',', 'NNP', ',', 'WDT', 'RB', 'VBP', 'NN', 'CC', 'NN', ',', 'CC', 'RB', 'VB', 'NN', 'NNS', 'VBG', 'RB', 'DT', 'NN', 'NN', '.']",46
sentiment_analysis,3,35,"Moreover , to exploit commonsense knowledge , we leverage external knowledge bases to facilitate the understanding of each word in the utterances by referring to related knowledge entities .","['Moreover', ',', 'to', 'exploit', 'commonsense', 'knowledge', ',', 'we', 'leverage', 'external', 'knowledge', 'bases', 'to', 'facilitate', 'the', 'understanding', 'of', 'each', 'word', 'in', 'the', 'utterances', 'by', 'referring', 'to', 'related', 'knowledge', 'entities', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'TO', 'VB', 'NN', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'NNS', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNS', 'IN', 'VBG', 'TO', 'VBN', 'NN', 'NNS', '.']",29
sentiment_analysis,3,36,The referring process is dynamic and balances between relatedness and affectiveness of the retrieved knowledge entities using a context - aware affective graph attention mechanism .,"['The', 'referring', 'process', 'is', 'dynamic', 'and', 'balances', 'between', 'relatedness', 'and', 'affectiveness', 'of', 'the', 'retrieved', 'knowledge', 'entities', 'using', 'a', 'context', '-', 'aware', 'affective', 'graph', 'attention', 'mechanism', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBG', 'NN', 'VBZ', 'JJ', 'CC', 'NNS', 'IN', 'NN', 'CC', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNS', 'VBG', 'DT', 'NN', ':', 'JJ', 'JJ', 'NN', 'NN', 'NN', '.']",26
sentiment_analysis,3,199,c LSTM : A contextual LSTM model .,"['c', 'LSTM', ':', 'A', 'contextual', 'LSTM', 'model', '.']","['B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['JJ', 'NNP', ':', 'DT', 'JJ', 'NNP', 'NN', '.']",8
sentiment_analysis,3,200,An utterance - level bidirectional LSTM is used to encode each utterance .,"['An', 'utterance', '-', 'level', 'bidirectional', 'LSTM', 'is', 'used', 'to', 'encode', 'each', 'utterance', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', ':', 'NN', 'JJ', 'NNP', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NN', '.']",13
sentiment_analysis,3,201,A context - level unidirectional LSTM is used to encode the context .,"['A', 'context', '-', 'level', 'unidirectional', 'LSTM', 'is', 'used', 'to', 'encode', 'the', 'context', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['DT', 'JJ', ':', 'NN', 'JJ', 'NNP', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NN', '.']",13
sentiment_analysis,3,204,CNN+cLSTM : An CNN is used to extract utterance features .,"['CNN+cLSTM', ':', 'An', 'CNN', 'is', 'used', 'to', 'extract', 'utterance', 'features', '.']","['B-n', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['NN', ':', 'DT', 'NNP', 'VBZ', 'VBN', 'TO', 'VB', 'JJ', 'NNS', '.']",11
sentiment_analysis,3,205,An c LSTM is then applied to learn context representations .,"['An', 'c', 'LSTM', 'is', 'then', 'applied', 'to', 'learn', 'context', 'representations', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NNP', 'VBZ', 'RB', 'VBN', 'TO', 'VB', 'JJ', 'NNS', '.']",11
sentiment_analysis,3,206,BERT BASE :,"['BERT', 'BASE', ':']","['B-n', 'I-n', 'O']","['NNP', 'NNP', ':']",3
sentiment_analysis,3,208,We treat each utterance with its context as a single document .,"['We', 'treat', 'each', 'utterance', 'with', 'its', 'context', 'as', 'a', 'single', 'document', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",12
sentiment_analysis,3,209,We limit the document length to the last 100 tokens to allow larger batch size .,"['We', 'limit', 'the', 'document', 'length', 'to', 'the', 'last', '100', 'tokens', 'to', 'allow', 'larger', 'batch', 'size', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'DT', 'JJ', 'CD', 'NNS', 'TO', 'VB', 'JJR', 'NN', 'NN', '.']",16
sentiment_analysis,3,211,DialogueRNN : The stateof - the - art model for emotion detection in textual conversations .,"['DialogueRNN', ':', 'The', 'stateof', '-', 'the', '-', 'art', 'model', 'for', 'emotion', 'detection', 'in', 'textual', 'conversations', '.']","['B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NN', ':', 'DT', 'NN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'NN', 'NN', 'IN', 'JJ', 'NNS', '.']",16
sentiment_analysis,3,212,It models both context and speakers information .,"['It', 'models', 'both', 'context', 'and', 'speakers', 'information', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'NNS', 'DT', 'NN', 'CC', 'NNS', 'NN', '.']",8
sentiment_analysis,3,216,KET SingleSelfAttn :,"['KET', 'SingleSelfAttn', ':']","['B-n', 'I-n', 'O']","['NNP', 'NNP', ':']",3
sentiment_analysis,3,217,We replace the hierarchical self - attention by a single self - attention layer to learn context representations .,"['We', 'replace', 'the', 'hierarchical', 'self', '-', 'attention', 'by', 'a', 'single', 'self', '-', 'attention', 'layer', 'to', 'learn', 'context', 'representations', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VB', 'DT', 'JJ', 'NN', ':', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'NN', 'NN', 'TO', 'VB', 'JJ', 'NNS', '.']",19
sentiment_analysis,3,218,Contextual utterances are concatenated together prior to the single self - attention layer .,"['Contextual', 'utterances', 'are', 'concatenated', 'together', 'prior', 'to', 'the', 'single', 'self', '-', 'attention', 'layer', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'NNS', 'VBP', 'VBN', 'RB', 'RB', 'TO', 'DT', 'JJ', 'NN', ':', 'NN', 'NN', '.']",14
sentiment_analysis,3,219,KET StdAttn :,"['KET', 'StdAttn', ':']","['B-n', 'I-n', 'O']","['NNP', 'NNP', ':']",3
sentiment_analysis,3,220,We replace the dynamic contextaware affective graph attention by the standard graph attention .,"['We', 'replace', 'the', 'dynamic', 'contextaware', 'affective', 'graph', 'attention', 'by', 'the', 'standard', 'graph', 'attention', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VB', 'DT', 'JJ', 'NN', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'NN', '.']",14
sentiment_analysis,3,222,We preprocessed all datasets by lower - casing and tokenization using Spacy 2 .,"['We', 'preprocessed', 'all', 'datasets', 'by', 'lower', '-', 'casing', 'and', 'tokenization', 'using', 'Spacy', '2', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O']","['PRP', 'VBD', 'DT', 'NNS', 'IN', 'JJR', ':', 'NN', 'CC', 'NN', 'VBG', 'NNP', 'CD', '.']",14
sentiment_analysis,3,224,We use the released code for BERT BASE and DialogueRNN .,"['We', 'use', 'the', 'released', 'code', 'for', 'BERT', 'BASE', 'and', 'DialogueRNN', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'CC', 'NNP', '.']",11
sentiment_analysis,3,225,"For each dataset , all models are fine - tuned based on their performance on the validation set .","['For', 'each', 'dataset', ',', 'all', 'models', 'are', 'fine', '-', 'tuned', 'based', 'on', 'their', 'performance', 'on', 'the', 'validation', 'set', '.']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'DT', 'NNS', 'VBP', 'JJ', ':', 'VBN', 'VBN', 'IN', 'PRP$', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",19
sentiment_analysis,3,226,"For our model in all datasets , we use Adam optimization ( Kingma and Ba , 2014 ) with a batch size of 64 and learning rate of 0.0001 throughout the training process .","['For', 'our', 'model', 'in', 'all', 'datasets', ',', 'we', 'use', 'Adam', 'optimization', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', 'with', 'a', 'batch', 'size', 'of', '64', 'and', 'learning', 'rate', 'of', '0.0001', 'throughout', 'the', 'training', 'process', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'PRP$', 'NN', 'IN', 'DT', 'NNS', ',', 'PRP', 'VBP', 'JJ', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', 'CC', 'VBG', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', 'NN', '.']",34
sentiment_analysis,3,227,We use Glo Ve embedding for initialization in the word and concept embedding layers,"['We', 'use', 'Glo', 'Ve', 'embedding', 'for', 'initialization', 'in', 'the', 'word', 'and', 'concept', 'embedding', 'layers']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['PRP', 'VBP', 'NNP', 'NNP', 'VBG', 'IN', 'NN', 'IN', 'DT', 'NN', 'CC', 'NN', 'VBG', 'NNS']",14
sentiment_analysis,3,228,"For the class weights in cross - entropy loss for each dataset , we set them as the ratio of the class distribution in the validation set to the class distribution in the training set .","['For', 'the', 'class', 'weights', 'in', 'cross', '-', 'entropy', 'loss', 'for', 'each', 'dataset', ',', 'we', 'set', 'them', 'as', 'the', 'ratio', 'of', 'the', 'class', 'distribution', 'in', 'the', 'validation', 'set', 'to', 'the', 'class', 'distribution', 'in', 'the', 'training', 'set', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'NNS', 'IN', 'NN', ':', 'NN', 'NN', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'PRP', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'VBN', 'TO', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",36
sentiment_analysis,3,237,"c LSTM performs reasonably well on short conversations ( i.e. , EC and DailyDialog ) , but the worst on long conversations ( i.e. , MELD , EmoryNLP and IEMOCAP ) .","['c', 'LSTM', 'performs', 'reasonably', 'well', 'on', 'short', 'conversations', '(', 'i.e.', ',', 'EC', 'and', 'DailyDialog', ')', ',', 'but', 'the', 'worst', 'on', 'long', 'conversations', '(', 'i.e.', ',', 'MELD', ',', 'EmoryNLP', 'and', 'IEMOCAP', ')', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'NNP', 'VBZ', 'RB', 'RB', 'IN', 'JJ', 'NNS', '(', 'FW', ',', 'NNP', 'CC', 'NNP', ')', ',', 'CC', 'DT', 'JJS', 'IN', 'JJ', 'NNS', '(', 'FW', ',', 'NNP', ',', 'NNP', 'CC', 'NNP', ')', '.']",32
sentiment_analysis,3,239,"In contrast , when the utterance - level LSTM in c LSTM is replaced by features extracted by CNN , i.e. , the CNN + c LSTM , the model performs significantly better than c LSTM on long conversations , which further validates that modelling long conversations using only RNN models may not be sufficient .","['In', 'contrast', ',', 'when', 'the', 'utterance', '-', 'level', 'LSTM', 'in', 'c', 'LSTM', 'is', 'replaced', 'by', 'features', 'extracted', 'by', 'CNN', ',', 'i.e.', ',', 'the', 'CNN', '+', 'c', 'LSTM', ',', 'the', 'model', 'performs', 'significantly', 'better', 'than', 'c', 'LSTM', 'on', 'long', 'conversations', ',', 'which', 'further', 'validates', 'that', 'modelling', 'long', 'conversations', 'using', 'only', 'RNN', 'models', 'may', 'not', 'be', 'sufficient', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ',', 'WRB', 'DT', 'NN', ':', 'NN', 'NNP', 'IN', 'JJ', 'NNP', 'VBZ', 'VBN', 'IN', 'NNS', 'VBN', 'IN', 'NNP', ',', 'NN', ',', 'DT', 'NNP', 'NNP', 'NN', 'NNP', ',', 'DT', 'NN', 'VBZ', 'RB', 'JJR', 'IN', 'JJ', 'NNP', 'IN', 'JJ', 'NNS', ',', 'WDT', 'RB', 'VBZ', 'IN', 'VBG', 'JJ', 'NNS', 'VBG', 'RB', 'NNP', 'NNS', 'MD', 'RB', 'VB', 'JJ', '.']",56
sentiment_analysis,3,240,BERT BASE achieves very competitive performance on all datasets except EC due to its strong representational power via bi-directional context modelling using the Transformer .,"['BERT', 'BASE', 'achieves', 'very', 'competitive', 'performance', 'on', 'all', 'datasets', 'except', 'EC', 'due', 'to', 'its', 'strong', 'representational', 'power', 'via', 'bi-directional', 'context', 'modelling', 'using', 'the', 'Transformer', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['NNP', 'NNP', 'VBZ', 'RB', 'JJ', 'NN', 'IN', 'DT', 'NNS', 'IN', 'NNP', 'JJ', 'TO', 'PRP$', 'JJ', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', 'VBG', 'DT', 'NNP', '.']",25
sentiment_analysis,3,243,"In particular , DialogueRNN performs better than our model on IEMOCAP , which maybe attributed to its detailed speaker information for modelling the emotion dynamics in each speaker as the conversation flows .","['In', 'particular', ',', 'DialogueRNN', 'performs', 'better', 'than', 'our', 'model', 'on', 'IEMOCAP', ',', 'which', 'maybe', 'attributed', 'to', 'its', 'detailed', 'speaker', 'information', 'for', 'modelling', 'the', 'emotion', 'dynamics', 'in', 'each', 'speaker', 'as', 'the', 'conversation', 'flows', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', ',', 'NNP', 'VBZ', 'JJR', 'IN', 'PRP$', 'NN', 'IN', 'NNP', ',', 'WDT', 'RB', 'VBD', 'TO', 'PRP$', 'JJ', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', '.']",33
sentiment_analysis,3,245,"This finding indicates that our model is robust across datasets with varying training sizes , context lengths and domains .","['This', 'finding', 'indicates', 'that', 'our', 'model', 'is', 'robust', 'across', 'datasets', 'with', 'varying', 'training', 'sizes', ',', 'context', 'lengths', 'and', 'domains', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'IN', 'PRP$', 'NN', 'VBZ', 'JJ', 'IN', 'NNS', 'IN', 'VBG', 'NN', 'NNS', ',', 'JJ', 'NNS', 'CC', 'NNS', '.']",20
sentiment_analysis,3,246,Our KET variants KET SingleSelfAttn and KET StdAttn perform comparably with the best baselines on all datasets except IEMOCAP .,"['Our', 'KET', 'variants', 'KET', 'SingleSelfAttn', 'and', 'KET', 'StdAttn', 'perform', 'comparably', 'with', 'the', 'best', 'baselines', 'on', 'all', 'datasets', 'except', 'IEMOCAP', '.']","['O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP$', 'NNP', 'NNS', 'NNP', 'NNP', 'CC', 'NNP', 'NNP', 'NN', 'RB', 'IN', 'DT', 'JJS', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'NNP', '.']",20
sentiment_analysis,3,247,"However , both variants perform noticeably worse than KET on all datasets except EC , validating the importance of our proposed hierarchical self - attention and dynamic context - aware affective graph attention mechanism .","['However', ',', 'both', 'variants', 'perform', 'noticeably', 'worse', 'than', 'KET', 'on', 'all', 'datasets', 'except', 'EC', ',', 'validating', 'the', 'importance', 'of', 'our', 'proposed', 'hierarchical', 'self', '-', 'attention', 'and', 'dynamic', 'context', '-', 'aware', 'affective', 'graph', 'attention', 'mechanism', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'DT', 'NNS', 'VBP', 'RB', 'JJR', 'IN', 'NNP', 'IN', 'DT', 'NNS', 'IN', 'NNP', ',', 'VBG', 'DT', 'NN', 'IN', 'PRP$', 'VBN', 'JJ', 'NN', ':', 'NN', 'CC', 'JJ', 'JJ', ':', 'JJ', 'JJ', 'NN', 'NN', 'NN', '.']",35
sentiment_analysis,3,248,One observation worth mentioning is that these two variants perform on a par with the KET model on EC .,"['One', 'observation', 'worth', 'mentioning', 'is', 'that', 'these', 'two', 'variants', 'perform', 'on', 'a', 'par', 'with', 'the', 'KET', 'model', 'on', 'EC', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['CD', 'NN', 'IN', 'VBG', 'VBZ', 'IN', 'DT', 'CD', 'NNS', 'VBP', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'NNP', '.']",20
sentiment_analysis,3,274,It is clear that both context and knowledge are essential to the strong performance of KET on all datasets .,"['It', 'is', 'clear', 'that', 'both', 'context', 'and', 'knowledge', 'are', 'essential', 'to', 'the', 'strong', 'performance', 'of', 'KET', 'on', 'all', 'datasets', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBZ', 'JJ', 'IN', 'DT', 'NN', 'CC', 'NN', 'VBP', 'JJ', 'TO', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'IN', 'DT', 'NNS', '.']",20
sentiment_analysis,3,275,"Note that removing context has a greater impact on long conversations than short conversations , which is expected because more contextual information is lost in long conversations .","['Note', 'that', 'removing', 'context', 'has', 'a', 'greater', 'impact', 'on', 'long', 'conversations', 'than', 'short', 'conversations', ',', 'which', 'is', 'expected', 'because', 'more', 'contextual', 'information', 'is', 'lost', 'in', 'long', 'conversations', '.']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'IN', 'VBG', 'NN', 'VBZ', 'DT', 'JJR', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', ',', 'WDT', 'VBZ', 'VBN', 'RB', 'RBR', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', 'NNS', '.']",28
sentiment_analysis,38,13,Representation learning ) plays a critical role in many modern machine learning systems .,"['Representation', 'learning', ')', 'plays', 'a', 'critical', 'role', 'in', 'many', 'modern', 'machine', 'learning', 'systems', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBG', ')', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'VBG', 'NNS', '.']",14
sentiment_analysis,38,49,We focus in on the task of sentiment analysis and attempt to learn an unsupervised representation that accurately contains this concept .,"['We', 'focus', 'in', 'on', 'the', 'task', 'of', 'sentiment', 'analysis', 'and', 'attempt', 'to', 'learn', 'an', 'unsupervised', 'representation', 'that', 'accurately', 'contains', 'this', 'concept', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', 'CC', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'WDT', 'RB', 'VBZ', 'DT', 'NN', '.']",22
sentiment_analysis,38,51,"As an approach , we consider the popular research benchmark of byte ( character ) level language modelling due to its further simplicity and generality .","['As', 'an', 'approach', ',', 'we', 'consider', 'the', 'popular', 'research', 'benchmark', 'of', 'byte', '(', 'character', ')', 'level', 'language', 'modelling', 'due', 'to', 'its', 'further', 'simplicity', 'and', 'generality', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NN', '(', 'NN', ')', 'NN', 'NN', 'VBG', 'JJ', 'TO', 'PRP$', 'JJ', 'NN', 'CC', 'NN', '.']",26
sentiment_analysis,38,53,We train on a very large corpus picked to have a similar distribution as our task of interest .,"['We', 'train', 'on', 'a', 'very', 'large', 'corpus', 'picked', 'to', 'have', 'a', 'similar', 'distribution', 'as', 'our', 'task', 'of', 'interest', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'RB', 'JJ', 'NN', 'VBD', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'PRP$', 'NN', 'IN', 'NN', '.']",19
sentiment_analysis,38,99,Review Sentiment Analysis,"['Review', 'Sentiment', 'Analysis']","['B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NN']",3
sentiment_analysis,38,105,The representation learned by our model achieves 91.8 % significantly outperforming the state of the art of 90.2 % by a 30 model ensemble .,"['The', 'representation', 'learned', 'by', 'our', 'model', 'achieves', '91.8', '%', 'significantly', 'outperforming', 'the', 'state', 'of', 'the', 'art', 'of', '90.2', '%', 'by', 'a', '30', 'model', 'ensemble', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBN', 'IN', 'PRP$', 'NN', 'VBZ', 'CD', 'NN', 'RB', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'CD', 'NN', 'IN', 'DT', 'CD', 'NN', 'NN', '.']",25
sentiment_analysis,38,107,It matches the performance of baselines using as few as a dozen labeled examples and outperforms all previous results with only a few hundred labeled examples .,"['It', 'matches', 'the', 'performance', 'of', 'baselines', 'using', 'as', 'few', 'as', 'a', 'dozen', 'labeled', 'examples', 'and', 'outperforms', 'all', 'previous', 'results', 'with', 'only', 'a', 'few', 'hundred', 'labeled', 'examples', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'DT', 'NN', 'IN', 'NNS', 'VBG', 'RB', 'JJ', 'IN', 'DT', 'NN', 'VBN', 'NNS', 'CC', 'NNS', 'DT', 'JJ', 'NNS', 'IN', 'RB', 'DT', 'JJ', 'CD', 'VBD', 'NNS', '.']",27
sentiment_analysis,38,109,"Confusingly , despite a 16 % relative error reduction on the binary subtask , it does not reach the state of the art of 53.6 % on the fine - grained subtask , achieving 52.9 % .","['Confusingly', ',', 'despite', 'a', '16', '%', 'relative', 'error', 'reduction', 'on', 'the', 'binary', 'subtask', ',', 'it', 'does', 'not', 'reach', 'the', 'state', 'of', 'the', 'art', 'of', '53.6', '%', 'on', 'the', 'fine', '-', 'grained', 'subtask', ',', 'achieving', '52.9', '%', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'IN', 'DT', 'CD', 'NN', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBZ', 'RB', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'CD', 'NN', 'IN', 'DT', 'JJ', ':', 'VBN', 'NN', ',', 'VBG', 'CD', 'NN', '.']",37
sentiment_analysis,38,112,L1 regularization is known to reduce sample complexity when there are many irrelevant features .,"['L1', 'regularization', 'is', 'known', 'to', 'reduce', 'sample', 'complexity', 'when', 'there', 'are', 'many', 'irrelevant', 'features', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'JJ', 'NN', 'WRB', 'EX', 'VBP', 'JJ', 'JJ', 'NNS', '.']",15
sentiment_analysis,38,117,"Fitting a threshold to this single unit achieves a test accuracy of 92.30 % which outperforms a strong supervised results on the dataset , the 91.87 % of NB - SVM trigram , but is still below the semi-supervised state of the art of 94.09 % .","['Fitting', 'a', 'threshold', 'to', 'this', 'single', 'unit', 'achieves', 'a', 'test', 'accuracy', 'of', '92.30', '%', 'which', 'outperforms', 'a', 'strong', 'supervised', 'results', 'on', 'the', 'dataset', ',', 'the', '91.87', '%', 'of', 'NB', '-', 'SVM', 'trigram', ',', 'but', 'is', 'still', 'below', 'the', 'semi-supervised', 'state', 'of', 'the', 'art', 'of', '94.09', '%', '.']","['B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['VBG', 'DT', 'NN', 'TO', 'DT', 'JJ', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'CD', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'NN', ',', 'DT', 'CD', 'NN', 'IN', 'NNP', ':', 'NNP', 'NN', ',', 'CC', 'VBZ', 'RB', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'CD', 'NN', '.']",47
sentiment_analysis,38,118,Using the full 4096 unit representation achieves 92.88 % .,"['Using', 'the', 'full', '4096', 'unit', 'representation', 'achieves', '92.88', '%', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['VBG', 'DT', 'JJ', 'CD', 'NN', 'NN', 'VBZ', 'CD', 'NN', '.']",10
sentiment_analysis,38,122,Capacity Ceiling,"['Capacity', 'Ceiling']","['B-n', 'I-n']","['NNP', 'NNP']",2
sentiment_analysis,38,124,We try our approach on the binary version of the Yelp Dataset Challenge in 2015 as introduced in .,"['We', 'try', 'our', 'approach', 'on', 'the', 'binary', 'version', 'of', 'the', 'Yelp', 'Dataset', 'Challenge', 'in', '2015', 'as', 'introduced', 'in', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'IN', 'CD', 'IN', 'VBN', 'IN', '.']",19
sentiment_analysis,38,127,"Using the full dataset , we achieve 95 . 22 % test accuracy .","['Using', 'the', 'full', 'dataset', ',', 'we', 'achieve', '95', '.', '22', '%', 'test', 'accuracy', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'CD', '.', 'CD', 'NN', 'NN', 'NN', '.']",14
sentiment_analysis,38,129,The observed capacity ceiling is an interesting phenomena and stumbling point for scaling our unsupervised representations .,"['The', 'observed', 'capacity', 'ceiling', 'is', 'an', 'interesting', 'phenomena', 'and', 'stumbling', 'point', 'for', 'scaling', 'our', 'unsupervised', 'representations', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'CC', 'JJ', 'NN', 'IN', 'VBG', 'PRP$', 'JJ', 'NNS', '.']",17
sentiment_analysis,38,134,"Additionally , there is a notable drop in the relative performance of our approach transitioning from sentence to document datasets .","['Additionally', ',', 'there', 'is', 'a', 'notable', 'drop', 'in', 'the', 'relative', 'performance', 'of', 'our', 'approach', 'transitioning', 'from', 'sentence', 'to', 'document', 'datasets', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'EX', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'PRP$', 'NN', 'VBG', 'IN', 'NN', 'TO', 'NN', 'NNS', '.']",21
sentiment_analysis,38,136,"Finally , as the amount of labeled data increases , the performance of the simple linear model we train on top of our static representation will eventually saturate .","['Finally', ',', 'as', 'the', 'amount', 'of', 'labeled', 'data', 'increases', ',', 'the', 'performance', 'of', 'the', 'simple', 'linear', 'model', 'we', 'train', 'on', 'top', 'of', 'our', 'static', 'representation', 'will', 'eventually', 'saturate', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-n', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['RB', ',', 'IN', 'DT', 'NN', 'IN', 'VBN', 'NN', 'NNS', ',', 'DT', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'PRP', 'VBP', 'IN', 'NN', 'IN', 'PRP$', 'JJ', 'NN', 'MD', 'RB', 'VB', '.']",29
sentiment_analysis,15,2,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,"['Recursive', 'Deep', 'Models', 'for', 'Semantic', 'Compositionality', 'Over', 'a', 'Sentiment', 'Treebank']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'DT', 'NNP', 'NNP']",10
sentiment_analysis,15,5,Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition .,"['Further', 'progress', 'towards', 'understanding', 'compositionality', 'in', 'tasks', 'such', 'as', 'sentiment', 'detection', 'requires', 'richer', 'supervised', 'training', 'and', 'evaluation', 'resources', 'and', 'more', 'powerful', 'models', 'of', 'composition', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'NN', 'NNS', 'VBG', 'NN', 'IN', 'NNS', 'JJ', 'IN', 'NN', 'NN', 'VBZ', 'JJR', 'JJ', 'NN', 'CC', 'NN', 'NNS', 'CC', 'JJR', 'JJ', 'NNS', 'IN', 'NN', '.']",25
sentiment_analysis,15,20,The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language .,"['The', 'Stanford', 'Sentiment', 'Treebank', 'is', 'the', 'first', 'corpus', 'with', 'fully', 'labeled', 'parse', 'trees', 'that', 'allows', 'for', 'a', 'complete', 'analysis', 'of', 'the', 'compositional', 'effects', 'of', 'sentiment', 'in', 'language', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'RB', 'VBN', 'NN', 'NNS', 'WDT', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NN', 'IN', 'NN', '.']",28
sentiment_analysis,15,21,"The corpus is based on the dataset introduced by and consists of 11,855 single sentences extracted from movie reviews .","['The', 'corpus', 'is', 'based', 'on', 'the', 'dataset', 'introduced', 'by', 'and', 'consists', 'of', '11,855', 'single', 'sentences', 'extracted', 'from', 'movie', 'reviews', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'VBN', 'IN', 'CC', 'NNS', 'IN', 'CD', 'JJ', 'NNS', 'VBN', 'IN', 'NN', 'NNS', '.']",20
sentiment_analysis,15,22,"It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees , each annotated by 3 human judges .","['It', 'was', 'parsed', 'with', 'the', 'Stanford', 'parser', 'and', 'includes', 'a', 'total', 'of', '215,154', 'unique', 'phrases', 'from', 'those', 'parse', 'trees', ',', 'each', 'annotated', 'by', '3', 'human', 'judges', '.']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'VBN', 'IN', 'DT', 'NNP', 'NN', 'CC', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NNS', ',', 'DT', 'VBN', 'IN', 'CD', 'JJ', 'NNS', '.']",27
sentiment_analysis,15,23,This new dataset allows us to analyze the intricacies of sentiment and to capture complex linguistic phenomena .,"['This', 'new', 'dataset', 'allows', 'us', 'to', 'analyze', 'the', 'intricacies', 'of', 'sentiment', 'and', 'to', 'capture', 'complex', 'linguistic', 'phenomena', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'PRP', 'TO', 'VB', 'DT', 'NNS', 'IN', 'NN', 'CC', 'TO', 'VB', 'JJ', 'JJ', 'NN', '.']",18
sentiment_analysis,15,25,The granularity and size of this dataset will enable the community to train compositional models that are based on supervised and structured machine learning techniques .,"['The', 'granularity', 'and', 'size', 'of', 'this', 'dataset', 'will', 'enable', 'the', 'community', 'to', 'train', 'compositional', 'models', 'that', 'are', 'based', 'on', 'supervised', 'and', 'structured', 'machine', 'learning', 'techniques', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'CC', 'NN', 'IN', 'DT', 'NN', 'MD', 'VB', 'DT', 'NN', 'TO', 'VB', 'JJ', 'NNS', 'WDT', 'VBP', 'VBN', 'IN', 'JJ', 'CC', 'JJ', 'NN', 'VBG', 'NNS', '.']",26
sentiment_analysis,15,27,"In order to capture the compositional effects with higher accuracy , we propose a new model called the Recursive Neural Tensor Network ( RNTN ) .","['In', 'order', 'to', 'capture', 'the', 'compositional', 'effects', 'with', 'higher', 'accuracy', ',', 'we', 'propose', 'a', 'new', 'model', 'called', 'the', 'Recursive', 'Neural', 'Tensor', 'Network', '(', 'RNTN', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'IN', 'JJR', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBD', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', '.']",26
sentiment_analysis,15,28,Recursive Neural Tensor Networks take as input phrases of any length .,"['Recursive', 'Neural', 'Tensor', 'Networks', 'take', 'as', 'input', 'phrases', 'of', 'any', 'length', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['JJ', 'NNP', 'NNP', 'NNP', 'VB', 'IN', 'NN', 'NNS', 'IN', 'DT', 'NN', '.']",12
sentiment_analysis,15,29,They represent a phrase through word vectors and a parse tree and then compute vectors for higher nodes in the tree using the same tensor - based composition function .,"['They', 'represent', 'a', 'phrase', 'through', 'word', 'vectors', 'and', 'a', 'parse', 'tree', 'and', 'then', 'compute', 'vectors', 'for', 'higher', 'nodes', 'in', 'the', 'tree', 'using', 'the', 'same', 'tensor', '-', 'based', 'composition', 'function', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'NNS', 'CC', 'DT', 'NN', 'NN', 'CC', 'RB', 'VB', 'NNS', 'IN', 'JJR', 'NNS', 'IN', 'DT', 'NN', 'VBG', 'DT', 'JJ', 'NN', ':', 'VBN', 'NN', 'NN', '.']",30
sentiment_analysis,15,202,Optimal performance for all models was achieved at word vector sizes between 25 and 35 dimensions and batch sizes between 20 and 30 .,"['Optimal', 'performance', 'for', 'all', 'models', 'was', 'achieved', 'at', 'word', 'vector', 'sizes', 'between', '25', 'and', '35', 'dimensions', 'and', 'batch', 'sizes', 'between', '20', 'and', '30', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['JJ', 'NN', 'IN', 'DT', 'NNS', 'VBD', 'VBN', 'IN', 'NN', 'NN', 'VBZ', 'IN', 'CD', 'CC', 'CD', 'NNS', 'CC', 'NN', 'NNS', 'IN', 'CD', 'CC', 'CD', '.']",24
sentiment_analysis,15,206,The RNTN would usually achieve its best performance on the dev set after training for 3 - 5 hours .,"['The', 'RNTN', 'would', 'usually', 'achieve', 'its', 'best', 'performance', 'on', 'the', 'dev', 'set', 'after', 'training', 'for', '3', '-', '5', 'hours', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'MD', 'RB', 'VB', 'PRP$', 'JJS', 'NN', 'IN', 'DT', 'NN', 'VBN', 'IN', 'VBG', 'IN', 'CD', ':', 'CD', 'NNS', '.']",20
sentiment_analysis,15,208,We use f = tanh in all experiments .,"['We', 'use', 'f', '=', 'tanh', 'in', 'all', 'experiments', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NNP', 'NN', 'IN', 'DT', 'NNS', '.']",9
sentiment_analysis,15,209,"We compare to commonly used methods that use bag of words features with Naive Bayes and SVMs , as well as Naive Bayes with bag of bigram features .","['We', 'compare', 'to', 'commonly', 'used', 'methods', 'that', 'use', 'bag', 'of', 'words', 'features', 'with', 'Naive', 'Bayes', 'and', 'SVMs', ',', 'as', 'well', 'as', 'Naive', 'Bayes', 'with', 'bag', 'of', 'bigram', 'features', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'TO', 'RB', 'VBN', 'NNS', 'WDT', 'VBP', 'NN', 'IN', 'NNS', 'NNS', 'IN', 'JJ', 'NNP', 'CC', 'NNP', ',', 'RB', 'RB', 'IN', 'JJ', 'NNP', 'IN', 'NN', 'IN', 'NN', 'NNS', '.']",29
sentiment_analysis,15,211,We also compare to a model that averages neural word vectors and ignores word order ( VecAvg ) .,"['We', 'also', 'compare', 'to', 'a', 'model', 'that', 'averages', 'neural', 'word', 'vectors', 'and', 'ignores', 'word', 'order', '(', 'VecAvg', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'TO', 'DT', 'NN', 'WDT', 'VBZ', 'JJ', 'NN', 'NNS', 'CC', 'NNS', 'NN', 'NN', '(', 'NNP', ')', '.']",19
sentiment_analysis,15,212,"The sentences in the treebank were split into a train ( 8544 ) , dev ( 1101 ) and test splits ( 2210 ) and these splits are made available with the data release .","['The', 'sentences', 'in', 'the', 'treebank', 'were', 'split', 'into', 'a', 'train', '(', '8544', ')', ',', 'dev', '(', '1101', ')', 'and', 'test', 'splits', '(', '2210', ')', 'and', 'these', 'splits', 'are', 'made', 'available', 'with', 'the', 'data', 'release', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'DT', 'NN', 'VBD', 'VBN', 'IN', 'DT', 'NN', '(', 'CD', ')', ',', 'FW', '(', 'CD', ')', 'CC', 'NN', 'NNS', '(', 'CD', ')', 'CC', 'DT', 'NNS', 'VBP', 'VBN', 'JJ', 'IN', 'DT', 'NN', 'NN', '.']",35
sentiment_analysis,15,217,showed that a fine grained classification into 5 classes is a reasonable approximation to capture most of the data variation .,"['showed', 'that', 'a', 'fine', 'grained', 'classification', 'into', '5', 'classes', 'is', 'a', 'reasonable', 'approximation', 'to', 'capture', 'most', 'of', 'the', 'data', 'variation', '.']","['B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBD', 'IN', 'DT', 'JJ', 'VBN', 'NN', 'IN', 'CD', 'NNS', 'VBZ', 'DT', 'JJ', 'NN', 'TO', 'VB', 'JJS', 'IN', 'DT', 'NNS', 'NN', '.']",21
sentiment_analysis,15,219,"The RNTN gets the highest performance , followed by the MV - RNN and RNN .","['The', 'RNTN', 'gets', 'the', 'highest', 'performance', ',', 'followed', 'by', 'the', 'MV', '-', 'RNN', 'and', 'RNN', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'VBZ', 'DT', 'JJS', 'NN', ',', 'VBN', 'IN', 'DT', 'NNP', ':', 'NN', 'CC', 'NNP', '.']",16
sentiment_analysis,15,220,"The recursive models work very well on shorter phrases , where negation and composition are important , while bag of features baselines perform well only with longer sentences .","['The', 'recursive', 'models', 'work', 'very', 'well', 'on', 'shorter', 'phrases', ',', 'where', 'negation', 'and', 'composition', 'are', 'important', ',', 'while', 'bag', 'of', 'features', 'baselines', 'perform', 'well', 'only', 'with', 'longer', 'sentences', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NNS', 'VBP', 'RB', 'RB', 'IN', 'JJR', 'NNS', ',', 'WRB', 'NN', 'CC', 'NN', 'VBP', 'JJ', ',', 'IN', 'NN', 'IN', 'NNS', 'NNS', 'RB', 'RB', 'RB', 'IN', 'JJR', 'NNS', '.']",29
sentiment_analysis,15,229,The combination of the new sentiment treebank and the RNTN pushes the state of the art on short phrases up to 85.4 % .,"['The', 'combination', 'of', 'the', 'new', 'sentiment', 'treebank', 'and', 'the', 'RNTN', 'pushes', 'the', 'state', 'of', 'the', 'art', 'on', 'short', 'phrases', 'up', 'to', '85.4', '%', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'CC', 'DT', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'RB', 'TO', 'CD', 'NN', '.']",24
sentiment_analysis,15,247,"The RNTN has the highest reversal accuracy , showing its ability to structurally learn negation of positive sentences .","['The', 'RNTN', 'has', 'the', 'highest', 'reversal', 'accuracy', ',', 'showing', 'its', 'ability', 'to', 'structurally', 'learn', 'negation', 'of', 'positive', 'sentences', '.']","['O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'VBZ', 'DT', 'JJS', 'NN', 'NN', ',', 'VBG', 'PRP$', 'NN', 'TO', 'RB', 'VB', 'NN', 'IN', 'JJ', 'NNS', '.']",19
sentiment_analysis,15,255,shows a typical case in which sentiment was made more positive by switching the main class from negative to neutral even though both not and dull were negative .,"['shows', 'a', 'typical', 'case', 'in', 'which', 'sentiment', 'was', 'made', 'more', 'positive', 'by', 'switching', 'the', 'main', 'class', 'from', 'negative', 'to', 'neutral', 'even', 'though', 'both', 'not', 'and', 'dull', 'were', 'negative', '.']","['B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBZ', 'DT', 'JJ', 'NN', 'IN', 'WDT', 'NN', 'VBD', 'VBN', 'RBR', 'JJ', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'TO', 'JJ', 'RB', 'IN', 'DT', 'RB', 'CC', 'VB', 'VBD', 'JJ', '.']",29
sentiment_analysis,15,259,Therefore we can conclude that the RNTN is best able to identify the effect of negations upon both positive and negative sentiment sentences . :,"['Therefore', 'we', 'can', 'conclude', 'that', 'the', 'RNTN', 'is', 'best', 'able', 'to', 'identify', 'the', 'effect', 'of', 'negations', 'upon', 'both', 'positive', 'and', 'negative', 'sentiment', 'sentences', '.', ':']","['O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'PRP', 'MD', 'VB', 'IN', 'DT', 'NNP', 'VBZ', 'RBS', 'JJ', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'JJ', 'CC', 'JJ', 'NN', 'NNS', '.', ':']",25
sentiment_analysis,46,2,A Multi-sentiment - resource Enhanced Attention Network for Sentiment Classification,"['A', 'Multi-sentiment', '-', 'resource', 'Enhanced', 'Attention', 'Network', 'for', 'Sentiment', 'Classification']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['DT', 'JJ', ':', 'NN', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",10
sentiment_analysis,46,15,"In this work , we propose a Multi- sentimentresource Enhanced Attention Network ( MEAN ) for sentence - level sentiment classification to integrate many kinds of sentiment linguistic knowledge into deep neural networks via multi -path attention mechanism .","['In', 'this', 'work', ',', 'we', 'propose', 'a', 'Multi-', 'sentimentresource', 'Enhanced', 'Attention', 'Network', '(', 'MEAN', ')', 'for', 'sentence', '-', 'level', 'sentiment', 'classification', 'to', 'integrate', 'many', 'kinds', 'of', 'sentiment', 'linguistic', 'knowledge', 'into', 'deep', 'neural', 'networks', 'via', 'multi', '-path', 'attention', 'mechanism', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', 'NN', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'IN', 'NN', ':', 'NN', 'NN', 'NN', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'NN', 'JJ', 'NN', 'IN', 'JJ', 'JJ', 'NNS', 'IN', 'NN', 'NNP', 'NN', 'NN', '.']",39
sentiment_analysis,46,16,"Specifically , we first design a coupled word embedding module to model the word representation from character - level and word - level semantics .","['Specifically', ',', 'we', 'first', 'design', 'a', 'coupled', 'word', 'embedding', 'module', 'to', 'model', 'the', 'word', 'representation', 'from', 'character', '-', 'level', 'and', 'word', '-', 'level', 'semantics', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'RB', 'VB', 'DT', 'JJ', 'NN', 'VBG', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'NN', ':', 'NN', 'CC', 'NN', ':', 'NN', 'NNS', '.']",25
sentiment_analysis,46,18,"Then , we propose a multisentiment - resource attention module to learn more comprehensive and meaningful sentiment - specific sentence representation by using the three types of sentiment resource words as attention sources attending to the context words respectively .","['Then', ',', 'we', 'propose', 'a', 'multisentiment', '-', 'resource', 'attention', 'module', 'to', 'learn', 'more', 'comprehensive', 'and', 'meaningful', 'sentiment', '-', 'specific', 'sentence', 'representation', 'by', 'using', 'the', 'three', 'types', 'of', 'sentiment', 'resource', 'words', 'as', 'attention', 'sources', 'attending', 'to', 'the', 'context', 'words', 'respectively', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', ':', 'NN', 'NN', 'NN', 'TO', 'VB', 'RBR', 'JJ', 'CC', 'JJ', 'NN', ':', 'JJ', 'NN', 'NN', 'IN', 'VBG', 'DT', 'CD', 'NNS', 'IN', 'NN', 'NN', 'NNS', 'IN', 'NN', 'NNS', 'VBG', 'TO', 'DT', 'NN', 'NNS', 'RB', '.']",40
sentiment_analysis,46,19,"In this way , we can attend to different sentimentrelevant information from different representation subspaces implied by different types of sentiment sources and capture the over all semantics of the sentiment , negation and intensity words for sentiment prediction .","['In', 'this', 'way', ',', 'we', 'can', 'attend', 'to', 'different', 'sentimentrelevant', 'information', 'from', 'different', 'representation', 'subspaces', 'implied', 'by', 'different', 'types', 'of', 'sentiment', 'sources', 'and', 'capture', 'the', 'over', 'all', 'semantics', 'of', 'the', 'sentiment', ',', 'negation', 'and', 'intensity', 'words', 'for', 'sentiment', 'prediction', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'MD', 'VB', 'TO', 'JJ', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NNS', 'VBN', 'IN', 'JJ', 'NNS', 'IN', 'NN', 'NNS', 'CC', 'NN', 'DT', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', ',', 'NN', 'CC', 'NN', 'NNS', 'IN', 'NN', 'NN', '.']",40
sentiment_analysis,46,90,RNTN : Recursive Tensor Neural Network ) is used to model correlations between different dimensions of child nodes vectors .,"['RNTN', ':', 'Recursive', 'Tensor', 'Neural', 'Network', ')', 'is', 'used', 'to', 'model', 'correlations', 'between', 'different', 'dimensions', 'of', 'child', 'nodes', 'vectors', '.']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NN', ':', 'JJ', 'NNP', 'NNP', 'NNP', ')', 'VBZ', 'VBN', 'TO', 'VB', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'NN', 'NNS', 'NNS', '.']",20
sentiment_analysis,46,91,LSTM / Bi-LSTM : Cho et al. ( 2014 ) employs Long Short - Term Memory and the bidirectional variant to capture sequential information .,"['LSTM', '/', 'Bi-LSTM', ':', 'Cho', 'et', 'al.', '(', '2014', ')', 'employs', 'Long', 'Short', '-', 'Term', 'Memory', 'and', 'the', 'bidirectional', 'variant', 'to', 'capture', 'sequential', 'information', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['NNP', 'VBD', 'NNP', ':', 'NNP', 'CC', 'NN', '(', 'CD', ')', 'VBZ', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'CC', 'DT', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'NN', '.']",25
sentiment_analysis,46,92,"Tree-LSTM : Memory cells was introduced by Tree - Structured Long Short - Term Memory and gates into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .","['Tree-LSTM', ':', 'Memory', 'cells', 'was', 'introduced', 'by', 'Tree', '-', 'Structured', 'Long', 'Short', '-', 'Term', 'Memory', 'and', 'gates', 'into', 'tree', '-', 'structured', 'neural', 'network', ',', 'which', 'is', 'beneficial', 'to', 'capture', 'semantic', 'relatedness', 'by', 'parsing', 'syntax', 'trees', '.']","['B-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', ':', 'NN', 'NNS', 'VBD', 'VBN', 'IN', 'NNP', ':', 'VBD', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'CC', 'VBZ', 'IN', 'JJ', ':', 'VBN', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'JJ', 'TO', 'VB', 'JJ', 'NN', 'IN', 'VBG', 'NN', 'NNS', '.']",36
sentiment_analysis,46,93,CNN : Convolutional Neural Networks ) is applied to generate task - specific sentence representation .,"['CNN', ':', 'Convolutional', 'Neural', 'Networks', ')', 'is', 'applied', 'to', 'generate', 'task', '-', 'specific', 'sentence', 'representation', '.']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', ':', 'JJ', 'NNP', 'NNP', ')', 'VBZ', 'VBN', 'TO', 'VB', 'NN', ':', 'JJ', 'NN', 'NN', '.']",16
sentiment_analysis,46,94,NCSL : designs a Neural Context - Sensitive Lexicon ( NSCL ) to obtain prior sentiment scores of words in the sentence .,"['NCSL', ':', 'designs', 'a', 'Neural', 'Context', '-', 'Sensitive', 'Lexicon', '(', 'NSCL', ')', 'to', 'obtain', 'prior', 'sentiment', 'scores', 'of', 'words', 'in', 'the', 'sentence', '.']","['B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['NN', ':', 'VBZ', 'DT', 'NNP', 'NNP', ':', 'JJ', 'NNP', '(', 'NNP', ')', 'TO', 'VB', 'JJ', 'NN', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'NN', '.']",23
sentiment_analysis,46,95,LR - Bi-LSTM : imposes linguistic roles into neural networks by applying linguistic regularization on intermediate outputs with KL divergence .,"['LR', '-', 'Bi-LSTM', ':', 'imposes', 'linguistic', 'roles', 'into', 'neural', 'networks', 'by', 'applying', 'linguistic', 'regularization', 'on', 'intermediate', 'outputs', 'with', 'KL', 'divergence', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ':', 'NN', ':', 'NNS', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'VBG', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NNP', 'NN', '.']",21
sentiment_analysis,46,96,Self - attention : proposes a selfattention mechanism to learn structured sentence embedding .,"['Self', '-', 'attention', ':', 'proposes', 'a', 'selfattention', 'mechanism', 'to', 'learn', 'structured', 'sentence', 'embedding', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', ':', 'VBZ', 'DT', 'NN', 'NN', 'TO', 'VB', 'JJ', 'NN', 'NN', '.']",14
sentiment_analysis,46,97,ID - LSTM : uses reinforcement learning to learn structured sentence representation for sentiment classification .,"['ID', '-', 'LSTM', ':', 'uses', 'reinforcement', 'learning', 'to', 'learn', 'structured', 'sentence', 'representation', 'for', 'sentiment', 'classification', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ':', 'NN', ':', 'VBZ', 'NN', 'VBG', 'TO', 'VB', 'JJ', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",16
sentiment_analysis,46,99,"In our experiments , the dimensions of characterlevel embedding and word embedding ( Glo Ve ) are both set to 300 .","['In', 'our', 'experiments', ',', 'the', 'dimensions', 'of', 'characterlevel', 'embedding', 'and', 'word', 'embedding', '(', 'Glo', 'Ve', ')', 'are', 'both', 'set', 'to', '300', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'PRP$', 'NNS', ',', 'DT', 'NNS', 'IN', 'NN', 'NN', 'CC', 'NN', 'NN', '(', 'NNP', 'NNP', ')', 'VBP', 'DT', 'VBN', 'TO', 'CD', '.']",22
sentiment_analysis,46,100,"Kernel sizes of multi-gram convolution for Char - CNN are set to 2 , 3 , respectively .","['Kernel', 'sizes', 'of', 'multi-gram', 'convolution', 'for', 'Char', '-', 'CNN', 'are', 'set', 'to', '2', ',', '3', ',', 'respectively', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['NNP', 'NNS', 'IN', 'JJ', 'NN', 'IN', 'NNP', ':', 'NN', 'VBP', 'VBN', 'TO', 'CD', ',', 'CD', ',', 'RB', '.']",18
sentiment_analysis,46,101,"All the weight matrices are initialized as random orthogonal matrices , and we set all the bias vectors as zero vectors .","['All', 'the', 'weight', 'matrices', 'are', 'initialized', 'as', 'random', 'orthogonal', 'matrices', ',', 'and', 'we', 'set', 'all', 'the', 'bias', 'vectors', 'as', 'zero', 'vectors', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PDT', 'DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'JJ', 'JJ', 'NNS', ',', 'CC', 'PRP', 'VBP', 'PDT', 'DT', 'JJ', 'NNS', 'IN', 'NN', 'NNS', '.']",22
sentiment_analysis,46,102,"We optimize the proposed model with RMSprop algorithm , using mini-batch training .","['We', 'optimize', 'the', 'proposed', 'model', 'with', 'RMSprop', 'algorithm', ',', 'using', 'mini-batch', 'training', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'VBN', 'NN', 'IN', 'NNP', 'NN', ',', 'VBG', 'JJ', 'NN', '.']",13
sentiment_analysis,46,103,The size of mini-batch is 60 .,"['The', 'size', 'of', 'mini-batch', 'is', '60', '.']","['O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NN', 'VBZ', 'CD', '.']",7
sentiment_analysis,46,104,"The dropout rate is 0.5 , and the coefficient ?","['The', 'dropout', 'rate', 'is', '0.5', ',', 'and', 'the', 'coefficient', '?']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'CD', ',', 'CC', 'DT', 'NN', '.']",10
sentiment_analysis,46,105,of L 2 normalization is set to 10 ?5 . is set to 10 ? 4 . ?,"['of', 'L', '2', 'normalization', 'is', 'set', 'to', '10', '?5', '.', 'is', 'set', 'to', '10', '?', '4', '.', '?']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', 'CD', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'NN', '.', 'VBZ', 'VBN', 'TO', 'CD', '.', 'CD', '.', '.']",18
sentiment_analysis,46,112,"First , our model brings a substantial improvement over the methods that do not leverage sentiment linguistic knowledge ( e.g. , RNTN , LSTM , BiLSTM , CNN and ID - LSTM ) on both datasets .","['First', ',', 'our', 'model', 'brings', 'a', 'substantial', 'improvement', 'over', 'the', 'methods', 'that', 'do', 'not', 'leverage', 'sentiment', 'linguistic', 'knowledge', '(', 'e.g.', ',', 'RNTN', ',', 'LSTM', ',', 'BiLSTM', ',', 'CNN', 'and', 'ID', '-', 'LSTM', ')', 'on', 'both', 'datasets', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNS', 'WDT', 'VBP', 'RB', 'VB', 'JJ', 'JJ', 'NN', '(', 'JJ', ',', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', 'CC', 'NNP', ':', 'NNP', ')', 'IN', 'DT', 'NNS', '.']",37
sentiment_analysis,46,114,"Second , our model also consistently outperforms LR - Bi - LSTM which integrates linguistic roles of sentiment , negation and intensity words into neural networks via the linguistic regularization .","['Second', ',', 'our', 'model', 'also', 'consistently', 'outperforms', 'LR', '-', 'Bi', '-', 'LSTM', 'which', 'integrates', 'linguistic', 'roles', 'of', 'sentiment', ',', 'negation', 'and', 'intensity', 'words', 'into', 'neural', 'networks', 'via', 'the', 'linguistic', 'regularization', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['JJ', ',', 'PRP$', 'NN', 'RB', 'RB', 'VBZ', 'NNP', ':', 'NNP', ':', 'NNP', 'WDT', 'VBZ', 'JJ', 'NNS', 'IN', 'NN', ',', 'NN', 'CC', 'NN', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",31
sentiment_analysis,46,115,"For example , our model achieves 2.4 % improvements over the MR dataset and 0.8 % improvements over the SST dataset compared to LR - Bi - LSTM .","['For', 'example', ',', 'our', 'model', 'achieves', '2.4', '%', 'improvements', 'over', 'the', 'MR', 'dataset', 'and', '0.8', '%', 'improvements', 'over', 'the', 'SST', 'dataset', 'compared', 'to', 'LR', '-', 'Bi', '-', 'LSTM', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'CD', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'CC', 'CD', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'VBN', 'TO', 'NNP', ':', 'NNP', ':', 'NN', '.']",29
sentiment_analysis,36,2,Transformation Networks for Target - Oriented Sentiment Classification,"['Transformation', 'Networks', 'for', 'Target', '-', 'Oriented', 'Sentiment', 'Classification']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NN', 'NNS', 'IN', 'NNP', ':', 'VBN', 'NNP', 'NNP']",8
sentiment_analysis,36,29,"We propose a new architecture , named Target - Specific Transformation Networks ( TNet ) , to solve the above issues in the task of target sentiment classification .","['We', 'propose', 'a', 'new', 'architecture', ',', 'named', 'Target', '-', 'Specific', 'Transformation', 'Networks', '(', 'TNet', ')', ',', 'to', 'solve', 'the', 'above', 'issues', 'in', 'the', 'task', 'of', 'target', 'sentiment', 'classification', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', ',', 'VBN', 'NNP', ':', 'JJ', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', 'NN', '.']",29
sentiment_analysis,36,30,TNet firstly encodes the context information into word embeddings and generates the contextualized word representations with LSTMs .,"['TNet', 'firstly', 'encodes', 'the', 'context', 'information', 'into', 'word', 'embeddings', 'and', 'generates', 'the', 'contextualized', 'word', 'representations', 'with', 'LSTMs', '.']","['B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', 'RB', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'NN', 'NNS', 'CC', 'VBZ', 'DT', 'JJ', 'NN', 'NNS', 'IN', 'NNP', '.']",18
sentiment_analysis,36,31,"To integrate the target information into the word representations , TNet introduces a novel Target - Specific Transformation ( TST ) component for generating the target - specific word representations .","['To', 'integrate', 'the', 'target', 'information', 'into', 'the', 'word', 'representations', ',', 'TNet', 'introduces', 'a', 'novel', 'Target', '-', 'Specific', 'Transformation', '(', 'TST', ')', 'component', 'for', 'generating', 'the', 'target', '-', 'specific', 'word', 'representations', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NNS', ',', 'NNP', 'VBZ', 'DT', 'JJ', 'NNP', ':', 'JJ', 'NNP', '(', 'NNP', ')', 'NN', 'IN', 'VBG', 'DT', 'NN', ':', 'JJ', 'NN', 'NNS', '.']",31
sentiment_analysis,36,32,"Contrary to the previous attention - based approaches which apply the same target representation to determine the attention scores of individual context words , TST firstly generates different representations of the target conditioned on individual context words , then it consolidates each context word with its tailor - made target representation to obtain the transformed word representation .","['Contrary', 'to', 'the', 'previous', 'attention', '-', 'based', 'approaches', 'which', 'apply', 'the', 'same', 'target', 'representation', 'to', 'determine', 'the', 'attention', 'scores', 'of', 'individual', 'context', 'words', ',', 'TST', 'firstly', 'generates', 'different', 'representations', 'of', 'the', 'target', 'conditioned', 'on', 'individual', 'context', 'words', ',', 'then', 'it', 'consolidates', 'each', 'context', 'word', 'with', 'its', 'tailor', '-', 'made', 'target', 'representation', 'to', 'obtain', 'the', 'transformed', 'word', 'representation', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['JJ', 'TO', 'DT', 'JJ', 'NN', ':', 'VBN', 'NNS', 'WDT', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NNS', 'IN', 'JJ', 'NN', 'NNS', ',', 'NNP', 'RB', 'VBZ', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'VBN', 'IN', 'JJ', 'NN', 'NNS', ',', 'RB', 'PRP', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'PRP$', 'NN', ':', 'VBN', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', '.']",58
sentiment_analysis,36,37,"As the context information carried by the representations from the LSTM layer will be lost after the non-linear TST , we design a contextpreserving mechanism to contextualize the generated target - specific word representations .","['As', 'the', 'context', 'information', 'carried', 'by', 'the', 'representations', 'from', 'the', 'LSTM', 'layer', 'will', 'be', 'lost', 'after', 'the', 'non-linear', 'TST', ',', 'we', 'design', 'a', 'contextpreserving', 'mechanism', 'to', 'contextualize', 'the', 'generated', 'target', '-', 'specific', 'word', 'representations', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'NN', 'VBN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'MD', 'VB', 'VBN', 'IN', 'DT', 'JJ', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', ':', 'JJ', 'NN', 'NNS', '.']",35
sentiment_analysis,36,39,"To help the CNN feature extractor locate sentiment indicators more accurately , we adopt a proximity strategy to scale the input of convolutional layer with positional relevance between a word and the target .","['To', 'help', 'the', 'CNN', 'feature', 'extractor', 'locate', 'sentiment', 'indicators', 'more', 'accurately', ',', 'we', 'adopt', 'a', 'proximity', 'strategy', 'to', 'scale', 'the', 'input', 'of', 'convolutional', 'layer', 'with', 'positional', 'relevance', 'between', 'a', 'word', 'and', 'the', 'target', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NNP', 'NN', 'NN', 'JJ', 'NN', 'NNS', 'RBR', 'RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'IN', 'DT', 'NN', 'CC', 'DT', 'NN', '.']",34
sentiment_analysis,36,150,SVM : It is a traditional support vector machine based model with extensive feature engineering ;,"['SVM', ':', 'It', 'is', 'a', 'traditional', 'support', 'vector', 'machine', 'based', 'model', 'with', 'extensive', 'feature', 'engineering', ';']","['B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NN', ':', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'NN', 'VBN', 'NN', 'IN', 'JJ', 'NN', 'NN', ':']",16
sentiment_analysis,36,151,AdaRNN : It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree ;,"['AdaRNN', ':', 'It', 'learns', 'the', 'sentence', 'representation', 'toward', 'target', 'for', 'sentiment', 'prediction', 'via', 'semantic', 'composition', 'over', 'dependency', 'tree', ';']","['B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NN', ':', 'PRP', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'NN', 'IN', 'NN', 'NN', 'IN', 'JJ', 'NN', 'IN', 'NN', 'NN', ':']",19
sentiment_analysis,36,152,"AE - LSTM , and ATAE - LSTM : AE - LSTM is a simple LSTM model incorporating the target embedding as input , while ATAE - LSTM extends AE - LSTM with attention ;","['AE', '-', 'LSTM', ',', 'and', 'ATAE', '-', 'LSTM', ':', 'AE', '-', 'LSTM', 'is', 'a', 'simple', 'LSTM', 'model', 'incorporating', 'the', 'target', 'embedding', 'as', 'input', ',', 'while', 'ATAE', '-', 'LSTM', 'extends', 'AE', '-', 'LSTM', 'with', 'attention', ';']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-p', 'B-n', 'O']","['NNP', ':', 'NNP', ',', 'CC', 'NNP', ':', 'NN', ':', 'NNP', ':', 'NNP', 'VBZ', 'DT', 'JJ', 'NNP', 'NN', 'VBG', 'DT', 'NN', 'VBG', 'IN', 'NN', ',', 'IN', 'NNP', ':', 'NNP', 'VBZ', 'NNP', ':', 'NN', 'IN', 'NN', ':']",35
sentiment_analysis,36,153,IAN : IAN employs two LSTMs to learn the representations of the context and the target phrase interactively ;,"['IAN', ':', 'IAN', 'employs', 'two', 'LSTMs', 'to', 'learn', 'the', 'representations', 'of', 'the', 'context', 'and', 'the', 'target', 'phrase', 'interactively', ';']","['B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['NN', ':', 'NNP', 'VBZ', 'CD', 'NNP', 'TO', 'VB', 'DT', 'NNS', 'IN', 'DT', 'NN', 'CC', 'DT', 'NN', 'NN', 'RB', ':']",19
sentiment_analysis,36,154,CNN - ASP :,"['CNN', '-', 'ASP', ':']","['B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', ':']",4
sentiment_analysis,36,155,It is a CNN - based model implemented by us which directly concatenates target representation to each word embedding ;,"['It', 'is', 'a', 'CNN', '-', 'based', 'model', 'implemented', 'by', 'us', 'which', 'directly', 'concatenates', 'target', 'representation', 'to', 'each', 'word', 'embedding', ';']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'DT', 'NNP', ':', 'VBN', 'NN', 'VBN', 'IN', 'PRP', 'WDT', 'RB', 'VBZ', 'NN', 'NN', 'TO', 'DT', 'NN', 'NN', ':']",20
sentiment_analysis,36,156,TD - LSTM :,"['TD', '-', 'LSTM', ':']","['B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', ':']",4
sentiment_analysis,36,157,"It employs two LSTMs to model the left and right contexts of the target separately , then performs predictions based on concatenated context representations ;","['It', 'employs', 'two', 'LSTMs', 'to', 'model', 'the', 'left', 'and', 'right', 'contexts', 'of', 'the', 'target', 'separately', ',', 'then', 'performs', 'predictions', 'based', 'on', 'concatenated', 'context', 'representations', ';']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'CD', 'NNP', 'TO', 'VB', 'DT', 'NN', 'CC', 'JJ', 'NN', 'IN', 'DT', 'NN', 'RB', ',', 'RB', 'VBZ', 'NNS', 'VBN', 'IN', 'JJ', 'JJ', 'NNS', ':']",25
sentiment_analysis,36,158,MemNet : It applies attention mechanism over the word embeddings multiple times and predicts sentiments based on the top - most sentence representations ;,"['MemNet', ':', 'It', 'applies', 'attention', 'mechanism', 'over', 'the', 'word', 'embeddings', 'multiple', 'times', 'and', 'predicts', 'sentiments', 'based', 'on', 'the', 'top', '-', 'most', 'sentence', 'representations', ';']","['B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', ':', 'PRP', 'VBZ', 'NN', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'JJ', 'NNS', 'CC', 'VBZ', 'NNS', 'VBN', 'IN', 'DT', 'JJ', ':', 'JJS', 'JJ', 'NNS', ':']",24
sentiment_analysis,36,159,"BILSTM - ATT -G : It models left and right contexts using two attention - based LSTMs and introduces gates to measure the importance of left context , right context , and the entire sentence for the prediction ;","['BILSTM', '-', 'ATT', '-G', ':', 'It', 'models', 'left', 'and', 'right', 'contexts', 'using', 'two', 'attention', '-', 'based', 'LSTMs', 'and', 'introduces', 'gates', 'to', 'measure', 'the', 'importance', 'of', 'left', 'context', ',', 'right', 'context', ',', 'and', 'the', 'entire', 'sentence', 'for', 'the', 'prediction', ';']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['NNP', ':', 'NNP', 'NN', ':', 'PRP', 'NNS', 'VBD', 'CC', 'JJ', 'NN', 'VBG', 'CD', 'NN', ':', 'VBN', 'NNP', 'CC', 'NNS', 'NNS', 'TO', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NN', ',', 'JJ', 'NN', ',', 'CC', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', ':']",39
sentiment_analysis,36,160,RAM : RAM is a multilayer architecture where each layer consists of attention - based aggregation of word features and a GRU cell to learn the sentence representation .,"['RAM', ':', 'RAM', 'is', 'a', 'multilayer', 'architecture', 'where', 'each', 'layer', 'consists', 'of', 'attention', '-', 'based', 'aggregation', 'of', 'word', 'features', 'and', 'a', 'GRU', 'cell', 'to', 'learn', 'the', 'sentence', 'representation', '.']","['B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['NN', ':', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'WRB', 'DT', 'NN', 'VBZ', 'IN', 'NN', ':', 'VBN', 'NN', 'IN', 'NN', 'NNS', 'CC', 'DT', 'NNP', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', '.']",29
sentiment_analysis,36,188,LSTM - based models relying on sequential information can perform well for formal sentences by capturing more useful context features ;,"['LSTM', '-', 'based', 'models', 'relying', 'on', 'sequential', 'information', 'can', 'perform', 'well', 'for', 'formal', 'sentences', 'by', 'capturing', 'more', 'useful', 'context', 'features', ';']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'VBN', 'NNS', 'VBG', 'IN', 'JJ', 'NN', 'MD', 'VB', 'RB', 'IN', 'JJ', 'NNS', 'IN', 'VBG', 'JJR', 'JJ', 'NN', 'NNS', ':']",21
sentiment_analysis,36,189,"For ungrammatical text , CNN - based models may have some advantages because CNN aims to extract the most informative n-gram features and is thus less sensitive to informal texts without strong sequential patterns .","['For', 'ungrammatical', 'text', ',', 'CNN', '-', 'based', 'models', 'may', 'have', 'some', 'advantages', 'because', 'CNN', 'aims', 'to', 'extract', 'the', 'most', 'informative', 'n-gram', 'features', 'and', 'is', 'thus', 'less', 'sensitive', 'to', 'informal', 'texts', 'without', 'strong', 'sequential', 'patterns', '.']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', 'NN', ',', 'NNP', ':', 'VBN', 'NNS', 'MD', 'VB', 'DT', 'NNS', 'IN', 'NNP', 'NNS', 'TO', 'VB', 'DT', 'RBS', 'JJ', 'JJ', 'NNS', 'CC', 'VBZ', 'RB', 'RBR', 'JJ', 'TO', 'JJ', 'NN', 'IN', 'JJ', 'JJ', 'NNS', '.']",35
sentiment_analysis,36,192,"After removing the deep transformation ( i.e. , the techniques introduced in Section 2.2 ) , both TNet - LF and TNet - AS are reduced to TNet w/o transformation ( where position relevance is kept ) , and their results in both accuracy and F 1 measure are incomparable with those of TNet .","['After', 'removing', 'the', 'deep', 'transformation', '(', 'i.e.', ',', 'the', 'techniques', 'introduced', 'in', 'Section', '2.2', ')', ',', 'both', 'TNet', '-', 'LF', 'and', 'TNet', '-', 'AS', 'are', 'reduced', 'to', 'TNet', 'w/o', 'transformation', '(', 'where', 'position', 'relevance', 'is', 'kept', ')', ',', 'and', 'their', 'results', 'in', 'both', 'accuracy', 'and', 'F', '1', 'measure', 'are', 'incomparable', 'with', 'those', 'of', 'TNet', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'O']","['IN', 'VBG', 'DT', 'JJ', 'NN', '(', 'FW', ',', 'DT', 'NNS', 'VBN', 'IN', 'NNP', 'CD', ')', ',', 'DT', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NN', 'VBP', 'VBN', 'TO', 'NNP', 'NN', 'NN', '(', 'WRB', 'NN', 'NN', 'VBZ', 'VBN', ')', ',', 'CC', 'PRP$', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NNP', 'CD', 'NN', 'VBP', 'JJ', 'IN', 'DT', 'IN', 'NNP', '.']",55
sentiment_analysis,36,193,It shows that the integration of target information into the word - level representations is crucial for good performance .,"['It', 'shows', 'that', 'the', 'integration', 'of', 'target', 'information', 'into', 'the', 'word', '-', 'level', 'representations', 'is', 'crucial', 'for', 'good', 'performance', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', 'IN', 'DT', 'NN', ':', 'NN', 'NNS', 'VBZ', 'JJ', 'IN', 'JJ', 'NN', '.']",20
sentiment_analysis,36,194,"Comparing the results of TNet and TNet w/o context ( where TST and position relevance are kept ) , we observe that the performance of TNet w/o context drops significantly on LAPTOP and REST 7 , while on TWITTER , TNet w/o context performs very competitive ( p- values with TNet - LF and TNet - AS are 0.066 and 0.053 respectively for Accuracy ) .","['Comparing', 'the', 'results', 'of', 'TNet', 'and', 'TNet', 'w/o', 'context', '(', 'where', 'TST', 'and', 'position', 'relevance', 'are', 'kept', ')', ',', 'we', 'observe', 'that', 'the', 'performance', 'of', 'TNet', 'w/o', 'context', 'drops', 'significantly', 'on', 'LAPTOP', 'and', 'REST', '7', ',', 'while', 'on', 'TWITTER', ',', 'TNet', 'w/o', 'context', 'performs', 'very', 'competitive', '(', 'p-', 'values', 'with', 'TNet', '-', 'LF', 'and', 'TNet', '-', 'AS', 'are', '0.066', 'and', '0.053', 'respectively', 'for', 'Accuracy', ')', '.']","['B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'DT', 'NNS', 'IN', 'NNP', 'CC', 'NNP', 'VBP', 'NN', '(', 'WRB', 'NNP', 'CC', 'NN', 'NN', 'VBP', 'VBN', ')', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NN', 'NN', 'VBZ', 'RB', 'IN', 'NNP', 'CC', 'NNP', 'CD', ',', 'IN', 'IN', 'NNP', ',', 'NNP', 'VBD', 'JJ', 'NNS', 'RB', 'JJ', '(', 'JJ', 'NNS', 'IN', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NNP', 'VBP', 'CD', 'CC', 'CD', 'RB', 'IN', 'NNP', ')', '.']",66
sentiment_analysis,36,196,"TNet w/o context performs consistently better than TNet w/o transformation , which verifies the efficacy of the target specific transformation ( TST ) , before applying context - preserving .","['TNet', 'w/o', 'context', 'performs', 'consistently', 'better', 'than', 'TNet', 'w/o', 'transformation', ',', 'which', 'verifies', 'the', 'efficacy', 'of', 'the', 'target', 'specific', 'transformation', '(', 'TST', ')', ',', 'before', 'applying', 'context', '-', 'preserving', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'NN', 'NNS', 'RB', 'JJR', 'IN', 'NNP', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'JJ', 'NN', '(', 'NNP', ')', ',', 'IN', 'VBG', 'JJ', ':', 'NN', '.']",30
sentiment_analysis,36,198,"All of the produced p-values are less than 0.05 , suggesting that the improvements brought in by position information are significant .","['All', 'of', 'the', 'produced', 'p-values', 'are', 'less', 'than', '0.05', ',', 'suggesting', 'that', 'the', 'improvements', 'brought', 'in', 'by', 'position', 'information', 'are', 'significant', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'IN', 'DT', 'VBN', 'NNS', 'VBP', 'JJR', 'IN', 'CD', ',', 'VBG', 'IN', 'DT', 'NNS', 'VBN', 'IN', 'IN', 'NN', 'NN', 'VBP', 'JJ', '.']",22
sentiment_analysis,22,2,Hierarchical Attention Based Position-aware Network for Aspect-level Sentiment Analysis,"['Hierarchical', 'Attention', 'Based', 'Position-aware', 'Network', 'for', 'Aspect-level', 'Sentiment', 'Analysis']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'VBD', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NN']",9
sentiment_analysis,22,12,"Aspect - level sentiment analysis is a fine - grained task in sentiment analysis , which aims to identify the sentiment polarity ( i.e. , negative , neutral , or positive ) of a specific opinion target expressed in a comment / review by a reviewer .","['Aspect', '-', 'level', 'sentiment', 'analysis', 'is', 'a', 'fine', '-', 'grained', 'task', 'in', 'sentiment', 'analysis', ',', 'which', 'aims', 'to', 'identify', 'the', 'sentiment', 'polarity', '(', 'i.e.', ',', 'negative', ',', 'neutral', ',', 'or', 'positive', ')', 'of', 'a', 'specific', 'opinion', 'target', 'expressed', 'in', 'a', 'comment', '/', 'review', 'by', 'a', 'reviewer', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', 'NN', 'NN', 'VBZ', 'DT', 'JJ', ':', 'VBN', 'NN', 'IN', 'NN', 'NN', ',', 'WDT', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'NN', '(', 'JJ', ',', 'JJ', ',', 'JJ', ',', 'CC', 'JJ', ')', 'IN', 'DT', 'JJ', 'NN', 'NN', 'VBN', 'IN', 'DT', 'NN', 'NNP', 'NN', 'IN', 'DT', 'NN', '.']",47
sentiment_analysis,22,37,"Based on the analysis above , in this paper , we propose a hierarchical attention based positionaware network ( HAPN ) for aspect - level sentiment classification .","['Based', 'on', 'the', 'analysis', 'above', ',', 'in', 'this', 'paper', ',', 'we', 'propose', 'a', 'hierarchical', 'attention', 'based', 'positionaware', 'network', '(', 'HAPN', ')', 'for', 'aspect', '-', 'level', 'sentiment', 'classification', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'NN', 'IN', ',', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBN', 'NN', 'NN', '(', 'NNP', ')', 'IN', 'JJ', ':', 'NN', 'NN', 'NN', '.']",28
sentiment_analysis,22,38,A position - aware encoding layer is introduced for modelling the sentence to achieve the position - aware abstract representation of each word .,"['A', 'position', '-', 'aware', 'encoding', 'layer', 'is', 'introduced', 'for', 'modelling', 'the', 'sentence', 'to', 'achieve', 'the', 'position', '-', 'aware', 'abstract', 'representation', 'of', 'each', 'word', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', ':', 'JJ', 'VBG', 'NN', 'VBZ', 'VBN', 'IN', 'VBG', 'DT', 'NN', 'TO', 'VB', 'DT', 'NN', ':', 'JJ', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",24
sentiment_analysis,22,39,"On this basis , a succinct fusion mechanism is further proposed to fuse the information of aspects and the contexts , achieving the final sentence representation .","['On', 'this', 'basis', ',', 'a', 'succinct', 'fusion', 'mechanism', 'is', 'further', 'proposed', 'to', 'fuse', 'the', 'information', 'of', 'aspects', 'and', 'the', 'contexts', ',', 'achieving', 'the', 'final', 'sentence', 'representation', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'RBR', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNS', 'CC', 'DT', 'NN', ',', 'VBG', 'DT', 'JJ', 'NN', 'NN', '.']",27
sentiment_analysis,22,40,"Finally , we feed the achieved sentence representation into a softmax layer to predict the sentiment polarity .","['Finally', ',', 'we', 'feed', 'the', 'achieved', 'sentence', 'representation', 'into', 'a', 'softmax', 'layer', 'to', 'predict', 'the', 'sentiment', 'polarity', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', '.']",18
sentiment_analysis,22,43,We make our source code public at https://github.com/DUT-LiuYang/Aspect-Sentiment-Analysis.,"['We', 'make', 'our', 'source', 'code', 'public', 'at', 'https://github.com/DUT-LiuYang/Aspect-Sentiment-Analysis.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']","['PRP', 'VBP', 'PRP$', 'NN', 'NN', 'NN', 'IN', 'NN']",8
sentiment_analysis,22,128,"We use 300 - dimension word vectors pre-trained by GloVe ( whose vocabulary size is 1.9M ) for our experiments , as previous works did .","['We', 'use', '300', '-', 'dimension', 'word', 'vectors', 'pre-trained', 'by', 'GloVe', '(', 'whose', 'vocabulary', 'size', 'is', '1.9M', ')', 'for', 'our', 'experiments', ',', 'as', 'previous', 'works', 'did', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'CD', ':', 'NN', 'NN', 'NNS', 'JJ', 'IN', 'NNP', '(', 'WP$', 'JJ', 'NN', 'VBZ', 'CD', ')', 'IN', 'PRP$', 'NNS', ',', 'IN', 'JJ', 'NNS', 'VBD', '.']",26
sentiment_analysis,22,129,"All out - of - vocabulary words are initialized as zero vectors , and all biases are set to zero .","['All', 'out', '-', 'of', '-', 'vocabulary', 'words', 'are', 'initialized', 'as', 'zero', 'vectors', ',', 'and', 'all', 'biases', 'are', 'set', 'to', 'zero', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'RP', ':', 'IN', ':', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'NN', 'NNS', ',', 'CC', 'DT', 'NNS', 'VBP', 'VBN', 'TO', 'CD', '.']",21
sentiment_analysis,22,130,The dimensions of hidden states and fused embeddings are set to 300 .,"['The', 'dimensions', 'of', 'hidden', 'states', 'and', 'fused', 'embeddings', 'are', 'set', 'to', '300', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NNS', 'IN', 'JJ', 'NNS', 'CC', 'VBN', 'NNS', 'VBP', 'VBN', 'TO', 'CD', '.']",13
sentiment_analysis,22,131,The dimension of position embeddings is set to 50 .,"['The', 'dimension', 'of', 'position', 'embeddings', 'is', 'set', 'to', '50', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NN', 'NNS', 'VBZ', 'VBN', 'TO', 'CD', '.']",10
sentiment_analysis,22,132,Keras is used for implementing our neural network model .,"['Keras', 'is', 'used', 'for', 'implementing', 'our', 'neural', 'network', 'model', '.']","['B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'VBN', 'IN', 'VBG', 'PRP$', 'JJ', 'NN', 'NN', '.']",10
sentiment_analysis,22,133,"In model training , we set the learning rate to 0.001 , the batch size to 64 , and dropout rate to 0.5 .","['In', 'model', 'training', ',', 'we', 'set', 'the', 'learning', 'rate', 'to', '0.001', ',', 'the', 'batch', 'size', 'to', '64', ',', 'and', 'dropout', 'rate', 'to', '0.5', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'CD', ',', 'DT', 'NN', 'NN', 'TO', 'CD', ',', 'CC', 'NN', 'NN', 'TO', 'CD', '.']",24
sentiment_analysis,22,134,The paired t- test is used for the significance testing .,"['The', 'paired', 't-', 'test', 'is', 'used', 'for', 'the', 'significance', 'testing', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",11
sentiment_analysis,22,137,Majority assigns the sentiment polarity with most frequent occurrences in the training set to each sample in test set .,"['Majority', 'assigns', 'the', 'sentiment', 'polarity', 'with', 'most', 'frequent', 'occurrences', 'in', 'the', 'training', 'set', 'to', 'each', 'sample', 'in', 'test', 'set', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'JJS', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'VBN', 'TO', 'DT', 'NN', 'IN', 'NN', 'NN', '.']",20
sentiment_analysis,22,139,Bi - LSTM and Bi - GRU adopt a Bi - LSTM and a Bi - GRU network to model the sentence and use the hidden state of the final word for prediction respectively .,"['Bi', '-', 'LSTM', 'and', 'Bi', '-', 'GRU', 'adopt', 'a', 'Bi', '-', 'LSTM', 'and', 'a', 'Bi', '-', 'GRU', 'network', 'to', 'model', 'the', 'sentence', 'and', 'use', 'the', 'hidden', 'state', 'of', 'the', 'final', 'word', 'for', 'prediction', 'respectively', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O']","['NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NNP', 'VB', 'DT', 'NNP', ':', 'NNP', 'CC', 'DT', 'NNP', ':', 'NNP', 'NN', 'TO', 'VB', 'DT', 'NN', 'CC', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'RB', '.']",35
sentiment_analysis,22,141,TD - LSTM adopts two LSTMs to model the left context with target and the right context with target respectively ; It takes the hidden states of LSTM at last time - step to represent the sentence for prediction .,"['TD', '-', 'LSTM', 'adopts', 'two', 'LSTMs', 'to', 'model', 'the', 'left', 'context', 'with', 'target', 'and', 'the', 'right', 'context', 'with', 'target', 'respectively', ';', 'It', 'takes', 'the', 'hidden', 'states', 'of', 'LSTM', 'at', 'last', 'time', '-', 'step', 'to', 'represent', 'the', 'sentence', 'for', 'prediction', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['NNP', ':', 'NNP', 'VBZ', 'CD', 'NNP', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'CC', 'DT', 'JJ', 'NN', 'IN', 'NN', 'RB', ':', 'PRP', 'VBZ', 'DT', 'JJ', 'NNS', 'IN', 'NNP', 'IN', 'JJ', 'NN', ':', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', '.']",40
sentiment_analysis,22,143,"MemNet applies attention multiple times on the word embeddings , and the output of last attention is fed to softmax for prediction .","['MemNet', 'applies', 'attention', 'multiple', 'times', 'on', 'the', 'word', 'embeddings', ',', 'and', 'the', 'output', 'of', 'last', 'attention', 'is', 'fed', 'to', 'softmax', 'for', 'prediction', '.']","['B-n', 'B-p', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O']","['NNP', 'NNS', 'NN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NNS', ',', 'CC', 'DT', 'NN', 'IN', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'IN', 'NN', '.']",23
sentiment_analysis,22,145,"IAN interactively learns attentions in the contexts and targets , and generates the representations for targets and contexts separately .","['IAN', 'interactively', 'learns', 'attentions', 'in', 'the', 'contexts', 'and', 'targets', ',', 'and', 'generates', 'the', 'representations', 'for', 'targets', 'and', 'contexts', 'separately', '.']","['B-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O']","['NNP', 'RB', 'VBZ', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NNS', ',', 'CC', 'VBZ', 'DT', 'NNS', 'IN', 'NNS', 'CC', 'NN', 'RB', '.']",20
sentiment_analysis,22,147,RAM ) is a multilayer architecture where each layer consists of attention - based aggregation of word features and a GRU cell to learn the sentence representation .,"['RAM', ')', 'is', 'a', 'multilayer', 'architecture', 'where', 'each', 'layer', 'consists', 'of', 'attention', '-', 'based', 'aggregation', 'of', 'word', 'features', 'and', 'a', 'GRU', 'cell', 'to', 'learn', 'the', 'sentence', 'representation', '.']","['B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['NNP', ')', 'VBZ', 'DT', 'JJ', 'NN', 'WRB', 'DT', 'NN', 'VBZ', 'IN', 'NN', ':', 'VBN', 'NN', 'IN', 'NN', 'NNS', 'CC', 'DT', 'NNP', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', '.']",28
sentiment_analysis,22,149,"LCR - Rot employs three Bi- LSTMs to model the left context , the target and the right context .","['LCR', '-', 'Rot', 'employs', 'three', 'Bi-', 'LSTMs', 'to', 'model', 'the', 'left', 'context', ',', 'the', 'target', 'and', 'the', 'right', 'context', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'VBZ', 'CD', 'JJ', 'NNP', 'TO', 'VB', 'DT', 'JJ', 'NN', ',', 'DT', 'NN', 'CC', 'DT', 'JJ', 'NN', '.']",20
sentiment_analysis,22,152,AOA - LSTM introduces an attention - over- attention ( AOA ) based network to model aspects and sentences in a joint way and explicitly capture the interaction between aspects and context sentences .,"['AOA', '-', 'LSTM', 'introduces', 'an', 'attention', '-', 'over-', 'attention', '(', 'AOA', ')', 'based', 'network', 'to', 'model', 'aspects', 'and', 'sentences', 'in', 'a', 'joint', 'way', 'and', 'explicitly', 'capture', 'the', 'interaction', 'between', 'aspects', 'and', 'context', 'sentences', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'VBZ', 'DT', 'NN', ':', 'JJ', 'NN', '(', 'NNP', ')', 'VBN', 'NN', 'TO', 'VB', 'NNS', 'CC', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'CC', 'RB', 'VB', 'DT', 'NN', 'IN', 'NNS', 'CC', 'JJ', 'NNS', '.']",34
sentiment_analysis,22,155,"( 2 ) The TD - LSTM model , which has been shown to be better than LSTM , gets the worst performance of all RNN based models and the accuracy achieved by TD - LSTM is 2.94 % and 2.4 % lower than those by Bi - LSTM on the two datasets respectively .","['(', '2', ')', 'The', 'TD', '-', 'LSTM', 'model', ',', 'which', 'has', 'been', 'shown', 'to', 'be', 'better', 'than', 'LSTM', ',', 'gets', 'the', 'worst', 'performance', 'of', 'all', 'RNN', 'based', 'models', 'and', 'the', 'accuracy', 'achieved', 'by', 'TD', '-', 'LSTM', 'is', '2.94', '%', 'and', '2.4', '%', 'lower', 'than', 'those', 'by', 'Bi', '-', 'LSTM', 'on', 'the', 'two', 'datasets', 'respectively', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'CD', ')', 'DT', 'NNP', ':', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'VBN', 'VBN', 'TO', 'VB', 'JJR', 'IN', 'NNP', ',', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NNP', 'VBN', 'NNS', 'CC', 'DT', 'NN', 'VBN', 'IN', 'NNP', ':', 'NNP', 'VBZ', 'CD', 'NN', 'CC', 'CD', 'NN', 'JJR', 'IN', 'DT', 'IN', 'NNP', ':', 'NN', 'IN', 'DT', 'CD', 'NNS', 'RB', '.']",55
sentiment_analysis,22,159,"( 3 ) Compared with the state - of - the - art methods , our model achieves the best performance , which illustrates the effectiveness of the proposed approach .","['(', '3', ')', 'Compared', 'with', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'methods', ',', 'our', 'model', 'achieves', 'the', 'best', 'performance', ',', 'which', 'illustrates', 'the', 'effectiveness', 'of', 'the', 'proposed', 'approach', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'CD', ')', 'VBN', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'JJS', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'VBN', 'NN', '.']",31
sentiment_analysis,22,160,"Our method achieves accuracies of 82.23 % as well as 77 . 27 % on the Restaurant and Laptop dataset respectively , which are 0.89 % and 2.03 % higher than the current best method .","['Our', 'method', 'achieves', 'accuracies', 'of', '82.23', '%', 'as', 'well', 'as', '77', '.', '27', '%', 'on', 'the', 'Restaurant', 'and', 'Laptop', 'dataset', 'respectively', ',', 'which', 'are', '0.89', '%', 'and', '2.03', '%', 'higher', 'than', 'the', 'current', 'best', 'method', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'VBZ', 'NNS', 'IN', 'CD', 'NN', 'RB', 'RB', 'IN', 'CD', '.', 'CD', 'NN', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NN', 'RB', ',', 'WDT', 'VBP', 'CD', 'NN', 'CC', 'CD', 'NN', 'JJR', 'IN', 'DT', 'JJ', 'JJS', 'NN', '.']",36
sentiment_analysis,22,170,"After introducing the position embeddings , the accuracy has an increase of 0.62 % and 2.67 % on two datasets .","['After', 'introducing', 'the', 'position', 'embeddings', ',', 'the', 'accuracy', 'has', 'an', 'increase', 'of', '0.62', '%', 'and', '2.67', '%', 'on', 'two', 'datasets', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'VBG', 'DT', 'NN', 'NNS', ',', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'CD', 'NNS', '.']",21
sentiment_analysis,22,172,"In addition , another observation is that Bi - GRU - PW performs even worse than Bi - GRU .","['In', 'addition', ',', 'another', 'observation', 'is', 'that', 'Bi', '-', 'GRU', '-', 'PW', 'performs', 'even', 'worse', 'than', 'Bi', '-', 'GRU', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'DT', 'NN', 'VBZ', 'IN', 'NNP', ':', 'NNP', ':', 'NN', 'NNS', 'RB', 'JJR', 'IN', 'NNP', ':', 'NNP', '.']",20
sentiment_analysis,22,173,The accuracy achieved by Bi - GRU - PW is 0.72 % as well as 1.41 % lower than that by Bi - GRU on the Restaurant and Laptop dataset respectively .,"['The', 'accuracy', 'achieved', 'by', 'Bi', '-', 'GRU', '-', 'PW', 'is', '0.72', '%', 'as', 'well', 'as', '1.41', '%', 'lower', 'than', 'that', 'by', 'Bi', '-', 'GRU', 'on', 'the', 'Restaurant', 'and', 'Laptop', 'dataset', 'respectively', '.']","['O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['DT', 'NN', 'VBN', 'IN', 'NNP', ':', 'NNP', ':', 'NN', 'VBZ', 'CD', 'NN', 'RB', 'RB', 'IN', 'CD', 'NN', 'JJR', 'IN', 'DT', 'IN', 'NNP', ':', 'NN', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NN', 'RB', '.']",32
sentiment_analysis,22,180,HAPN achieves improvement of 0.35 % and 0.78 % on accuracy respectively on the two dataset .,"['HAPN', 'achieves', 'improvement', 'of', '0.35', '%', 'and', '0.78', '%', 'on', 'accuracy', 'respectively', 'on', 'the', 'two', 'dataset', '.']","['B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'NN', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'NN', 'RB', 'IN', 'DT', 'CD', 'NN', '.']",17
sentiment_analysis,22,192,( 1 ) The information fusion operation is only used to calculate the Source2context attention value .,"['(', '1', ')', 'The', 'information', 'fusion', 'operation', 'is', 'only', 'used', 'to', 'calculate', 'the', 'Source2context', 'attention', 'value', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'DT', 'NN', 'NN', 'NN', 'VBZ', 'RB', 'VBN', 'TO', 'VB', 'DT', 'NNP', 'NN', 'NN', '.']",17
sentiment_analysis,22,193,The output of Source2aspect attention is only used for information fusion .,"['The', 'output', 'of', 'Source2aspect', 'attention', 'is', 'only', 'used', 'for', 'information', 'fusion', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'NNP', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'NN', 'NN', '.']",12
sentiment_analysis,22,195,"And the achieved model is "" Bi - GRU - PE "" reported in the , achieving the accuracies of 80.89 % and 76.02 % on the two datasets respectively , which are 1.34 % and 1.25 % lower than the proposed model .","['And', 'the', 'achieved', 'model', 'is', '""', 'Bi', '-', 'GRU', '-', 'PE', '""', 'reported', 'in', 'the', ',', 'achieving', 'the', 'accuracies', 'of', '80.89', '%', 'and', '76.02', '%', 'on', 'the', 'two', 'datasets', 'respectively', ',', 'which', 'are', '1.34', '%', 'and', '1.25', '%', 'lower', 'than', 'the', 'proposed', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['CC', 'DT', 'JJ', 'NN', 'VBZ', 'JJ', 'NNP', ':', 'NNP', ':', 'NN', 'NN', 'VBN', 'IN', 'DT', ',', 'VBG', 'DT', 'NNS', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'DT', 'CD', 'NNS', 'RB', ',', 'WDT', 'VBP', 'CD', 'NN', 'CC', 'CD', 'NN', 'JJR', 'IN', 'DT', 'VBN', 'NN', '.']",44
sentiment_analysis,30,2,Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect - based Sentiment Analysis,"['Recurrent', 'Entity', 'Networks', 'with', 'Delayed', 'Memory', 'Update', 'for', 'Targeted', 'Aspect', '-', 'based', 'Sentiment', 'Analysis']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', ':', 'VBN', 'NN', 'NN']",14
sentiment_analysis,30,4,"While neural networks have been shown to achieve impressive results for sentence - level sentiment analysis , targeted aspect - based sentiment analysis ( TABSA ) - extraction of finegrained opinion polarity w.r.t. a pre-defined set of aspects - remains a difficult task .","['While', 'neural', 'networks', 'have', 'been', 'shown', 'to', 'achieve', 'impressive', 'results', 'for', 'sentence', '-', 'level', 'sentiment', 'analysis', ',', 'targeted', 'aspect', '-', 'based', 'sentiment', 'analysis', '(', 'TABSA', ')', '-', 'extraction', 'of', 'finegrained', 'opinion', 'polarity', 'w.r.t.', 'a', 'pre-defined', 'set', 'of', 'aspects', '-', 'remains', 'a', 'difficult', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', 'NNS', 'VBP', 'VBN', 'VBN', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'NN', ':', 'NN', 'NN', 'NN', ',', 'VBN', 'NN', ':', 'VBN', 'NN', 'NN', '(', 'NNP', ')', ':', 'NN', 'IN', 'JJ', 'NN', 'NN', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NNS', ':', 'VBZ', 'DT', 'JJ', 'NN', '.']",44
sentiment_analysis,30,26,"In this work , we propose a novel model architecture for TABSA , augmented with multiple "" memory chains "" , and equipped with a delayed memory update mechanism , to keep track of numerous entities independently .","['In', 'this', 'work', ',', 'we', 'propose', 'a', 'novel', 'model', 'architecture', 'for', 'TABSA', ',', 'augmented', 'with', 'multiple', '""', 'memory', 'chains', '""', ',', 'and', 'equipped', 'with', 'a', 'delayed', 'memory', 'update', 'mechanism', ',', 'to', 'keep', 'track', 'of', 'numerous', 'entities', 'independently', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NNP', ',', 'VBD', 'IN', 'JJ', 'NNP', 'NN', 'NNS', 'VBP', ',', 'CC', 'VBD', 'IN', 'DT', 'JJ', 'NN', 'JJ', 'NN', ',', 'TO', 'VB', 'NN', 'IN', 'JJ', 'NNS', 'RB', '.']",38
sentiment_analysis,30,90,"We initialise our model with GloVe ( 300 - D , trained on 42B tokens , 1.9 M vocab , not updated during training : ) 4 and pre-process the corpus with tokenisation using NLTK ) and case folding .","['We', 'initialise', 'our', 'model', 'with', 'GloVe', '(', '300', '-', 'D', ',', 'trained', 'on', '42B', 'tokens', ',', '1.9', 'M', 'vocab', ',', 'not', 'updated', 'during', 'training', ':', ')', '4', 'and', 'pre-process', 'the', 'corpus', 'with', 'tokenisation', 'using', 'NLTK', ')', 'and', 'case', 'folding', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'IN', 'NNP', '(', 'CD', ':', 'NN', ',', 'VBD', 'IN', 'CD', 'NNS', ',', 'CD', 'NNP', 'NN', ',', 'RB', 'VBN', 'IN', 'NN', ':', ')', 'CD', 'CC', 'JJ', 'DT', 'NN', 'IN', 'NN', 'VBG', 'NNP', ')', 'CC', 'NN', 'NN', '.']",40
sentiment_analysis,30,91,Training is carried out over 800 epochs with the FTRL optimiser and a batch size of 128 and learning rate of 0.05 .,"['Training', 'is', 'carried', 'out', 'over', '800', 'epochs', 'with', 'the', 'FTRL', 'optimiser', 'and', 'a', 'batch', 'size', 'of', '128', 'and', 'learning', 'rate', 'of', '0.05', '.']","['B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['NN', 'VBZ', 'VBN', 'IN', 'IN', 'CD', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'CC', 'DT', 'NN', 'NN', 'IN', 'CD', 'CC', 'VBG', 'NN', 'IN', 'CD', '.']",23
sentiment_analysis,30,92,"We use the following hyper - parameters for weight matrices in both directions : R ? R 3003 , H , U , V , Ware all matrices of size R 300300 , v ? R 300 , and hidden size of the GRU in Equation is 300 .","['We', 'use', 'the', 'following', 'hyper', '-', 'parameters', 'for', 'weight', 'matrices', 'in', 'both', 'directions', ':', 'R', '?', 'R', '3003', ',', 'H', ',', 'U', ',', 'V', ',', 'Ware', 'all', 'matrices', 'of', 'size', 'R', '300300', ',', 'v', '?', 'R', '300', ',', 'and', 'hidden', 'size', 'of', 'the', 'GRU', 'in', 'Equation', 'is', '300', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', ':', 'NNS', 'IN', 'NN', 'NNS', 'IN', 'DT', 'NNS', ':', 'NN', '.', 'NN', 'CD', ',', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', 'DT', 'NNS', 'IN', 'NN', 'NNP', 'CD', ',', 'NN', '.', 'NNP', 'CD', ',', 'CC', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'IN', 'NNP', 'VBZ', 'CD', '.']",49
sentiment_analysis,30,93,Dropout is applied to the output of ? in the final classifier ( Equation ) with a rate of 0.2 .,"['Dropout', 'is', 'applied', 'to', 'the', 'output', 'of', '?', 'in', 'the', 'final', 'classifier', '(', 'Equation', ')', 'with', 'a', 'rate', 'of', '0.2', '.']","['B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-n', 'O']","['NN', 'VBZ', 'VBN', 'TO', 'DT', 'NN', 'IN', '.', 'IN', 'DT', 'JJ', 'NN', '(', 'NNP', ')', 'IN', 'DT', 'NN', 'IN', 'CD', '.']",21
sentiment_analysis,30,95,"Lastly , to curb overfitting , we regularise the last layer ( Equation ) with an L 2 penalty on its weights : ?","['Lastly', ',', 'to', 'curb', 'overfitting', ',', 'we', 'regularise', 'the', 'last', 'layer', '(', 'Equation', ')', 'with', 'an', 'L', '2', 'penalty', 'on', 'its', 'weights', ':', '?']","['O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O']","['RB', ',', 'TO', 'VB', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', '(', 'NNP', ')', 'IN', 'DT', 'NNP', 'CD', 'NN', 'IN', 'PRP$', 'NNS', ':', '.']",24
sentiment_analysis,30,97,"We empirically set the number of memory chains to 6 , with the keys of two of them set to the same embeddings as the target words LOC1 and LOC2 , resp. , and the other 4 chains with free key embeddings which are updated during training , and therefore free to capture any entities .","['We', 'empirically', 'set', 'the', 'number', 'of', 'memory', 'chains', 'to', '6', ',', 'with', 'the', 'keys', 'of', 'two', 'of', 'them', 'set', 'to', 'the', 'same', 'embeddings', 'as', 'the', 'target', 'words', 'LOC1', 'and', 'LOC2', ',', 'resp.', ',', 'and', 'the', 'other', '4', 'chains', 'with', 'free', 'key', 'embeddings', 'which', 'are', 'updated', 'during', 'training', ',', 'and', 'therefore', 'free', 'to', 'capture', 'any', 'entities', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBD', 'DT', 'NN', 'IN', 'NN', 'NNS', 'TO', 'CD', ',', 'IN', 'DT', 'NNS', 'IN', 'CD', 'IN', 'PRP', 'VBP', 'TO', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NNS', 'NNP', 'CC', 'NNP', ',', 'NN', ',', 'CC', 'DT', 'JJ', 'CD', 'NNS', 'IN', 'JJ', 'NN', 'NNS', 'WDT', 'VBP', 'VBN', 'IN', 'NN', ',', 'CC', 'RB', 'JJ', 'TO', 'VB', 'DT', 'NNS', '.']",56
sentiment_analysis,30,114,Our model achieves state - of - the - art results for both aspect detection and sentiment classification .,"['Our', 'model', 'achieves', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'for', 'both', 'aspect', 'detection', 'and', 'sentiment', 'classification', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'CC', 'NN', 'NN', '.']",19
sentiment_analysis,30,115,"It is impressive that the proposed model , equipped only with domainindependent general - purpose GloVe embeddings , outperforms Sentic LSTM , an approach heavily reliant on external knowledge bases and domainspecific embeddings .","['It', 'is', 'impressive', 'that', 'the', 'proposed', 'model', ',', 'equipped', 'only', 'with', 'domainindependent', 'general', '-', 'purpose', 'GloVe', 'embeddings', ',', 'outperforms', 'Sentic', 'LSTM', ',', 'an', 'approach', 'heavily', 'reliant', 'on', 'external', 'knowledge', 'bases', 'and', 'domainspecific', 'embeddings', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'JJ', 'IN', 'DT', 'VBN', 'NN', ',', 'VBD', 'RB', 'IN', 'JJ', 'JJ', ':', 'NN', 'NNP', 'NNS', ',', 'NNS', 'NNP', 'NNP', ',', 'DT', 'NN', 'RB', 'VB', 'IN', 'JJ', 'NN', 'NNS', 'CC', 'JJ', 'NNS', '.']",34
sentiment_analysis,30,116,Ent Net vs. our model .,"['Ent', 'Net', 'vs.', 'our', 'model', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'NNP', 'FW', 'PRP$', 'NN', '.']",6
sentiment_analysis,30,117,"We see consistent performance gains for our model in both aspect detection and sentiment classification , compared to EntNet , esp. for aspect detection , underlining the benefit of delayed update gate activation .","['We', 'see', 'consistent', 'performance', 'gains', 'for', 'our', 'model', 'in', 'both', 'aspect', 'detection', 'and', 'sentiment', 'classification', ',', 'compared', 'to', 'EntNet', ',', 'esp.', 'for', 'aspect', 'detection', ',', 'underlining', 'the', 'benefit', 'of', 'delayed', 'update', 'gate', 'activation', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'NNS', 'IN', 'PRP$', 'NN', 'IN', 'DT', 'JJ', 'NN', 'CC', 'NN', 'NN', ',', 'VBN', 'TO', 'NNP', ',', 'NN', 'IN', 'JJ', 'NN', ',', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'NN', '.']",34
sentiment_analysis,31,2,Parameterized Convolutional Neural Networks for Aspect Level Sentiment Classification,"['Parameterized', 'Convolutional', 'Neural', 'Networks', 'for', 'Aspect', 'Level', 'Sentiment', 'Classification']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'NNP']",9
sentiment_analysis,31,8,Continuous growing of user generated text in social media platforms such as Twitter drives sentiment classification increasingly popular .,"['Continuous', 'growing', 'of', 'user', 'generated', 'text', 'in', 'social', 'media', 'platforms', 'such', 'as', 'Twitter', 'drives', 'sentiment', 'classification', 'increasingly', 'popular', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['JJ', 'VBG', 'IN', 'NN', 'VBN', 'NN', 'IN', 'JJ', 'NNS', 'NNS', 'JJ', 'IN', 'NNP', 'VBZ', 'JJ', 'NN', 'RB', 'JJ', '.']",19
sentiment_analysis,31,12,"Differing from general sentiment classification , aspect level sentiment classification identifies opinions from text about specific entities and their aspects .","['Differing', 'from', 'general', 'sentiment', 'classification', ',', 'aspect', 'level', 'sentiment', 'classification', 'identifies', 'opinions', 'from', 'text', 'about', 'specific', 'entities', 'and', 'their', 'aspects', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'IN', 'JJ', 'NN', 'NN', ',', 'VBP', 'JJ', 'NN', 'NN', 'NNS', 'NNS', 'IN', 'NN', 'IN', 'JJ', 'NNS', 'CC', 'PRP$', 'NNS', '.']",21
sentiment_analysis,31,20,"In the present work , we propose two simple yet effective convolutional neural networks with aspect information incorporated .","['In', 'the', 'present', 'work', ',', 'we', 'propose', 'two', 'simple', 'yet', 'effective', 'convolutional', 'neural', 'networks', 'with', 'aspect', 'information', 'incorporated', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'CD', 'JJ', 'RB', 'JJ', 'JJ', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'VBN', '.']",19
sentiment_analysis,31,22,"Specifically , we design two novel neural units that take target aspects into account .","['Specifically', ',', 'we', 'design', 'two', 'novel', 'neural', 'units', 'that', 'take', 'target', 'aspects', 'into', 'account', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['RB', ',', 'PRP', 'VBP', 'CD', 'JJ', 'JJ', 'NNS', 'WDT', 'VBP', 'NN', 'NNS', 'IN', 'NN', '.']",15
sentiment_analysis,31,23,"One is parameterized filter , the other is parameterized gate .","['One', 'is', 'parameterized', 'filter', ',', 'the', 'other', 'is', 'parameterized', 'gate', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['CD', 'VBZ', 'VBN', 'RB', ',', 'DT', 'JJ', 'VBZ', 'VBN', 'NN', '.']",11
sentiment_analysis,31,24,These units both are generated from aspect - specific features and are further applied on the sentence .,"['These', 'units', 'both', 'are', 'generated', 'from', 'aspect', '-', 'specific', 'features', 'and', 'are', 'further', 'applied', 'on', 'the', 'sentence', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'DT', 'VBP', 'VBN', 'IN', 'JJ', ':', 'JJ', 'NNS', 'CC', 'VBP', 'JJ', 'VBN', 'IN', 'DT', 'NN', '.']",18
sentiment_analysis,31,112,"We use rectifier as non-linear function f in the CNN g , CNN t and sigmoid in the CNN s , filter window sizes of 1 , 2 , 3 , 4 with 100 feature maps each , l 2 regularization term of 0.001 and minibatch size of 25 .","['We', 'use', 'rectifier', 'as', 'non-linear', 'function', 'f', 'in', 'the', 'CNN', 'g', ',', 'CNN', 't', 'and', 'sigmoid', 'in', 'the', 'CNN', 's', ',', 'filter', 'window', 'sizes', 'of', '1', ',', '2', ',', '3', ',', '4', 'with', '100', 'feature', 'maps', 'each', ',', 'l', '2', 'regularization', 'term', 'of', '0.001', 'and', 'minibatch', 'size', 'of', '25', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'JJR', 'IN', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', ',', 'NNP', 'NN', 'CC', 'NN', 'IN', 'DT', 'NNP', 'NN', ',', 'NN', 'NN', 'NNS', 'IN', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', 'IN', 'CD', 'NN', 'NNS', 'DT', ',', 'VBZ', 'CD', 'NN', 'NN', 'IN', 'CD', 'CC', 'VB', 'NN', 'IN', 'CD', '.']",50
sentiment_analysis,31,113,Parameterized filters and gates have the same size and number as normal filters .,"['Parameterized', 'filters', 'and', 'gates', 'have', 'the', 'same', 'size', 'and', 'number', 'as', 'normal', 'filters', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['VBN', 'NNS', 'CC', 'NNS', 'VBP', 'DT', 'JJ', 'NN', 'CC', 'NN', 'IN', 'JJ', 'NNS', '.']",14
sentiment_analysis,31,114,"They are generated uniformly by CNN with window sizes of 1 , 2 , 3 , 4 , eg. among 100 parameterized filters with size 3 , 25 of them are generated by aspect CNN with filter size 1 , 2 , 3 , 4 respectively .","['They', 'are', 'generated', 'uniformly', 'by', 'CNN', 'with', 'window', 'sizes', 'of', '1', ',', '2', ',', '3', ',', '4', ',', 'eg.', 'among', '100', 'parameterized', 'filters', 'with', 'size', '3', ',', '25', 'of', 'them', 'are', 'generated', 'by', 'aspect', 'CNN', 'with', 'filter', 'size', '1', ',', '2', ',', '3', ',', '4', 'respectively', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'VBN', 'RB', 'IN', 'NNP', 'IN', 'JJ', 'NNS', 'IN', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ',', 'NN', 'IN', 'CD', 'JJ', 'NNS', 'IN', 'NN', 'CD', ',', 'CD', 'IN', 'PRP', 'VBP', 'VBN', 'IN', 'JJ', 'NNP', 'IN', 'NN', 'NN', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', 'RB', '.']",47
sentiment_analysis,31,115,The word embeddings are initialized with 300 - dimensional Glove vectors and are fixed during training .,"['The', 'word', 'embeddings', 'are', 'initialized', 'with', '300', '-', 'dimensional', 'Glove', 'vectors', 'and', 'are', 'fixed', 'during', 'training', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'CD', ':', 'JJ', 'NNP', 'NNS', 'CC', 'VBP', 'VBN', 'IN', 'NN', '.']",17
sentiment_analysis,31,116,"For the out of vocabulary words we initialize them randomly from uniform distribution U ( ? 0.01 , 0.01 ) .","['For', 'the', 'out', 'of', 'vocabulary', 'words', 'we', 'initialize', 'them', 'randomly', 'from', 'uniform', 'distribution', 'U', '(', '?', '0.01', ',', '0.01', ')', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'PRP', 'VBP', 'PRP', 'RB', 'IN', 'JJ', 'NN', 'NNP', '(', '.', 'CD', ',', 'CD', ')', '.']",21
sentiment_analysis,31,117,We apply dropout on the final classification features of PG - CNN .,"['We', 'apply', 'dropout', 'on', 'the', 'final', 'classification', 'features', 'of', 'PG', '-', 'CNN', '.']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'RB', 'IN', 'DT', 'JJ', 'NN', 'NNS', 'IN', 'NNP', ':', 'NN', '.']",13
sentiment_analysis,31,118,The dropout rate is chosen as 0.3 .,"['The', 'dropout', 'rate', 'is', 'chosen', 'as', '0.3', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', '.']",8
sentiment_analysis,31,119,Training is done through mini-batch stochastic gradient descent with Adam update rule .,"['Training', 'is', 'done', 'through', 'mini-batch', 'stochastic', 'gradient', 'descent', 'with', 'Adam', 'update', 'rule', '.']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NN', 'VBZ', 'VBN', 'IN', 'NN', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'JJ', 'NN', '.']",13
sentiment_analysis,31,120,The initial learning rate is 0.001 .,"['The', 'initial', 'learning', 'rate', 'is', '0.001', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'CD', '.']",7
sentiment_analysis,31,121,"If the training loss does not drop after every three epochs , we decrease the learning rate by half .","['If', 'the', 'training', 'loss', 'does', 'not', 'drop', 'after', 'every', 'three', 'epochs', ',', 'we', 'decrease', 'the', 'learning', 'rate', 'by', 'half', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NN', 'NN', 'VBZ', 'RB', 'VB', 'IN', 'DT', 'CD', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'NN', '.']",20
sentiment_analysis,31,122,We adopt early stopping based on the validation loss on development sets .,"['We', 'adopt', 'early', 'stopping', 'based', 'on', 'the', 'validation', 'loss', 'on', 'development', 'sets', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'VBN', 'IN', 'DT', 'NN', 'NN', 'IN', 'NN', 'NNS', '.']",13
sentiment_analysis,31,127,TD - LSTM uses two LSTM networks to model the preceding and following contexts surrounding the aspect term .,"['TD', '-', 'LSTM', 'uses', 'two', 'LSTM', 'networks', 'to', 'model', 'the', 'preceding', 'and', 'following', 'contexts', 'surrounding', 'the', 'aspect', 'term', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', ':', 'JJ', 'NNS', 'CD', 'NNP', 'NNS', 'TO', 'VB', 'DT', 'NN', 'CC', 'VBG', 'NN', 'VBG', 'DT', 'JJ', 'NN', '.']",19
sentiment_analysis,31,129,AT - LSTM combines the sentence hidden states from a LSTM with the aspect term embedding to generate the attention vector .,"['AT', '-', 'LSTM', 'combines', 'the', 'sentence', 'hidden', 'states', 'from', 'a', 'LSTM', 'with', 'the', 'aspect', 'term', 'embedding', 'to', 'generate', 'the', 'attention', 'vector', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'VBZ', 'DT', 'NN', 'JJ', 'NNS', 'IN', 'DT', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'VBG', 'TO', 'VB', 'DT', 'NN', 'NN', '.']",22
sentiment_analysis,31,131,ATAE - LSTM further extends AT - LSTM by appending the aspect embedding into each word vector .,"['ATAE', '-', 'LSTM', 'further', 'extends', 'AT', '-', 'LSTM', 'by', 'appending', 'the', 'aspect', 'embedding', 'into', 'each', 'word', 'vector', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'JJ', 'NNS', 'NNP', ':', 'NN', 'IN', 'VBG', 'DT', 'NN', 'VBG', 'IN', 'DT', 'NN', 'NN', '.']",18
sentiment_analysis,31,132,AF - LSTM introduces a word - aspect fusion attention to learn associative relationships between aspect and context words .,"['AF', '-', 'LSTM', 'introduces', 'a', 'word', '-', 'aspect', 'fusion', 'attention', 'to', 'learn', 'associative', 'relationships', 'between', 'aspect', 'and', 'context', 'words', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'VBZ', 'DT', 'NN', ':', 'NN', 'NN', 'NN', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'NN', 'CC', 'NN', 'NNS', '.']",20
sentiment_analysis,31,133,CNN uses the architecture proposed in without explicitly considering aspect .,"['CNN', 'uses', 'the', 'architecture', 'proposed', 'in', 'without', 'explicitly', 'considering', 'aspect', '.']","['B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'VBN', 'IN', 'IN', 'RB', 'VBG', 'NN', '.']",11
sentiment_analysis,31,137,"Our two models achieve the best performance when compared to these baselines as shown in , which shows that our proposed neural units effectively captures the aspect - specific features .","['Our', 'two', 'models', 'achieve', 'the', 'best', 'performance', 'when', 'compared', 'to', 'these', 'baselines', 'as', 'shown', 'in', ',', 'which', 'shows', 'that', 'our', 'proposed', 'neural', 'units', 'effectively', 'captures', 'the', 'aspect', '-', 'specific', 'features', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'CD', 'NNS', 'VBP', 'DT', 'JJS', 'NN', 'WRB', 'VBN', 'TO', 'DT', 'NNS', 'IN', 'VBN', 'IN', ',', 'WDT', 'VBZ', 'IN', 'PRP$', 'VBN', 'JJ', 'NNS', 'RB', 'VBZ', 'DT', 'JJ', ':', 'JJ', 'NNS', '.']",31
sentiment_analysis,31,138,"Compared to one recently proposed model AF - LSTM , our method achieve 2 % - 5 % improvements .","['Compared', 'to', 'one', 'recently', 'proposed', 'model', 'AF', '-', 'LSTM', ',', 'our', 'method', 'achieve', '2', '%', '-', '5', '%', 'improvements', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'TO', 'CD', 'RB', 'VBN', 'NN', 'NNP', ':', 'NN', ',', 'PRP$', 'NN', 'VBP', 'CD', 'NN', ':', 'CD', 'NN', 'NNS', '.']",20
sentiment_analysis,31,139,"Surprisingly , a vanilla CNN works quite well on this problem .","['Surprisingly', ',', 'a', 'vanilla', 'CNN', 'works', 'quite', 'well', 'on', 'this', 'problem', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['RB', ',', 'DT', 'NN', 'NNP', 'VBZ', 'RB', 'RB', 'IN', 'DT', 'NN', '.']",12
sentiment_analysis,31,140,"It even beats these welldesigned LSTM models , which further proves that using CNN - based methods is a direction worth exploring .","['It', 'even', 'beats', 'these', 'welldesigned', 'LSTM', 'models', ',', 'which', 'further', 'proves', 'that', 'using', 'CNN', '-', 'based', 'methods', 'is', 'a', 'direction', 'worth', 'exploring', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBZ', 'DT', 'VBD', 'NNP', 'NNS', ',', 'WDT', 'VBP', 'NNS', 'IN', 'VBG', 'NNP', ':', 'VBN', 'NNS', 'VBZ', 'DT', 'NN', 'IN', 'VBG', '.']",23
sentiment_analysis,12,2,Progressive Self - Supervised Attention Learning for Aspect - Level Sentiment Analysis,"['Progressive', 'Self', '-', 'Supervised', 'Attention', 'Learning', 'for', 'Aspect', '-', 'Level', 'Sentiment', 'Analysis']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['JJ', 'NNP', ':', 'VBD', 'NNP', 'NNP', 'IN', 'NNP', ':', 'NN', 'NN', 'NN']",12
sentiment_analysis,12,4,"In aspect - level sentiment classification ( ASC ) , it is prevalent to equip dominant neural models with attention mechanisms , for the sake of acquiring the importance of each context word on the given aspect .","['In', 'aspect', '-', 'level', 'sentiment', 'classification', '(', 'ASC', ')', ',', 'it', 'is', 'prevalent', 'to', 'equip', 'dominant', 'neural', 'models', 'with', 'attention', 'mechanisms', ',', 'for', 'the', 'sake', 'of', 'acquiring', 'the', 'importance', 'of', 'each', 'context', 'word', 'on', 'the', 'given', 'aspect', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', ':', 'NN', 'NN', 'NN', '(', 'NNP', ')', ',', 'PRP', 'VBZ', 'JJ', 'TO', 'VB', 'JJ', 'JJ', 'NNS', 'IN', 'NN', 'NNS', ',', 'IN', 'DT', 'NN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'VBN', 'NN', '.']",38
sentiment_analysis,12,6,"In this paper , we propose a progressive self - supervised attention learning approach for neural ASC models , which automatically mines useful attention supervision information from a training corpus to refine attention mechanisms .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'progressive', 'self', '-', 'supervised', 'attention', 'learning', 'approach', 'for', 'neural', 'ASC', 'models', ',', 'which', 'automatically', 'mines', 'useful', 'attention', 'supervision', 'information', 'from', 'a', 'training', 'corpus', 'to', 'refine', 'attention', 'mechanisms', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', ':', 'VBN', 'NN', 'VBG', 'NN', 'IN', 'JJ', 'NNP', 'NNS', ',', 'WDT', 'RB', 'VBZ', 'JJ', 'NN', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', 'NN', 'NNS', '.']",35
sentiment_analysis,12,13,"Aspect - level sentiment classification ( ASC ) , as an indispensable task in sentiment analysis , aims at inferring the sentiment polarity of an input sentence in a certain aspect .","['Aspect', '-', 'level', 'sentiment', 'classification', '(', 'ASC', ')', ',', 'as', 'an', 'indispensable', 'task', 'in', 'sentiment', 'analysis', ',', 'aims', 'at', 'inferring', 'the', 'sentiment', 'polarity', 'of', 'an', 'input', 'sentence', 'in', 'a', 'certain', 'aspect', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', 'NN', 'NN', '(', 'NNP', ')', ',', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NN', ',', 'VBZ', 'IN', 'VBG', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",32
sentiment_analysis,12,18,"However , the existing attention mechanism in ASC suffers from a major drawback .","['However', ',', 'the', 'existing', 'attention', 'mechanism', 'in', 'ASC', 'suffers', 'from', 'a', 'major', 'drawback', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'DT', 'VBG', 'NN', 'NN', 'IN', 'NNP', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",14
sentiment_analysis,12,30,"In this paper , we propose a novel progressive self - supervised attention learning approach for neural ASC models .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', 'progressive', 'self', '-', 'supervised', 'attention', 'learning', 'approach', 'for', 'neural', 'ASC', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', ':', 'VBN', 'NN', 'VBG', 'NN', 'IN', 'JJ', 'NNP', 'NNS', '.']",20
sentiment_analysis,12,31,"Our method is able to automatically and incrementally mine attention supervision information from a training corpus , which can be exploited to guide the training of attention mechanisms in ASC models .","['Our', 'method', 'is', 'able', 'to', 'automatically', 'and', 'incrementally', 'mine', 'attention', 'supervision', 'information', 'from', 'a', 'training', 'corpus', ',', 'which', 'can', 'be', 'exploited', 'to', 'guide', 'the', 'training', 'of', 'attention', 'mechanisms', 'in', 'ASC', 'models', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'JJ', 'TO', 'RB', 'CC', 'RB', 'JJ', 'NN', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'WDT', 'MD', 'VB', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', 'NNS', 'IN', 'NNP', 'NNS', '.']",32
sentiment_analysis,12,32,The basic idea behind our approach roots in the following fact : the context word with the maximum attention weight has the greatest impact on the sentiment prediction of an input sentence .,"['The', 'basic', 'idea', 'behind', 'our', 'approach', 'roots', 'in', 'the', 'following', 'fact', ':', 'the', 'context', 'word', 'with', 'the', 'maximum', 'attention', 'weight', 'has', 'the', 'greatest', 'impact', 'on', 'the', 'sentiment', 'prediction', 'of', 'an', 'input', 'sentence', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'PRP$', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', ':', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",33
sentiment_analysis,12,150,We used pre-trained Glo Ve vectors to initialize the word embeddings with vector dimension 300 .,"['We', 'used', 'pre-trained', 'Glo', 'Ve', 'vectors', 'to', 'initialize', 'the', 'word', 'embeddings', 'with', 'vector', 'dimension', '300', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-n', 'O']","['PRP', 'VBD', 'JJ', 'NNP', 'NNP', 'NNS', 'TO', 'VB', 'DT', 'NN', 'VBZ', 'IN', 'NN', 'NN', 'CD', '.']",16
sentiment_analysis,12,151,"For out - of - vocabulary words , we randomly sampled their embeddings from the uniform distribution , as implemented in .","['For', 'out', '-', 'of', '-', 'vocabulary', 'words', ',', 'we', 'randomly', 'sampled', 'their', 'embeddings', 'from', 'the', 'uniform', 'distribution', ',', 'as', 'implemented', 'in', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'RP', ':', 'IN', ':', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'VBN', 'PRP$', 'NNS', 'IN', 'DT', 'JJ', 'NN', ',', 'IN', 'VBN', 'IN', '.']",22
sentiment_analysis,12,153,"To alleviate overfitting , we employed dropout strategy ( Hinton et al. , 2012 ) on the input word embeddings of the LSTM and the ultimate aspect - related sentence representation .","['To', 'alleviate', 'overfitting', ',', 'we', 'employed', 'dropout', 'strategy', '(', 'Hinton', 'et', 'al.', ',', '2012', ')', 'on', 'the', 'input', 'word', 'embeddings', 'of', 'the', 'LSTM', 'and', 'the', 'ultimate', 'aspect', '-', 'related', 'sentence', 'representation', '.']","['B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'NN', ',', 'PRP', 'VBD', 'RP', 'NN', '(', 'NNP', 'VBZ', 'RB', ',', 'CD', ')', 'IN', 'DT', 'NN', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'CC', 'DT', 'JJ', 'NN', ':', 'JJ', 'NN', 'NN', '.']",32
sentiment_analysis,12,154,"Adam ( Kingma and Ba , 2015 ) was adopted as the optimizer with the learning rate 0.001 .","['Adam', '(', 'Kingma', 'and', 'Ba', ',', '2015', ')', 'was', 'adopted', 'as', 'the', 'optimizer', 'with', 'the', 'learning', 'rate', '0.001', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O']","['NNP', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'VBD', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'VBG', 'NN', 'CD', '.']",19
sentiment_analysis,12,155,"When implementing our approach , we empirically set the maximum iteration number K as 5 , ? in Equation 3 as 0.1 on LAPTOP data set , 0.5 on REST data set and 0.1 on TWITTER data set , respectively .","['When', 'implementing', 'our', 'approach', ',', 'we', 'empirically', 'set', 'the', 'maximum', 'iteration', 'number', 'K', 'as', '5', ',', '?', 'in', 'Equation', '3', 'as', '0.1', 'on', 'LAPTOP', 'data', 'set', ',', '0.5', 'on', 'REST', 'data', 'set', 'and', '0.1', 'on', 'TWITTER', 'data', 'set', ',', 'respectively', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['WRB', 'VBG', 'PRP$', 'NN', ',', 'PRP', 'RB', 'VBD', 'DT', 'JJ', 'NN', 'NN', 'NNP', 'IN', 'CD', ',', '.', 'IN', 'NNP', 'CD', 'IN', 'CD', 'IN', 'NNP', 'NN', 'NN', ',', 'CD', 'IN', 'NNP', 'NN', 'NN', 'CC', 'CD', 'IN', 'NNP', 'NN', 'NN', ',', 'RB', '.']",41
sentiment_analysis,12,156,All hyper - parameters were tuned on 20 % randomly held - out training data .,"['All', 'hyper', '-', 'parameters', 'were', 'tuned', 'on', '20', '%', 'randomly', 'held', '-', 'out', 'training', 'data', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJR', ':', 'NNS', 'VBD', 'VBN', 'IN', 'CD', 'NN', 'RB', 'VBD', ':', 'RP', 'NN', 'NNS', '.']",16
sentiment_analysis,12,174,"First , both of our reimplemented MN and TNet are comparable to their original models reported in .","['First', ',', 'both', 'of', 'our', 'reimplemented', 'MN', 'and', 'TNet', 'are', 'comparable', 'to', 'their', 'original', 'models', 'reported', 'in', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['RB', ',', 'DT', 'IN', 'PRP$', 'VBN', 'NNP', 'CC', 'NNP', 'VBP', 'JJ', 'TO', 'PRP$', 'JJ', 'NNS', 'VBN', 'IN', '.']",18
sentiment_analysis,12,176,"When we replace the CNN of TNet with an attention mechanism , TNet - ATT is slightly inferior to TNet .","['When', 'we', 'replace', 'the', 'CNN', 'of', 'TNet', 'with', 'an', 'attention', 'mechanism', ',', 'TNet', '-', 'ATT', 'is', 'slightly', 'inferior', 'to', 'TNet', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['WRB', 'PRP', 'VB', 'DT', 'NNP', 'IN', 'NNP', 'IN', 'DT', 'NN', 'NN', ',', 'NNP', ':', 'NNP', 'VBZ', 'RB', 'JJ', 'TO', 'NNP', '.']",21
sentiment_analysis,12,177,"Moreover , when we perform additional K+1 - iteration of training on these models , their performance has not changed significantly , suggesting simply increasing training time is unable to enhance the performance of the neural ASC models .","['Moreover', ',', 'when', 'we', 'perform', 'additional', 'K+1', '-', 'iteration', 'of', 'training', 'on', 'these', 'models', ',', 'their', 'performance', 'has', 'not', 'changed', 'significantly', ',', 'suggesting', 'simply', 'increasing', 'training', 'time', 'is', 'unable', 'to', 'enhance', 'the', 'performance', 'of', 'the', 'neural', 'ASC', 'models', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'WRB', 'PRP', 'VBP', 'JJ', 'NNP', ':', 'NN', 'IN', 'VBG', 'IN', 'DT', 'NNS', ',', 'PRP$', 'NN', 'VBZ', 'RB', 'VBN', 'RB', ',', 'VBG', 'RB', 'VBG', 'NN', 'NN', 'VBZ', 'JJ', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'NNS', '.']",39
sentiment_analysis,12,183,"Finally , when we use both kinds of attention supervision information , no matter for which metric , MN ( + AS ) remarkably outperforms MN on all test sets .","['Finally', ',', 'when', 'we', 'use', 'both', 'kinds', 'of', 'attention', 'supervision', 'information', ',', 'no', 'matter', 'for', 'which', 'metric', ',', 'MN', '(', '+', 'AS', ')', 'remarkably', 'outperforms', 'MN', 'on', 'all', 'test', 'sets', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'WRB', 'PRP', 'VBP', 'DT', 'NNS', 'IN', 'NN', 'NN', 'NN', ',', 'DT', 'NN', 'IN', 'WDT', 'JJ', ',', 'NNP', '(', 'NNP', 'NNP', ')', 'RB', 'VBZ', 'NNP', 'IN', 'DT', 'NN', 'NNS', '.']",31
sentiment_analysis,35,2,Exploiting Coarse - to - Fine Task Transfer for Aspect - level Sentiment Classification,"['Exploiting', 'Coarse', '-', 'to', '-', 'Fine', 'Task', 'Transfer', 'for', 'Aspect', '-', 'level', 'Sentiment', 'Classification']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['VBG', 'NNP', ':', 'TO', ':', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', ':', 'NN', 'NNP', 'NNP']",14
sentiment_analysis,35,4,"Aspect - level sentiment classification ( ASC ) aims at identifying sentiment polarities towards aspects in a sentence , where the aspect can behave as a general Aspect Category ( AC ) or a specific Aspect Term ( AT ) .","['Aspect', '-', 'level', 'sentiment', 'classification', '(', 'ASC', ')', 'aims', 'at', 'identifying', 'sentiment', 'polarities', 'towards', 'aspects', 'in', 'a', 'sentence', ',', 'where', 'the', 'aspect', 'can', 'behave', 'as', 'a', 'general', 'Aspect', 'Category', '(', 'AC', ')', 'or', 'a', 'specific', 'Aspect', 'Term', '(', 'AT', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'IN', 'VBG', 'NN', 'NNS', 'NNS', 'NNS', 'IN', 'DT', 'NN', ',', 'WRB', 'DT', 'NN', 'MD', 'VB', 'IN', 'DT', 'JJ', 'NNP', 'NNP', '(', 'NNP', ')', 'CC', 'DT', 'JJ', 'NNP', 'NNP', '(', 'NNP', ')', '.']",41
sentiment_analysis,35,22,"To model aspect - oriented sentiment analysis , equipping Recurrent Neural Networks ( RNNs ) with the attention Copyright c 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .","['To', 'model', 'aspect', '-', 'oriented', 'sentiment', 'analysis', ',', 'equipping', 'Recurrent', 'Neural', 'Networks', '(', 'RNNs', ')', 'with', 'the', 'attention', 'Copyright', 'c', '2019', ',', 'Association', 'for', 'the', 'Advancement', 'of', 'Artificial', 'Intelligence', '(', 'www.aaai.org', ')', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'JJ', ':', 'VBN', 'NN', 'NN', ',', 'VBG', 'JJ', 'NNP', 'NNP', '(', 'NNP', ')', 'IN', 'DT', 'NN', 'NNP', 'NN', 'CD', ',', 'NNP', 'IN', 'DT', 'NNP', 'IN', 'NNP', 'NNP', '(', 'NN', ')', '.']",33
sentiment_analysis,35,38,"To resolve the challenges , we propose a novel framework named Multi - Granularity Alignment Network ( MGAN ) to simultaneously align aspect granularity and aspect- specific feature representations across domains .","['To', 'resolve', 'the', 'challenges', ',', 'we', 'propose', 'a', 'novel', 'framework', 'named', 'Multi', '-', 'Granularity', 'Alignment', 'Network', '(', 'MGAN', ')', 'to', 'simultaneously', 'align', 'aspect', 'granularity', 'and', 'aspect-', 'specific', 'feature', 'representations', 'across', 'domains', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['TO', 'VB', 'DT', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBN', 'NNP', ':', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'TO', 'RB', 'VB', 'JJ', 'NN', 'CC', 'JJ', 'JJ', 'NN', 'NNS', 'IN', 'NNS', '.']",32
sentiment_analysis,35,39,"Specifically , the MGAN consists of two networks for learning aspect - specific representations for the two domains , respectively .","['Specifically', ',', 'the', 'MGAN', 'consists', 'of', 'two', 'networks', 'for', 'learning', 'aspect', '-', 'specific', 'representations', 'for', 'the', 'two', 'domains', ',', 'respectively', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['RB', ',', 'DT', 'NNP', 'VBZ', 'IN', 'CD', 'NNS', 'IN', 'VBG', 'JJ', ':', 'JJ', 'NNS', 'IN', 'DT', 'CD', 'NNS', ',', 'RB', '.']",21
sentiment_analysis,35,40,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .","['First', ',', 'to', 'reduce', 'the', 'task', 'discrepancy', 'between', 'domains', ',', 'i.e.', ',', 'modeling', 'the', 'two', 'tasks', 'at', 'the', 'same', 'fine', '-', 'grained', 'level', ',', 'we', 'propose', 'a', 'novel', 'Coarse2', 'Fine', '(', 'C2F', ')', 'attention', 'module', 'to', 'help', 'the', 'source', 'task', 'automatically', 'capture', 'the', 'corresponding', 'aspect', 'term', 'in', 'the', 'context', 'towards', 'the', 'given', 'aspect', 'category', '(', 'e.g.', ',', '""', 'salmon', '""', 'to', 'the', '""', 'food', '""', ')', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'NNS', ',', 'FW', ',', 'VBG', 'DT', 'CD', 'NNS', 'IN', 'DT', 'JJ', 'JJ', ':', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'RB', 'VB', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'VBN', 'JJ', 'NN', '(', 'JJ', ',', 'JJ', 'NN', 'NN', 'TO', 'DT', 'NN', 'NN', 'NN', ')', '.']",67
sentiment_analysis,35,45,"To prevent false alignment , we adopt the Contrastive Feature Alignment ( CFA ) ( Motiian et al. 2017 ) to semantically align aspect - specific representations .","['To', 'prevent', 'false', 'alignment', ',', 'we', 'adopt', 'the', 'Contrastive', 'Feature', 'Alignment', '(', 'CFA', ')', '(', 'Motiian', 'et', 'al.', '2017', ')', 'to', 'semantically', 'align', 'aspect', '-', 'specific', 'representations', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', '(', 'JJ', 'NN', 'IN', 'CD', ')', 'TO', 'RB', 'VB', 'JJ', ':', 'JJ', 'NNS', '.']",28
sentiment_analysis,35,46,"The CFA considers both semantic alignment by maximally ensuring the equivalent distributions from different domains but the same class , and semantic separation by guaranteeing distributions from both different classes and domains to be as dissimilar as possible .","['The', 'CFA', 'considers', 'both', 'semantic', 'alignment', 'by', 'maximally', 'ensuring', 'the', 'equivalent', 'distributions', 'from', 'different', 'domains', 'but', 'the', 'same', 'class', ',', 'and', 'semantic', 'separation', 'by', 'guaranteeing', 'distributions', 'from', 'both', 'different', 'classes', 'and', 'domains', 'to', 'be', 'as', 'dissimilar', 'as', 'possible', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'NNS', 'DT', 'JJ', 'NN', 'IN', 'RB', 'VBG', 'DT', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', 'CC', 'DT', 'JJ', 'NN', ',', 'CC', 'JJ', 'NN', 'IN', 'VBG', 'NNS', 'IN', 'DT', 'JJ', 'NNS', 'CC', 'NNS', 'TO', 'VB', 'RB', 'JJ', 'IN', 'JJ', '.']",39
sentiment_analysis,35,184,The word embeddings are initialized with 200 - dimension GloVE vectors and fine - tuned during the training .,"['The', 'word', 'embeddings', 'are', 'initialized', 'with', '200', '-', 'dimension', 'GloVE', 'vectors', 'and', 'fine', '-', 'tuned', 'during', 'the', 'training', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'CD', ':', 'NN', 'NNP', 'NNS', 'CC', 'JJ', ':', 'VBN', 'IN', 'DT', 'NN', '.']",19
sentiment_analysis,35,186,The fc layer size is 300 .,"['The', 'fc', 'layer', 'size', 'is', '300', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'NN', 'VBZ', 'CD', '.']",7
sentiment_analysis,35,187,The Adam ( Kingma and Ba 2014 ) is used as the optimizer with the initial learning rate 10 ? 4 .,"['The', 'Adam', '(', 'Kingma', 'and', 'Ba', '2014', ')', 'is', 'used', 'as', 'the', 'optimizer', 'with', 'the', 'initial', 'learning', 'rate', '10', '?', '4', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', '(', 'NNP', 'CC', 'NNP', 'CD', ')', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'CD', '.', 'CD', '.']",22
sentiment_analysis,35,188,Gradients with the 2 norm larger than 40 are normalized to be 40 .,"['Gradients', 'with', 'the', '2', 'norm', 'larger', 'than', '40', 'are', 'normalized', 'to', 'be', '40', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O']","['NNS', 'IN', 'DT', 'CD', 'NN', 'JJR', 'IN', 'CD', 'VBP', 'VBN', 'TO', 'VB', 'CD', '.']",14
sentiment_analysis,35,189,"All weights in networks are randomly initialized from a uniform distribution U ( ? 0.01 , 0.01 ) .","['All', 'weights', 'in', 'networks', 'are', 'randomly', 'initialized', 'from', 'a', 'uniform', 'distribution', 'U', '(', '?', '0.01', ',', '0.01', ')', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NNP', '(', '.', 'CD', ',', 'CD', ')', '.']",19
sentiment_analysis,35,190,"The batch sizes are 64 and 32 for source and target domains , respectively .","['The', 'batch', 'sizes', 'are', '64', 'and', '32', 'for', 'source', 'and', 'target', 'domains', ',', 'respectively', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['DT', 'NN', 'NNS', 'VBP', 'CD', 'CC', 'CD', 'IN', 'NN', 'CC', 'NN', 'NNS', ',', 'RB', '.']",15
sentiment_analysis,35,193,"To alleviate overfitting , we apply dropout on the word embeddings of the context with dropout rate 0.5 .","['To', 'alleviate', 'overfitting', ',', 'we', 'apply', 'dropout', 'on', 'the', 'word', 'embeddings', 'of', 'the', 'context', 'with', 'dropout', 'rate', '0.5', '.']","['B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'NN', ',', 'PRP', 'VBP', 'RB', 'IN', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', 'CD', '.']",19
sentiment_analysis,35,194,We also perform early stopping on the validation set during the training process .,"['We', 'also', 'perform', 'early', 'stopping', 'on', 'the', 'validation', 'set', 'during', 'the', 'training', 'process', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'JJ', 'NN', 'IN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",14
sentiment_analysis,35,195,The hyperparameters are tuned on 10 % randomly held - out training data of the target domain in R1?L task and are fixed to be used in all transfer pairs .,"['The', 'hyperparameters', 'are', 'tuned', 'on', '10', '%', 'randomly', 'held', '-', 'out', 'training', 'data', 'of', 'the', 'target', 'domain', 'in', 'R1?L', 'task', 'and', 'are', 'fixed', 'to', 'be', 'used', 'in', 'all', 'transfer', 'pairs', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'CD', 'NN', 'RB', 'VBD', ':', 'RP', 'VBG', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'NNP', 'NN', 'CC', 'VBP', 'VBN', 'TO', 'VB', 'VBN', 'IN', 'DT', 'NN', 'NNS', '.']",31
sentiment_analysis,35,198,Non-Transfer,['Non-Transfer'],['B-n'],['NN'],1
sentiment_analysis,35,199,"To demonstrate the benefits from coarse - tofine task transfer , we compare with the following state - of the - art AT - level methods without transfer : Target Network ( TN ) :","['To', 'demonstrate', 'the', 'benefits', 'from', 'coarse', '-', 'tofine', 'task', 'transfer', ',', 'we', 'compare', 'with', 'the', 'following', 'state', '-', 'of', 'the', '-', 'art', 'AT', '-', 'level', 'methods', 'without', 'transfer', ':', 'Target', 'Network', '(', 'TN', ')', ':']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NNS', 'IN', 'JJ', ':', 'NN', 'NN', 'NN', ',', 'PRP', 'VBP', 'IN', 'DT', 'VBG', 'NN', ':', 'IN', 'DT', ':', 'NN', 'NNP', ':', 'NN', 'NNS', 'IN', 'NN', ':', 'NN', 'NNP', '(', 'NNP', ')', ':']",35
sentiment_analysis,35,200,It is our proposed base model ( BiLSTM + C2A + Pas ) trained on D t for the target task .,"['It', 'is', 'our', 'proposed', 'base', 'model', '(', 'BiLSTM', '+', 'C2A', '+', 'Pas', ')', 'trained', 'on', 'D', 't', 'for', 'the', 'target', 'task', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBZ', 'PRP$', 'JJ', 'NN', 'NN', '(', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', ')', 'VBD', 'IN', 'NNP', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",22
sentiment_analysis,35,202,Transfer,['Transfer'],['B-n'],['NN'],1
sentiment_analysis,35,204,Source- only ( SO ) :,"['Source-', 'only', '(', 'SO', ')', ':']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'RB', '(', 'NNP', ')', ':']",6
sentiment_analysis,35,205,It uses a source network trained on D s to initialize a target network and then tests it on D t .,"['It', 'uses', 'a', 'source', 'network', 'trained', 'on', 'D', 's', 'to', 'initialize', 'a', 'target', 'network', 'and', 'then', 'tests', 'it', 'on', 'D', 't', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBZ', 'DT', 'NN', 'NN', 'VBD', 'IN', 'NNP', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'CC', 'RB', 'VBZ', 'PRP', 'IN', 'NNP', 'NN', '.']",22
sentiment_analysis,35,206,Fine-tuning ( FT ) : It advances SO with further finetuning the target network on D t .,"['Fine-tuning', '(', 'FT', ')', ':', 'It', 'advances', 'SO', 'with', 'further', 'finetuning', 'the', 'target', 'network', 'on', 'D', 't', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', '(', 'NNP', ')', ':', 'PRP', 'VBZ', 'RB', 'IN', 'JJ', 'VBG', 'DT', 'NN', 'NN', 'IN', 'NNP', 'NN', '.']",18
sentiment_analysis,35,207,M- DAN : It is a multi-adversarial version of Domain Adversarial Network ( DAN ) ) based on multiple domain discriminators .,"['M-', 'DAN', ':', 'It', 'is', 'a', 'multi-adversarial', 'version', 'of', 'Domain', 'Adversarial', 'Network', '(', 'DAN', ')', ')', 'based', 'on', 'multiple', 'domain', 'discriminators', '.']","['B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['JJ', 'NNP', ':', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', ')', 'VBN', 'IN', 'JJ', 'NN', 'NNS', '.']",22
sentiment_analysis,35,209,"M - MMD : Similar with M - DAN , M - MMD aligns different class distributions between domains based on multiple Maximum Mean Discrepancy ( MMD ) ) .","['M', '-', 'MMD', ':', 'Similar', 'with', 'M', '-', 'DAN', ',', 'M', '-', 'MMD', 'aligns', 'different', 'class', 'distributions', 'between', 'domains', 'based', 'on', 'multiple', 'Maximum', 'Mean', 'Discrepancy', '(', 'MMD', ')', ')', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['NNP', ':', 'NN', ':', 'JJ', 'IN', 'NNP', ':', 'NNP', ',', 'NNP', ':', 'NNP', 'VBZ', 'JJ', 'NN', 'NNS', 'IN', 'NNS', 'VBN', 'IN', 'JJ', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', ')', '.']",30
sentiment_analysis,35,214,Comparison with Non - Transfer,"['Comparison', 'with', 'Non', '-', 'Transfer']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'IN', 'NNP', ':', 'NN']",5
sentiment_analysis,35,220,"( 2 ) MGAN consistently outperforms the MGAN w / o C2 F , where C2F module of the source network is removed and the source position information is missed ( we set all p s i to 1 ) , by 1.41 % , 1.03 % , 1.09 % for accuracy and 1.79 % , 3.62 % and 1.16 % for Macro - F1 on average .","['(', '2', ')', 'MGAN', 'consistently', 'outperforms', 'the', 'MGAN', 'w', '/', 'o', 'C2', 'F', ',', 'where', 'C2F', 'module', 'of', 'the', 'source', 'network', 'is', 'removed', 'and', 'the', 'source', 'position', 'information', 'is', 'missed', '(', 'we', 'set', 'all', 'p', 's', 'i', 'to', '1', ')', ',', 'by', '1.41', '%', ',', '1.03', '%', ',', '1.09', '%', 'for', 'accuracy', 'and', '1.79', '%', ',', '3.62', '%', 'and', '1.16', '%', 'for', 'Macro', '-', 'F1', 'on', 'average', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['(', 'CD', ')', 'NNP', 'RB', 'VBZ', 'DT', 'NNP', 'VBD', 'NNP', 'MD', 'NNP', 'NNP', ',', 'WRB', 'NNP', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'CC', 'DT', 'NN', 'NN', 'NN', 'VBZ', 'VBN', '(', 'PRP', 'VBP', 'DT', 'VBP', 'NN', 'NN', 'TO', 'CD', ')', ',', 'IN', 'CD', 'NN', ',', 'CD', 'NN', ',', 'CD', 'NN', 'IN', 'NN', 'CC', 'CD', 'NN', ',', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'NNP', ':', 'NN', 'IN', 'NN', '.']",68
sentiment_analysis,35,223,"The MGAN w / o PI , which does not utilize the position information , performs very poorly .","['The', 'MGAN', 'w', '/', 'o', 'PI', ',', 'which', 'does', 'not', 'utilize', 'the', 'position', 'information', ',', 'performs', 'very', 'poorly', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NNP', 'VBD', 'NNP', 'MD', 'NNP', ',', 'WDT', 'VBZ', 'RB', 'VB', 'DT', 'NN', 'NN', ',', 'VBZ', 'RB', 'RB', '.']",19
sentiment_analysis,35,224,Comparison with Transfer,"['Comparison', 'with', 'Transfer']","['B-n', 'I-n', 'I-n']","['NN', 'IN', 'NN']",3
sentiment_analysis,35,227,SO performs poorly due to no adaptation applied .,"['SO', 'performs', 'poorly', 'due', 'to', 'no', 'adaptation', 'applied', '.']","['B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'JJ', 'RB', 'JJ', 'TO', 'DT', 'NN', 'VBD', '.']",9
sentiment_analysis,35,228,The popular technique FT can not achieve satisfactory results since fine - tuning may cause the oblivion of useful knowledge from the source task .,"['The', 'popular', 'technique', 'FT', 'can', 'not', 'achieve', 'satisfactory', 'results', 'since', 'fine', '-', 'tuning', 'may', 'cause', 'the', 'oblivion', 'of', 'useful', 'knowledge', 'from', 'the', 'source', 'task', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NNP', 'MD', 'RB', 'VB', 'NN', 'NNS', 'IN', 'JJ', ':', 'NN', 'MD', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",25
sentiment_analysis,35,229,"The full model MGAN outperforms M - DAN and M - MMD by 1.80 % and 1.33 % for accuracy and 1.90 % and 1.66 % for Marco - F1 on average , respectively .","['The', 'full', 'model', 'MGAN', 'outperforms', 'M', '-', 'DAN', 'and', 'M', '-', 'MMD', 'by', '1.80', '%', 'and', '1.33', '%', 'for', 'accuracy', 'and', '1.90', '%', 'and', '1.66', '%', 'for', 'Marco', '-', 'F1', 'on', 'average', ',', 'respectively', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NNP', 'VBZ', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NN', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'NN', 'CC', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'NNP', ':', 'NN', 'IN', 'NN', ',', 'RB', '.']",35
sentiment_analysis,35,231,"Remarkably , MGAN considers both of them in a point - wise surrogate , which altogether improves the performance of our method .","['Remarkably', ',', 'MGAN', 'considers', 'both', 'of', 'them', 'in', 'a', 'point', '-', 'wise', 'surrogate', ',', 'which', 'altogether', 'improves', 'the', 'performance', 'of', 'our', 'method', '.']","['O', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'NNP', 'VBZ', 'DT', 'IN', 'PRP', 'IN', 'DT', 'NN', ':', 'NN', 'NN', ',', 'WDT', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'PRP$', 'NN', '.']",23
sentiment_analysis,35,232,"Besides , MGAN outperforms its ablation MGAN w/ o SS removing the semantic separation loss of the CFA by 0.81 % for accuracy and 1.00 % for Macro - F1 on average , which implies that the semantic separation plays an important role in alleviating false alignment .","['Besides', ',', 'MGAN', 'outperforms', 'its', 'ablation', 'MGAN', 'w/', 'o', 'SS', 'removing', 'the', 'semantic', 'separation', 'loss', 'of', 'the', 'CFA', 'by', '0.81', '%', 'for', 'accuracy', 'and', '1.00', '%', 'for', 'Macro', '-', 'F1', 'on', 'average', ',', 'which', 'implies', 'that', 'the', 'semantic', 'separation', 'plays', 'an', 'important', 'role', 'in', 'alleviating', 'false', 'alignment', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', ',', 'NNP', 'VBZ', 'PRP$', 'NN', 'NNP', 'VBZ', 'JJ', 'NNP', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NNP', 'IN', 'CD', 'NN', 'IN', 'NN', 'CC', 'CD', 'NN', 'IN', 'NNP', ':', 'NN', 'IN', 'NN', ',', 'WDT', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'JJ', 'NN', '.']",48
sentiment_analysis,35,233,Effect of C2F Attention Module,"['Effect', 'of', 'C2F', 'Attention', 'Module']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NN', 'IN', 'NNP', 'NNP', 'NNP']",5
sentiment_analysis,35,237,"Then , compared with MGAN w / o C2F , MGAN further uses C2F to capture more specific aspect terms from the context towards the aspect category , such as "" shells "" to food seafood sea , which helps the source task capture more fine - grained semantics of aspect category and detailed position information like the target task , such that the sentiment attention can be positionaware and identify more relevant sentiment features towards the aspect .","['Then', ',', 'compared', 'with', 'MGAN', 'w', '/', 'o', 'C2F', ',', 'MGAN', 'further', 'uses', 'C2F', 'to', 'capture', 'more', 'specific', 'aspect', 'terms', 'from', 'the', 'context', 'towards', 'the', 'aspect', 'category', ',', 'such', 'as', '""', 'shells', '""', 'to', 'food', 'seafood', 'sea', ',', 'which', 'helps', 'the', 'source', 'task', 'capture', 'more', 'fine', '-', 'grained', 'semantics', 'of', 'aspect', 'category', 'and', 'detailed', 'position', 'information', 'like', 'the', 'target', 'task', ',', 'such', 'that', 'the', 'sentiment', 'attention', 'can', 'be', 'positionaware', 'and', 'identify', 'more', 'relevant', 'sentiment', 'features', 'towards', 'the', 'aspect', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'VBN', 'IN', 'NNP', 'VBP', 'NNP', 'NN', 'NNP', ',', 'NNP', 'JJ', 'NNS', 'NNP', 'TO', 'VB', 'RBR', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'JJ', 'IN', 'JJ', 'NNS', 'VBP', 'TO', 'NN', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'NN', 'NN', 'JJR', 'JJ', ':', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'CC', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'JJ', 'IN', 'DT', 'NN', 'NN', 'MD', 'VB', 'JJ', 'CC', 'VB', 'JJR', 'JJ', 'NN', 'NNS', 'VBP', 'DT', 'NN', '.']",79
sentiment_analysis,35,240,While MGAN w / o C2F locates wrong sentiment contexts and fails in ( c ) .,"['While', 'MGAN', 'w', '/', 'o', 'C2F', 'locates', 'wrong', 'sentiment', 'contexts', 'and', 'fails', 'in', '(', 'c', ')', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', 'VBP', 'NNP', 'MD', 'NNP', 'VBZ', 'JJ', 'NN', 'NN', 'CC', 'NNS', 'IN', '(', 'NN', ')', '.']",17
sentiment_analysis,35,241,"As such , benefited from distilled knowledge from the source task , MGAN can better model the complicated relatedness between the context and aspect term for the target domain L , but MGAN w / o C2F performs poorly though it make true predictions in ( d ) and ( e ) .","['As', 'such', ',', 'benefited', 'from', 'distilled', 'knowledge', 'from', 'the', 'source', 'task', ',', 'MGAN', 'can', 'better', 'model', 'the', 'complicated', 'relatedness', 'between', 'the', 'context', 'and', 'aspect', 'term', 'for', 'the', 'target', 'domain', 'L', ',', 'but', 'MGAN', 'w', '/', 'o', 'C2F', 'performs', 'poorly', 'though', 'it', 'make', 'true', 'predictions', 'in', '(', 'd', ')', 'and', '(', 'e', ')', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', ',', 'VBD', 'IN', 'VBN', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'NNP', 'MD', 'VB', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'CC', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', 'NNP', ',', 'CC', 'NNP', 'VBP', 'NNP', 'MD', 'VB', 'NNS', 'RB', 'IN', 'PRP', 'VBP', 'JJ', 'NNS', 'IN', '(', 'NN', ')', 'CC', '(', 'NN', ')', '.']",53
sentiment_analysis,0,2,MULTIMODAL SPEECH EMOTION RECOGNITION USING AUDIO AND TEXT,"['MULTIMODAL', 'SPEECH', 'EMOTION', 'RECOGNITION', 'USING', 'AUDIO', 'AND', 'TEXT']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",8
sentiment_analysis,0,23,"To overcome these limitations , we propose a model that uses high - level text transcription , as well as low - level audio signals , to utilize the information contained within low - resource datasets to a greater degree .","['To', 'overcome', 'these', 'limitations', ',', 'we', 'propose', 'a', 'model', 'that', 'uses', 'high', '-', 'level', 'text', 'transcription', ',', 'as', 'well', 'as', 'low', '-', 'level', 'audio', 'signals', ',', 'to', 'utilize', 'the', 'information', 'contained', 'within', 'low', '-', 'resource', 'datasets', 'to', 'a', 'greater', 'degree', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'WDT', 'VBZ', 'JJ', ':', 'NN', 'JJ', 'NN', ',', 'RB', 'RB', 'IN', 'JJ', ':', 'NN', 'NN', 'NNS', ',', 'TO', 'VB', 'DT', 'NN', 'VBD', 'IN', 'JJ', ':', 'NN', 'NNS', 'TO', 'DT', 'JJR', 'NN', '.']",41
sentiment_analysis,0,27,"In this paper , we propose a novel deep dual recurrent encoder model that simultaneously utilizes audio and text data in recognizing emotions from speech .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', 'deep', 'dual', 'recurrent', 'encoder', 'model', 'that', 'simultaneously', 'utilizes', 'audio', 'and', 'text', 'data', 'in', 'recognizing', 'emotions', 'from', 'speech', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'JJ', 'NN', 'NN', 'NN', 'WDT', 'RB', 'VBZ', 'NN', 'CC', 'NN', 'NNS', 'IN', 'VBG', 'NNS', 'IN', 'NN', '.']",26
sentiment_analysis,0,123,"Among the variants of the RNN function , we use GRUs as they yield comparable performance to that of the LSTM and include a smaller number of weight parameters .","['Among', 'the', 'variants', 'of', 'the', 'RNN', 'function', ',', 'we', 'use', 'GRUs', 'as', 'they', 'yield', 'comparable', 'performance', 'to', 'that', 'of', 'the', 'LSTM', 'and', 'include', 'a', 'smaller', 'number', 'of', 'weight', 'parameters', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'NN', ',', 'PRP', 'VBP', 'VBN', 'IN', 'PRP', 'VBP', 'JJ', 'NN', 'TO', 'DT', 'IN', 'DT', 'NNP', 'CC', 'VBP', 'DT', 'JJR', 'NN', 'IN', 'JJ', 'NNS', '.']",30
sentiment_analysis,0,124,"We use a max encoder step of 750 for the audio input , based on the implementation choices presented in and 128 for the text input because it covers the maximum length of the transcripts .","['We', 'use', 'a', 'max', 'encoder', 'step', 'of', '750', 'for', 'the', 'audio', 'input', ',', 'based', 'on', 'the', 'implementation', 'choices', 'presented', 'in', 'and', '128', 'for', 'the', 'text', 'input', 'because', 'it', 'covers', 'the', 'maximum', 'length', 'of', 'the', 'transcripts', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'JJ', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', 'NN', ',', 'VBN', 'IN', 'DT', 'NN', 'NNS', 'VBN', 'IN', 'CC', 'CD', 'IN', 'DT', 'NN', 'NN', 'IN', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNS', '.']",36
sentiment_analysis,0,125,"The vocabulary size of the dataset is 3,747 , including the "" UNK "" token , which represents unknown words , and the "" PAD "" token , which is used to indicate padding information added while preparing mini-batch data .","['The', 'vocabulary', 'size', 'of', 'the', 'dataset', 'is', '3,747', ',', 'including', 'the', '""', 'UNK', '""', 'token', ',', 'which', 'represents', 'unknown', 'words', ',', 'and', 'the', '""', 'PAD', '""', 'token', ',', 'which', 'is', 'used', 'to', 'indicate', 'padding', 'information', 'added', 'while', 'preparing', 'mini-batch', 'data', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'CD', ',', 'VBG', 'DT', 'JJ', 'NNP', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'JJ', 'NNS', ',', 'CC', 'DT', 'NNP', 'NNP', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'VBN', 'TO', 'VB', 'VBG', 'NN', 'VBD', 'IN', 'VBG', 'JJ', 'NNS', '.']",41
sentiment_analysis,0,126,"The number of hidden units and the number of layers in the RNN for each model ( ARE , TRE , MDRE and MDREA ) are selected based on extensive hyperparameter search experiments .","['The', 'number', 'of', 'hidden', 'units', 'and', 'the', 'number', 'of', 'layers', 'in', 'the', 'RNN', 'for', 'each', 'model', '(', 'ARE', ',', 'TRE', ',', 'MDRE', 'and', 'MDREA', ')', 'are', 'selected', 'based', 'on', 'extensive', 'hyperparameter', 'search', 'experiments', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'JJ', 'NNS', 'CC', 'DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NNP', 'IN', 'DT', 'NN', '(', 'NNP', ',', 'NNP', ',', 'NNP', 'CC', 'NNP', ')', 'VBP', 'VBN', 'VBN', 'IN', 'JJ', 'NN', 'NN', 'NNS', '.']",34
sentiment_analysis,0,127,The weights of the hidden units are initialized using orthogonal,"['The', 'weights', 'of', 'the', 'hidden', 'units', 'are', 'initialized', 'using', 'orthogonal']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n']","['DT', 'NNS', 'IN', 'DT', 'JJ', 'NNS', 'VBP', 'VBN', 'VBG', 'JJ']",10
sentiment_analysis,0,133,"weights ] , and the text embedding layer is initialized from pretrained word - embedding vectors .","['weights', ']', ',', 'and', 'the', 'text', 'embedding', 'layer', 'is', 'initialized', 'from', 'pretrained', 'word', '-', 'embedding', 'vectors', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'NN', ',', 'CC', 'DT', 'NN', 'VBG', 'NN', 'VBZ', 'VBN', 'IN', 'VBN', 'NN', ':', 'NN', 'NNS', '.']",17
sentiment_analysis,0,134,"In preparing the textual dataset , we first use the released transcripts of the IEMOCAP dataset for simplicity .","['In', 'preparing', 'the', 'textual', 'dataset', ',', 'we', 'first', 'use', 'the', 'released', 'transcripts', 'of', 'the', 'IEMOCAP', 'dataset', 'for', 'simplicity', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'VBG', 'DT', 'JJ', 'NN', ',', 'PRP', 'RB', 'VBP', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'IN', 'NN', '.']",19
sentiment_analysis,0,144,"First , our ARE model shows the baseline performance because we use minimal audio features , such as the MFCC and prosodic features with simple architectures .","['First', ',', 'our', 'ARE', 'model', 'shows', 'the', 'baseline', 'performance', 'because', 'we', 'use', 'minimal', 'audio', 'features', ',', 'such', 'as', 'the', 'MFCC', 'and', 'prosodic', 'features', 'with', 'simple', 'architectures', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'NNP', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'PRP', 'VBP', 'JJ', 'NN', 'NNS', ',', 'JJ', 'IN', 'DT', 'NNP', 'CC', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', '.']",27
sentiment_analysis,0,145,"On the other hand , the TRE model shows higher performance gain compared to the ARE .","['On', 'the', 'other', 'hand', ',', 'the', 'TRE', 'model', 'shows', 'higher', 'performance', 'gain', 'compared', 'to', 'the', 'ARE', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'DT', 'NNP', 'NN', 'VBZ', 'JJR', 'NN', 'NN', 'VBN', 'TO', 'DT', 'NNP', '.']",17
sentiment_analysis,0,146,"From this result , we note that textual data are informative in emotion prediction tasks , and the recurrent encoder model is effective in understanding these types of sequential data .","['From', 'this', 'result', ',', 'we', 'note', 'that', 'textual', 'data', 'are', 'informative', 'in', 'emotion', 'prediction', 'tasks', ',', 'and', 'the', 'recurrent', 'encoder', 'model', 'is', 'effective', 'in', 'understanding', 'these', 'types', 'of', 'sequential', 'data', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'IN', 'JJ', 'NNS', 'VBP', 'JJ', 'IN', 'NN', 'NN', 'NNS', ',', 'CC', 'DT', 'NN', 'NN', 'NN', 'VBZ', 'JJ', 'IN', 'VBG', 'DT', 'NNS', 'IN', 'JJ', 'NNS', '.']",31
sentiment_analysis,0,147,"Second , the newly proposed model , MDRE , shows a substantial performance gain .","['Second', ',', 'the', 'newly', 'proposed', 'model', ',', 'MDRE', ',', 'shows', 'a', 'substantial', 'performance', 'gain', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['JJ', ',', 'DT', 'RB', 'VBN', 'NN', ',', 'NNP', ',', 'VBZ', 'DT', 'JJ', 'NN', 'NN', '.']",15
sentiment_analysis,0,148,It thus achieves the state - of - the - art performance with a WAP value of 0.718 .,"['It', 'thus', 'achieves', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'with', 'a', 'WAP', 'value', 'of', '0.718', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'RB', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'CD', '.']",19
sentiment_analysis,0,150,"Lastly , the attention model , MDREA , also outperforms the best existing research results ( WAP 0.690 to 0.688 ) .","['Lastly', ',', 'the', 'attention', 'model', ',', 'MDREA', ',', 'also', 'outperforms', 'the', 'best', 'existing', 'research', 'results', '(', 'WAP', '0.690', 'to', '0.688', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'NN', ',', 'NNP', ',', 'RB', 'VBZ', 'DT', 'JJS', 'VBG', 'NN', 'NNS', '(', 'NNP', 'CD', 'TO', 'CD', ')', '.']",22
sentiment_analysis,0,156,The label accuracy of the processed transcripts is 5.53 % WER .,"['The', 'label', 'accuracy', 'of', 'the', 'processed', 'transcripts', 'is', '5.53', '%', 'WER', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'VBZ', 'CD', 'NN', 'NNP', '.']",12
sentiment_analysis,0,157,"The TRE - ASR , MDRE - ASR and MDREA - ASR models reflect degraded performance compared to that of the TRE , MDRE and MDREA models .","['The', 'TRE', '-', 'ASR', ',', 'MDRE', '-', 'ASR', 'and', 'MDREA', '-', 'ASR', 'models', 'reflect', 'degraded', 'performance', 'compared', 'to', 'that', 'of', 'the', 'TRE', ',', 'MDRE', 'and', 'MDREA', 'models', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', ':', 'NNP', ',', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NNP', 'NNS', 'VBP', 'VBN', 'NN', 'VBN', 'TO', 'DT', 'IN', 'DT', 'NNP', ',', 'NNP', 'CC', 'NNP', 'NNS', '.']",28
sentiment_analysis,0,162,"The ARE model ( ) incorrectly classifies most instances of happy as neutral ( 43.51 % ) ; thus , it shows reduced accuracy ( 35.15 % ) in predicting the the happy class .","['The', 'ARE', 'model', '(', ')', 'incorrectly', 'classifies', 'most', 'instances', 'of', 'happy', 'as', 'neutral', '(', '43.51', '%', ')', ';', 'thus', ',', 'it', 'shows', 'reduced', 'accuracy', '(', '35.15', '%', ')', 'in', 'predicting', 'the', 'the', 'happy', 'class', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NNP', '(', ')', 'RB', 'VBZ', 'JJS', 'NNS', 'IN', 'JJ', 'IN', 'JJ', '(', 'CD', 'NN', ')', ':', 'RB', ',', 'PRP', 'VBZ', 'VBN', 'NN', '(', 'CD', 'NN', ')', 'IN', 'VBG', 'DT', 'DT', 'JJ', 'NN', '.']",35
sentiment_analysis,0,163,"Overall , most of the emotion classes are frequently confused with the neutral class .","['Overall', ',', 'most', 'of', 'the', 'emotion', 'classes', 'are', 'frequently', 'confused', 'with', 'the', 'neutral', 'class', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['JJ', ',', 'JJS', 'IN', 'DT', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",15
sentiment_analysis,0,165,"Interestingly , the TRE model ( ) shows greater prediction gains in predicting the happy class when compared to the ARE model ( 35.15 % to 75. 73 % ) .","['Interestingly', ',', 'the', 'TRE', 'model', '(', ')', 'shows', 'greater', 'prediction', 'gains', 'in', 'predicting', 'the', 'happy', 'class', 'when', 'compared', 'to', 'the', 'ARE', 'model', '(', '35.15', '%', 'to', '75.', '73', '%', ')', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NNP', 'NN', '(', ')', 'VBZ', 'JJR', 'NN', 'NNS', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'WRB', 'VBN', 'TO', 'DT', 'NNP', 'NNP', '(', 'CD', 'NN', 'TO', 'CD', 'CD', 'NN', ')', '.']",31
sentiment_analysis,0,168,The MDRE model ) compensates for the weaknesses of the previous two models ( ARE and TRE ) and benefits from their strengths to a surprising degree .,"['The', 'MDRE', 'model', ')', 'compensates', 'for', 'the', 'weaknesses', 'of', 'the', 'previous', 'two', 'models', '(', 'ARE', 'and', 'TRE', ')', 'and', 'benefits', 'from', 'their', 'strengths', 'to', 'a', 'surprising', 'degree', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNP', 'NN', ')', 'VBZ', 'IN', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'CD', 'NNS', '(', 'NNP', 'CC', 'NNP', ')', 'CC', 'NNS', 'IN', 'PRP$', 'NNS', 'TO', 'DT', 'JJ', 'NN', '.']",28
sentiment_analysis,0,170,"Furthermore , the occurrence of the incorrect "" sad - to - happy "" cases in the TRE model is reduced from 16 . 20 % to 9.15 % .","['Furthermore', ',', 'the', 'occurrence', 'of', 'the', 'incorrect', '""', 'sad', '-', 'to', '-', 'happy', '""', 'cases', 'in', 'the', 'TRE', 'model', 'is', 'reduced', 'from', '16', '.', '20', '%', 'to', '9.15', '%', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'NN', ':', 'TO', ':', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'VBZ', 'VBN', 'IN', 'CD', '.', 'CD', 'NN', 'TO', 'CD', 'NN', '.']",30
sentiment_analysis,40,2,Recurrent Attention Network on Memory for Aspect Sentiment Analysis,"['Recurrent', 'Attention', 'Network', 'on', 'Memory', 'for', 'Aspect', 'Sentiment', 'Analysis']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'IN', 'NNP', 'IN', 'NNP', 'NNP', 'NN']",9
sentiment_analysis,40,30,"In this paper , we propose a novel framework to solve the above problems in target sentiment analysis .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', 'framework', 'to', 'solve', 'the', 'above', 'problems', 'in', 'target', 'sentiment', 'analysis', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'NNS', 'IN', 'NN', 'NN', 'NN', '.']",19
sentiment_analysis,40,31,"Specifically , our framework first adopts a bidirectional LSTM ( BLSTM ) to produce the memory ( i.e. the states of time steps generated by LSTM ) from the input , as bidirectional recurrent neural networks ( RNNs ) were found effective for a similar purpose in machine translation .","['Specifically', ',', 'our', 'framework', 'first', 'adopts', 'a', 'bidirectional', 'LSTM', '(', 'BLSTM', ')', 'to', 'produce', 'the', 'memory', '(', 'i.e.', 'the', 'states', 'of', 'time', 'steps', 'generated', 'by', 'LSTM', ')', 'from', 'the', 'input', ',', 'as', 'bidirectional', 'recurrent', 'neural', 'networks', '(', 'RNNs', ')', 'were', 'found', 'effective', 'for', 'a', 'similar', 'purpose', 'in', 'machine', 'translation', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP$', 'NN', 'RB', 'VBZ', 'DT', 'JJ', 'NNP', '(', 'NNP', ')', 'TO', 'VB', 'DT', 'NN', '(', 'IN', 'DT', 'NNS', 'IN', 'NN', 'NNS', 'VBN', 'IN', 'NNP', ')', 'IN', 'DT', 'NN', ',', 'IN', 'JJ', 'NN', 'JJ', 'NNS', '(', 'NNP', ')', 'VBD', 'VBN', 'JJ', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NN', '.']",50
sentiment_analysis,40,32,"The memory slices are then weighted according to their relative positions to the target , so that different targets from the same sentence have their own tailor - made memories .","['The', 'memory', 'slices', 'are', 'then', 'weighted', 'according', 'to', 'their', 'relative', 'positions', 'to', 'the', 'target', ',', 'so', 'that', 'different', 'targets', 'from', 'the', 'same', 'sentence', 'have', 'their', 'own', 'tailor', '-', 'made', 'memories', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'VBG', 'TO', 'PRP$', 'JJ', 'NNS', 'TO', 'DT', 'NN', ',', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'VBP', 'PRP$', 'JJ', 'NN', ':', 'VBN', 'NNS', '.']",31
sentiment_analysis,40,33,"After that , we pay multiple attentions on the position - weighted memory and nonlinearly combine the attention results with a recurrent network , i.e. GRUs .","['After', 'that', ',', 'we', 'pay', 'multiple', 'attentions', 'on', 'the', 'position', '-', 'weighted', 'memory', 'and', 'nonlinearly', 'combine', 'the', 'attention', 'results', 'with', 'a', 'recurrent', 'network', ',', 'i.e.', 'GRUs', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'IN', 'DT', 'NN', ':', 'JJ', 'NN', 'CC', 'RB', 'VB', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', ',', 'FW', 'NNP', '.']",27
sentiment_analysis,40,34,"Finally , we apply softmax on the output of the GRU network to predict the sentiment on the target .","['Finally', ',', 'we', 'apply', 'softmax', 'on', 'the', 'output', 'of', 'the', 'GRU', 'network', 'to', 'predict', 'the', 'sentiment', 'on', 'the', 'target', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']","['RB', ',', 'PRP', 'VBP', 'VB', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",20
sentiment_analysis,40,35,Our framework introduces a novel way of applying multiple - attention mechanism to synthesize important features in difficult sentence structures .,"['Our', 'framework', 'introduces', 'a', 'novel', 'way', 'of', 'applying', 'multiple', '-', 'attention', 'mechanism', 'to', 'synthesize', 'important', 'features', 'in', 'difficult', 'sentence', 'structures', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'JJ', ':', 'NN', 'NN', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NNS', '.']",21
sentiment_analysis,40,149,Average Context :,"['Average', 'Context', ':']","['B-n', 'I-n', 'O']","['JJ', 'NN', ':']",3
sentiment_analysis,40,150,There are two versions of this method .,"['There', 'are', 'two', 'versions', 'of', 'this', 'method', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O']","['EX', 'VBP', 'CD', 'NNS', 'IN', 'DT', 'NN', '.']",8
sentiment_analysis,40,151,"The first one , named AC - S , averages the word vectors before the target and the word vectors after the target separately .","['The', 'first', 'one', ',', 'named', 'AC', '-', 'S', ',', 'averages', 'the', 'word', 'vectors', 'before', 'the', 'target', 'and', 'the', 'word', 'vectors', 'after', 'the', 'target', 'separately', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O']","['DT', 'JJ', 'CD', ',', 'VBN', 'NNP', ':', 'NNP', ',', 'VBZ', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'RB', '.']",25
sentiment_analysis,40,152,"The second one , named AC , averages the word vectors of the full context .","['The', 'second', 'one', ',', 'named', 'AC', ',', 'averages', 'the', 'word', 'vectors', 'of', 'the', 'full', 'context', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'CD', ',', 'VBN', 'NNP', ',', 'VBZ', 'DT', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",16
sentiment_analysis,40,153,"SVM : The traditional state - of - the - art method using SVMs on surface features , lexicon features and parsing features , which is the best team in SemEval 2014 .","['SVM', ':', 'The', 'traditional', 'state', '-', 'of', '-', 'the', '-', 'art', 'method', 'using', 'SVMs', 'on', 'surface', 'features', ',', 'lexicon', 'features', 'and', 'parsing', 'features', ',', 'which', 'is', 'the', 'best', 'team', 'in', 'SemEval', '2014', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', ':', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'VBG', 'NNP', 'IN', 'NN', 'NNS', ',', 'NN', 'NNS', 'CC', 'VBG', 'NNS', ',', 'WDT', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'JJ', 'CD', '.']",33
sentiment_analysis,40,154,"Rec - NN : It firstly uses rules to transform the dependency tree and put the opinion target at the root , and then performs semantic composition with Recursive NNs for sentiment prediction .","['Rec', '-', 'NN', ':', 'It', 'firstly', 'uses', 'rules', 'to', 'transform', 'the', 'dependency', 'tree', 'and', 'put', 'the', 'opinion', 'target', 'at', 'the', 'root', ',', 'and', 'then', 'performs', 'semantic', 'composition', 'with', 'Recursive', 'NNs', 'for', 'sentiment', 'prediction', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ':', 'NN', ':', 'PRP', 'RB', 'VBZ', 'NNS', 'TO', 'VB', 'DT', 'NN', 'NN', 'CC', 'VBD', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', ',', 'CC', 'RB', 'NNS', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'IN', 'NN', 'NN', '.']",34
sentiment_analysis,40,155,TD- LSTM : It uses a forward LSTM and a backward LSTM to abstract the information before and after the target .,"['TD-', 'LSTM', ':', 'It', 'uses', 'a', 'forward', 'LSTM', 'and', 'a', 'backward', 'LSTM', 'to', 'abstract', 'the', 'information', 'before', 'and', 'after', 'the', 'target', '.']","['B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['JJ', 'NNP', ':', 'PRP', 'VBZ', 'DT', 'NN', 'NNP', 'CC', 'DT', 'NN', 'NNP', 'TO', 'VB', 'DT', 'NN', 'IN', 'CC', 'IN', 'DT', 'NN', '.']",22
sentiment_analysis,40,158,TD - LSTM - A : We developed TD - LSTM to make it have one attention on the outputs of 3 https://github.com/svn2github/word2vec,"['TD', '-', 'LSTM', '-', 'A', ':', 'We', 'developed', 'TD', '-', 'LSTM', 'to', 'make', 'it', 'have', 'one', 'attention', 'on', 'the', 'outputs', 'of', '3', 'https://github.com/svn2github/word2vec']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O']","['NNP', ':', 'NNP', ':', 'NN', ':', 'PRP', 'VBD', 'NNP', ':', 'NN', 'TO', 'VB', 'PRP', 'VB', 'CD', 'NN', 'IN', 'DT', 'NNS', 'IN', 'CD', 'NN']",23
sentiment_analysis,40,160,"MemNet : It applies attention multiple times on the word embeddings , and the last attention 's output is fed to softmax for prediction , without combining the results of different attentions .","['MemNet', ':', 'It', 'applies', 'attention', 'multiple', 'times', 'on', 'the', 'word', 'embeddings', ',', 'and', 'the', 'last', 'attention', ""'s"", 'output', 'is', 'fed', 'to', 'softmax', 'for', 'prediction', ',', 'without', 'combining', 'the', 'results', 'of', 'different', 'attentions', '.']","['B-n', 'O', 'O', 'B-p', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', ':', 'PRP', 'VBZ', 'NN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NNS', ',', 'CC', 'DT', 'JJ', 'NN', 'POS', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'IN', 'NN', ',', 'IN', 'VBG', 'DT', 'NNS', 'IN', 'JJ', 'NNS', '.']",33
sentiment_analysis,40,167,"As shown by the results in , our RAM consistently outperforms all compared methods on these four datasets .","['As', 'shown', 'by', 'the', 'results', 'in', ',', 'our', 'RAM', 'consistently', 'outperforms', 'all', 'compared', 'methods', 'on', 'these', 'four', 'datasets', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBN', 'IN', 'DT', 'NNS', 'IN', ',', 'PRP$', 'NNP', 'RB', 'VBZ', 'DT', 'VBN', 'NNS', 'IN', 'DT', 'CD', 'NNS', '.']",19
sentiment_analysis,40,168,"AC and AC - S perform poorly , because averaging context is equivalent to paying identical attention to each word which would hide the true sentiment word .","['AC', 'and', 'AC', '-', 'S', 'perform', 'poorly', ',', 'because', 'averaging', 'context', 'is', 'equivalent', 'to', 'paying', 'identical', 'attention', 'to', 'each', 'word', 'which', 'would', 'hide', 'the', 'true', 'sentiment', 'word', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'CC', 'NNP', ':', 'NNP', 'VB', 'RB', ',', 'IN', 'VBG', 'NN', 'VBZ', 'JJ', 'TO', 'VBG', 'JJ', 'NN', 'TO', 'DT', 'NN', 'WDT', 'MD', 'VB', 'DT', 'JJ', 'NN', 'NN', '.']",28
sentiment_analysis,40,169,Rec - NN is better than TD - LSTM but not as good as our method .,"['Rec', '-', 'NN', 'is', 'better', 'than', 'TD', '-', 'LSTM', 'but', 'not', 'as', 'good', 'as', 'our', 'method', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['NNP', ':', 'NN', 'VBZ', 'JJR', 'IN', 'NNP', ':', 'NN', 'CC', 'RB', 'RB', 'JJ', 'IN', 'PRP$', 'NN', '.']",17
sentiment_analysis,40,172,"TD - LSTM performs less competitive than our method on all the datasets , particularly on the tweet dataset , because in this dataset sentiment words are usually far from person names , for which case the multiple - attention mechanism is designed to work .","['TD', '-', 'LSTM', 'performs', 'less', 'competitive', 'than', 'our', 'method', 'on', 'all', 'the', 'datasets', ',', 'particularly', 'on', 'the', 'tweet', 'dataset', ',', 'because', 'in', 'this', 'dataset', 'sentiment', 'words', 'are', 'usually', 'far', 'from', 'person', 'names', ',', 'for', 'which', 'case', 'the', 'multiple', '-', 'attention', 'mechanism', 'is', 'designed', 'to', 'work', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', 'NNS', 'RBR', 'JJ', 'IN', 'PRP$', 'NN', 'IN', 'PDT', 'DT', 'NNS', ',', 'RB', 'IN', 'DT', 'NN', 'NN', ',', 'IN', 'IN', 'DT', 'NN', 'NN', 'NNS', 'VBP', 'RB', 'RB', 'IN', 'NN', 'NNS', ',', 'IN', 'WDT', 'NN', 'DT', 'NN', ':', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', '.']",46
sentiment_analysis,40,173,"TD - LSTM - A also performs worse than our method , because it s two attentions , i.e. one for the text before the target and the other for the after , can not tackle some cases where more than one features being attended are at the same side of the target .","['TD', '-', 'LSTM', '-', 'A', 'also', 'performs', 'worse', 'than', 'our', 'method', ',', 'because', 'it', 's', 'two', 'attentions', ',', 'i.e.', 'one', 'for', 'the', 'text', 'before', 'the', 'target', 'and', 'the', 'other', 'for', 'the', 'after', ',', 'can', 'not', 'tackle', 'some', 'cases', 'where', 'more', 'than', 'one', 'features', 'being', 'attended', 'are', 'at', 'the', 'same', 'side', 'of', 'the', 'target', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NNP', ':', 'NN', 'RB', 'VBZ', 'JJR', 'IN', 'PRP$', 'NN', ',', 'IN', 'PRP', 'VBD', 'CD', 'NNS', ',', 'FW', 'CD', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'CC', 'DT', 'JJ', 'IN', 'DT', 'IN', ',', 'MD', 'RB', 'VB', 'DT', 'NNS', 'WRB', 'JJR', 'IN', 'CD', 'NNS', 'VBG', 'VBN', 'VBP', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",54
sentiment_analysis,40,175,"MemNet adopts multiple attentions in order to improve the attention results , given the assumption that the result of an attention at a later hop should be better than that at the beginning .","['MemNet', 'adopts', 'multiple', 'attentions', 'in', 'order', 'to', 'improve', 'the', 'attention', 'results', ',', 'given', 'the', 'assumption', 'that', 'the', 'result', 'of', 'an', 'attention', 'at', 'a', 'later', 'hop', 'should', 'be', 'better', 'than', 'that', 'at', 'the', 'beginning', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'JJ', 'NNS', 'IN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NNS', ',', 'VBN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'MD', 'VB', 'JJR', 'IN', 'DT', 'IN', 'DT', 'NN', '.']",34
sentiment_analysis,47,2,Left - Center - Right Separated Neural Network for Aspect - based Sentiment Analysis with Rotatory Attention,"['Left', '-', 'Center', '-', 'Right', 'Separated', 'Neural', 'Network', 'for', 'Aspect', '-', 'based', 'Sentiment', 'Analysis', 'with', 'Rotatory', 'Attention']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['NNP', ':', 'NNP', ':', 'RB', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', ':', 'VBN', 'JJ', 'NN', 'IN', 'NNP', 'NNP']",17
sentiment_analysis,47,16,"Aspect - based sentiment analysis is a fine - grained classification task in sentiment analysis , identifying sentiment polarity of a sentence expressed toward a target .","['Aspect', '-', 'based', 'sentiment', 'analysis', 'is', 'a', 'fine', '-', 'grained', 'classification', 'task', 'in', 'sentiment', 'analysis', ',', 'identifying', 'sentiment', 'polarity', 'of', 'a', 'sentence', 'expressed', 'toward', 'a', 'target', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'VBN', 'NN', 'NN', 'VBZ', 'DT', 'JJ', ':', 'VBN', 'NN', 'NN', 'IN', 'NN', 'NN', ',', 'VBG', 'JJ', 'NN', 'IN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', '.']",27
sentiment_analysis,47,17,"In the early studies , methods for the aspect - based sentiment classification task were similar as that used in standard sentiment classification task .","['In', 'the', 'early', 'studies', ',', 'methods', 'for', 'the', 'aspect', '-', 'based', 'sentiment', 'classification', 'task', 'were', 'similar', 'as', 'that', 'used', 'in', 'standard', 'sentiment', 'classification', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NNS', ',', 'NNS', 'IN', 'DT', 'NN', ':', 'VBN', 'NN', 'NN', 'NN', 'VBD', 'JJ', 'IN', 'DT', 'VBD', 'IN', 'JJ', 'NN', 'NN', 'NN', '.']",25
sentiment_analysis,47,38,"With the attempt to better address the two problems , in this paper we propose a left - center - right separated neural network with rotatory attention mechanism ( LCR - Rot ) .","['With', 'the', 'attempt', 'to', 'better', 'address', 'the', 'two', 'problems', ',', 'in', 'this', 'paper', 'we', 'propose', 'a', 'left', '-', 'center', '-', 'right', 'separated', 'neural', 'network', 'with', 'rotatory', 'attention', 'mechanism', '(', 'LCR', '-', 'Rot', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'TO', 'JJR', 'VB', 'DT', 'CD', 'NNS', ',', 'IN', 'DT', 'NN', 'PRP', 'VBP', 'DT', 'JJ', ':', 'NN', ':', 'RB', 'VBN', 'JJ', 'NN', 'IN', 'NN', 'NN', 'NN', '(', 'NNP', ':', 'NN', ')', '.']",34
sentiment_analysis,47,39,"Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a review ( left context , target phrase and right context ) .","['Specifically', ',', 'we', 'design', 'a', 'left', '-', 'center', '-', 'right', 'separated', 'LSTMs', 'that', 'contains', 'three', 'LSTMs', ',', 'i.e.', ',', 'left', '-', ',', 'center', '-', 'and', 'right', '-', 'LSTM', ',', 'respectively', 'modeling', 'the', 'three', 'parts', 'of', 'a', 'review', '(', 'left', 'context', ',', 'target', 'phrase', 'and', 'right', 'context', ')', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'JJ', ':', 'NN', ':', 'RB', 'VBN', 'NNP', 'IN', 'VBZ', 'CD', 'NNP', ',', 'NN', ',', 'VBD', ':', ',', 'NN', ':', 'CC', 'JJ', ':', 'NNP', ',', 'RB', 'VBG', 'DT', 'CD', 'NNS', 'IN', 'DT', 'NN', '(', 'JJ', 'NN', ',', 'NN', 'NN', 'CC', 'JJ', 'NN', ')', '.']",48
sentiment_analysis,47,40,"On this basis , we further propose a rotatory attention mechanism to take into account the interaction between targets and contexts to better represent targets and contexts .","['On', 'this', 'basis', ',', 'we', 'further', 'propose', 'a', 'rotatory', 'attention', 'mechanism', 'to', 'take', 'into', 'account', 'the', 'interaction', 'between', 'targets', 'and', 'contexts', 'to', 'better', 'represent', 'targets', 'and', 'contexts', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'VB', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'IN', 'NN', 'DT', 'NN', 'IN', 'NNS', 'CC', 'NN', 'TO', 'RBR', 'JJ', 'NNS', 'CC', 'NN', '.']",28
sentiment_analysis,47,41,The target2context attention is used to capture the most indicative sentiment words in left / right contexts .,"['The', 'target2context', 'attention', 'is', 'used', 'to', 'capture', 'the', 'most', 'indicative', 'sentiment', 'words', 'in', 'left', '/', 'right', 'contexts', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'RBS', 'JJ', 'NN', 'NNS', 'IN', 'JJ', 'NN', 'JJ', 'NN', '.']",18
sentiment_analysis,47,42,"Subsequently , the context2target attention is used to capture the most important word in the target .","['Subsequently', ',', 'the', 'context2target', 'attention', 'is', 'used', 'to', 'capture', 'the', 'most', 'important', 'word', 'in', 'the', 'target', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['RB', ',', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'RBS', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",17
sentiment_analysis,47,43,This leads to a two - side representation of the target : left - aware target and right - aware target .,"['This', 'leads', 'to', 'a', 'two', '-', 'side', 'representation', 'of', 'the', 'target', ':', 'left', '-', 'aware', 'target', 'and', 'right', '-', 'aware', 'target', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'TO', 'DT', 'CD', ':', 'NN', 'NN', 'IN', 'DT', 'NN', ':', 'JJ', ':', 'JJ', 'NN', 'CC', 'JJ', ':', 'JJ', 'NN', '.']",22
sentiment_analysis,47,44,"Finally , we concatenate the component representations as the final representation of the sentence and feed it into a softmax layer to predict the sentiment polarity .","['Finally', ',', 'we', 'concatenate', 'the', 'component', 'representations', 'as', 'the', 'final', 'representation', 'of', 'the', 'sentence', 'and', 'feed', 'it', 'into', 'a', 'softmax', 'layer', 'to', 'predict', 'the', 'sentiment', 'polarity', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'CC', 'VB', 'PRP', 'IN', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', '.']",27
sentiment_analysis,47,131,"In our work , the dimension of word embedding vectors and hidden state vectors is 300 .","['In', 'our', 'work', ',', 'the', 'dimension', 'of', 'word', 'embedding', 'vectors', 'and', 'hidden', 'state', 'vectors', 'is', '300', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'PRP$', 'NN', ',', 'DT', 'NN', 'IN', 'NN', 'VBG', 'NNS', 'CC', 'JJ', 'NN', 'NNS', 'VBZ', 'CD', '.']",17
sentiment_analysis,47,132,"We use GloVe 2 vectors with 300 dimensions to initialize the word embeddings , the same as .","['We', 'use', 'GloVe', '2', 'vectors', 'with', '300', 'dimensions', 'to', 'initialize', 'the', 'word', 'embeddings', ',', 'the', 'same', 'as', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', 'CD', 'NNS', 'IN', 'CD', 'NNS', 'TO', 'VB', 'DT', 'NN', 'NNS', ',', 'DT', 'JJ', 'IN', '.']",18
sentiment_analysis,47,133,"All out - ofvocabulary words and weight matrices are randomly initialized by a uniform distribution U ( - 0.1 , 0.1 ) , and all bias are set to zero .","['All', 'out', '-', 'ofvocabulary', 'words', 'and', 'weight', 'matrices', 'are', 'randomly', 'initialized', 'by', 'a', 'uniform', 'distribution', 'U', '(', '-', '0.1', ',', '0.1', ')', ',', 'and', 'all', 'bias', 'are', 'set', 'to', 'zero', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'RP', ':', 'JJ', 'NNS', 'CC', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NNP', '(', ':', 'CD', ',', 'CD', ')', ',', 'CC', 'DT', 'NNS', 'VBP', 'VBN', 'TO', 'CD', '.']",31
sentiment_analysis,47,134,Tensor Flow is used for implementing our neural network model .,"['Tensor', 'Flow', 'is', 'used', 'for', 'implementing', 'our', 'neural', 'network', 'model', '.']","['B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'VBZ', 'VBN', 'IN', 'VBG', 'PRP$', 'JJ', 'NN', 'NN', '.']",11
sentiment_analysis,47,135,"In model training , the learning rate is set to 0.1 , the weight for L 2 - norm regularization is set to 1 e - 5 , and dropout rate is set to 0.5 .","['In', 'model', 'training', ',', 'the', 'learning', 'rate', 'is', 'set', 'to', '0.1', ',', 'the', 'weight', 'for', 'L', '2', '-', 'norm', 'regularization', 'is', 'set', 'to', '1', 'e', '-', '5', ',', 'and', 'dropout', 'rate', 'is', 'set', 'to', '0.5', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'NN', 'NN', ',', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', ',', 'DT', 'NN', 'IN', 'NNP', 'CD', ':', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'SYM', ':', 'CD', ',', 'CC', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",36
sentiment_analysis,47,136,We train the model use stochastic gradient descent optimizer with momentum of 0.9 .,"['We', 'train', 'the', 'model', 'use', 'stochastic', 'gradient', 'descent', 'optimizer', 'with', 'momentum', 'of', '0.9', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'JJ', 'NN', 'NN', 'NN', 'IN', 'NN', 'IN', 'CD', '.']",14
sentiment_analysis,47,137,The paired t- test is used for the significance testing .,"['The', 'paired', 't-', 'test', 'is', 'used', 'for', 'the', 'significance', 'testing', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",11
sentiment_analysis,47,141,"Majority assigns the sentiment polarity that has the largest probability in the training set ; 2 . Simple SVM is a SVM classifier with simple features such as unigrams and bigrams ; 3 . Feature - enhanced SVM is a SVM classifier with a state - of - the - art feature template which contains n-gram features , parse features and lexicon features ; 4 . LSTM represents a standard LSTM for aspect - based sentiment classification task ; 5 . TD - LSTM adopts two LSTMs to model the left context with target and the right context with target respectively ; 74.30 66.50 66.50 TD- LSTM 75.60 68.10 70.80 AE - LSTM 76.60 68.90 - ATAE - LSTM 77.20 68.70 - GRNN- G3 79.55 * 71.47 * 70.09 * MemNet 79.98 * 70.33 * 70.52 * IAN 78.60 72.10 - LCR - Rot ( our approach ) 81.34 75.24 72.69 : The performance ( classification accuracy ) of different methods on three datasets .","['Majority', 'assigns', 'the', 'sentiment', 'polarity', 'that', 'has', 'the', 'largest', 'probability', 'in', 'the', 'training', 'set', ';', '2', '.', 'Simple', 'SVM', 'is', 'a', 'SVM', 'classifier', 'with', 'simple', 'features', 'such', 'as', 'unigrams', 'and', 'bigrams', ';', '3', '.', 'Feature', '-', 'enhanced', 'SVM', 'is', 'a', 'SVM', 'classifier', 'with', 'a', 'state', '-', 'of', '-', 'the', '-', 'art', 'feature', 'template', 'which', 'contains', 'n-gram', 'features', ',', 'parse', 'features', 'and', 'lexicon', 'features', ';', '4', '.', 'LSTM', 'represents', 'a', 'standard', 'LSTM', 'for', 'aspect', '-', 'based', 'sentiment', 'classification', 'task', ';', '5', '.', 'TD', '-', 'LSTM', 'adopts', 'two', 'LSTMs', 'to', 'model', 'the', 'left', 'context', 'with', 'target', 'and', 'the', 'right', 'context', 'with', 'target', 'respectively', ';', '74.30', '66.50', '66.50', 'TD-', 'LSTM', '75.60', '68.10', '70.80', 'AE', '-', 'LSTM', '76.60', '68.90', '-', 'ATAE', '-', 'LSTM', '77.20', '68.70', '-', 'GRNN-', 'G3', '79.55', '*', '71.47', '*', '70.09', '*', 'MemNet', '79.98', '*', '70.33', '*', '70.52', '*', 'IAN', '78.60', '72.10', '-', 'LCR', '-', 'Rot', '(', 'our', 'approach', ')', '81.34', '75.24', '72.69', ':', 'The', 'performance', '(', 'classification', 'accuracy', ')', 'of', 'different', 'methods', 'on', 'three', 'datasets', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NN', 'VBN', ':', 'CD', '.', 'JJ', 'NNP', 'VBZ', 'DT', 'NNP', 'NN', 'IN', 'JJ', 'NNS', 'JJ', 'IN', 'NNS', 'CC', 'NNS', ':', 'CD', '.', 'NNP', ':', 'VBD', 'NNP', 'VBZ', 'DT', 'NNP', 'NN', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'NN', 'WDT', 'VBZ', 'JJ', 'NNS', ',', 'NN', 'NNS', 'CC', 'NN', 'NNS', ':', 'CD', '.', 'NNP', 'VBZ', 'DT', 'JJ', 'NNP', 'IN', 'JJ', ':', 'VBN', 'NN', 'NN', 'NN', ':', 'CD', '.', 'NNP', ':', 'NNP', 'VBZ', 'CD', 'NNP', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'CC', 'DT', 'JJ', 'NN', 'IN', 'NN', 'RB', ':', 'CD', 'CD', 'CD', 'JJ', 'NNP', 'CD', 'CD', 'CD', 'NNP', ':', 'VBD', 'CD', 'CD', ':', 'NNP', ':', 'VBD', 'CD', 'CD', ':', 'JJ', 'NNP', 'CD', 'VBD', 'CD', 'JJ', 'CD', 'JJ', 'NNP', 'CD', 'VBD', 'CD', 'JJ', 'CD', 'JJ', 'NNP', 'CD', 'CD', ':', 'NNP', ':', 'NN', '(', 'PRP$', 'NN', ')', 'CD', 'CD', 'CD', ':', 'DT', 'NN', '(', 'NN', 'NN', ')', 'IN', 'JJ', 'NNS', 'IN', 'CD', 'NNS', '.']",165
sentiment_analysis,47,143,6 . AE - LSTM is an upgraded version of LSTM .,"['6', '.', 'AE', '-', 'LSTM', 'is', 'an', 'upgraded', 'version', 'of', 'LSTM', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['CD', '.', 'NNP', ':', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', '.']",12
sentiment_analysis,47,146,7 . ATAE - LSTM is developed based on AE - LSTM .,"['7', '.', 'ATAE', '-', 'LSTM', 'is', 'developed', 'based', 'on', 'AE', '-', 'LSTM', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['CD', '.', 'NNP', ':', 'NNP', 'VBZ', 'VBN', 'VBN', 'IN', 'NNP', ':', 'NN', '.']",13
sentiment_analysis,47,148,8 . GRNN - G3 adopts a Gated - RNN to represent sentence and use a three - way structure to leverage contexts .,"['8', '.', 'GRNN', '-', 'G3', 'adopts', 'a', 'Gated', '-', 'RNN', 'to', 'represent', 'sentence', 'and', 'use', 'a', 'three', '-', 'way', 'structure', 'to', 'leverage', 'contexts', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['CD', '.', 'NNP', ':', 'NNP', 'VBZ', 'DT', 'NNP', ':', 'NN', 'TO', 'VB', 'NN', 'CC', 'VB', 'DT', 'CD', ':', 'NN', 'NN', 'TO', 'VB', 'NN', '.']",24
sentiment_analysis,47,150,MemNet is a deep memory network which considers the content and position of target .,"['MemNet', 'is', 'a', 'deep', 'memory', 'network', 'which', 'considers', 'the', 'content', 'and', 'position', 'of', 'target', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'CC', 'NN', 'IN', 'NN', '.']",15
sentiment_analysis,47,152,"IAN interactively learns attentions in the contexts and targets , and generate the representations for targets and contexts separately .","['IAN', 'interactively', 'learns', 'attentions', 'in', 'the', 'contexts', 'and', 'targets', ',', 'and', 'generate', 'the', 'representations', 'for', 'targets', 'and', 'contexts', 'separately', '.']","['B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O']","['NNP', 'RB', 'VBZ', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NNS', ',', 'CC', 'VB', 'DT', 'NNS', 'IN', 'NNS', 'CC', 'NN', 'RB', '.']",20
sentiment_analysis,47,155,"We can find that the Majority method is the worst , which means the majority sentiment polarity occupies 53.50 % , 65.00 % and 50 % of all samples on the Restaurant , Laptop and Twitter testing datasets respectively .","['We', 'can', 'find', 'that', 'the', 'Majority', 'method', 'is', 'the', 'worst', ',', 'which', 'means', 'the', 'majority', 'sentiment', 'polarity', 'occupies', '53.50', '%', ',', '65.00', '%', 'and', '50', '%', 'of', 'all', 'samples', 'on', 'the', 'Restaurant', ',', 'Laptop', 'and', 'Twitter', 'testing', 'datasets', 'respectively', '.']","['O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'MD', 'VB', 'IN', 'DT', 'NNP', 'NN', 'VBZ', 'DT', 'JJS', ',', 'WDT', 'VBZ', 'DT', 'NN', 'NN', 'NN', 'VBZ', 'CD', 'NN', ',', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNP', ',', 'NNP', 'CC', 'NNP', 'VBG', 'NNS', 'RB', '.']",40
sentiment_analysis,47,156,The Simple SVM model performs better than Majority .,"['The', 'Simple', 'SVM', 'model', 'performs', 'better', 'than', 'Majority', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['DT', 'NNP', 'NNP', 'NN', 'NNS', 'RBR', 'IN', 'NNP', '.']",9
sentiment_analysis,47,157,"With the help of feature engineering , the Feature - enhanced SVM achieves much better results .","['With', 'the', 'help', 'of', 'feature', 'engineering', ',', 'the', 'Feature', '-', 'enhanced', 'SVM', 'achieves', 'much', 'better', 'results', '.']","['B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'IN', 'NN', 'NN', ',', 'DT', 'NNP', ':', 'VBD', 'NNP', 'NNS', 'RB', 'JJR', 'NNS', '.']",17
sentiment_analysis,47,159,Our model achieves significantly better results than feature - enhanced SVM .,"['Our', 'model', 'achieves', 'significantly', 'better', 'results', 'than', 'feature', '-', 'enhanced', 'SVM', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'RB', 'JJR', 'NNS', 'IN', 'JJ', ':', 'JJ', 'NNP', '.']",12
sentiment_analysis,47,161,"Among LSTM based neural networks described in this paper , the basic LSTM approach performs the worst .","['Among', 'LSTM', 'based', 'neural', 'networks', 'described', 'in', 'this', 'paper', ',', 'the', 'basic', 'LSTM', 'approach', 'performs', 'the', 'worst', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['IN', 'NNP', 'VBN', 'JJ', 'NNS', 'VBN', 'IN', 'DT', 'NN', ',', 'DT', 'JJ', 'NNP', 'NN', 'VBZ', 'DT', 'JJS', '.']",18
sentiment_analysis,47,162,TD - LSTM obtains an improvement of 1 - 2 % over LSTM when target signals are taken into consideration .,"['TD', '-', 'LSTM', 'obtains', 'an', 'improvement', 'of', '1', '-', '2', '%', 'over', 'LSTM', 'when', 'target', 'signals', 'are', 'taken', 'into', 'consideration', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['NNP', ':', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'CD', ':', 'CD', 'NN', 'IN', 'NNP', 'WRB', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'NN', '.']",21
sentiment_analysis,47,165,"MemNet achieves better results than other models on the Restaurant dataset , since it considers not only the contexts of targets but also the position of each context word related to the target .","['MemNet', 'achieves', 'better', 'results', 'than', 'other', 'models', 'on', 'the', 'Restaurant', 'dataset', ',', 'since', 'it', 'considers', 'not', 'only', 'the', 'contexts', 'of', 'targets', 'but', 'also', 'the', 'position', 'of', 'each', 'context', 'word', 'related', 'to', 'the', 'target', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'JJR', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NNP', 'NN', ',', 'IN', 'PRP', 'VBZ', 'RB', 'RB', 'DT', 'NN', 'IN', 'NNS', 'CC', 'RB', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBN', 'TO', 'DT', 'NN', '.']",34
sentiment_analysis,47,166,IAN considers separate representations of targets and obtains better result on the Laptop dataset .,"['IAN', 'considers', 'separate', 'representations', 'of', 'targets', 'and', 'obtains', 'better', 'result', 'on', 'the', 'Laptop', 'dataset', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', 'NNS', 'JJ', 'NNS', 'IN', 'NNS', 'CC', 'NNS', 'RBR', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",15
sentiment_analysis,47,167,GRNN - G3 achieves competitive results on all datasets because of its three - way structure and special gated - RNN model .,"['GRNN', '-', 'G3', 'achieves', 'competitive', 'results', 'on', 'all', 'datasets', 'because', 'of', 'its', 'three', '-', 'way', 'structure', 'and', 'special', 'gated', '-', 'RNN', 'model', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NNP', 'VBZ', 'JJ', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'IN', 'PRP$', 'CD', ':', 'NN', 'NN', 'CC', 'JJ', 'VBN', ':', 'JJ', 'NN', '.']",23
sentiment_analysis,47,168,"In the contrast , our LCR - Rot model achieves the best results on the all datasets among all models .","['In', 'the', 'contrast', ',', 'our', 'LCR', '-', 'Rot', 'model', 'achieves', 'the', 'best', 'results', 'on', 'the', 'all', 'datasets', 'among', 'all', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP$', 'NNP', ':', 'NNP', 'NN', 'VBZ', 'DT', 'JJS', 'NNS', 'IN', 'DT', 'DT', 'NNS', 'IN', 'DT', 'NNS', '.']",21
semantic_role_labeling,1,2,Linguistically - Informed Self - Attention for Semantic Role Labeling,"['Linguistically', '-', 'Informed', 'Self', '-', 'Attention', 'for', 'Semantic', 'Role', 'Labeling']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['RB', ':', 'VBN', 'NNP', ':', 'NN', 'IN', 'JJ', 'NNP', 'NNP']",10
semantic_role_labeling,1,4,Current state - of - the - art semantic role labeling ( SRL ) uses a deep neural network with no explicit linguistic features .,"['Current', 'state', '-', 'of', '-', 'the', '-', 'art', 'semantic', 'role', 'labeling', '(', 'SRL', ')', 'uses', 'a', 'deep', 'neural', 'network', 'with', 'no', 'explicit', 'linguistic', 'features', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NNS', '.']",25
semantic_role_labeling,1,5,"However , prior work has shown that gold syntax trees can dramatically improve SRL decoding , suggesting the possibility of increased accuracy from explicit modeling of syntax .","['However', ',', 'prior', 'work', 'has', 'shown', 'that', 'gold', 'syntax', 'trees', 'can', 'dramatically', 'improve', 'SRL', 'decoding', ',', 'suggesting', 'the', 'possibility', 'of', 'increased', 'accuracy', 'from', 'explicit', 'modeling', 'of', 'syntax', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', 'NN', 'NNS', 'MD', 'RB', 'VB', 'NNP', 'NN', ',', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'IN', 'NN', '.']",28
semantic_role_labeling,1,21,"In response , we propose linguistically - informed self - attention ( LISA ) : a model that combines multi-task learning with stacked layers of multi-head self - attention ; the model is trained to : ( 1 ) jointly predict parts of speech and predicates ; ( 2 ) perform parsing ; and ( 3 ) attend to syntactic parse parents , while ( 4 ) assigning semantic role labels .","['In', 'response', ',', 'we', 'propose', 'linguistically', '-', 'informed', 'self', '-', 'attention', '(', 'LISA', ')', ':', 'a', 'model', 'that', 'combines', 'multi-task', 'learning', 'with', 'stacked', 'layers', 'of', 'multi-head', 'self', '-', 'attention', ';', 'the', 'model', 'is', 'trained', 'to', ':', '(', '1', ')', 'jointly', 'predict', 'parts', 'of', 'speech', 'and', 'predicates', ';', '(', '2', ')', 'perform', 'parsing', ';', 'and', '(', '3', ')', 'attend', 'to', 'syntactic', 'parse', 'parents', ',', 'while', '(', '4', ')', 'assigning', 'semantic', 'role', 'labels', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'RB', ':', 'VBN', 'PRP', ':', 'NN', '(', 'NNP', ')', ':', 'DT', 'NN', 'WDT', 'VBZ', 'NN', 'VBG', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NN', ':', 'NN', ':', 'DT', 'NN', 'VBZ', 'VBN', 'TO', ':', '(', 'CD', ')', 'RB', 'JJ', 'NNS', 'IN', 'NN', 'CC', 'NNS', ':', '(', 'CD', ')', 'NN', 'NN', ':', 'CC', '(', 'CD', ')', 'NN', 'TO', 'JJ', 'NN', 'NNS', ',', 'IN', '(', 'CD', ')', 'VBG', 'JJ', 'NN', 'NNS', '.']",72
semantic_role_labeling,1,22,"Whereas prior work typically requires separate models to provide linguistic analysis , including most syntaxfree neural models which still rely on external predicate detection , our model is truly end - to - end : earlier layers are trained to predict prerequisite parts - of - speech and predicates , the latter of which are supplied to later layers for scoring .","['Whereas', 'prior', 'work', 'typically', 'requires', 'separate', 'models', 'to', 'provide', 'linguistic', 'analysis', ',', 'including', 'most', 'syntaxfree', 'neural', 'models', 'which', 'still', 'rely', 'on', 'external', 'predicate', 'detection', ',', 'our', 'model', 'is', 'truly', 'end', '-', 'to', '-', 'end', ':', 'earlier', 'layers', 'are', 'trained', 'to', 'predict', 'prerequisite', 'parts', '-', 'of', '-', 'speech', 'and', 'predicates', ',', 'the', 'latter', 'of', 'which', 'are', 'supplied', 'to', 'later', 'layers', 'for', 'scoring', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', 'JJ', 'NN', 'RB', 'VBZ', 'JJ', 'NNS', 'TO', 'VB', 'JJ', 'NN', ',', 'VBG', 'JJS', 'JJ', 'JJ', 'NNS', 'WDT', 'RB', 'VBP', 'IN', 'JJ', 'NN', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'JJ', 'JJ', ':', 'TO', ':', 'NN', ':', 'JJR', 'NNS', 'VBP', 'VBN', 'TO', 'VB', 'NN', 'NNS', ':', 'IN', ':', 'NN', 'CC', 'NNS', ',', 'DT', 'NN', 'IN', 'WDT', 'VBP', 'VBN', 'TO', 'RBR', 'NNS', 'IN', 'VBG', '.']",62
semantic_role_labeling,1,23,"Though prior work re-encodes each sentence to predict each desired task and again with respect to each predicate to perform SRL , we more efficiently encode each sentence only once , predict its predicates , part - of - speech tags and labeled syntactic parse , then predict the semantic roles for all predicates in the sentence in parallel .","['Though', 'prior', 'work', 're-encodes', 'each', 'sentence', 'to', 'predict', 'each', 'desired', 'task', 'and', 'again', 'with', 'respect', 'to', 'each', 'predicate', 'to', 'perform', 'SRL', ',', 'we', 'more', 'efficiently', 'encode', 'each', 'sentence', 'only', 'once', ',', 'predict', 'its', 'predicates', ',', 'part', '-', 'of', '-', 'speech', 'tags', 'and', 'labeled', 'syntactic', 'parse', ',', 'then', 'predict', 'the', 'semantic', 'roles', 'for', 'all', 'predicates', 'in', 'the', 'sentence', 'in', 'parallel', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', 'NN', 'NNS', 'DT', 'NN', 'TO', 'VB', 'DT', 'VBN', 'NN', 'CC', 'RB', 'IN', 'NN', 'TO', 'DT', 'NN', 'TO', 'VB', 'NNP', ',', 'PRP', 'RBR', 'RB', 'VBZ', 'DT', 'NN', 'RB', 'RB', ',', 'VBP', 'PRP$', 'NNS', ',', 'NN', ':', 'IN', ':', 'NN', 'NNS', 'CC', 'VBD', 'JJ', 'NN', ',', 'RB', 'VBP', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NN', '.']",60
semantic_role_labeling,1,128,"We train the model using Nadam ( Dozat , 2016 ) SGD combined with the learning rate schedule in .","['We', 'train', 'the', 'model', 'using', 'Nadam', '(', 'Dozat', ',', '2016', ')', 'SGD', 'combined', 'with', 'the', 'learning', 'rate', 'schedule', 'in', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'VBG', 'NNP', '(', 'NNP', ',', 'CD', ')', 'NNP', 'VBN', 'IN', 'DT', 'VBG', 'NN', 'NN', 'IN', '.']",20
semantic_role_labeling,1,129,"In addition to MTL , we regularize our model using dropout .","['In', 'addition', 'to', 'MTL', ',', 'we', 'regularize', 'our', 'model', 'using', 'dropout', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'NN', 'TO', 'NNP', ',', 'PRP', 'VBP', 'PRP$', 'NN', 'VBG', 'NN', '.']",12
semantic_role_labeling,1,130,We use gradient clipping to avoid exploding gradients .,"['We', 'use', 'gradient', 'clipping', 'to', 'avoid', 'exploding', 'gradients', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'TO', 'VB', 'VBG', 'NNS', '.']",9
semantic_role_labeling,1,151,"We present results on the CoNLL - 2005 shared task and the CoNLL - 2012 English subset of OntoNotes 5.0 , achieving state - of - the - art results for a single model with predicted predicates on both corpora .","['We', 'present', 'results', 'on', 'the', 'CoNLL', '-', '2005', 'shared', 'task', 'and', 'the', 'CoNLL', '-', '2012', 'English', 'subset', 'of', 'OntoNotes', '5.0', ',', 'achieving', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'for', 'a', 'single', 'model', 'with', 'predicted', 'predicates', 'on', 'both', 'corpora', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'JJ', 'NNS', 'IN', 'DT', 'NNP', ':', 'CD', 'VBD', 'NN', 'CC', 'DT', 'NNP', ':', 'CD', 'JJ', 'NN', 'IN', 'NNP', 'CD', ',', 'VBG', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NNS', '.']",41
semantic_role_labeling,1,159,"We demonstrate that our models benefit from injecting state - of - the - art predicted parses at test time ( + D&M ) by fixing the attention to parses predicted by Dozat and Manning ( 2017 ) , the winner of the 2017 CoNLL shared task which we re-train using ELMo embeddings .","['We', 'demonstrate', 'that', 'our', 'models', 'benefit', 'from', 'injecting', 'state', '-', 'of', '-', 'the', '-', 'art', 'predicted', 'parses', 'at', 'test', 'time', '(', '+', 'D&M', ')', 'by', 'fixing', 'the', 'attention', 'to', 'parses', 'predicted', 'by', 'Dozat', 'and', 'Manning', '(', '2017', ')', ',', 'the', 'winner', 'of', 'the', '2017', 'CoNLL', 'shared', 'task', 'which', 'we', 're-train', 'using', 'ELMo', 'embeddings', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'PRP$', 'NNS', 'VBP', 'IN', 'VBG', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'VBD', 'NNS', 'IN', 'NN', 'NN', '(', 'JJ', 'NNP', ')', 'IN', 'VBG', 'DT', 'NN', 'TO', 'NNS', 'VBN', 'IN', 'NNP', 'CC', 'NNP', '(', 'CD', ')', ',', 'DT', 'NN', 'IN', 'DT', 'CD', 'NNP', 'VBD', 'NN', 'WDT', 'PRP', 'VBP', 'VBG', 'NNP', 'NNS', '.']",54
semantic_role_labeling,1,165,"For models using GloVe embeddings , our syntax - free SA model already achieves a new state - of - the - art by jointly predicting predicates , POS and SRL .","['For', 'models', 'using', 'GloVe', 'embeddings', ',', 'our', 'syntax', '-', 'free', 'SA', 'model', 'already', 'achieves', 'a', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'by', 'jointly', 'predicting', 'predicates', ',', 'POS', 'and', 'SRL', '.']","['B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O']","['IN', 'NNS', 'VBG', 'NNP', 'NNS', ',', 'PRP$', 'JJ', ':', 'JJ', 'NNP', 'NN', 'RB', 'VBZ', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'RB', 'VBG', 'NNS', ',', 'NNP', 'CC', 'NNP', '.']",32
semantic_role_labeling,1,166,"LISA with it s own parses performs comparably to SA , but when supplied with D&M parses LISA out - performs the previous state - of - the - art by 2.5 F1 points .","['LISA', 'with', 'it', 's', 'own', 'parses', 'performs', 'comparably', 'to', 'SA', ',', 'but', 'when', 'supplied', 'with', 'D&M', 'parses', 'LISA', 'out', '-', 'performs', 'the', 'previous', 'state', '-', 'of', '-', 'the', '-', 'art', 'by', '2.5', 'F1', 'points', '.']","['B-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NN', 'IN', 'PRP', 'VBZ', 'JJ', 'NNS', 'VBZ', 'RB', 'TO', 'NNP', ',', 'CC', 'WRB', 'VBN', 'IN', 'NNP', 'NNS', 'NNP', 'IN', ':', 'NNS', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'CD', 'NNP', 'NNS', '.']",35
semantic_role_labeling,1,167,"On the out - ofdomain Brown test set , LISA also performs comparably to its syntax - free counterpart with its own parses , but with D&M parses LISA performs exceptionally well , more than 3.5 F1 points higher than He et al ..","['On', 'the', 'out', '-', 'ofdomain', 'Brown', 'test', 'set', ',', 'LISA', 'also', 'performs', 'comparably', 'to', 'its', 'syntax', '-', 'free', 'counterpart', 'with', 'its', 'own', 'parses', ',', 'but', 'with', 'D&M', 'parses', 'LISA', 'performs', 'exceptionally', 'well', ',', 'more', 'than', '3.5', 'F1', 'points', 'higher', 'than', 'He', 'et', 'al', '..']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'RP', ':', 'NN', 'NNP', 'NN', 'NN', ',', 'NNP', 'RB', 'VBZ', 'RB', 'TO', 'PRP$', 'JJ', ':', 'JJ', 'NN', 'IN', 'PRP$', 'JJ', 'NNS', ',', 'CC', 'IN', 'NNP', 'NNS', 'NNP', 'NNS', 'RB', 'RB', ',', 'JJR', 'IN', 'CD', 'NNP', 'NNS', 'JJR', 'IN', 'PRP', 'VBZ', 'JJ', 'NN']",44
semantic_role_labeling,1,168,Incorporating ELMo em-beddings improves all scores .,"['Incorporating', 'ELMo', 'em-beddings', 'improves', 'all', 'scores', '.']","['B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['VBG', 'NNP', 'JJ', 'NNS', 'DT', 'NNS', '.']",7
semantic_role_labeling,4,2,A Span Selection Model for Semantic Role Labeling,"['A', 'Span', 'Selection', 'Model', 'for', 'Semantic', 'Role', 'Labeling']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['DT', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'VBG']",8
semantic_role_labeling,4,4,We present a simple and accurate span - based model for semantic role labeling ( SRL ) .,"['We', 'present', 'a', 'simple', 'and', 'accurate', 'span', '-', 'based', 'model', 'for', 'semantic', 'role', 'labeling', '(', 'SRL', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'CC', 'NN', 'NN', ':', 'VBN', 'NN', 'IN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', '.']",18
semantic_role_labeling,4,11,"Given a sentence and a target predicate , SRL systems have to predict semantic arguments of the predicate .","['Given', 'a', 'sentence', 'and', 'a', 'target', 'predicate', ',', 'SRL', 'systems', 'have', 'to', 'predict', 'semantic', 'arguments', 'of', 'the', 'predicate', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'DT', 'NN', 'CC', 'DT', 'NN', 'NN', ',', 'NNP', 'NNS', 'VBP', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']",19
semantic_role_labeling,4,23,"To fill this gap , this paper presents a simple and accurate span - based model .","['To', 'fill', 'this', 'gap', ',', 'this', 'paper', 'presents', 'a', 'simple', 'and', 'accurate', 'span', '-', 'based', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'CC', 'JJ', 'NN', ':', 'VBN', 'NN', '.']",17
semantic_role_labeling,4,24,"Inspired by recent span - based models in syntactic parsing and coreference resolution , our model directly scores all possible labeled spans based on span representations induced from neural networks .","['Inspired', 'by', 'recent', 'span', '-', 'based', 'models', 'in', 'syntactic', 'parsing', 'and', 'coreference', 'resolution', ',', 'our', 'model', 'directly', 'scores', 'all', 'possible', 'labeled', 'spans', 'based', 'on', 'span', 'representations', 'induced', 'from', 'neural', 'networks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['VBN', 'IN', 'JJ', 'NN', ':', 'VBN', 'NNS', 'IN', 'JJ', 'NN', 'CC', 'NN', 'NN', ',', 'PRP$', 'NN', 'RB', 'VBZ', 'DT', 'JJ', 'VBN', 'NNS', 'VBN', 'IN', 'JJ', 'NNS', 'VBN', 'IN', 'JJ', 'NNS', '.']",31
semantic_role_labeling,4,25,"At decoding time , we greedily select higher scoring labeled spans .","['At', 'decoding', 'time', ',', 'we', 'greedily', 'select', 'higher', 'scoring', 'labeled', 'spans', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBG', 'NN', ',', 'PRP', 'RB', 'VBP', 'JJR', 'VBG', 'VBN', 'NNS', '.']",12
semantic_role_labeling,4,26,The model parameters are learned by optimizing loglikelihood of correct labeled spans .,"['The', 'model', 'parameters', 'are', 'learned', 'by', 'optimizing', 'loglikelihood', 'of', 'correct', 'labeled', 'spans', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'VBG', 'NN', 'IN', 'NN', 'VBN', 'NNS', '.']",13
semantic_role_labeling,4,157,"For comparison , as a model based on BIO tagging approaches , we use the BiLSTM - CRF model proposed by .","['For', 'comparison', ',', 'as', 'a', 'model', 'based', 'on', 'BIO', 'tagging', 'approaches', ',', 'we', 'use', 'the', 'BiLSTM', '-', 'CRF', 'model', 'proposed', 'by', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['IN', 'NN', ',', 'IN', 'DT', 'NN', 'VBN', 'IN', 'NNP', 'VBG', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NNP', ':', 'NNP', 'NN', 'VBN', 'IN', '.']",22
semantic_role_labeling,4,160,"As the base function f base , we use 4 BiLSTM layers with 300 dimensional hidden units .","['As', 'the', 'base', 'function', 'f', 'base', ',', 'we', 'use', '4', 'BiLSTM', 'layers', 'with', '300', 'dimensional', 'hidden', 'units', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'NN', 'NN', 'NN', ',', 'PRP', 'VBP', 'CD', 'NNP', 'NNS', 'IN', 'CD', 'JJ', 'JJ', 'NNS', '.']",18
semantic_role_labeling,4,161,"To optimize the model parameters , we use Adam .","['To', 'optimize', 'the', 'model', 'parameters', ',', 'we', 'use', 'Adam', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O']","['TO', 'VB', 'DT', 'NN', 'NNS', ',', 'PRP', 'VBP', 'NNP', '.']",10
semantic_role_labeling,4,165,"To validate the model performance , we use two types of word embeddings .","['To', 'validate', 'the', 'model', 'performance', ',', 'we', 'use', 'two', 'types', 'of', 'word', 'embeddings', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', 'NN', ',', 'PRP', 'VBP', 'CD', 'NNS', 'IN', 'NN', 'NNS', '.']",14
semantic_role_labeling,4,166,"Typical word embeddings , SENNA 6 ( Collobert et al. , 2011 ) Contextualized word embeddings , ELMo 7 SENNA and ELMo can be regarded as different types of embeddings in terms of the context sensitivity .","['Typical', 'word', 'embeddings', ',', 'SENNA', '6', '(', 'Collobert', 'et', 'al.', ',', '2011', ')', 'Contextualized', 'word', 'embeddings', ',', 'ELMo', '7', 'SENNA', 'and', 'ELMo', 'can', 'be', 'regarded', 'as', 'different', 'types', 'of', 'embeddings', 'in', 'terms', 'of', 'the', 'context', 'sensitivity', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NNS', ',', 'NNP', 'CD', '(', 'NNP', 'NNP', 'NN', ',', 'CD', ')', 'VBN', 'NN', 'NNS', ',', 'NNP', 'CD', 'NNP', 'CC', 'NNP', 'MD', 'VB', 'VBN', 'IN', 'JJ', 'NNS', 'IN', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",37
semantic_role_labeling,4,171,These embeddings are fixed during training .,"['These', 'embeddings', 'are', 'fixed', 'during', 'training', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'NN', '.']",7
semantic_role_labeling,4,173,"As the objective function , we use the crossentropy L ? in Eq. 3 with L2 weight decay ,","['As', 'the', 'objective', 'function', ',', 'we', 'use', 'the', 'crossentropy', 'L', '?', 'in', 'Eq.', '3', 'with', 'L2', 'weight', 'decay', ',']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NNP', '.', 'IN', 'NNP', 'CD', 'IN', 'NNP', 'NN', 'NN', ',']",19
semantic_role_labeling,4,177,We report averaged scores across five different runs of the model training .,"['We', 'report', 'averaged', 'scores', 'across', 'five', 'different', 'runs', 'of', 'the', 'model', 'training', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O']","['PRP', 'VBP', 'VBD', 'NNS', 'IN', 'CD', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",13
semantic_role_labeling,4,179,"Overall , our span - based ensemble model using ELMo achieved the best F1 scores , 87.4 F1 and 87.0 F1 on the CoNLL - 2005 and CoNLL - 2012 datasets , respectively .","['Overall', ',', 'our', 'span', '-', 'based', 'ensemble', 'model', 'using', 'ELMo', 'achieved', 'the', 'best', 'F1', 'scores', ',', '87.4', 'F1', 'and', '87.0', 'F1', 'on', 'the', 'CoNLL', '-', '2005', 'and', 'CoNLL', '-', '2012', 'datasets', ',', 'respectively', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['JJ', ',', 'PRP$', 'SYM', ':', 'VBN', 'JJ', 'NN', 'VBG', 'NNP', 'VBD', 'DT', 'JJS', 'NN', 'NNS', ',', 'CD', 'NNP', 'CC', 'CD', 'NNP', 'IN', 'DT', 'NNP', ':', 'CD', 'CC', 'NNP', ':', 'CD', 'NNS', ',', 'RB', '.']",34
semantic_role_labeling,4,180,"In comparison with the CRF - based single model , our span - based single model consistently yielded better F 1 scores regardless of the word embeddings , SENNA and ELMO .","['In', 'comparison', 'with', 'the', 'CRF', '-', 'based', 'single', 'model', ',', 'our', 'span', '-', 'based', 'single', 'model', 'consistently', 'yielded', 'better', 'F', '1', 'scores', 'regardless', 'of', 'the', 'word', 'embeddings', ',', 'SENNA', 'and', 'ELMO', '.']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'IN', 'DT', 'NNP', ':', 'VBN', 'JJ', 'NN', ',', 'PRP$', 'SYM', ':', 'VBN', 'JJ', 'NN', 'RB', 'VBN', 'RBR', 'NNP', 'CD', 'NNS', 'RB', 'IN', 'DT', 'NN', 'NNS', ',', 'NNP', 'CC', 'NNP', '.']",32
semantic_role_labeling,4,183,Our single and ensemble models using ELMO achieved the best F 1 scores on all the test sets except the Brown test set .,"['Our', 'single', 'and', 'ensemble', 'models', 'using', 'ELMO', 'achieved', 'the', 'best', 'F', '1', 'scores', 'on', 'all', 'the', 'test', 'sets', 'except', 'the', 'Brown', 'test', 'set', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'JJ', 'CC', 'JJ', 'NNS', 'VBG', 'NNP', 'VBD', 'DT', 'JJS', 'JJ', 'CD', 'NNS', 'IN', 'PDT', 'DT', 'NN', 'VBZ', 'IN', 'DT', 'NNP', 'NN', 'NN', '.']",24
semantic_role_labeling,2,2,Deep Semantic Role Labeling : What Works and What 's Next,"['Deep', 'Semantic', 'Role', 'Labeling', ':', 'What', 'Works', 'and', 'What', ""'s"", 'Next']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', 'NNP', ':', 'WP', 'VBZ', 'CC', 'WP', 'VBZ', 'JJ']",11
semantic_role_labeling,2,4,"We introduce a new deep learning model for semantic role labeling ( SRL ) that significantly improves the state of the art , along with detailed analyses to reveal its strengths and limitations .","['We', 'introduce', 'a', 'new', 'deep', 'learning', 'model', 'for', 'semantic', 'role', 'labeling', '(', 'SRL', ')', 'that', 'significantly', 'improves', 'the', 'state', 'of', 'the', 'art', ',', 'along', 'with', 'detailed', 'analyses', 'to', 'reveal', 'its', 'strengths', 'and', 'limitations', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'WDT', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'IN', 'IN', 'JJ', 'NNS', 'TO', 'VB', 'PRP$', 'NNS', 'CC', 'NNS', '.']",34
semantic_role_labeling,2,10,Recently breakthroughs involving end - to - end deep models for SRL without syntactic input seem to overturn the long - held belief that syntactic parsing is a prerequisite for this task .,"['Recently', 'breakthroughs', 'involving', 'end', '-', 'to', '-', 'end', 'deep', 'models', 'for', 'SRL', 'without', 'syntactic', 'input', 'seem', 'to', 'overturn', 'the', 'long', '-', 'held', 'belief', 'that', 'syntactic', 'parsing', 'is', 'a', 'prerequisite', 'for', 'this', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'VBZ', 'VBG', 'VB', ':', 'TO', ':', 'VB', 'JJ', 'NNS', 'IN', 'NNP', 'IN', 'JJ', 'NN', 'VBP', 'TO', 'VB', 'DT', 'JJ', ':', 'NN', 'NN', 'IN', 'JJ', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",33
semantic_role_labeling,2,11,"In this paper , we show that this result can be pushed further using deep highway bidirectional LSTMs with constrained decoding , again significantly moving the state of the art ( another 2 points on CoNLL 2005 ) .","['In', 'this', 'paper', ',', 'we', 'show', 'that', 'this', 'result', 'can', 'be', 'pushed', 'further', 'using', 'deep', 'highway', 'bidirectional', 'LSTMs', 'with', 'constrained', 'decoding', ',', 'again', 'significantly', 'moving', 'the', 'state', 'of', 'the', 'art', '(', 'another', '2', 'points', 'on', 'CoNLL', '2005', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'MD', 'VB', 'VBN', 'RBR', 'VBG', 'JJ', 'NN', 'JJ', 'NNP', 'IN', 'JJ', 'NN', ',', 'RB', 'RB', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '(', 'DT', 'CD', 'NNS', 'IN', 'NNP', 'CD', ')', '.']",39
semantic_role_labeling,2,14,"Fol - lowing , we treat SRL as a BIO tagging problem and use deep bidirectional LSTMs .","['Fol', '-', 'lowing', ',', 'we', 'treat', 'SRL', 'as', 'a', 'BIO', 'tagging', 'problem', 'and', 'use', 'deep', 'bidirectional', 'LSTMs', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', ',', 'PRP', 'VBP', 'NNP', 'IN', 'DT', 'NNP', 'NN', 'NN', 'CC', 'NN', 'JJ', 'JJ', 'NNP', '.']",18
semantic_role_labeling,2,15,"However , we differ by ( 1 ) simplifying the input and output layers , ( 2 ) introducing highway connections , ( 3 ) using recurrent dropout , ( 4 ) decoding with BIOconstraints , and ( 5 ) ensembling with a product of experts .","['However', ',', 'we', 'differ', 'by', '(', '1', ')', 'simplifying', 'the', 'input', 'and', 'output', 'layers', ',', '(', '2', ')', 'introducing', 'highway', 'connections', ',', '(', '3', ')', 'using', 'recurrent', 'dropout', ',', '(', '4', ')', 'decoding', 'with', 'BIOconstraints', ',', 'and', '(', '5', ')', 'ensembling', 'with', 'a', 'product', 'of', 'experts', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'IN', '(', 'CD', ')', 'VBG', 'DT', 'NN', 'CC', 'NN', 'NNS', ',', '(', 'CD', ')', 'VBG', 'NN', 'NNS', ',', '(', 'CD', ')', 'VBG', 'NN', 'NN', ',', '(', 'CD', ')', 'VBG', 'IN', 'NNP', ',', 'CC', '(', 'CD', ')', 'VBG', 'IN', 'DT', 'NN', 'IN', 'NNS', '.']",47
semantic_role_labeling,2,87,"Our network consists of 8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ) with 300 dimensional hidden units , and a softmax layer for predicting the output distribution .","['Our', 'network', 'consists', 'of', '8', 'BiLSTM', 'layers', '(', '4', 'forward', 'LSTMs', 'and', '4', 'reversed', 'LSTMs', ')', 'with', '300', 'dimensional', 'hidden', 'units', ',', 'and', 'a', 'softmax', 'layer', 'for', 'predicting', 'the', 'output', 'distribution', '.']","['O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'IN', 'CD', 'NNP', 'NNS', '(', 'CD', 'RB', 'NNP', 'CC', 'CD', 'JJ', 'NNP', ')', 'IN', 'CD', 'JJ', 'JJ', 'NNS', ',', 'CC', 'DT', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'NN', '.']",32
semantic_role_labeling,2,89,"All the weight matrices in BiL - STMs are initialized with random orthonormal matrices as described in. ,","['All', 'the', 'weight', 'matrices', 'in', 'BiL', '-', 'STMs', 'are', 'initialized', 'with', 'random', 'orthonormal', 'matrices', 'as', 'described', 'in.', ',']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PDT', 'DT', 'NN', 'NNS', 'IN', 'NNP', ':', 'NN', 'VBP', 'VBN', 'IN', 'JJ', 'JJ', 'NNS', 'IN', 'JJ', 'NN', ',']",18
semantic_role_labeling,2,91,All tokens are lower - cased and initialized with 100 - dimensional GloVe embeddings pre-trained on 6B tokens and updated during training .,"['All', 'tokens', 'are', 'lower', '-', 'cased', 'and', 'initialized', 'with', '100', '-', 'dimensional', 'GloVe', 'embeddings', 'pre-trained', 'on', '6B', 'tokens', 'and', 'updated', 'during', 'training', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NNS', 'VBP', 'JJR', ':', 'VBN', 'CC', 'VBN', 'IN', 'CD', ':', 'JJ', 'NNP', 'VBZ', 'JJ', 'IN', 'CD', 'NNS', 'CC', 'JJ', 'IN', 'NN', '.']",23
semantic_role_labeling,2,92,Tokens that are not covered by GloVe are replaced with a randomly initialized UNK embedding .,"['Tokens', 'that', 'are', 'not', 'covered', 'by', 'GloVe', 'are', 'replaced', 'with', 'a', 'randomly', 'initialized', 'UNK', 'embedding', '.']","['B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'WDT', 'VBP', 'RB', 'VBN', 'IN', 'NNP', 'VBP', 'VBN', 'IN', 'DT', 'RB', 'VBN', 'NNP', 'NN', '.']",16
semantic_role_labeling,2,93,"Training We use Adadelta ( Zeiler , 2012 ) with = 1e ?6 and ? = 0.95 and mini-batches of size 80 .","['Training', 'We', 'use', 'Adadelta', '(', 'Zeiler', ',', '2012', ')', 'with', '=', '1e', '?6', 'and', '?', '=', '0.95', 'and', 'mini-batches', 'of', 'size', '80', '.']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'B-n', 'O']","['VBG', 'PRP', 'VBP', 'NNP', '(', 'NNP', ',', 'CD', ')', 'IN', '$', 'CD', 'NN', 'CC', '.', '$', 'CD', 'CC', 'NNS', 'IN', 'NN', 'CD', '.']",23
semantic_role_labeling,2,94,We set RNN - dropout probability to 0.1 and clip gradients with norm larger than 1 .,"['We', 'set', 'RNN', '-', 'dropout', 'probability', 'to', '0.1', 'and', 'clip', 'gradients', 'with', 'norm', 'larger', 'than', '1', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'NNP', ':', 'NN', 'NN', 'TO', 'CD', 'CC', 'JJ', 'NNS', 'IN', 'NN', 'JJR', 'IN', 'CD', '.']",17
semantic_role_labeling,2,95,All the models are trained for 500 epochs with early stopping based on development results .,"['All', 'the', 'models', 'are', 'trained', 'for', '500', 'epochs', 'with', 'early', 'stopping', 'based', 'on', 'development', 'results', '.']","['O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PDT', 'DT', 'NNS', 'VBP', 'VBN', 'IN', 'CD', 'NNS', 'IN', 'JJ', 'VBG', 'VBN', 'IN', 'NN', 'NNS', '.']",16
semantic_role_labeling,2,103,Our ensemble ( PoE ) has an absolute improvement of 2.1 F1 on both CoNLL 2005 and CoNLL 2012 over the previous state of the art .,"['Our', 'ensemble', '(', 'PoE', ')', 'has', 'an', 'absolute', 'improvement', 'of', '2.1', 'F1', 'on', 'both', 'CoNLL', '2005', 'and', 'CoNLL', '2012', 'over', 'the', 'previous', 'state', 'of', 'the', 'art', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'JJ', '(', 'NNP', ')', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NNP', 'IN', 'DT', 'NNP', 'CD', 'CC', 'NNP', 'CD', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",27
semantic_role_labeling,2,104,Our single model also achieves more than a 0.4 improvement on both datasets .,"['Our', 'single', 'model', 'also', 'achieves', 'more', 'than', 'a', '0.4', 'improvement', 'on', 'both', 'datasets', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP$', 'JJ', 'NN', 'RB', 'VBZ', 'JJR', 'IN', 'DT', 'CD', 'NN', 'IN', 'DT', 'NNS', '.']",14
semantic_role_labeling,2,105,"In comparison with the best reported results , our percentage of completely correct predicates improves by 5.9 points .","['In', 'comparison', 'with', 'the', 'best', 'reported', 'results', ',', 'our', 'percentage', 'of', 'completely', 'correct', 'predicates', 'improves', 'by', '5.9', 'points', '.']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'NN', 'IN', 'DT', 'JJS', 'JJ', 'NNS', ',', 'PRP$', 'NN', 'IN', 'RB', 'JJ', 'NNS', 'NNS', 'IN', 'CD', 'NNS', '.']",19
semantic_role_labeling,2,106,"While the continuing trend of improving SRL without syntax seems to suggest that neural end - to - end systems no longer needs parsers , our analysis in Section 4.4 will show that accurate syntactic information can improve these deep models .","['While', 'the', 'continuing', 'trend', 'of', 'improving', 'SRL', 'without', 'syntax', 'seems', 'to', 'suggest', 'that', 'neural', 'end', '-', 'to', '-', 'end', 'systems', 'no', 'longer', 'needs', 'parsers', ',', 'our', 'analysis', 'in', 'Section', '4.4', 'will', 'show', 'that', 'accurate', 'syntactic', 'information', 'can', 'improve', 'these', 'deep', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'VBG', 'NN', 'IN', 'VBG', 'NNP', 'IN', 'NN', 'VBZ', 'TO', 'VB', 'IN', 'JJ', 'NN', ':', 'TO', ':', 'NN', 'NNS', 'DT', 'JJR', 'VBZ', 'NNS', ',', 'PRP$', 'NN', 'IN', 'NNP', 'CD', 'MD', 'VB', 'IN', 'JJ', 'JJ', 'NN', 'MD', 'VB', 'DT', 'JJ', 'NNS', '.']",42
semantic_role_labeling,2,108,"Without dropout , the model overfits at around 300 epochs at 78 F1 .","['Without', 'dropout', ',', 'the', 'model', 'overfits', 'at', 'around', '300', 'epochs', 'at', '78', 'F1', '.']","['B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'DT', 'NN', 'VBZ', 'IN', 'IN', 'CD', 'NNS', 'IN', 'CD', 'NNP', '.']",14
semantic_role_labeling,2,109,"Orthonormal parameter initialization is surprisingly important - without this , the model achieves only 65 F1 within the first 50 epochs .","['Orthonormal', 'parameter', 'initialization', 'is', 'surprisingly', 'important', '-', 'without', 'this', ',', 'the', 'model', 'achieves', 'only', '65', 'F1', 'within', 'the', 'first', '50', 'epochs', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['JJ', 'NN', 'NN', 'VBZ', 'RB', 'JJ', ':', 'IN', 'DT', ',', 'DT', 'NN', 'VBZ', 'RB', 'CD', 'NNP', 'IN', 'DT', 'JJ', 'CD', 'NN', '.']",22
semantic_role_labeling,2,110,All 8 layer ablations suffer a loss of more than 1.7 in absolute F 1 compared to the full model .,"['All', '8', 'layer', 'ablations', 'suffer', 'a', 'loss', 'of', 'more', 'than', '1.7', 'in', 'absolute', 'F', '1', 'compared', 'to', 'the', 'full', 'model', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'CD', 'NN', 'NNS', 'VBP', 'DT', 'NN', 'IN', 'JJR', 'IN', 'CD', 'IN', 'JJ', 'NNP', 'CD', 'VBN', 'TO', 'DT', 'JJ', 'NN', '.']",21
semantic_role_labeling,3,2,Deep Semantic Role Labeling with Self - Attention,"['Deep', 'Semantic', 'Role', 'Labeling', 'with', 'Self', '-', 'Attention']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', 'VBG', 'IN', 'NNP', ':', 'NN']",8
semantic_role_labeling,3,4,Semantic Role Labeling ( SRL ) is believed to be a crucial step towards natural language understanding and has been widely studied .,"['Semantic', 'Role', 'Labeling', '(', 'SRL', ')', 'is', 'believed', 'to', 'be', 'a', 'crucial', 'step', 'towards', 'natural', 'language', 'understanding', 'and', 'has', 'been', 'widely', 'studied', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', '(', 'NNP', ')', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NNS', 'JJ', 'NN', 'NN', 'CC', 'VBZ', 'VBN', 'RB', 'VBN', '.']",23
semantic_role_labeling,3,7,"In this paper , we present a simple and effective architecture for SRL which aims to address these problems .","['In', 'this', 'paper', ',', 'we', 'present', 'a', 'simple', 'and', 'effective', 'architecture', 'for', 'SRL', 'which', 'aims', 'to', 'address', 'these', 'problems', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'CC', 'JJ', 'NN', 'IN', 'NNP', 'WDT', 'VBZ', 'TO', 'VB', 'DT', 'NNS', '.']",20
semantic_role_labeling,3,12,"Semantic Role Labeling is a shallow semantic parsing task , whose goal is to determine essentially "" who did what to whom "" , "" when "" and "" where "" .","['Semantic', 'Role', 'Labeling', 'is', 'a', 'shallow', 'semantic', 'parsing', 'task', ',', 'whose', 'goal', 'is', 'to', 'determine', 'essentially', '""', 'who', 'did', 'what', 'to', 'whom', '""', ',', '""', 'when', '""', 'and', '""', 'where', '""', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'NN', ',', 'WP$', 'NN', 'VBZ', 'TO', 'VB', 'RB', 'NNP', 'WP', 'VBD', 'WP', 'TO', 'WP', 'NNP', ',', 'NN', 'WRB', 'NN', 'CC', 'VB', 'WRB', 'NN', '.']",32
semantic_role_labeling,3,29,"To address these problems above , we present a deep attentional neural network ( DEEPATT ) for the task of SRL 1 .","['To', 'address', 'these', 'problems', 'above', ',', 'we', 'present', 'a', 'deep', 'attentional', 'neural', 'network', '(', 'DEEPATT', ')', 'for', 'the', 'task', 'of', 'SRL', '1', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'B-n', 'O', 'O']","['TO', 'VB', 'DT', 'NNS', 'IN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'JJ', 'NN', '(', 'NNP', ')', 'IN', 'DT', 'NN', 'IN', 'NNP', 'CD', '.']",23
semantic_role_labeling,3,30,Our models rely on the self - attention mechanism which directly draws the global dependencies of the inputs .,"['Our', 'models', 'rely', 'on', 'the', 'self', '-', 'attention', 'mechanism', 'which', 'directly', 'draws', 'the', 'global', 'dependencies', 'of', 'the', 'inputs', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['PRP$', 'NNS', 'RB', 'IN', 'DT', 'NN', ':', 'NN', 'NN', 'WDT', 'RB', 'VBZ', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NNS', '.']",19
semantic_role_labeling,3,31,"In contrast to RNNs , a major advantage of self - attention is that it conducts direct connections between two arbitrary tokens in a sentence .","['In', 'contrast', 'to', 'RNNs', ',', 'a', 'major', 'advantage', 'of', 'self', '-', 'attention', 'is', 'that', 'it', 'conducts', 'direct', 'connections', 'between', 'two', 'arbitrary', 'tokens', 'in', 'a', 'sentence', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['IN', 'NN', 'TO', 'NNP', ',', 'DT', 'JJ', 'NN', 'IN', 'NN', ':', 'NN', 'VBZ', 'IN', 'PRP', 'VBZ', 'JJ', 'NNS', 'IN', 'CD', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']",26
semantic_role_labeling,3,32,"Therefore , distant elements can interact with each other by shorter paths ( O ( 1 ) v.s. O ( n ) ) , which allows unimpeded information flow through the network .","['Therefore', ',', 'distant', 'elements', 'can', 'interact', 'with', 'each', 'other', 'by', 'shorter', 'paths', '(', 'O', '(', '1', ')', 'v.s.', 'O', '(', 'n', ')', ')', ',', 'which', 'allows', 'unimpeded', 'information', 'flow', 'through', 'the', 'network', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['RB', ',', 'JJ', 'NNS', 'MD', 'VB', 'IN', 'DT', 'JJ', 'IN', 'JJR', 'NNS', '(', 'NNP', '(', 'CD', ')', 'NN', 'NNP', '(', 'RB', ')', ')', ',', 'WDT', 'VBZ', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', '.']",33
semantic_role_labeling,3,33,"Self - attention also provides a more flexible way to select , represent and synthesize the information of the inputs and is complementary to RNN based models .","['Self', '-', 'attention', 'also', 'provides', 'a', 'more', 'flexible', 'way', 'to', 'select', ',', 'represent', 'and', 'synthesize', 'the', 'information', 'of', 'the', 'inputs', 'and', 'is', 'complementary', 'to', 'RNN', 'based', 'models', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', 'RB', 'VBZ', 'DT', 'RBR', 'JJ', 'NN', 'TO', 'VB', ',', 'NN', 'CC', 'VB', 'DT', 'NN', 'IN', 'DT', 'NNS', 'CC', 'VBZ', 'JJ', 'TO', 'NNP', 'VBN', 'NNS', '.']",28
semantic_role_labeling,3,34,"Along with self - attention , DEEP - ATT comes with three variants which uses recurrent ( RNN ) , convolutional ( CNN ) and feed - forward ( FFN ) neural network to further enhance the representations .","['Along', 'with', 'self', '-', 'attention', ',', 'DEEP', '-', 'ATT', 'comes', 'with', 'three', 'variants', 'which', 'uses', 'recurrent', '(', 'RNN', ')', ',', 'convolutional', '(', 'CNN', ')', 'and', 'feed', '-', 'forward', '(', 'FFN', ')', 'neural', 'network', 'to', 'further', 'enhance', 'the', 'representations', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['IN', 'IN', 'NN', ':', 'NN', ',', 'NNP', ':', 'NNP', 'VBZ', 'IN', 'CD', 'NNS', 'WDT', 'VBZ', 'NN', '(', 'NNP', ')', ',', 'JJ', '(', 'NNP', ')', 'CC', '$', ':', 'NN', '(', 'NNP', ')', 'JJ', 'NN', 'TO', 'JJ', 'VB', 'DT', 'NNS', '.']",39
semantic_role_labeling,3,157,We initialize the weights of all sub-layers as random orthogonal matrices .,"['We', 'initialize', 'the', 'weights', 'of', 'all', 'sub-layers', 'as', 'random', 'orthogonal', 'matrices', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'JJ', 'JJ', 'NNS', '.']",12
semantic_role_labeling,3,158,"For other parameters , we initialize them by sampling each element from a Gaussian distribution with mean 0 and variance 1 ? d .","['For', 'other', 'parameters', ',', 'we', 'initialize', 'them', 'by', 'sampling', 'each', 'element', 'from', 'a', 'Gaussian', 'distribution', 'with', 'mean', '0', 'and', 'variance', '1', '?', 'd', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'PRP', 'IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'CD', 'CC', 'NN', 'CD', '.', 'NN', '.']",24
semantic_role_labeling,3,159,The embedding layer can be initialized randomly or using pre-trained word embeddings .,"['The', 'embedding', 'layer', 'can', 'be', 'initialized', 'randomly', 'or', 'using', 'pre-trained', 'word', 'embeddings', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'VBG', 'NN', 'MD', 'VB', 'VBN', 'RB', 'CC', 'VBG', 'JJ', 'NN', 'NNS', '.']",13
semantic_role_labeling,3,164,The dimension of word embeddings and predicate mask embeddings is set to 100 and the number of hidden layers is set to 10 .,"['The', 'dimension', 'of', 'word', 'embeddings', 'and', 'predicate', 'mask', 'embeddings', 'is', 'set', 'to', '100', 'and', 'the', 'number', 'of', 'hidden', 'layers', 'is', 'set', 'to', '10', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NN', 'NNS', 'CC', 'VB', 'NN', 'NNS', 'VBZ', 'VBN', 'TO', 'CD', 'CC', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'VBZ', 'VBN', 'TO', 'CD', '.']",24
semantic_role_labeling,3,165,We set the number of hidden units d to 200 .,"['We', 'set', 'the', 'number', 'of', 'hidden', 'units', 'd', 'to', '200', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'VBP', 'TO', 'CD', '.']",11
semantic_role_labeling,3,166,The number of heads h is set to 8 .,"['The', 'number', 'of', 'heads', 'h', 'is', 'set', 'to', '8', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NNS', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",10
semantic_role_labeling,3,168,Dropout layers are added before residual connections with a keep probability of 0.8 .,"['Dropout', 'layers', 'are', 'added', 'before', 'residual', 'connections', 'with', 'a', 'keep', 'probability', 'of', '0.8', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'NNS', 'VBP', 'VBN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', '.']",14
semantic_role_labeling,3,169,"Dropout is also applied before the attention softmax layer and the feed - froward ReLU hidden layer , and the keep probabilities are set to 0.9 .","['Dropout', 'is', 'also', 'applied', 'before', 'the', 'attention', 'softmax', 'layer', 'and', 'the', 'feed', '-', 'froward', 'ReLU', 'hidden', 'layer', ',', 'and', 'the', 'keep', 'probabilities', 'are', 'set', 'to', '0.9', '.']","['B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['NN', 'VBZ', 'RB', 'VBN', 'IN', 'DT', 'NN', 'NN', 'NN', 'CC', 'DT', 'NN', ':', 'NN', 'NNP', 'NN', 'NN', ',', 'CC', 'DT', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'CD', '.']",27
semantic_role_labeling,3,170,We also employ label smoothing technique with a smoothing value of 0.1 during training .,"['We', 'also', 'employ', 'label', 'smoothing', 'technique', 'with', 'a', 'smoothing', 'value', 'of', '0.1', 'during', 'training', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'RB', 'VBP', 'JJ', 'NN', 'NN', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', 'IN', 'NN', '.']",15
semantic_role_labeling,3,171,Learning Parameter optimization is performed using stochastic gradient descent .,"['Learning', 'Parameter', 'optimization', 'is', 'performed', 'using', 'stochastic', 'gradient', 'descent', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['VBG', 'NNP', 'NN', 'VBZ', 'VBN', 'VBG', 'JJ', 'NN', 'NN', '.']",10
semantic_role_labeling,3,172,We adopt Adadelta ) ( = 10 6 and ? = 0.95 ) as the optimizer .,"['We', 'adopt', 'Adadelta', ')', '(', '=', '10', '6', 'and', '?', '=', '0.95', ')', 'as', 'the', 'optimizer', '.']","['O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'NNP', ')', '(', '$', 'CD', 'CD', 'CC', '.', '$', 'CD', ')', 'IN', 'DT', 'NN', '.']",17
semantic_role_labeling,3,173,"To avoid exploding gradients problem , we clip the norm of gradients with a predefined threshold 1.0 .","['To', 'avoid', 'exploding', 'gradients', 'problem', ',', 'we', 'clip', 'the', 'norm', 'of', 'gradients', 'with', 'a', 'predefined', 'threshold', '1.0', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'VBG', 'NNS', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'CD', '.']",18
semantic_role_labeling,3,174,Each SGD contains a mini-batch of approximately 4096 tokens for the CoNLL - 2005 dataset and 8192 tokens for the CoNLL - 2012 dataset .,"['Each', 'SGD', 'contains', 'a', 'mini-batch', 'of', 'approximately', '4096', 'tokens', 'for', 'the', 'CoNLL', '-', '2005', 'dataset', 'and', '8192', 'tokens', 'for', 'the', 'CoNLL', '-', '2012', 'dataset', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'RB', 'CD', 'NNS', 'IN', 'DT', 'NNP', ':', 'CD', 'NN', 'CC', 'CD', 'NNS', 'IN', 'DT', 'NNP', ':', 'CD', 'NN', '.']",25
semantic_role_labeling,3,175,The learning rate is initialized to 1.0 .,"['The', 'learning', 'rate', 'is', 'initialized', 'to', '1.0', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",8
semantic_role_labeling,3,176,"After training 400 k steps , we halve the learning rate every 100 K steps .","['After', 'training', '400', 'k', 'steps', ',', 'we', 'halve', 'the', 'learning', 'rate', 'every', '100', 'K', 'steps', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'VBG', 'CD', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'DT', 'CD', 'NNP', 'NNS', '.']",16
semantic_role_labeling,3,177,We train all models for 600 K steps .,"['We', 'train', 'all', 'models', 'for', '600', 'K', 'steps', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNS', 'IN', 'CD', 'NNP', 'NNS', '.']",9
semantic_role_labeling,3,178,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about two days to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .","['For', 'DEEP', '-', 'ATT', 'with', 'FFN', 'sub', '-', 'layers', ',', 'the', 'whole', 'training', 'stage', 'takes', 'about', 'two', 'days', 'to', 'finish', 'on', 'a', 'single', 'Titan', 'X', 'GPU', ',', 'which', 'is', '2.5', 'times', 'faster', 'than', 'the', 'previous', 'approach', ')', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ':', 'NN', 'IN', 'NNP', 'SYM', ':', 'NNS', ',', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'IN', 'CD', 'NNS', 'TO', 'VB', 'IN', 'DT', 'JJ', 'NNP', 'NNP', 'NNP', ',', 'WDT', 'VBZ', 'CD', 'NNS', 'RBR', 'IN', 'DT', 'JJ', 'NN', ')', '.']",38
semantic_role_labeling,3,180,"In Remarkably , we get 74.1 F 1 score on the out - of - domain dataset , which outperforms the previous state - of - the - art system by 2.0 F 1 score .","['In', 'Remarkably', ',', 'we', 'get', '74.1', 'F', '1', 'score', 'on', 'the', 'out', '-', 'of', '-', 'domain', 'dataset', ',', 'which', 'outperforms', 'the', 'previous', 'state', '-', 'of', '-', 'the', '-', 'art', 'system', 'by', '2.0', 'F', '1', 'score', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'RB', ',', 'PRP', 'VBP', 'CD', 'NNP', 'CD', 'NN', 'IN', 'DT', 'RP', ':', 'IN', ':', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'CD', 'NNP', 'CD', 'NN', '.']",36
semantic_role_labeling,3,182,"When ensembling 5 models with FFN nonlinear sub - layers , our approach achieves an F 1 score of 84.6 and 83.9 on the two datasets respectively , which has an absolute improvement of 1.4 and 0.5 over the previous state - of - the - art .","['When', 'ensembling', '5', 'models', 'with', 'FFN', 'nonlinear', 'sub', '-', 'layers', ',', 'our', 'approach', 'achieves', 'an', 'F', '1', 'score', 'of', '84.6', 'and', '83.9', 'on', 'the', 'two', 'datasets', 'respectively', ',', 'which', 'has', 'an', 'absolute', 'improvement', 'of', '1.4', 'and', '0.5', 'over', 'the', 'previous', 'state', '-', 'of', '-', 'the', '-', 'art', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['WRB', 'VBG', 'CD', 'NNS', 'IN', 'NNP', 'VBP', 'JJ', ':', 'NNS', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'NNP', 'CD', 'NN', 'IN', 'CD', 'CC', 'CD', 'IN', 'DT', 'CD', 'NNS', 'RB', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'CD', 'CC', 'CD', 'IN', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', '.']",48
semantic_role_labeling,3,183,These results are consistent with our intuition that the self - attention layers is helpful to capture structural information and long distance dependencies .,"['These', 'results', 'are', 'consistent', 'with', 'our', 'intuition', 'that', 'the', 'self', '-', 'attention', 'layers', 'is', 'helpful', 'to', 'capture', 'structural', 'information', 'and', 'long', 'distance', 'dependencies', '.']","['O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'JJ', 'IN', 'PRP$', 'NN', 'IN', 'DT', 'NN', ':', 'NN', 'NNS', 'VBZ', 'JJ', 'TO', 'VB', 'JJ', 'NN', 'CC', 'JJ', 'NN', 'NNS', '.']",24
semantic_role_labeling,0,2,Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling,"['Jointly', 'Predicting', 'Predicates', 'and', 'Arguments', 'in', 'Neural', 'Semantic', 'Role', 'Labeling']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']","['RB', 'VBG', 'NNS', 'CC', 'NNS', 'IN', 'NNP', 'NNP', 'NNP', 'VBG']",10
semantic_role_labeling,0,10,"Semantic role labeling ( SRL ) captures predicateargument relations , such as "" who did what to whom . ""","['Semantic', 'role', 'labeling', '(', 'SRL', ')', 'captures', 'predicateargument', 'relations', ',', 'such', 'as', '""', 'who', 'did', 'what', 'to', 'whom', '.', '""']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'JJ', 'NNS', ',', 'JJ', 'IN', 'NNS', 'WP', 'VBD', 'WP', 'TO', 'WP', '.', 'NN']",20
semantic_role_labeling,0,11,"Recent high - performing SRL models are BIO - taggers , labeling argument spans for a single predicate at a time ( as shown in .","['Recent', 'high', '-', 'performing', 'SRL', 'models', 'are', 'BIO', '-', 'taggers', ',', 'labeling', 'argument', 'spans', 'for', 'a', 'single', 'predicate', 'at', 'a', 'time', '(', 'as', 'shown', 'in', '.']","['O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'JJ', ':', 'VBG', 'NNP', 'NNS', 'VBP', 'NNP', ':', 'NNS', ',', 'VBG', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '(', 'IN', 'VBN', 'IN', '.']",26
semantic_role_labeling,0,13,We propose an end - to - end approach for predicting all the predicates and their argument spans in one forward pass .,"['We', 'propose', 'an', 'end', '-', 'to', '-', 'end', 'approach', 'for', 'predicting', 'all', 'the', 'predicates', 'and', 'their', 'argument', 'spans', 'in', 'one', 'forward', 'pass', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', ':', 'TO', ':', 'VB', 'NN', 'IN', 'VBG', 'PDT', 'DT', 'NNS', 'CC', 'PRP$', 'NN', 'NNS', 'IN', 'CD', 'NN', 'NN', '.']",23
semantic_role_labeling,0,14,"Our model builds on a recent coreference resolution model , by making central use of learned , contextualized span representations .","['Our', 'model', 'builds', 'on', 'a', 'recent', 'coreference', 'resolution', 'model', ',', 'by', 'making', 'central', 'use', 'of', 'learned', ',', 'contextualized', 'span', 'representations', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', ',', 'IN', 'VBG', 'JJ', 'NN', 'IN', 'VBN', ',', 'VBN', 'NN', 'NNS', '.']",21
semantic_role_labeling,0,15,We use these representations to predict SRL graphs directly over text spans .,"['We', 'use', 'these', 'representations', 'to', 'predict', 'SRL', 'graphs', 'directly', 'over', 'text', 'spans', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNS', 'TO', 'VB', 'NNP', 'NN', 'RB', 'IN', 'JJ', 'NNS', '.']",13
semantic_role_labeling,0,16,"Each edge is identified by independently predicting which role , if any , holds between every possible pair of text spans , while using aggressive beam 1 Code and models : https://github.com/luheng/lsgn pruning for efficiency .","['Each', 'edge', 'is', 'identified', 'by', 'independently', 'predicting', 'which', 'role', ',', 'if', 'any', ',', 'holds', 'between', 'every', 'possible', 'pair', 'of', 'text', 'spans', ',', 'while', 'using', 'aggressive', 'beam', '1', 'Code', 'and', 'models', ':', 'https://github.com/luheng/lsgn', 'pruning', 'for', 'efficiency', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'IN', 'RB', 'VBG', 'WDT', 'NN', ',', 'IN', 'DT', ',', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', ',', 'IN', 'VBG', 'JJ', 'NN', 'CD', 'NNP', 'CC', 'NNS', ':', 'NN', 'VBG', 'IN', 'NN', '.']",36
semantic_role_labeling,0,17,The final graph is simply the union of predicted SRL roles ( edges ) and their associated text spans ( nodes ) .,"['The', 'final', 'graph', 'is', 'simply', 'the', 'union', 'of', 'predicted', 'SRL', 'roles', '(', 'edges', ')', 'and', 'their', 'associated', 'text', 'spans', '(', 'nodes', ')', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'RB', 'DT', 'NN', 'IN', 'JJ', 'NNP', 'NNS', '(', 'NNS', ')', 'CC', 'PRP$', 'VBN', 'NN', 'NNS', '(', 'NNS', ')', '.']",23
semantic_role_labeling,0,19,"The span representations also generalize the token - level representations in BIObased models , letting the model dynamically decide which spans and roles to include , without using previously standard syntactic features .","['The', 'span', 'representations', 'also', 'generalize', 'the', 'token', '-', 'level', 'representations', 'in', 'BIObased', 'models', ',', 'letting', 'the', 'model', 'dynamically', 'decide', 'which', 'spans', 'and', 'roles', 'to', 'include', ',', 'without', 'using', 'previously', 'standard', 'syntactic', 'features', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NNS', 'RB', 'VBP', 'DT', 'JJ', ':', 'NN', 'NNS', 'IN', 'JJ', 'NNS', ',', 'VBG', 'DT', 'NN', 'RB', 'VB', 'WDT', 'NNS', 'CC', 'NNS', 'TO', 'VB', ',', 'IN', 'VBG', 'RB', 'VBN', 'JJ', 'NNS', '.']",33
semantic_role_labeling,0,20,"To the best of our knowledge , this is the first span - based SRL model that does not assume that predicates are given .","['To', 'the', 'best', 'of', 'our', 'knowledge', ',', 'this', 'is', 'the', 'first', 'span', '-', 'based', 'SRL', 'model', 'that', 'does', 'not', 'assume', 'that', 'predicates', 'are', 'given', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'DT', 'JJS', 'IN', 'PRP$', 'NN', ',', 'DT', 'VBZ', 'DT', 'JJ', 'NN', ':', 'VBN', 'NNP', 'NN', 'WDT', 'VBZ', 'RB', 'VB', 'IN', 'NNS', 'VBP', 'VBN', '.']",25
semantic_role_labeling,0,71,"As shown in , 2 our joint model outperforms the previous best pipeline system by an F1 difference of anywhere between 1.3 and 6.0 in every setting .","['As', 'shown', 'in', ',', '2', 'our', 'joint', 'model', 'outperforms', 'the', 'previous', 'best', 'pipeline', 'system', 'by', 'an', 'F1', 'difference', 'of', 'anywhere', 'between', '1.3', 'and', '6.0', 'in', 'every', 'setting', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['IN', 'VBN', 'IN', ',', 'CD', 'PRP$', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'JJS', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'RB', 'IN', 'CD', 'CC', 'CD', 'IN', 'DT', 'NN', '.']",28
semantic_role_labeling,0,73,"On all datasets , our model is able to predict over 40 % of the sentences completely correctly .","['On', 'all', 'datasets', ',', 'our', 'model', 'is', 'able', 'to', 'predict', 'over', '40', '%', 'of', 'the', 'sentences', 'completely', 'correctly', '.']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'PRP$', 'NN', 'VBZ', 'JJ', 'TO', 'VB', 'IN', 'CD', 'NN', 'IN', 'DT', 'NNS', 'RB', 'RB', '.']",19
topic_models,0,2,Learning document embeddings along with their uncertainties,"['Learning', 'document', 'embeddings', 'along', 'with', 'their', 'uncertainties']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['VBG', 'JJ', 'NNS', 'IN', 'IN', 'PRP$', 'NNS']",7
topic_models,0,8,We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings .,"['We', 'also', 'present', 'a', 'generative', 'Gaussian', 'linear', 'classifier', 'for', 'topic', 'identification', 'that', 'exploits', 'the', 'uncertainty', 'in', 'document', 'embeddings', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBD', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NNS', '.']",19
topic_models,0,18,We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings .,"['We', 'also', 'present', 'a', 'generative', 'Gaussian', 'linear', 'classifier', 'for', 'topic', 'identification', 'that', 'exploits', 'the', 'uncertainty', 'in', 'document', 'embeddings', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBD', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NNS', '.']",19
topic_models,0,24,"L EARNING word and document embeddings have proven to be useful in wide range of information retrieval , speech and natural language processing applications -.","['L', 'EARNING', 'word', 'and', 'document', 'embeddings', 'have', 'proven', 'to', 'be', 'useful', 'in', 'wide', 'range', 'of', 'information', 'retrieval', ',', 'speech', 'and', 'natural', 'language', 'processing', 'applications', '-.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'NN', 'CC', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'VB', 'JJ', 'IN', 'JJ', 'NN', 'IN', 'NN', 'NN', ',', 'NN', 'CC', 'JJ', 'NN', 'NN', 'NNS', 'VBP']",25
topic_models,0,38,"In this paper , we present Bayesian subspace multinomial model ( Bayesian SMM ) as a generative model for bag - ofwords representation of documents .","['In', 'this', 'paper', ',', 'we', 'present', 'Bayesian', 'subspace', 'multinomial', 'model', '(', 'Bayesian', 'SMM', ')', 'as', 'a', 'generative', 'model', 'for', 'bag', '-', 'ofwords', 'representation', 'of', 'documents', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'JJ', 'NN', '(', 'JJ', 'NNP', ')', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', ':', 'NNS', 'NN', 'IN', 'NNS', '.']",26
topic_models,0,39,"We show that our model can learn to represent each document in the form of a Gaussian distribution , thereby encoding the uncertainty in its covariance .","['We', 'show', 'that', 'our', 'model', 'can', 'learn', 'to', 'represent', 'each', 'document', 'in', 'the', 'form', 'of', 'a', 'Gaussian', 'distribution', ',', 'thereby', 'encoding', 'the', 'uncertainty', 'in', 'its', 'covariance', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'PRP$', 'NN', 'MD', 'VB', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'RB', 'VBG', 'DT', 'NN', 'IN', 'PRP$', 'NN', '.']",27
topic_models,0,40,"Further , we propose a generative Gaussian classifier that exploits this uncertainty for topic identification ( ID ) .","['Further', ',', 'we', 'propose', 'a', 'generative', 'Gaussian', 'classifier', 'that', 'exploits', 'this', 'uncertainty', 'for', 'topic', 'identification', '(', 'ID', ')', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNP', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'NN', 'NN', '(', 'NNP', ')', '.']",19
topic_models,0,41,"The proposed VB framework can be extended in a straightforward way for subspace n-gram model , that can model n-gram distribution of words in sentences .","['The', 'proposed', 'VB', 'framework', 'can', 'be', 'extended', 'in', 'a', 'straightforward', 'way', 'for', 'subspace', 'n-gram', 'model', ',', 'that', 'can', 'model', 'n-gram', 'distribution', 'of', 'words', 'in', 'sentences', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBN', 'NNP', 'NN', 'MD', 'VB', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'JJ', 'NN', ',', 'WDT', 'MD', 'VB', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'NNS', '.']",26
topic_models,0,274,"The embedding dimension was chosen from K = { 100 , . . . , 800 } , and regularization weight from ? = { 0.0001 , . . . , 10.0 }.","['The', 'embedding', 'dimension', 'was', 'chosen', 'from', 'K', '=', '{', '100', ',', '.', '.', '.', ',', '800', '}', ',', 'and', 'regularization', 'weight', 'from', '?', '=', '{', '0.0001', ',', '.', '.', '.', ',', '10.0', '}.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['DT', 'VBG', 'NN', 'VBD', 'VBN', 'IN', 'NNP', 'NNP', '(', 'CD', ',', '.', '.', '.', ',', 'CD', ')', ',', 'CC', 'NN', 'NN', 'IN', '.', 'NNP', '(', 'CD', ',', '.', '.', '.', ',', 'CD', 'NN']",33
topic_models,0,285,"1 ) NVDM : Since NVDM and our proposed Bayesian SMM share similarities , we chose to extract the embeddings from NVDM and use them for training linear classifiers .","['1', ')', 'NVDM', ':', 'Since', 'NVDM', 'and', 'our', 'proposed', 'Bayesian', 'SMM', 'share', 'similarities', ',', 'we', 'chose', 'to', 'extract', 'the', 'embeddings', 'from', 'NVDM', 'and', 'use', 'them', 'for', 'training', 'linear', 'classifiers', '.']","['O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['CD', ')', 'NN', ':', 'IN', 'NNP', 'CC', 'PRP$', 'VBN', 'NNP', 'NNP', 'NN', 'NNS', ',', 'PRP', 'VBD', 'TO', 'VB', 'DT', 'NNS', 'IN', 'NNP', 'CC', 'VB', 'PRP', 'IN', 'VBG', 'JJ', 'NNS', '.']",30
topic_models,0,293,"2 ) SMM : Our second baseline system is non-Bayesian SMM with 1 regularization over the rows in T matrix , i.e. , 1 SMM .","['2', ')', 'SMM', ':', 'Our', 'second', 'baseline', 'system', 'is', 'non-Bayesian', 'SMM', 'with', '1', 'regularization', 'over', 'the', 'rows', 'in', 'T', 'matrix', ',', 'i.e.', ',', '1', 'SMM', '.']","['O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['CD', ')', 'NN', ':', 'PRP$', 'JJ', 'NN', 'NN', 'VBZ', 'JJ', 'NNP', 'IN', 'CD', 'NN', 'IN', 'DT', 'NNS', 'IN', 'NNP', 'NN', ',', 'NN', ',', 'CD', 'NNP', '.']",26
topic_models,0,299,3 ) ULMFiT : The third baseline system is the universal language model fine - tuned for classification ( ULMFiT ) .,"['3', ')', 'ULMFiT', ':', 'The', 'third', 'baseline', 'system', 'is', 'the', 'universal', 'language', 'model', 'fine', '-', 'tuned', 'for', 'classification', '(', 'ULMFiT', ')', '.']","['O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O']","['LS', ')', 'NN', ':', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'JJ', ':', 'VBN', 'IN', 'NN', '(', 'NNP', ')', '.']",22
topic_models,0,306,4 ) TF - IDF :,"['4', ')', 'TF', '-', 'IDF', ':']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['CD', ')', 'NNP', ':', 'NN', ':']",6
topic_models,0,307,"The fourth baseline system is a standard term frequency - inverse document frequency ( TF - IDF ) based document representation , followed by multi-class logistic regression ( LR ) .","['The', 'fourth', 'baseline', 'system', 'is', 'a', 'standard', 'term', 'frequency', '-', 'inverse', 'document', 'frequency', '(', 'TF', '-', 'IDF', ')', 'based', 'document', 'representation', ',', 'followed', 'by', 'multi-class', 'logistic', 'regression', '(', 'LR', ')', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', ':', 'NN', 'NN', 'NN', '(', 'NNP', ':', 'NNP', ')', 'VBN', 'NN', 'NN', ',', 'VBN', 'IN', 'NN', 'JJ', 'NN', '(', 'NNP', ')', '.']",31
topic_models,0,362,"presents the classification results on Fisher speech corpora with manual and automatic transcriptions , where the first two rows are the results from earlier published works .","['presents', 'the', 'classification', 'results', 'on', 'Fisher', 'speech', 'corpora', 'with', 'manual', 'and', 'automatic', 'transcriptions', ',', 'where', 'the', 'first', 'two', 'rows', 'are', 'the', 'results', 'from', 'earlier', 'published', 'works', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNS', 'DT', 'NN', 'NNS', 'IN', 'NNP', 'NN', 'NN', 'IN', 'JJ', 'CC', 'JJ', 'NNS', ',', 'WRB', 'DT', 'JJ', 'CD', 'NNS', 'VBP', 'DT', 'NNS', 'IN', 'JJR', 'VBN', 'NNS', '.']",27
topic_models,0,367,"We can see that our proposed systems achieve consistently better accuracies ; notably , GLCU which exploits the uncertainty in document embeddings has much lower cross - entropy than its counterpart , GLC .","['We', 'can', 'see', 'that', 'our', 'proposed', 'systems', 'achieve', 'consistently', 'better', 'accuracies', ';', 'notably', ',', 'GLCU', 'which', 'exploits', 'the', 'uncertainty', 'in', 'document', 'embeddings', 'has', 'much', 'lower', 'cross', '-', 'entropy', 'than', 'its', 'counterpart', ',', 'GLC', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'B-n', 'O']","['PRP', 'MD', 'VB', 'IN', 'PRP$', 'VBN', 'NNS', 'VBP', 'RB', 'JJR', 'NNS', ':', 'RB', ',', 'NNP', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'NN', 'NNS', 'VBZ', 'RB', 'JJR', 'NN', ':', 'NN', 'IN', 'PRP$', 'NN', ',', 'NNP', '.']",34
topic_models,0,370,presents classification results on 20 Newsgroups dataset .,"['presents', 'classification', 'results', 'on', '20', 'Newsgroups', 'dataset', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNS', 'NN', 'NNS', 'IN', 'CD', 'NNP', 'NN', '.']",8
topic_models,0,376,"We see that the topic ID systems based on Bayesian SMM and logistic regression is better than all the other models , except for the purely discriminative CNN model .","['We', 'see', 'that', 'the', 'topic', 'ID', 'systems', 'based', 'on', 'Bayesian', 'SMM', 'and', 'logistic', 'regression', 'is', 'better', 'than', 'all', 'the', 'other', 'models', ',', 'except', 'for', 'the', 'purely', 'discriminative', 'CNN', 'model', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NN', 'NNP', 'NNS', 'VBN', 'IN', 'JJ', 'NNP', 'CC', 'JJ', 'NN', 'VBZ', 'JJR', 'IN', 'PDT', 'DT', 'JJ', 'NNS', ',', 'IN', 'IN', 'DT', 'RB', 'JJ', 'NNP', 'NN', '.']",30
topic_models,0,377,"We can also see that all the topic ID systems based on Bayesian SMM are consistently better than variational auto encoder inspired NVDM , and ( non-Bayesian ) SMM .","['We', 'can', 'also', 'see', 'that', 'all', 'the', 'topic', 'ID', 'systems', 'based', 'on', 'Bayesian', 'SMM', 'are', 'consistently', 'better', 'than', 'variational', 'auto', 'encoder', 'inspired', 'NVDM', ',', 'and', '(', 'non-Bayesian', ')', 'SMM', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'MD', 'RB', 'VB', 'IN', 'PDT', 'DT', 'NN', 'NNP', 'NNS', 'VBN', 'IN', 'JJ', 'NNP', 'VBP', 'RB', 'JJR', 'IN', 'JJ', 'NN', 'NN', 'VBD', 'NNP', ',', 'CC', '(', 'JJ', ')', 'NNP', '.']",30
negation_scope_resolution,0,2,NegBERT : A Transfer Learning Approach for Negation Detection and Scope Resolution,"['NegBERT', ':', 'A', 'Transfer', 'Learning', 'Approach', 'for', 'Negation', 'Detection', 'and', 'Scope', 'Resolution']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NN', ':', 'DT', 'NN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'CC', 'NNP', 'NNP']",12
negation_scope_resolution,0,34,"Motivated by the success of transfer learning , we apply BERT to negation detection and scope resolution .","['Motivated', 'by', 'the', 'success', 'of', 'transfer', 'learning', ',', 'we', 'apply', 'BERT', 'to', 'negation', 'detection', 'and', 'scope', 'resolution', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', ',', 'PRP', 'VBP', 'NNP', 'TO', 'VB', 'NN', 'CC', 'NN', 'NN', '.']",18
negation_scope_resolution,0,35,"We explore the set of design choices involved , and experiment on all 3 public datasets available : the BioScope Corpus ( Abstracts and Full Papers ) , the Sherlock Dataset and the SFU Review Corpus .","['We', 'explore', 'the', 'set', 'of', 'design', 'choices', 'involved', ',', 'and', 'experiment', 'on', 'all', '3', 'public', 'datasets', 'available', ':', 'the', 'BioScope', 'Corpus', '(', 'Abstracts', 'and', 'Full', 'Papers', ')', ',', 'the', 'Sherlock', 'Dataset', 'and', 'the', 'SFU', 'Review', 'Corpus', '.']","['O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'NNS', 'VBN', ',', 'CC', 'NN', 'IN', 'DT', 'CD', 'JJ', 'NNS', 'JJ', ':', 'DT', 'NNP', 'NNP', '(', 'NNPS', 'CC', 'NNP', 'NNP', ')', ',', 'DT', 'NNP', 'NNP', 'CC', 'DT', 'NNP', 'NNP', 'NNP', '.']",37
negation_scope_resolution,0,189,We use Google 's BERT as the base model to generate contextual embeddings for the sentence .,"['We', 'use', 'Google', ""'s"", 'BERT', 'as', 'the', 'base', 'model', 'to', 'generate', 'contextual', 'embeddings', 'for', 'the', 'sentence', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'NNP', 'POS', 'NNP', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']",17
negation_scope_resolution,0,190,The input to the BERT model is a sequence of tokenized and encoded tokens of a sentence .,"['The', 'input', 'to', 'the', 'BERT', 'model', 'is', 'a', 'sequence', 'of', 'tokenized', 'and', 'encoded', 'tokens', 'of', 'a', 'sentence', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['DT', 'NN', 'TO', 'DT', 'NNP', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'CC', 'VBD', 'NNS', 'IN', 'DT', 'NN', '.']",18
negation_scope_resolution,0,191,"We then use a vector of dimension R H x N_C to compute scores per token , for the classification task at hand .","['We', 'then', 'use', 'a', 'vector', 'of', 'dimension', 'R', 'H', 'x', 'N_C', 'to', 'compute', 'scores', 'per', 'token', ',', 'for', 'the', 'classification', 'task', 'at', 'hand', '.']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', 'IN', 'NN', 'NNP', 'NNP', 'NNP', 'NNP', 'TO', 'VB', 'NNS', 'IN', 'NN', ',', 'IN', 'DT', 'NN', 'NN', 'IN', 'NN', '.']",24
negation_scope_resolution,0,192,"BERT outputs a vector of size R H per token of the input , which we feed to a common classification layer of dimen-sion R Hx5 for cue detection and R Hx2 for scope resolution .","['BERT', 'outputs', 'a', 'vector', 'of', 'size', 'R', 'H', 'per', 'token', 'of', 'the', 'input', ',', 'which', 'we', 'feed', 'to', 'a', 'common', 'classification', 'layer', 'of', 'dimen-sion', 'R', 'Hx5', 'for', 'cue', 'detection', 'and', 'R', 'Hx2', 'for', 'scope', 'resolution', '.']","['B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'IN', 'NN', 'NNP', 'NNP', 'IN', 'NN', 'IN', 'DT', 'NN', ',', 'WDT', 'PRP', 'VBP', 'TO', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NN', 'NNP', 'NNP', 'IN', 'NN', 'NN', 'CC', 'NNP', 'NNP', 'IN', 'NN', 'NN', '.']",36
negation_scope_resolution,0,193,"We use early stopping on dev data for 6 epochs as tolerance and F 1 score as the early stopping metric , use the Adam optimizer with an initial learning rate of 3 e - 5 , and the Categorical Cross Entropy Loss with class weights as described above to avoid training on the padded label outputs .","['We', 'use', 'early', 'stopping', 'on', 'dev', 'data', 'for', '6', 'epochs', 'as', 'tolerance', 'and', 'F', '1', 'score', 'as', 'the', 'early', 'stopping', 'metric', ',', 'use', 'the', 'Adam', 'optimizer', 'with', 'an', 'initial', 'learning', 'rate', 'of', '3', 'e', '-', '5', ',', 'and', 'the', 'Categorical', 'Cross', 'Entropy', 'Loss', 'with', 'class', 'weights', 'as', 'described', 'above', 'to', 'avoid', 'training', 'on', 'the', 'padded', 'label', 'outputs', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'NN', 'NNS', 'IN', 'CD', 'NNS', 'IN', 'NN', 'CC', 'NNP', 'CD', 'NN', 'IN', 'DT', 'JJ', 'NN', 'JJ', ',', 'VB', 'DT', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'SYM', ':', 'CD', ',', 'CC', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NN', 'NNS', 'IN', 'VBN', 'IN', 'TO', 'VB', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNS', '.']",58
negation_scope_resolution,0,194,"We perform cue detection and scope resolution for all 3 datasets , and train on 1 and test on all datasets .","['We', 'perform', 'cue', 'detection', 'and', 'scope', 'resolution', 'for', 'all', '3', 'datasets', ',', 'and', 'train', 'on', '1', 'and', 'test', 'on', 'all', 'datasets', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'CC', 'NN', 'NN', 'IN', 'DT', 'CD', 'NNS', ',', 'CC', 'NN', 'IN', 'CD', 'CC', 'VB', 'IN', 'DT', 'NNS', '.']",22
negation_scope_resolution,0,197,"For all other corpuses , we use a default 70 - 15 - 15 split for the train - dev - test data .","['For', 'all', 'other', 'corpuses', ',', 'we', 'use', 'a', 'default', '70', '-', '15', '-', '15', 'split', 'for', 'the', 'train', '-', 'dev', '-', 'test', 'data', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'CD', ':', 'CD', ':', 'CD', 'NN', 'IN', 'DT', 'NN', ':', 'NN', ':', 'NN', 'NNS', '.']",24
negation_scope_resolution,0,198,"We trained the models on free GPUs available via Google Colaboratory , the training scripts are publicly available .","['We', 'trained', 'the', 'models', 'on', 'free', 'GPUs', 'available', 'via', 'Google', 'Colaboratory', ',', 'the', 'training', 'scripts', 'are', 'publicly', 'available', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'DT', 'NNS', 'IN', 'JJ', 'NNP', 'JJ', 'IN', 'NNP', 'NNP', ',', 'DT', 'NN', 'NNS', 'VBP', 'RB', 'JJ', '.']",19
negation_scope_resolution,0,201,"For cue detection , on the Sherlock dataset test data , we see that we outperform the best system [ FBK Chowdhury ] by 0.6 F1 measure .","['For', 'cue', 'detection', ',', 'on', 'the', 'Sherlock', 'dataset', 'test', 'data', ',', 'we', 'see', 'that', 'we', 'outperform', 'the', 'best', 'system', '[', 'FBK', 'Chowdhury', ']', 'by', '0.6', 'F1', 'measure', '.']","['B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'NN', ',', 'IN', 'DT', 'NNP', 'NN', 'NN', 'NNS', ',', 'PRP', 'VBP', 'IN', 'PRP', 'VBP', 'DT', 'JJS', 'NN', 'JJ', 'NNP', 'NNP', 'NNP', 'IN', 'CD', 'NNP', 'NN', '.']",28
negation_scope_resolution,0,202,"On the BioScope Abstracts , we perform reasonably well .","['On', 'the', 'BioScope', 'Abstracts', ',', 'we', 'perform', 'reasonably', 'well', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNP', 'NNP', ',', 'PRP', 'VBP', 'RB', 'RB', '.']",10
negation_scope_resolution,0,204,"On the BioScope Full papers , we are able to achieve 90.43 F1 when training on the same data , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .","['On', 'the', 'BioScope', 'Full', 'papers', ',', 'we', 'are', 'able', 'to', 'achieve', '90.43', 'F1', 'when', 'training', 'on', 'the', 'same', 'data', ',', 'but', 'we', 'do', 'note', 'that', 'the', 'amount', 'of', 'training', 'data', 'available', 'is', 'significantly', 'lower', 'than', 'for', 'the', 'other', 'datasets', ',', 'and', 'while', 'general', 'Deep', 'Learning', 'based', 'approaches', 'can', 'not', 'perform', 'well', 'in', 'such', 'situations', ',', 'we', 'still', 'manage', 'to', 'perform', 'well', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNP', 'NNP', 'NNS', ',', 'PRP', 'VBP', 'JJ', 'TO', 'VB', 'CD', 'NNP', 'WRB', 'VBG', 'IN', 'DT', 'JJ', 'NN', ',', 'CC', 'PRP', 'VBP', 'VB', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', 'JJ', 'VBZ', 'RB', 'JJR', 'IN', 'IN', 'DT', 'JJ', 'NNS', ',', 'CC', 'IN', 'JJ', 'NNP', 'NNP', 'VBN', 'NNS', 'MD', 'RB', 'VB', 'RB', 'IN', 'JJ', 'NNS', ',', 'PRP', 'RB', 'VB', 'TO', 'VB', 'RB', '.']",62
negation_scope_resolution,0,205,"On the SFU Review Corpus , we achieve an F1 of 87.08 .","['On', 'the', 'SFU', 'Review', 'Corpus', ',', 'we', 'achieve', 'an', 'F1', 'of', '87.08', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NNP', 'NNP', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NNP', 'IN', 'CD', '.']",13
negation_scope_resolution,0,208,For scope resolution :,"['For', 'scope', 'resolution', ':']","['O', 'B-n', 'I-n', 'O']","['IN', 'NN', 'NN', ':']",4
negation_scope_resolution,0,209,"On the Sherlock dataset , we achieve an F1 of 92.36 , outperforming the previous State of the Art by a significant margin ( almost 3.0 F1 ) .","['On', 'the', 'Sherlock', 'dataset', ',', 'we', 'achieve', 'an', 'F1', 'of', '92.36', ',', 'outperforming', 'the', 'previous', 'State', 'of', 'the', 'Art', 'by', 'a', 'significant', 'margin', '(', 'almost', '3.0', 'F1', ')', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNP', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', 'IN', 'CD', ',', 'VBG', 'DT', 'JJ', 'NNP', 'IN', 'DT', 'NNP', 'IN', 'DT', 'JJ', 'NN', '(', 'RB', 'CD', 'NNP', ')', '.']",29
negation_scope_resolution,0,210,"On the BioScope Abstracts , we achieve an F1 of 95.68 , outperforming the best architecture by 3.57 F1 .","['On', 'the', 'BioScope', 'Abstracts', ',', 'we', 'achieve', 'an', 'F1', 'of', '95.68', ',', 'outperforming', 'the', 'best', 'architecture', 'by', '3.57', 'F1', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNP', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NNP', 'IN', 'CD', ',', 'VBG', 'DT', 'JJS', 'NN', 'IN', 'CD', 'NNP', '.']",20
negation_scope_resolution,0,211,"On the Bioscope Full Papers , we outperform the best architecture by 2.64 F1 when training on the same dataset","['On', 'the', 'Bioscope', 'Full', 'Papers', ',', 'we', 'outperform', 'the', 'best', 'architecture', 'by', '2.64', 'F1', 'when', 'training', 'on', 'the', 'same', 'dataset']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n']","['IN', 'DT', 'NNP', 'NNP', 'NNP', ',', 'PRP', 'VBP', 'DT', 'JJS', 'NN', 'IN', 'CD', 'NNP', 'WRB', 'VBG', 'IN', 'DT', 'JJ', 'NN']",20
negation_scope_resolution,0,212,"On the SFU Review Corpus , we outperform the best system to date by 1.02 F1 .","['On', 'the', 'SFU', 'Review', 'Corpus', ',', 'we', 'outperform', 'the', 'best', 'system', 'to', 'date', 'by', '1.02', 'F1', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNP', 'NNP', 'NNP', ',', 'PRP', 'VBP', 'DT', 'JJS', 'NN', 'TO', 'NN', 'IN', 'CD', 'NNP', '.']",17
negation_scope_resolution,0,213,"For negation cue detection , we observe a significant gap between our model , NegBERT , and the current state - of the - art systems , while we outperform the baseline systems .","['For', 'negation', 'cue', 'detection', ',', 'we', 'observe', 'a', 'significant', 'gap', 'between', 'our', 'model', ',', 'NegBERT', ',', 'and', 'the', 'current', 'state', '-', 'of', 'the', '-', 'art', 'systems', ',', 'while', 'we', 'outperform', 'the', 'baseline', 'systems', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'PRP$', 'NN', ',', 'NNP', ',', 'CC', 'DT', 'JJ', 'NN', ':', 'IN', 'DT', ':', 'NN', 'NNS', ',', 'IN', 'PRP', 'VBP', 'DT', 'NN', 'NNS', '.']",34
negation_scope_resolution,0,217,"When we trained on BioScope Abstracts and tested on the BioScope Full Papers , we surprisingly observed a stateof - the - art result of 91.24 ( a gain of 3.89 F1 points over training on BioScope Full Papers ) , which is far beyond the achievable results on training and evaluating on the Bio-Medical sub corpora .","['When', 'we', 'trained', 'on', 'BioScope', 'Abstracts', 'and', 'tested', 'on', 'the', 'BioScope', 'Full', 'Papers', ',', 'we', 'surprisingly', 'observed', 'a', 'stateof', '-', 'the', '-', 'art', 'result', 'of', '91.24', '(', 'a', 'gain', 'of', '3.89', 'F1', 'points', 'over', 'training', 'on', 'BioScope', 'Full', 'Papers', ')', ',', 'which', 'is', 'far', 'beyond', 'the', 'achievable', 'results', 'on', 'training', 'and', 'evaluating', 'on', 'the', 'Bio-Medical', 'sub', 'corpora', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['WRB', 'PRP', 'VBD', 'IN', 'NNP', 'NNP', 'CC', 'VBN', 'IN', 'DT', 'NNP', 'NNP', 'NNP', ',', 'PRP', 'RB', 'VBD', 'DT', 'JJ', ':', 'DT', ':', 'NN', 'NN', 'IN', 'CD', '(', 'DT', 'NN', 'IN', 'CD', 'NNP', 'NNS', 'IN', 'NN', 'IN', 'NNP', 'NNP', 'NNP', ')', ',', 'WDT', 'VBZ', 'RB', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NN', 'CC', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",58
semantic_parsing,1,2,TRANX : A Transition - based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation,"['TRANX', ':', 'A', 'Transition', '-', 'based', 'Neural', 'Abstract', 'Syntax', 'Parser', 'for', 'Semantic', 'Parsing', 'and', 'Code', 'Generation']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NN', ':', 'DT', 'NNP', ':', 'VBN', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'CC', 'NNP', 'NNP']",16
semantic_parsing,1,18,"Inspired by this existing research , we have developed TRANX , a TRANsition - based abstract syntaX parser for semantic parsing and code generation .","['Inspired', 'by', 'this', 'existing', 'research', ',', 'we', 'have', 'developed', 'TRANX', ',', 'a', 'TRANsition', '-', 'based', 'abstract', 'syntaX', 'parser', 'for', 'semantic', 'parsing', 'and', 'code', 'generation', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'VBG', 'NN', ',', 'PRP', 'VBP', 'VBN', 'NNP', ',', 'DT', 'NNP', ':', 'VBN', 'JJ', 'NNS', 'NN', 'IN', 'JJ', 'NN', 'CC', 'NN', 'NN', '.']",25
semantic_parsing,1,19,TRANX is designed with the following principles in mind :,"['TRANX', 'is', 'designed', 'with', 'the', 'following', 'principles', 'in', 'mind', ':']","['B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['NNP', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NN', ':']",10
semantic_parsing,1,20,"Generalization ability TRANX employs ASTs as a general - purpose intermediate meaning representation , and the task - dependent grammar is provided to the system as external knowledge to guide the parsing process , therefore decoupling the semantic parsing procedure with specificities of grammars .","['Generalization', 'ability', 'TRANX', 'employs', 'ASTs', 'as', 'a', 'general', '-', 'purpose', 'intermediate', 'meaning', 'representation', ',', 'and', 'the', 'task', '-', 'dependent', 'grammar', 'is', 'provided', 'to', 'the', 'system', 'as', 'external', 'knowledge', 'to', 'guide', 'the', 'parsing', 'process', ',', 'therefore', 'decoupling', 'the', 'semantic', 'parsing', 'procedure', 'with', 'specificities', 'of', 'grammars', '.']","['B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'NNP', 'VBZ', 'NNP', 'IN', 'DT', 'JJ', ':', 'JJ', 'JJ', 'NN', 'NN', ',', 'CC', 'DT', 'NN', ':', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'DT', 'NN', 'IN', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', ',', 'RB', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NNS', 'IN', 'NNS', '.']",45
semantic_parsing,1,21,Extensibility TRANX uses a simple transition system to parse NL utterances into tree -,"['Extensibility', 'TRANX', 'uses', 'a', 'simple', 'transition', 'system', 'to', 'parse', 'NL', 'utterances', 'into', 'tree', '-']","['B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O']","['NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'NNP', 'NNPS', 'IN', 'JJ', ':']",14
semantic_parsing,1,24,Effectiveness,['Effectiveness'],['B-n'],['NN'],1
semantic_parsing,1,25,"We test TRANX on four semantic parsing ( ATIS , GEO ) and code generation ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .","['We', 'test', 'TRANX', 'on', 'four', 'semantic', 'parsing', '(', 'ATIS', ',', 'GEO', ')', 'and', 'code', 'generation', '(', 'DJANGO', ',', 'WIKISQL', ')', 'tasks', ',', 'and', 'demonstrate', 'that', 'TRANX', 'is', 'capable', 'of', 'generalizing', 'to', 'different', 'domains', 'while', 'registering', 'strong', 'performance', ',', 'out', '-', 'performing', 'existing', 'neural', 'networkbased', 'approaches', 'on', 'three', 'of', 'the', 'four', 'datasets', '(', 'GEO', ',', 'ATIS', ',', 'DJANGO', ')', '.']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'TO', 'IN', 'CD', 'JJ', 'NN', '(', 'NNP', ',', 'NNP', ')', 'CC', 'JJ', 'NN', '(', 'NNP', ',', 'NNP', ')', 'NNS', ',', 'CC', 'VB', 'DT', 'NNP', 'VBZ', 'JJ', 'IN', 'VBG', 'TO', 'JJ', 'NNS', 'IN', 'VBG', 'JJ', 'NN', ',', 'IN', ':', 'VBG', 'VBG', 'JJ', 'JJ', 'NNS', 'IN', 'CD', 'IN', 'DT', 'CD', 'NNS', '(', 'NNP', ',', 'NNP', ',', 'NNP', ')', '.']",59
semantic_parsing,1,126,Semantic Parsing Tab.,"['Semantic', 'Parsing', 'Tab.']","['B-n', 'I-n', 'O']","['JJ', 'NNP', 'NNP']",3
semantic_parsing,1,129,Our system outperforms existing neural network - based approaches .,"['Our', 'system', 'outperforms', 'existing', 'neural', 'network', '-', 'based', 'approaches', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'VBG', 'JJ', 'NN', ':', 'VBN', 'NNS', '.']",10
semantic_parsing,1,131,"Interestingly , we found the model without parent feeding achieves slightly better accuracy on GEO , probably because that its relative simple grammar does not require extra handling of parent information .","['Interestingly', ',', 'we', 'found', 'the', 'model', 'without', 'parent', 'feeding', 'achieves', 'slightly', 'better', 'accuracy', 'on', 'GEO', ',', 'probably', 'because', 'that', 'its', 'relative', 'simple', 'grammar', 'does', 'not', 'require', 'extra', 'handling', 'of', 'parent', 'information', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBD', 'DT', 'NN', 'IN', 'NN', 'VBG', 'NNS', 'RB', 'RB', 'NN', 'IN', 'NNP', ',', 'RB', 'IN', 'DT', 'PRP$', 'JJ', 'JJ', 'NN', 'VBZ', 'RB', 'VB', 'JJ', 'NN', 'IN', 'NN', 'NN', '.']",32
semantic_parsing,1,132,Code Generation Tab.,"['Code', 'Generation', 'Tab.']","['B-n', 'I-n', 'O']","['NNP', 'NNP', 'NNP']",3
semantic_parsing,1,133,2 lists the results on DJANGO .,"['2', 'lists', 'the', 'results', 'on', 'DJANGO', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'O']","['CD', 'VBZ', 'DT', 'NNS', 'IN', 'NNP', '.']",7
semantic_parsing,1,134,TRANX achieves state - of - the - art results on DJANGO .,"['TRANX', 'achieves', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'on', 'DJANGO', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['NNP', 'VBZ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'NNP', '.']",13
semantic_parsing,1,135,"We also find parent feeding yields + 1 point gain in accuracy , suggesting the importance of modeling parental connections in ASTs with complex domain grammars ( e.g. , Python ) .","['We', 'also', 'find', 'parent', 'feeding', 'yields', '+', '1', 'point', 'gain', 'in', 'accuracy', ',', 'suggesting', 'the', 'importance', 'of', 'modeling', 'parental', 'connections', 'in', 'ASTs', 'with', 'complex', 'domain', 'grammars', '(', 'e.g.', ',', 'Python', ')', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'JJ', 'NN', 'NNS', 'VBD', 'CD', 'NN', 'NN', 'IN', 'NN', ',', 'VBG', 'DT', 'NN', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'NNP', 'IN', 'JJ', 'NN', 'NNS', '(', 'NN', ',', 'NNP', ')', '.']",32
semantic_parsing,1,138,"3 . We find TRANX , although just with simple extensions to adapt to this dataset , achieves impressive results and outperforms many task - specific methods .","['3', '.', 'We', 'find', 'TRANX', ',', 'although', 'just', 'with', 'simple', 'extensions', 'to', 'adapt', 'to', 'this', 'dataset', ',', 'achieves', 'impressive', 'results', 'and', 'outperforms', 'many', 'task', '-', 'specific', 'methods', '.']","['O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['LS', '.', 'PRP', 'VBP', 'JJ', ',', 'IN', 'RB', 'IN', 'JJ', 'NNS', 'TO', 'VB', 'TO', 'DT', 'NN', ',', 'VBZ', 'JJ', 'NNS', 'CC', 'VBZ', 'JJ', 'NN', ':', 'JJ', 'NNS', '.']",28
semantic_parsing,2,2,Coarse - to - Fine Decoding for Neural Semantic Parsing,"['Coarse', '-', 'to', '-', 'Fine', 'Decoding', 'for', 'Neural', 'Semantic', 'Parsing']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', ':', 'TO', ':', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",10
semantic_parsing,2,4,Semantic parsing aims at mapping natural language utterances into structured meaning representations .,"['Semantic', 'parsing', 'aims', 'at', 'mapping', 'natural', 'language', 'utterances', 'into', 'structured', 'meaning', 'representations', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NNS', 'IN', 'VBG', 'JJ', 'NN', 'NNS', 'IN', 'JJ', 'NN', 'NNS', '.']",13
semantic_parsing,2,14,"In this work , we propose to decompose the decoding process into two stages .","['In', 'this', 'work', ',', 'we', 'propose', 'to', 'decompose', 'the', 'decoding', 'process', 'into', 'two', 'stages', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'CD', 'NNS', '.']",15
semantic_parsing,2,15,"The first decoder focuses on predicting a rough sketch of the meaning representation , which omits low - level details , such as arguments and variable names .","['The', 'first', 'decoder', 'focuses', 'on', 'predicting', 'a', 'rough', 'sketch', 'of', 'the', 'meaning', 'representation', ',', 'which', 'omits', 'low', '-', 'level', 'details', ',', 'such', 'as', 'arguments', 'and', 'variable', 'names', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NNS', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'WDT', 'VBZ', 'JJ', ':', 'NN', 'NNS', ',', 'JJ', 'IN', 'NNS', 'CC', 'JJ', 'NNS', '.']",28
semantic_parsing,2,17,"Then , a second decoder fills in missing details by conditioning on the natural language input and the sketch itself .","['Then', ',', 'a', 'second', 'decoder', 'fills', 'in', 'missing', 'details', 'by', 'conditioning', 'on', 'the', 'natural', 'language', 'input', 'and', 'the', 'sketch', 'itself', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O']","['RB', ',', 'DT', 'JJ', 'NN', 'NNS', 'IN', 'VBG', 'NNS', 'IN', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NN', 'CC', 'DT', 'NN', 'PRP', '.']",21
semantic_parsing,2,18,"Specifically , the sketch constrains the generation process and is encoded into vectors to guide decoding .","['Specifically', ',', 'the', 'sketch', 'constrains', 'the', 'generation', 'process', 'and', 'is', 'encoded', 'into', 'vectors', 'to', 'guide', 'decoding', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O']","['RB', ',', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'CC', 'VBZ', 'VBN', 'IN', 'NNS', 'TO', 'VB', 'NN', '.']",17
semantic_parsing,2,20,"Firstly , the decomposition disentangles high - level from low - level semantic information , which enables the decoders to model meaning at different levels of granularity .","['Firstly', ',', 'the', 'decomposition', 'disentangles', 'high', '-', 'level', 'from', 'low', '-', 'level', 'semantic', 'information', ',', 'which', 'enables', 'the', 'decoders', 'to', 'model', 'meaning', 'at', 'different', 'levels', 'of', 'granularity', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'VBZ', 'JJ', ':', 'NN', 'IN', 'JJ', ':', 'NN', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NNS', 'TO', 'VB', 'VBG', 'IN', 'JJ', 'NNS', 'IN', 'NN', '.']",28
semantic_parsing,2,22,"Secondly , the model can explicitly share knowledge of coarse structures for the examples that have the same sketch ( i.e. , basic meaning ) , even though their actual meaning representations are different ( e.g. , due to different details ) .","['Secondly', ',', 'the', 'model', 'can', 'explicitly', 'share', 'knowledge', 'of', 'coarse', 'structures', 'for', 'the', 'examples', 'that', 'have', 'the', 'same', 'sketch', '(', 'i.e.', ',', 'basic', 'meaning', ')', ',', 'even', 'though', 'their', 'actual', 'meaning', 'representations', 'are', 'different', '(', 'e.g.', ',', 'due', 'to', 'different', 'details', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'DT', 'NN', 'MD', 'RB', 'NN', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NNS', 'WDT', 'VBP', 'DT', 'JJ', 'NN', '(', 'FW', ',', 'JJ', 'NN', ')', ',', 'RB', 'IN', 'PRP$', 'JJ', 'NN', 'NNS', 'VBP', 'JJ', '(', 'NN', ',', 'JJ', 'TO', 'JJ', 'NNS', ')', '.']",43
semantic_parsing,2,23,"Thirdly , after generating the sketch , the decoder knows what the basic meaning of the utterance looks like , and the model can use it as global context to improve the prediction of the final details .","['Thirdly', ',', 'after', 'generating', 'the', 'sketch', ',', 'the', 'decoder', 'knows', 'what', 'the', 'basic', 'meaning', 'of', 'the', 'utterance', 'looks', 'like', ',', 'and', 'the', 'model', 'can', 'use', 'it', 'as', 'global', 'context', 'to', 'improve', 'the', 'prediction', 'of', 'the', 'final', 'details', '.']","['O', 'O', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'IN', 'VBG', 'DT', 'NN', ',', 'DT', 'NN', 'VBZ', 'WP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'IN', ',', 'CC', 'DT', 'NN', 'MD', 'VB', 'PRP', 'IN', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNS', '.']",38
semantic_parsing,2,230,Our implementation and pretrained models are available at https :// github.com/donglixp/coarse2fine.,"['Our', 'implementation', 'and', 'pretrained', 'models', 'are', 'available', 'at', 'https', '://', 'github.com/donglixp/coarse2fine.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['PRP$', 'NN', 'CC', 'JJ', 'NNS', 'VBP', 'JJ', 'IN', 'JJ', 'CD', 'NN']",11
semantic_parsing,2,237,"Dimensions of hidden vectors and word embeddings were selected from { 250 , 300 } and { 150 , 200 , 250 , 300 } , respectively .","['Dimensions', 'of', 'hidden', 'vectors', 'and', 'word', 'embeddings', 'were', 'selected', 'from', '{', '250', ',', '300', '}', 'and', '{', '150', ',', '200', ',', '250', ',', '300', '}', ',', 'respectively', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['NNS', 'IN', 'JJ', 'NNS', 'CC', 'NN', 'NNS', 'VBD', 'VBN', 'IN', '(', 'CD', ',', 'CD', ')', 'CC', '(', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ')', ',', 'RB', '.']",28
semantic_parsing,2,238,"The dropout rate was selected from { 0.3 , 0.5 } .","['The', 'dropout', 'rate', 'was', 'selected', 'from', '{', '0.3', ',', '0.5', '}', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBD', 'VBN', 'IN', '(', 'CD', ',', 'CD', ')', '.']",12
semantic_parsing,2,239,Label smoothing was employed for GEO and ATIS .,"['Label', 'smoothing', 'was', 'employed', 'for', 'GEO', 'and', 'ATIS', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O']","['NNP', 'NN', 'VBD', 'VBN', 'IN', 'NNP', 'CC', 'NNP', '.']",9
semantic_parsing,2,240,The smoothing parameter was set to 0.1 .,"['The', 'smoothing', 'parameter', 'was', 'set', 'to', '0.1', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBD', 'VBN', 'TO', 'CD', '.']",8
semantic_parsing,2,242,"Word embeddings were initialized by GloVe , and were shared by table encoder and input encoder in Section 4.3 .","['Word', 'embeddings', 'were', 'initialized', 'by', 'GloVe', ',', 'and', 'were', 'shared', 'by', 'table', 'encoder', 'and', 'input', 'encoder', 'in', 'Section', '4.3', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['NNP', 'NNS', 'VBD', 'VBN', 'IN', 'NNP', ',', 'CC', 'VBD', 'VBN', 'IN', 'JJ', 'NN', 'CC', 'NN', 'NN', 'IN', 'NNP', 'CD', '.']",20
semantic_parsing,2,243,We appended 10 - dimensional part - of - speech tag vectors to embeddings of the question words in WIKISQL .,"['We', 'appended', '10', '-', 'dimensional', 'part', '-', 'of', '-', 'speech', 'tag', 'vectors', 'to', 'embeddings', 'of', 'the', 'question', 'words', 'in', 'WIKISQL', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBD', 'CD', ':', 'JJ', 'NN', ':', 'IN', ':', 'NN', 'NN', 'NNS', 'TO', 'NNS', 'IN', 'DT', 'NN', 'NNS', 'IN', 'NNP', '.']",21
semantic_parsing,2,244,The part - of - speech tags were obtained by the spaCy toolkit .,"['The', 'part', '-', 'of', '-', 'speech', 'tags', 'were', 'obtained', 'by', 'the', 'spaCy', 'toolkit', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', ':', 'IN', ':', 'NN', 'NNS', 'VBD', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",14
semantic_parsing,2,245,We used the RMSProp optimizer to train the models .,"['We', 'used', 'the', 'RMSProp', 'optimizer', 'to', 'train', 'the', 'models', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBD', 'DT', 'NNP', 'NN', 'TO', 'VB', 'DT', 'NNS', '.']",10
semantic_parsing,2,246,"The learning rate was selected from { 0.002 , 0.005 } .","['The', 'learning', 'rate', 'was', 'selected', 'from', '{', '0.002', ',', '0.005', '}', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBD', 'VBN', 'IN', '(', 'CD', ',', 'CD', ')', '.']",12
semantic_parsing,2,247,"The batch size was 200 for WIKISQL , and was 64 for other datasets .","['The', 'batch', 'size', 'was', '200', 'for', 'WIKISQL', ',', 'and', 'was', '64', 'for', 'other', 'datasets', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBD', 'CD', 'IN', 'NNP', ',', 'CC', 'VBD', 'CD', 'IN', 'JJ', 'NNS', '.']",15
semantic_parsing,2,248,Early stopping was used to determine the number of epochs .,"['Early', 'stopping', 'was', 'used', 'to', 'determine', 'the', 'number', 'of', 'epochs', '.']","['B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', 'NN', 'VBD', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', '.']",11
semantic_parsing,2,257,"Overall , we observe that COARSE2FINE outperforms ONESTAGE , which suggests that disentangling high - level from low - level information dur - 62.3 SNM + COPY 71 and .","['Overall', ',', 'we', 'observe', 'that', 'COARSE2FINE', 'outperforms', 'ONESTAGE', ',', 'which', 'suggests', 'that', 'disentangling', 'high', '-', 'level', 'from', 'low', '-', 'level', 'information', 'dur', '-', '62.3', 'SNM', '+', 'COPY', '71', 'and', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', ',', 'PRP', 'VBP', 'DT', 'NNP', 'MD', 'NNP', ',', 'WDT', 'VBZ', 'IN', 'VBG', 'JJ', ':', 'NN', 'IN', 'JJ', ':', 'NN', 'NN', 'SYM', ':', 'CD', 'NNP', 'NNP', 'NNP', 'CD', 'CC', '.']",30
semantic_parsing,2,260,"Compared with previous neural models that utilize syntax or grammatical information ( SEQ2 TREE , ASN ; the second block in ) , our method performs competitively despite the use of relatively simple decoders .","['Compared', 'with', 'previous', 'neural', 'models', 'that', 'utilize', 'syntax', 'or', 'grammatical', 'information', '(', 'SEQ2', 'TREE', ',', 'ASN', ';', 'the', 'second', 'block', 'in', ')', ',', 'our', 'method', 'performs', 'competitively', 'despite', 'the', 'use', 'of', 'relatively', 'simple', 'decoders', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'JJ', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'CC', 'JJ', 'NN', '(', 'NNP', 'NNP', ',', 'NNP', ':', 'DT', 'JJ', 'NN', 'IN', ')', ',', 'PRP$', 'NN', 'NNS', 'RB', 'IN', 'DT', 'NN', 'IN', 'RB', 'JJ', 'NNS', '.']",35
semantic_parsing,2,262,"As can be seen , predicting the sketch correctly boosts performance .","['As', 'can', 'be', 'seen', ',', 'predicting', 'the', 'sketch', 'correctly', 'boosts', 'performance', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-n', 'B-p', 'B-n', 'O']","['IN', 'MD', 'VB', 'VBN', ',', 'VBG', 'DT', 'NN', 'RB', 'VBZ', 'NN', '.']",12
semantic_parsing,2,267,Again we observe that the sketch encoder is beneficial and that there is an 8.9 point difference in accuracy between COARSE2FINE and the oracle .,"['Again', 'we', 'observe', 'that', 'the', 'sketch', 'encoder', 'is', 'beneficial', 'and', 'that', 'there', 'is', 'an', '8.9', 'point', 'difference', 'in', 'accuracy', 'between', 'COARSE2FINE', 'and', 'the', 'oracle', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'JJ', 'CC', 'IN', 'EX', 'VBZ', 'DT', 'CD', 'NN', 'NN', 'IN', 'NN', 'IN', 'NNP', 'CC', 'DT', 'NN', '.']",25
semantic_parsing,2,269,Our model is superior to ONESTAGE as well as to previous best performing systems .,"['Our', 'model', 'is', 'superior', 'to', 'ONESTAGE', 'as', 'well', 'as', 'to', 'previous', 'best', 'performing', 'systems', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'JJ', 'TO', 'NNP', 'RB', 'RB', 'IN', 'TO', 'JJ', 'JJS', 'VBG', 'NNS', '.']",15
semantic_parsing,2,270,"COARSE2FINE 's accuracies on aggregation agg op and agg col are 90.2 % and 92.0 % , respectively , which is comparable to SQLNET .","['COARSE2FINE', ""'s"", 'accuracies', 'on', 'aggregation', 'agg', 'op', 'and', 'agg', 'col', 'are', '90.2', '%', 'and', '92.0', '%', ',', 'respectively', ',', 'which', 'is', 'comparable', 'to', 'SQLNET', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['NNP', 'POS', 'NNS', 'IN', 'NN', 'NN', 'NN', 'CC', 'NN', 'NN', 'VBP', 'CD', 'NN', 'CC', 'CD', 'NN', ',', 'RB', ',', 'WDT', 'VBZ', 'JJ', 'TO', 'NNP', '.']",25
semantic_parsing,2,271,So the most gain is obtained by the improved decoder of the WHERE clause .,"['So', 'the', 'most', 'gain', 'is', 'obtained', 'by', 'the', 'improved', 'decoder', 'of', 'the', 'WHERE', 'clause', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', 'DT', 'RBS', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",15
semantic_parsing,2,272,"We also find that a tableaware input encoder is critical for doing well on this task , since the same question might lead to different SQL queries depending on the table schemas .","['We', 'also', 'find', 'that', 'a', 'tableaware', 'input', 'encoder', 'is', 'critical', 'for', 'doing', 'well', 'on', 'this', 'task', ',', 'since', 'the', 'same', 'question', 'might', 'lead', 'to', 'different', 'SQL', 'queries', 'depending', 'on', 'the', 'table', 'schemas', '.']","['O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'IN', 'DT', 'NN', 'NN', 'NN', 'VBZ', 'JJ', 'IN', 'VBG', 'RB', 'IN', 'DT', 'NN', ',', 'IN', 'DT', 'JJ', 'NN', 'MD', 'VB', 'TO', 'JJ', 'NNP', 'NNS', 'VBG', 'IN', 'DT', 'NN', 'NN', '.']",33
semantic_parsing,2,279,Sketches produced by COARSE2FINE are more accurate across the board .,"['Sketches', 'produced', 'by', 'COARSE2FINE', 'are', 'more', 'accurate', 'across', 'the', 'board', '.']","['B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['NNS', 'VBN', 'IN', 'NNP', 'VBP', 'RBR', 'JJ', 'IN', 'DT', 'NN', '.']",11
semantic_parsing,2,282,"On WIKISQL , the sketches predicted by COARSE2FINE are marginally better compared with ONESTAGE .","['On', 'WIKISQL', ',', 'the', 'sketches', 'predicted', 'by', 'COARSE2FINE', 'are', 'marginally', 'better', 'compared', 'with', 'ONESTAGE', '.']","['B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'NNP', ',', 'DT', 'NNS', 'VBN', 'IN', 'NNP', 'VBP', 'RB', 'RB', 'VBN', 'IN', 'NNP', '.']",15
semantic_parsing,0,2,Spider : A Large - Scale Human - Labeled Dataset for Complex and Cross - Domain Semantic Parsing and Text - to - SQL Task,"['Spider', ':', 'A', 'Large', '-', 'Scale', 'Human', '-', 'Labeled', 'Dataset', 'for', 'Complex', 'and', 'Cross', '-', 'Domain', 'Semantic', 'Parsing', 'and', 'Text', '-', 'to', '-', 'SQL', 'Task']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', ':', 'DT', 'NNP', ':', 'NN', 'NNP', ':', 'VBD', 'NNP', 'IN', 'NNP', 'CC', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', ':', 'TO', ':', 'NNP', 'NNP']",25
semantic_parsing,0,11,Our dataset and task are publicly available at https://yale-lily. github.io/ spider .,"['Our', 'dataset', 'and', 'task', 'are', 'publicly', 'available', 'at', 'https://yale-lily.', 'github.io/', 'spider', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'CC', 'NN', 'VBP', 'RB', 'JJ', 'IN', 'JJ', 'NN', 'NN', '.']",12
semantic_parsing,0,13,Semantic parsing ( SP ) is one of the most important tasks in natural language processing ( NLP ) .,"['Semantic', 'parsing', '(', 'SP', ')', 'is', 'one', 'of', 'the', 'most', 'important', 'tasks', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', '(', 'NNP', ')', 'VBZ', 'CD', 'IN', 'DT', 'RBS', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', '.']",20
semantic_parsing,0,19,Existing datasets for SP have two shortcomings .,"['Existing', 'datasets', 'for', 'SP', 'have', 'two', 'shortcomings', '.']","['O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O']","['VBG', 'NNS', 'IN', 'NNP', 'VBP', 'CD', 'NNS', '.']",8
semantic_parsing,0,34,"To address the need for a large and high - quality dataset for a new complex and cross-domain semantic parsing task , we introduce Spider , which consists of 200 databases with multiple tables , 10,181 questions , and 5,693 corresponding complex SQL queries , all written by 11 college students spending a total of 1,000 man-hours .","['To', 'address', 'the', 'need', 'for', 'a', 'large', 'and', 'high', '-', 'quality', 'dataset', 'for', 'a', 'new', 'complex', 'and', 'cross-domain', 'semantic', 'parsing', 'task', ',', 'we', 'introduce', 'Spider', ',', 'which', 'consists', 'of', '200', 'databases', 'with', 'multiple', 'tables', ',', '10,181', 'questions', ',', 'and', '5,693', 'corresponding', 'complex', 'SQL', 'queries', ',', 'all', 'written', 'by', '11', 'college', 'students', 'spending', 'a', 'total', 'of', '1,000', 'man-hours', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'CC', 'JJ', ':', 'NN', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'CC', 'JJ', 'JJ', 'NN', 'NN', ',', 'PRP', 'VBP', 'NNP', ',', 'WDT', 'VBZ', 'IN', 'CD', 'NNS', 'IN', 'JJ', 'NNS', ',', 'CD', 'NNS', ',', 'CC', 'CD', 'NN', 'JJ', 'NNP', 'NNS', ',', 'DT', 'VBN', 'IN', 'CD', 'NN', 'NNS', 'VBG', 'DT', 'NN', 'IN', 'CD', 'NNS', '.']",58
semantic_parsing,0,35,"As illustrates , given a database with multiple tables including foreign keys , our corpus creates and annotates complex questions and SQL queries including different SQL clauses such as joining and nested query .","['As', 'illustrates', ',', 'given', 'a', 'database', 'with', 'multiple', 'tables', 'including', 'foreign', 'keys', ',', 'our', 'corpus', 'creates', 'and', 'annotates', 'complex', 'questions', 'and', 'SQL', 'queries', 'including', 'different', 'SQL', 'clauses', 'such', 'as', 'joining', 'and', 'nested', 'query', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNS', ',', 'VBN', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'VBG', 'JJ', 'NNS', ',', 'PRP$', 'NN', 'NNS', 'CC', 'NNS', 'JJ', 'NNS', 'CC', 'NNP', 'NNS', 'VBG', 'JJ', 'NNP', 'NNS', 'JJ', 'IN', 'NN', 'CC', 'JJ', 'NN', '.']",34
semantic_parsing,0,248,"The performances of the Seq2Seq - based basic models including Seq2Seq , Seq2Seq + Attention , and Seq2Seq + Copying are very low .","['The', 'performances', 'of', 'the', 'Seq2Seq', '-', 'based', 'basic', 'models', 'including', 'Seq2Seq', ',', 'Seq2Seq', '+', 'Attention', ',', 'and', 'Seq2Seq', '+', 'Copying', 'are', 'very', 'low', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'DT', 'NNP', ':', 'VBN', 'JJ', 'NNS', 'VBG', 'NNP', ',', 'NNP', 'NNP', 'NNP', ',', 'CC', 'NNP', 'NNP', 'NNP', 'VBP', 'RB', 'JJ', '.']",24
semantic_parsing,0,253,"In contrast , SQLNet and TypeSQL that utilize SQL structure information to guide the SQL generation process significantly outperform other Seq2Seq models .","['In', 'contrast', ',', 'SQLNet', 'and', 'TypeSQL', 'that', 'utilize', 'SQL', 'structure', 'information', 'to', 'guide', 'the', 'SQL', 'generation', 'process', 'significantly', 'outperform', 'other', 'Seq2Seq', 'models', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'NNP', 'CC', 'NNP', 'IN', 'JJ', 'NNP', 'NN', 'NN', 'TO', 'VB', 'DT', 'NNP', 'NN', 'NN', 'RB', 'VBZ', 'JJ', 'NNP', 'NNS', '.']",23
semantic_parsing,0,255,"As Component Matching results in shows , all models struggle with WHERE clause prediction the most .","['As', 'Component', 'Matching', 'results', 'in', 'shows', ',', 'all', 'models', 'struggle', 'with', 'WHERE', 'clause', 'prediction', 'the', 'most', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['IN', 'NNP', 'NNP', 'NNS', 'IN', 'NNS', ',', 'DT', 'NNS', 'VBP', 'IN', 'NNP', 'NN', 'NN', 'DT', 'RBS', '.']",17
semantic_parsing,0,258,"In general , the over all performances of all models are low , indicating that our task is challenging and there is still a large room for improvement .","['In', 'general', ',', 'the', 'over', 'all', 'performances', 'of', 'all', 'models', 'are', 'low', ',', 'indicating', 'that', 'our', 'task', 'is', 'challenging', 'and', 'there', 'is', 'still', 'a', 'large', 'room', 'for', 'improvement', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', ',', 'DT', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNS', 'VBP', 'JJ', ',', 'VBG', 'IN', 'PRP$', 'NN', 'VBZ', 'VBG', 'CC', 'EX', 'VBZ', 'RB', 'DT', 'JJ', 'NN', 'IN', 'NN', '.']",29
natural_language_inference,16,4,Natural language sentence matching is a fundamental technology for a variety of tasks .,"['Natural', 'language', 'sentence', 'matching', 'is', 'a', 'fundamental', 'technology', 'for', 'a', 'variety', 'of', 'tasks', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNS', '.']",14
natural_language_inference,16,15,Natural language sentence matching ( NLSM ) is the task of comparing two sentences and identifying the relationship between them .,"['Natural', 'language', 'sentence', 'matching', '(', 'NLSM', ')', 'is', 'the', 'task', 'of', 'comparing', 'two', 'sentences', 'and', 'identifying', 'the', 'relationship', 'between', 'them', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'CD', 'NNS', 'CC', 'VBG', 'DT', 'NN', 'IN', 'PRP', '.']",21
natural_language_inference,16,17,"For example , in a paraphrase identification task , NLSM is used to determine whether two sentences are paraphrase or not .","['For', 'example', ',', 'in', 'a', 'paraphrase', 'identification', 'task', ',', 'NLSM', 'is', 'used', 'to', 'determine', 'whether', 'two', 'sentences', 'are', 'paraphrase', 'or', 'not', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ',', 'IN', 'DT', 'NN', 'NN', 'NN', ',', 'NNP', 'VBZ', 'VBN', 'TO', 'VB', 'IN', 'CD', 'NNS', 'VBP', 'NN', 'CC', 'RB', '.']",22
natural_language_inference,16,32,"In this paper , to tackle these limitations , we propose a bilateral multi-perspective matching ( BiMPM ) model for NLSM tasks .","['In', 'this', 'paper', ',', 'to', 'tackle', 'these', 'limitations', ',', 'we', 'propose', 'a', 'bilateral', 'multi-perspective', 'matching', '(', 'BiMPM', ')', 'model', 'for', 'NLSM', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'TO', 'VB', 'DT', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', '(', 'NNP', ')', 'NN', 'IN', 'NNP', 'NNS', '.']",23
natural_language_inference,16,33,"Our model essentially belongs to the "" matching aggregation "" framework .","['Our', 'model', 'essentially', 'belongs', 'to', 'the', '""', 'matching', 'aggregation', '""', 'framework', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'RB', 'VBZ', 'TO', 'DT', 'NN', 'VBG', 'NN', 'NNP', 'NN', '.']",12
natural_language_inference,16,39,"Then , another BiLSTM layer is utilized to aggregate the matching results into a fixed - length matching vector .","['Then', ',', 'another', 'BiLSTM', 'layer', 'is', 'utilized', 'to', 'aggregate', 'the', 'matching', 'results', 'into', 'a', 'fixed', '-', 'length', 'matching', 'vector', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NNP', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'JJ', ':', 'NN', 'JJ', 'NN', '.']",20
natural_language_inference,16,40,"Finally , based on the matching vector , a decision is made through a fully connected layer .","['Finally', ',', 'based', 'on', 'the', 'matching', 'vector', ',', 'a', 'decision', 'is', 'made', 'through', 'a', 'fully', 'connected', 'layer', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'VBN', 'IN', 'DT', 'NN', 'NN', ',', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'RB', 'VBN', 'NN', '.']",18
natural_language_inference,16,130,We initialize word embeddings in the word representation layer with the 300 - dimensional GloVe word vectors pretrained from the 840B Common Crawl corpus .,"['We', 'initialize', 'word', 'embeddings', 'in', 'the', 'word', 'representation', 'layer', 'with', 'the', '300', '-', 'dimensional', 'GloVe', 'word', 'vectors', 'pretrained', 'from', 'the', '840B', 'Common', 'Crawl', 'corpus', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'NN', 'IN', 'DT', 'CD', ':', 'JJ', 'NNP', 'NN', 'NNS', 'VBD', 'IN', 'DT', 'CD', 'NNP', 'NNP', 'NN', '.']",25
natural_language_inference,16,131,"For the out - of - vocabulary ( OOV ) words , we initialize the word embeddings randomly .","['For', 'the', 'out', '-', 'of', '-', 'vocabulary', '(', 'OOV', ')', 'words', ',', 'we', 'initialize', 'the', 'word', 'embeddings', 'randomly', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O']","['IN', 'DT', 'RP', ':', 'IN', ':', 'NN', '(', 'NNP', ')', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBZ', 'RB', '.']",19
natural_language_inference,16,132,"For the charactercomposed embeddings , we initialize each character as a 20 - dimensional vector , and compose each word into a 50 dimensional vector with a LSTM layer .","['For', 'the', 'charactercomposed', 'embeddings', ',', 'we', 'initialize', 'each', 'character', 'as', 'a', '20', '-', 'dimensional', 'vector', ',', 'and', 'compose', 'each', 'word', 'into', 'a', '50', 'dimensional', 'vector', 'with', 'a', 'LSTM', 'layer', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'CD', ':', 'JJ', 'NN', ',', 'CC', 'VB', 'DT', 'NN', 'IN', 'DT', 'CD', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",30
natural_language_inference,16,133,We set the hidden size as 100 for all BiLSTM layers .,"['We', 'set', 'the', 'hidden', 'size', 'as', '100', 'for', 'all', 'BiLSTM', 'layers', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'CD', 'IN', 'DT', 'NNP', 'NNS', '.']",12
natural_language_inference,16,134,"We apply dropout to every layers in , and set the dropout ratio as 0.1 .","['We', 'apply', 'dropout', 'to', 'every', 'layers', 'in', ',', 'and', 'set', 'the', 'dropout', 'ratio', 'as', '0.1', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'RB', 'TO', 'DT', 'NNS', 'IN', ',', 'CC', 'VBD', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",16
natural_language_inference,16,135,"To train the model , we minimize the cross entropy of the training set , and use the ADAM optimizer [ Kingma and Ba , 2014 ] to update parameters .","['To', 'train', 'the', 'model', ',', 'we', 'minimize', 'the', 'cross', 'entropy', 'of', 'the', 'training', 'set', ',', 'and', 'use', 'the', 'ADAM', 'optimizer', '[', 'Kingma', 'and', 'Ba', ',', '2014', ']', 'to', 'update', 'parameters', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'CC', 'VB', 'DT', 'NNP', 'NN', 'NNP', 'NNP', 'CC', 'NNP', ',', 'CD', 'NN', 'TO', 'VB', 'NNS', '.']",31
natural_language_inference,16,136,We set the learning rate as 0.001 .,"['We', 'set', 'the', 'learning', 'rate', 'as', '0.001', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",8
natural_language_inference,16,137,"During training , we do not update the pre-trained word embeddings .","['During', 'training', ',', 'we', 'do', 'not', 'update', 'the', 'pre-trained', 'word', 'embeddings', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'RB', 'VB', 'DT', 'JJ', 'NN', 'NNS', '.']",12
natural_language_inference,16,163,"In this Sub-section , we compare our model with state - of - theart models on the paraphrase identification task .","['In', 'this', 'Sub-section', ',', 'we', 'compare', 'our', 'model', 'with', 'state', '-', 'of', '-', 'theart', 'models', 'on', 'the', 'paraphrase', 'identification', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'PRP$', 'NN', 'IN', 'NN', ':', 'IN', ':', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'NN', '.']",21
natural_language_inference,16,164,"We still experiment on the "" Quora Question Pairs "" dataset , and use the same dataset partition as Sub-section 4.2 .","['We', 'still', 'experiment', 'on', 'the', '""', 'Quora', 'Question', 'Pairs', '""', 'dataset', ',', 'and', 'use', 'the', 'same', 'dataset', 'partition', 'as', 'Sub-section', '4.2', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NN', ',', 'CC', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'CD', '.']",22
natural_language_inference,16,167,"First , under the Siamese framework , we implement two baseline models : "" Siamese - CNN "" and "" Siamese - LSTM "" .","['First', ',', 'under', 'the', 'Siamese', 'framework', ',', 'we', 'implement', 'two', 'baseline', 'models', ':', '""', 'Siamese', '-', 'CNN', '""', 'and', '""', 'Siamese', '-', 'LSTM', '""', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['RB', ',', 'IN', 'DT', 'NNP', 'NN', ',', 'PRP', 'VBP', 'CD', 'NN', 'NNS', ':', 'VB', 'JJ', ':', 'NNP', 'NNP', 'CC', 'NNP', 'NNP', ':', 'NNP', 'NNP', '.']",25
natural_language_inference,16,171,"Second , based on the two baseline models , we implement two more baseline models "" Multi - Perspective - CNN "" and "" Multi - Perspective - LSTM "" .","['Second', ',', 'based', 'on', 'the', 'two', 'baseline', 'models', ',', 'we', 'implement', 'two', 'more', 'baseline', 'models', '""', 'Multi', '-', 'Perspective', '-', 'CNN', '""', 'and', '""', 'Multi', '-', 'Perspective', '-', 'LSTM', '""', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['NNP', ',', 'VBN', 'IN', 'DT', 'CD', 'NN', 'NNS', ',', 'PRP', 'VBP', 'CD', 'JJR', 'NN', 'NNS', 'VBP', 'NNP', ':', 'NNP', ':', 'NNP', 'NNP', 'CC', 'NNP', 'NNP', ':', 'NNP', ':', 'NNP', 'NNP', '.']",31
natural_language_inference,16,173,"Third , we re-implement the "" L.D.C. "" model proposed by , which is a model under the "" matchingaggregation "" framework and acquires the state - of - the - art performance on several tasks .","['Third', ',', 'we', 're-implement', 'the', '""', 'L.D.C.', '""', 'model', 'proposed', 'by', ',', 'which', 'is', 'a', 'model', 'under', 'the', '""', 'matchingaggregation', '""', 'framework', 'and', 'acquires', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'on', 'several', 'tasks', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O']","['NNP', ',', 'PRP', 'VBP', 'DT', 'NNP', 'NNP', 'NNP', 'NN', 'VBN', 'IN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NNP', 'NN', 'CC', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'JJ', 'NNS', '.']",37
natural_language_inference,16,175,"We can see that "" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" ) works much better than "" Siamese - CNN "" ( or "" Siamese - LSTM "" ) , which further indicates that our multi-perspective cosine matching func - Models Accuracy 81.4 82.1 83.5 85.0 85.1 86.1 86.3 86.8 87.3 87.5 tion ( Eq. ) is very effective for matching vectors .","['We', 'can', 'see', 'that', '""', 'Multi', '-', 'Perspective', '-', 'CNN', '""', '(', 'or', '""', 'Multi-', 'Perspective', '-', 'LSTM', '""', ')', 'works', 'much', 'better', 'than', '""', 'Siamese', '-', 'CNN', '""', '(', 'or', '""', 'Siamese', '-', 'LSTM', '""', ')', ',', 'which', 'further', 'indicates', 'that', 'our', 'multi-perspective', 'cosine', 'matching', 'func', '-', 'Models', 'Accuracy', '81.4', '82.1', '83.5', '85.0', '85.1', '86.1', '86.3', '86.8', '87.3', '87.5', 'tion', '(', 'Eq.', ')', 'is', 'very', 'effective', 'for', 'matching', 'vectors', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'MD', 'VB', 'DT', 'NNP', 'NNP', ':', 'NNP', ':', 'NNP', 'NNP', '(', 'CC', 'VB', 'NNP', 'NNP', ':', 'NNP', 'NNP', ')', 'VBZ', 'RB', 'JJR', 'IN', 'JJR', 'JJ', ':', 'NN', 'NN', '(', 'CC', 'VB', 'JJ', ':', 'NNP', 'NNP', ')', ',', 'WDT', 'RB', 'VBZ', 'IN', 'PRP$', 'JJ', 'NN', 'VBG', 'JJ', ':', 'NNS', 'VBP', 'CD', 'CD', 'CD', 'CD', 'CD', 'CD', 'CD', 'CD', 'CD', 'CD', 'NN', '(', 'NNP', ')', 'VBZ', 'RB', 'JJ', 'IN', 'VBG', 'NNS', '.']",71
natural_language_inference,16,176,"Our "" BiMPM "" model outperforms the "" L.D.C. "" model by more than two percent .","['Our', '""', 'BiMPM', '""', 'model', 'outperforms', 'the', '""', 'L.D.C.', '""', 'model', 'by', 'more', 'than', 'two', 'percent', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'NNP', 'NNP', 'NN', 'VBZ', 'DT', 'NNP', 'NNP', 'NNP', 'NN', 'IN', 'JJR', 'IN', 'CD', 'NN', '.']",17
natural_language_inference,16,179,"In this Sub-section , we evaluate our model on the natural language inference task over the SNLI dataset .","['In', 'this', 'Sub-section', ',', 'we', 'evaluate', 'our', 'model', 'on', 'the', 'natural', 'language', 'inference', 'task', 'over', 'the', 'SNLI', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'PRP$', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",19
natural_language_inference,16,183,"First , we can see that "" Only P ? Q "" works significantly better than "" Only P ? Q "" , which tells us that , for natural language inference , matching the hypothesis against the premise is more effective than the other way around .","['First', ',', 'we', 'can', 'see', 'that', '""', 'Only', 'P', '?', 'Q', '""', 'works', 'significantly', 'better', 'than', '""', 'Only', 'P', '?', 'Q', '""', ',', 'which', 'tells', 'us', 'that', ',', 'for', 'natural', 'language', 'inference', ',', 'matching', 'the', 'hypothesis', 'against', 'the', 'premise', 'is', 'more', 'effective', 'than', 'the', 'other', 'way', 'around', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'MD', 'VB', 'DT', 'NNP', 'RB', 'NNP', '.', 'NNP', 'NNP', 'VBZ', 'RB', 'JJR', 'IN', 'JJR', 'RB', 'NNP', '.', 'NNP', 'NNP', ',', 'WDT', 'VBZ', 'PRP', 'IN', ',', 'IN', 'JJ', 'NN', 'NN', ',', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'RBR', 'JJ', 'IN', 'DT', 'JJ', 'NN', 'IN', '.']",48
natural_language_inference,16,184,"Second , our "" BiMPM "" model works much better than "" Only P ? Q "" , which reveals that matching premise against the hypothesis can also bring some benefits .","['Second', ',', 'our', '""', 'BiMPM', '""', 'model', 'works', 'much', 'better', 'than', '""', 'Only', 'P', '?', 'Q', '""', ',', 'which', 'reveals', 'that', 'matching', 'premise', 'against', 'the', 'hypothesis', 'can', 'also', 'bring', 'some', 'benefits', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', ',', 'PRP$', 'JJ', 'NNP', 'NNP', 'NN', 'VBZ', 'RB', 'JJR', 'IN', 'JJR', 'RB', 'NNP', '.', 'NNP', 'NNP', ',', 'WDT', 'VBZ', 'IN', 'VBG', 'NN', 'IN', 'DT', 'NN', 'MD', 'RB', 'VB', 'DT', 'NNS', '.']",32
natural_language_inference,16,185,"Finally , comparing our models with all the state - of - the - art models , we can observe that our single model "" BiMPM "" is on par with the state - of - the - art single models , and our ' BiMPM ( Ensemble ) "" works much better than "" ( Ensemble ) "" .","['Finally', ',', 'comparing', 'our', 'models', 'with', 'all', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', ',', 'we', 'can', 'observe', 'that', 'our', 'single', 'model', '""', 'BiMPM', '""', 'is', 'on', 'par', 'with', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'single', 'models', ',', 'and', 'our', ""'"", 'BiMPM', '(', 'Ensemble', ')', '""', 'works', 'much', 'better', 'than', '""', '(', 'Ensemble', ')', '""', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'O', 'O', 'O']","['RB', ',', 'VBG', 'PRP$', 'NNS', 'IN', 'PDT', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', ',', 'PRP', 'MD', 'VB', 'DT', 'PRP$', 'JJ', 'NN', 'NNP', 'NNP', 'NNP', 'VBZ', 'IN', 'NN', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'JJ', 'NNS', ',', 'CC', 'PRP$', ""''"", 'NNP', '(', 'NNP', ')', 'VBP', 'VBZ', 'RB', 'JJR', 'IN', 'NN', '(', 'JJ', ')', 'NN', '.']",60
natural_language_inference,16,186,"Therefore , our models achieve the state - of - the - art performance in both single and ensemble scenarios for the natural language inference task .","['Therefore', ',', 'our', 'models', 'achieve', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'in', 'both', 'single', 'and', 'ensemble', 'scenarios', 'for', 'the', 'natural', 'language', 'inference', 'task', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'NNS', 'VBP', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'DT', 'JJ', 'CC', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', '.']",27
natural_language_inference,16,188,"In this Sub-section , we study the effectiveness of our model for answer sentence selection tasks .","['In', 'this', 'Sub-section', ',', 'we', 'study', 'the', 'effectiveness', 'of', 'our', 'model', 'for', 'answer', 'sentence', 'selection', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'IN', 'JJR', 'NN', 'NN', 'NNS', '.']",17
natural_language_inference,16,190,We experiment on two datasets : TREC - QA and WikiQA .,"['We', 'experiment', 'on', 'two', 'datasets', ':', 'TREC', '-', 'QA', 'and', 'WikiQA', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O']","['PRP', 'VBP', 'IN', 'CD', 'NNS', ':', 'NNP', ':', 'NN', 'CC', 'NNP', '.']",12
natural_language_inference,16,192,We can see that the performance from our model is on par with the state - of - the - art models .,"['We', 'can', 'see', 'that', 'the', 'performance', 'from', 'our', 'model', 'is', 'on', 'par', 'with', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'IN', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'VBZ', 'IN', 'NN', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', '.']",23
natural_language_inference,77,2,Making Neural QA as Simple as Possible but not Simpler,"['Making', 'Neural', 'QA', 'as', 'Simple', 'as', 'Possible', 'but', 'not', 'Simpler']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', 'NNP', 'RB', 'NNP', 'IN', 'NNP', 'CC', 'RB', 'NNP']",10
natural_language_inference,77,4,Recent development of large - scale question answering ( QA ) datasets triggered a substantial amount of research into end - toend neural architectures for QA .,"['Recent', 'development', 'of', 'large', '-', 'scale', 'question', 'answering', '(', 'QA', ')', 'datasets', 'triggered', 'a', 'substantial', 'amount', 'of', 'research', 'into', 'end', '-', 'toend', 'neural', 'architectures', 'for', 'QA', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']","['JJ', 'NN', 'IN', 'JJ', ':', 'NN', 'NN', 'VBG', '(', 'NNP', ')', 'VBZ', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'JJ', ':', 'JJ', 'JJ', 'NNS', 'IN', 'NNP', '.']",27
natural_language_inference,77,11,Question answering is an important end - user task at the intersection of natural language processing ( NLP ) and information retrieval ( IR ) .,"['Question', 'answering', 'is', 'an', 'important', 'end', '-', 'user', 'task', 'at', 'the', 'intersection', 'of', 'natural', 'language', 'processing', '(', 'NLP', ')', 'and', 'information', 'retrieval', '(', 'IR', ')', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', ':', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'NN', 'NN', '(', 'NNP', ')', '.']",26
natural_language_inference,77,28,"In particular , we develop a simple neural , bag - of - words ( BoW ) - and a recurrent neural network ( RNN ) baseline , namely FastQA .","['In', 'particular', ',', 'we', 'develop', 'a', 'simple', 'neural', ',', 'bag', '-', 'of', '-', 'words', '(', 'BoW', ')', '-', 'and', 'a', 'recurrent', 'neural', 'network', '(', 'RNN', ')', 'baseline', ',', 'namely', 'FastQA', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O']","['IN', 'JJ', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', ',', 'JJ', ':', 'IN', ':', 'NNS', '(', 'NNP', ')', ':', 'CC', 'DT', 'JJ', 'JJ', 'NN', '(', 'NNP', ')', 'NN', ',', 'RB', 'NNP', '.']",31
natural_language_inference,77,29,"Crucially , both models do not make use of a complex interaction layer but model interaction between question and context only through computable features on the word level .","['Crucially', ',', 'both', 'models', 'do', 'not', 'make', 'use', 'of', 'a', 'complex', 'interaction', 'layer', 'but', 'model', 'interaction', 'between', 'question', 'and', 'context', 'only', 'through', 'computable', 'features', 'on', 'the', 'word', 'level', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'DT', 'NNS', 'VBP', 'RB', 'VB', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'CC', 'NN', 'NN', 'IN', 'NN', 'CC', 'NN', 'RB', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",29
natural_language_inference,77,141,BoW Model,"['BoW', 'Model']","['B-n', 'I-n']","['NNP', 'NNP']",2
natural_language_inference,77,142,The BoW model is trained on spans up to length 10 to keep the computation tractable .,"['The', 'BoW', 'model', 'is', 'trained', 'on', 'spans', 'up', 'to', 'length', '10', 'to', 'keep', 'the', 'computation', 'tractable', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NN', 'VBZ', 'VBN', 'IN', 'NNS', 'RB', 'TO', 'VB', 'CD', 'TO', 'VB', 'DT', 'NN', 'NN', '.']",17
natural_language_inference,77,144,As pre-processing steps we lowercase all inputs and tokenize it using spacy 4 .,"['As', 'pre-processing', 'steps', 'we', 'lowercase', 'all', 'inputs', 'and', 'tokenize', 'it', 'using', 'spacy', '4', '.']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-p', 'B-n', 'O', 'O']","['IN', 'JJ', 'NNS', 'PRP', 'VBP', 'DT', 'NNS', 'CC', 'VB', 'PRP', 'VBG', 'JJ', 'CD', '.']",14
natural_language_inference,77,145,The binary word in question feature is computed on lemmas provided by spacy and restricted to alphanumeric words that are not stopwords .,"['The', 'binary', 'word', 'in', 'question', 'feature', 'is', 'computed', 'on', 'lemmas', 'provided', 'by', 'spacy', 'and', 'restricted', 'to', 'alphanumeric', 'words', 'that', 'are', 'not', 'stopwords', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'NN', 'VBN', 'IN', 'NN', 'CC', 'VBN', 'TO', 'VB', 'NNS', 'WDT', 'VBP', 'RB', 'NNS', '.']",23
natural_language_inference,77,146,"Throughout all experiments we use a hidden dimensionality of n = 150 , dropout at the input embeddings with the same mask for all words and a rate of 0.2 and 300 - dimensional fixed word - embeddings from Glove .","['Throughout', 'all', 'experiments', 'we', 'use', 'a', 'hidden', 'dimensionality', 'of', 'n', '=', '150', ',', 'dropout', 'at', 'the', 'input', 'embeddings', 'with', 'the', 'same', 'mask', 'for', 'all', 'words', 'and', 'a', 'rate', 'of', '0.2', 'and', '300', '-', 'dimensional', 'fixed', 'word', '-', 'embeddings', 'from', 'Glove', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NNS', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNP', 'CD', ',', 'NN', 'IN', 'DT', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNS', 'CC', 'DT', 'NN', 'IN', 'CD', 'CC', 'CD', ':', 'JJ', 'VBN', 'NN', ':', 'NNS', 'IN', 'NNP', '.']",41
natural_language_inference,77,147,We employed ADAM for optimization with an initial learning - rate of 10 ?3 which was halved whenever the F 1 measure on the development set dropped between epochs .,"['We', 'employed', 'ADAM', 'for', 'optimization', 'with', 'an', 'initial', 'learning', '-', 'rate', 'of', '10', '?3', 'which', 'was', 'halved', 'whenever', 'the', 'F', '1', 'measure', 'on', 'the', 'development', 'set', 'dropped', 'between', 'epochs', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'NNP', 'IN', 'NN', 'IN', 'DT', 'JJ', 'VBG', ':', 'NN', 'IN', 'CD', 'NN', 'WDT', 'VBD', 'VBN', 'RB', 'DT', 'NNP', 'CD', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBD', 'IN', 'NNS', '.']",30
natural_language_inference,77,148,We used mini-batches of size 32 .,"['We', 'used', 'mini-batches', 'of', 'size', '32', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-n', 'O']","['PRP', 'VBD', 'NNS', 'IN', 'NN', 'CD', '.']",7
natural_language_inference,77,149,FastQA,['FastQA'],['B-n'],['NN'],1
natural_language_inference,77,151,We tokenize the input on whitespaces ( exclusive ) and non-alphanumeric characters ( inclusive ) .,"['We', 'tokenize', 'the', 'input', 'on', 'whitespaces', '(', 'exclusive', ')', 'and', 'non-alphanumeric', 'characters', '(', 'inclusive', ')', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'NNS', '(', 'JJ', ')', 'CC', 'JJ', 'NNS', '(', 'JJ', ')', '.']",16
natural_language_inference,77,152,The binary word in question feature is computed on the words as they appear in context .,"['The', 'binary', 'word', 'in', 'question', 'feature', 'is', 'computed', 'on', 'the', 'words', 'as', 'they', 'appear', 'in', 'context', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NNS', 'IN', 'PRP', 'VBP', 'IN', 'NN', '.']",17
natural_language_inference,77,153,"Throughout all experiments we use a hidden dimensionality of n = 300 , variational dropout at the input embeddings with the same mask for all words and a rate of 0.5 and 300 dimensional fixed word - embeddings from Glove .","['Throughout', 'all', 'experiments', 'we', 'use', 'a', 'hidden', 'dimensionality', 'of', 'n', '=', '300', ',', 'variational', 'dropout', 'at', 'the', 'input', 'embeddings', 'with', 'the', 'same', 'mask', 'for', 'all', 'words', 'and', 'a', 'rate', 'of', '0.5', 'and', '300', 'dimensional', 'fixed', 'word', '-', 'embeddings', 'from', 'Glove', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NNS', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNP', 'CD', ',', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNS', 'CC', 'DT', 'NN', 'IN', 'CD', 'CC', 'CD', 'JJ', 'VBN', 'NN', ':', 'NNS', 'IN', 'NNP', '.']",41
natural_language_inference,77,154,We employed ADAM for optimization with an initial learning - rate of 10 ?3 which was halved whenever the F 1 measure on the development set dropped between checkpoints .,"['We', 'employed', 'ADAM', 'for', 'optimization', 'with', 'an', 'initial', 'learning', '-', 'rate', 'of', '10', '?3', 'which', 'was', 'halved', 'whenever', 'the', 'F', '1', 'measure', 'on', 'the', 'development', 'set', 'dropped', 'between', 'checkpoints', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'NNP', 'IN', 'NN', 'IN', 'DT', 'JJ', 'VBG', ':', 'NN', 'IN', 'CD', 'NN', 'WDT', 'VBD', 'VBN', 'RB', 'DT', 'NNP', 'CD', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBD', 'IN', 'NNS', '.']",30
natural_language_inference,77,175,Our neural BoW baseline achieves good results on both datasets ( Tables 3 and 1 ) 5 .,"['Our', 'neural', 'BoW', 'baseline', 'achieves', 'good', 'results', 'on', 'both', 'datasets', '(', 'Tables', '3', 'and', '1', ')', '5', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'JJ', 'NNP', 'NN', 'NNS', 'JJ', 'NNS', 'IN', 'DT', 'NNS', '(', 'NNP', 'CD', 'CC', 'CD', ')', 'CD', '.']",18
natural_language_inference,77,176,"For instance , it outperforms a feature rich logistic - regression baseline on the SQuAD development set and nearly reaches the BiLSTM baseline system ( i.e. , FastQA without character embeddings and features ) .","['For', 'instance', ',', 'it', 'outperforms', 'a', 'feature', 'rich', 'logistic', '-', 'regression', 'baseline', 'on', 'the', 'SQuAD', 'development', 'set', 'and', 'nearly', 'reaches', 'the', 'BiLSTM', 'baseline', 'system', '(', 'i.e.', ',', 'FastQA', 'without', 'character', 'embeddings', 'and', 'features', ')', '.']","['O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ',', 'PRP', 'VBZ', 'DT', 'NN', 'JJ', 'JJ', ':', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NN', 'CC', 'RB', 'VBZ', 'DT', 'NNP', 'NN', 'NN', '(', 'FW', ',', 'NNP', 'IN', 'NN', 'NNS', 'CC', 'NNS', ')', '.']",35
natural_language_inference,77,179,It is very competitive to previously established stateof - the - art results on the two datasets and even improves those for News QA .,"['It', 'is', 'very', 'competitive', 'to', 'previously', 'established', 'stateof', '-', 'the', '-', 'art', 'results', 'on', 'the', 'two', 'datasets', 'and', 'even', 'improves', 'those', 'for', 'News', 'QA', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBZ', 'RB', 'JJ', 'TO', 'RB', 'VBN', 'NN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'CD', 'NNS', 'CC', 'RB', 'NNS', 'DT', 'IN', 'NNP', 'NNP', '.']",25
natural_language_inference,75,4,Directly reading documents and being able to answer questions from them is an unsolved challenge .,"['Directly', 'reading', 'documents', 'and', 'being', 'able', 'to', 'answer', 'questions', 'from', 'them', 'is', 'an', 'unsolved', 'challenge', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'VBG', 'NNS', 'CC', 'VBG', 'JJ', 'TO', 'VB', 'NNS', 'IN', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', '.']",16
natural_language_inference,75,5,"To avoid its inherent difficulty , question answering ( QA ) has been directed towards using Knowledge Bases ( KBs ) instead , which has proven effective .","['To', 'avoid', 'its', 'inherent', 'difficulty', ',', 'question', 'answering', '(', 'QA', ')', 'has', 'been', 'directed', 'towards', 'using', 'Knowledge', 'Bases', '(', 'KBs', ')', 'instead', ',', 'which', 'has', 'proven', 'effective', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'PRP$', 'JJ', 'NN', ',', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'VBN', 'VBN', 'NNS', 'VBG', 'NNP', 'NNP', '(', 'NNP', ')', 'RB', ',', 'WDT', 'VBZ', 'VBN', 'JJ', '.']",28
natural_language_inference,75,8,"To compare using KBs , information extraction or Wikipedia documents directly in a single framework we construct an analysis tool , WIKIMOVIES , a QA dataset that contains raw text alongside a preprocessed KB , in the domain of movies .","['To', 'compare', 'using', 'KBs', ',', 'information', 'extraction', 'or', 'Wikipedia', 'documents', 'directly', 'in', 'a', 'single', 'framework', 'we', 'construct', 'an', 'analysis', 'tool', ',', 'WIKIMOVIES', ',', 'a', 'QA', 'dataset', 'that', 'contains', 'raw', 'text', 'alongside', 'a', 'preprocessed', 'KB', ',', 'in', 'the', 'domain', 'of', 'movies', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'VBG', 'NNP', ',', 'NN', 'NN', 'CC', 'NNP', 'NNS', 'RB', 'IN', 'DT', 'JJ', 'NN', 'PRP', 'VBP', 'DT', 'NN', 'NN', ',', 'NNP', ',', 'DT', 'NNP', 'NN', 'WDT', 'VBZ', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NNP', ',', 'IN', 'DT', 'NN', 'IN', 'NNS', '.']",41
natural_language_inference,75,25,"In this work we propose the Key - Value Memory Network ( KV - MemNN ) , a new neural network architecture that generalizes the original Memory Network and can work with either knowledge source .","['In', 'this', 'work', 'we', 'propose', 'the', 'Key', '-', 'Value', 'Memory', 'Network', '(', 'KV', '-', 'MemNN', ')', ',', 'a', 'new', 'neural', 'network', 'architecture', 'that', 'generalizes', 'the', 'original', 'Memory', 'Network', 'and', 'can', 'work', 'with', 'either', 'knowledge', 'source', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'DT', 'NNP', ':', 'NNP', 'NNP', 'NNP', '(', 'NNP', ':', 'NN', ')', ',', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'NNP', 'NNP', 'CC', 'MD', 'VB', 'IN', 'DT', 'NN', 'NN', '.']",36
natural_language_inference,75,26,The KV - MemNN performs QA by first storing facts in a key - value structured memory before reasoning on them in order to predict an answer .,"['The', 'KV', '-', 'MemNN', 'performs', 'QA', 'by', 'first', 'storing', 'facts', 'in', 'a', 'key', '-', 'value', 'structured', 'memory', 'before', 'reasoning', 'on', 'them', 'in', 'order', 'to', 'predict', 'an', 'answer', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', ':', 'NN', 'NNS', 'NNP', 'IN', 'JJ', 'VBG', 'NNS', 'IN', 'DT', 'JJ', ':', 'NN', 'VBD', 'NN', 'IN', 'VBG', 'IN', 'PRP', 'IN', 'NN', 'TO', 'VB', 'DT', 'NN', '.']",28
natural_language_inference,75,27,"The memory is designed so that the model learns to use keys to address relevant memories with respect to the question , whose corresponding values are subsequently returned .","['The', 'memory', 'is', 'designed', 'so', 'that', 'the', 'model', 'learns', 'to', 'use', 'keys', 'to', 'address', 'relevant', 'memories', 'with', 'respect', 'to', 'the', 'question', ',', 'whose', 'corresponding', 'values', 'are', 'subsequently', 'returned', '.']","['O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'RB', 'IN', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'NNS', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'NN', 'TO', 'DT', 'NN', ',', 'WP$', 'VBG', 'NNS', 'VBP', 'RB', 'VBN', '.']",29
natural_language_inference,75,28,"This structure allows the model to encode prior knowledge for the considered task and to leverage possibly complex transforms between keys and values , while still being trained using standard backpropagation via stochastic gradient descent .","['This', 'structure', 'allows', 'the', 'model', 'to', 'encode', 'prior', 'knowledge', 'for', 'the', 'considered', 'task', 'and', 'to', 'leverage', 'possibly', 'complex', 'transforms', 'between', 'keys', 'and', 'values', ',', 'while', 'still', 'being', 'trained', 'using', 'standard', 'backpropagation', 'via', 'stochastic', 'gradient', 'descent', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'DT', 'NN', 'TO', 'VB', 'JJ', 'NN', 'IN', 'DT', 'VBN', 'NN', 'CC', 'TO', 'VB', 'RB', 'JJ', 'NNS', 'IN', 'NNS', 'CC', 'NNS', ',', 'IN', 'RB', 'VBG', 'VBN', 'VBG', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",36
natural_language_inference,75,161,WikiMovies,['WikiMovies'],['B-n'],['NNS'],1
natural_language_inference,75,170,"However , Key - Value Memory Networks outperform all other methods on all three data source types .","['However', ',', 'Key', '-', 'Value', 'Memory', 'Networks', 'outperform', 'all', 'other', 'methods', 'on', 'all', 'three', 'data', 'source', 'types', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'CD', 'NNS', 'NN', 'NNS', '.']",18
natural_language_inference,75,171,"Reading from Wikipedia documents directly ( Doc ) outperforms an IE - based KB ( IE ) , which is an encouraging result towards automated machine reading though a gap to a humanannotated KB still remains ( 93.9 vs. 76.2 ) .","['Reading', 'from', 'Wikipedia', 'documents', 'directly', '(', 'Doc', ')', 'outperforms', 'an', 'IE', '-', 'based', 'KB', '(', 'IE', ')', ',', 'which', 'is', 'an', 'encouraging', 'result', 'towards', 'automated', 'machine', 'reading', 'though', 'a', 'gap', 'to', 'a', 'humanannotated', 'KB', 'still', 'remains', '(', '93.9', 'vs.', '76.2', ')', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'IN', 'NNP', 'NNS', 'RB', '(', 'NNP', ')', 'VBZ', 'DT', 'NNP', ':', 'VBN', 'NNP', '(', 'NNP', ')', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'NNS', 'VBN', 'NN', 'VBG', 'IN', 'DT', 'NN', 'TO', 'DT', 'VBN', 'NNP', 'RB', 'VBZ', '(', 'CD', 'FW', 'CD', ')', '.']",42
natural_language_inference,75,187,WikiQA,['WikiQA'],['B-n'],['NN'],1
natural_language_inference,75,202,"Key - Value Memory Networks outperform a large set of other methods , although the results of the L.D.C. method of are very similar .","['Key', '-', 'Value', 'Memory', 'Networks', 'outperform', 'a', 'large', 'set', 'of', 'other', 'methods', ',', 'although', 'the', 'results', 'of', 'the', 'L.D.C.', 'method', 'of', 'are', 'very', 'similar', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NNP', 'NNP', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', ',', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'IN', 'VBP', 'RB', 'JJ', '.']",25
natural_language_inference,59,2,Neural Paraphrase Identification of Questions with Noisy Pretraining,"['Neural', 'Paraphrase', 'Identification', 'of', 'Questions', 'with', 'Noisy', 'Pretraining']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', 'IN', 'NNP', 'VBG']",8
natural_language_inference,59,4,We present a solution to the problem of paraphrase identification of questions .,"['We', 'present', 'a', 'solution', 'to', 'the', 'problem', 'of', 'paraphrase', 'identification', 'of', 'questions', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'TO', 'DT', 'NN', 'IN', 'NN', 'NN', 'IN', 'NNS', '.']",13
natural_language_inference,59,8,Question paraphrase identification is a widely useful NLP application .,"['Question', 'paraphrase', 'identification', 'is', 'a', 'widely', 'useful', 'NLP', 'application', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'NN', 'VBZ', 'DT', 'RB', 'JJ', 'NNP', 'NN', '.']",10
natural_language_inference,59,14,"We examine a simple model family , the decomposable attention model of , that has shown promise in modeling natural language inference and has inspired recent work on similar tasks .","['We', 'examine', 'a', 'simple', 'model', 'family', ',', 'the', 'decomposable', 'attention', 'model', 'of', ',', 'that', 'has', 'shown', 'promise', 'in', 'modeling', 'natural', 'language', 'inference', 'and', 'has', 'inspired', 'recent', 'work', 'on', 'similar', 'tasks', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', ',', 'DT', 'JJ', 'NN', 'NN', 'IN', ',', 'WDT', 'VBZ', 'VBN', 'NN', 'IN', 'VBG', 'JJ', 'NN', 'NN', 'CC', 'VBZ', 'VBN', 'JJ', 'NN', 'IN', 'JJ', 'NNS', '.']",31
natural_language_inference,59,16,"First , to mitigate data sparsity , we modify the input representation of the decomposable attention model to use sums of character n-gram embeddings instead of word embeddings .","['First', ',', 'to', 'mitigate', 'data', 'sparsity', ',', 'we', 'modify', 'the', 'input', 'representation', 'of', 'the', 'decomposable', 'attention', 'model', 'to', 'use', 'sums', 'of', 'character', 'n-gram', 'embeddings', 'instead', 'of', 'word', 'embeddings', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['RB', ',', 'TO', 'VB', 'NNS', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'NNS', 'IN', 'NN', 'JJ', 'NNS', 'RB', 'IN', 'NN', 'NNS', '.']",29
natural_language_inference,59,18,"Second , to significantly improve our model performance , we pretrain all our model parameters on the noisy , automatically collected question - paraphrase corpus Paralex , followed by fine - tuning the parameters on the Quora dataset .","['Second', ',', 'to', 'significantly', 'improve', 'our', 'model', 'performance', ',', 'we', 'pretrain', 'all', 'our', 'model', 'parameters', 'on', 'the', 'noisy', ',', 'automatically', 'collected', 'question', '-', 'paraphrase', 'corpus', 'Paralex', ',', 'followed', 'by', 'fine', '-', 'tuning', 'the', 'parameters', 'on', 'the', 'Quora', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['JJ', ',', 'TO', 'RB', 'VB', 'PRP$', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'PRP$', 'NN', 'NNS', 'IN', 'DT', 'NN', ',', 'RB', 'VBN', 'NN', ':', 'NN', 'NN', 'NNP', ',', 'VBN', 'IN', 'JJ', ':', 'VBG', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'NN', '.']",39
natural_language_inference,59,99,"We tuned the following hyperparameters by grid search on the development set ( settings for our best model are in parenthesis ) : embedding dimension ( 300 ) , shape of all feedforward networks ( two layers with 400 and 200 width ) , character n -gram sizes ( 5 ) , context size ( 1 ) , learning rate ( 0.1 for both pretraining and tuning ) , batch size ( 256 for pretraining and 64 for tuning ) , dropout ratio ( 0.1 for tuning ) and prediction threshold ( positive paraphrase for a score ? 0.3 ) .","['We', 'tuned', 'the', 'following', 'hyperparameters', 'by', 'grid', 'search', 'on', 'the', 'development', 'set', '(', 'settings', 'for', 'our', 'best', 'model', 'are', 'in', 'parenthesis', ')', ':', 'embedding', 'dimension', '(', '300', ')', ',', 'shape', 'of', 'all', 'feedforward', 'networks', '(', 'two', 'layers', 'with', '400', 'and', '200', 'width', ')', ',', 'character', 'n', '-gram', 'sizes', '(', '5', ')', ',', 'context', 'size', '(', '1', ')', ',', 'learning', 'rate', '(', '0.1', 'for', 'both', 'pretraining', 'and', 'tuning', ')', ',', 'batch', 'size', '(', '256', 'for', 'pretraining', 'and', '64', 'for', 'tuning', ')', ',', 'dropout', 'ratio', '(', '0.1', 'for', 'tuning', ')', 'and', 'prediction', 'threshold', '(', 'positive', 'paraphrase', 'for', 'a', 'score', '?', '0.3', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBD', 'DT', 'VBG', 'NNS', 'IN', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '(', 'NNS', 'IN', 'PRP$', 'JJS', 'NN', 'VBP', 'IN', 'NN', ')', ':', 'VBG', 'NN', '(', 'CD', ')', ',', 'NN', 'IN', 'DT', 'JJ', 'NNS', '(', 'CD', 'NNS', 'IN', 'CD', 'CC', 'CD', 'NN', ')', ',', 'JJR', 'JJ', 'NN', 'NNS', '(', 'CD', ')', ',', 'JJ', 'NN', '(', 'CD', ')', ',', 'VBG', 'NN', '(', 'CD', 'IN', 'DT', 'NN', 'CC', 'NN', ')', ',', 'NN', 'NN', '(', 'CD', 'IN', 'NN', 'CC', 'CD', 'IN', 'VBG', ')', ',', 'RB', 'NN', '(', 'CD', 'IN', 'VBG', ')', 'CC', 'NN', 'NN', '(', 'JJ', 'NN', 'IN', 'DT', 'NN', '.', 'CD', ')', '.']",101
natural_language_inference,59,115,"We observe that the simple FFNN baselines work better than more complex Siamese and Multi - Perspective CNN or LSTM models , more so if character n-gram based embeddings are used .","['We', 'observe', 'that', 'the', 'simple', 'FFNN', 'baselines', 'work', 'better', 'than', 'more', 'complex', 'Siamese', 'and', 'Multi', '-', 'Perspective', 'CNN', 'or', 'LSTM', 'models', ',', 'more', 'so', 'if', 'character', 'n-gram', 'based', 'embeddings', 'are', 'used', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'DT', 'JJ', 'NNP', 'NNS', 'VBP', 'JJR', 'IN', 'RBR', 'JJ', 'NNP', 'CC', 'NNP', ':', 'JJ', 'NNP', 'CC', 'NNP', 'NNS', ',', 'RBR', 'RB', 'IN', 'JJR', 'NN', 'VBN', 'NNS', 'VBP', 'VBN', '.']",32
natural_language_inference,59,116,"Our basic decomposable attention model DECATT word without pre-trained embeddings is better than most of the models , all of which used GloVe embeddings .","['Our', 'basic', 'decomposable', 'attention', 'model', 'DECATT', 'word', 'without', 'pre-trained', 'embeddings', 'is', 'better', 'than', 'most', 'of', 'the', 'models', ',', 'all', 'of', 'which', 'used', 'GloVe', 'embeddings', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['PRP$', 'JJ', 'JJ', 'NN', 'NN', 'NNP', 'NN', 'IN', 'JJ', 'NNS', 'VBZ', 'JJR', 'IN', 'JJS', 'IN', 'DT', 'NNS', ',', 'DT', 'IN', 'WDT', 'VBN', 'NNP', 'NNS', '.']",25
natural_language_inference,59,117,An interesting observation is that DECATT char model without any pretrained embeddings outperforms DE - CATT glove that uses task - agnostic GloVe embeddings .,"['An', 'interesting', 'observation', 'is', 'that', 'DECATT', 'char', 'model', 'without', 'any', 'pretrained', 'embeddings', 'outperforms', 'DE', '-', 'CATT', 'glove', 'that', 'uses', 'task', '-', 'agnostic', 'GloVe', 'embeddings', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'IN', 'NNP', 'VBP', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'VBZ', 'NNP', ':', 'NNP', 'VBP', 'WDT', 'VBZ', 'SYM', ':', 'JJ', 'NNP', 'NNS', '.']",25
natural_language_inference,59,118,"Furthermore , when character n-gram embeddings are pre-trained in a task - specific manner in DECATT paralex ? char model , we observe a significant boost in performance .","['Furthermore', ',', 'when', 'character', 'n-gram', 'embeddings', 'are', 'pre-trained', 'in', 'a', 'task', '-', 'specific', 'manner', 'in', 'DECATT', 'paralex', '?', 'char', 'model', ',', 'we', 'observe', 'a', 'significant', 'boost', 'in', 'performance', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['RB', ',', 'WRB', 'NN', 'JJ', 'NNS', 'VBP', 'JJ', 'IN', 'DT', 'NN', ':', 'JJ', 'NN', 'IN', 'NNP', 'NN', '.', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NN', '.']",29
natural_language_inference,59,122,"Finally , we note that our best performing model is pt - DECATT char , which leverages the full power of character embeddings and pretraining the model on Paralex .","['Finally', ',', 'we', 'note', 'that', 'our', 'best', 'performing', 'model', 'is', 'pt', '-', 'DECATT', 'char', ',', 'which', 'leverages', 'the', 'full', 'power', 'of', 'character', 'embeddings', 'and', 'pretraining', 'the', 'model', 'on', 'Paralex', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'IN', 'PRP$', 'JJS', 'NN', 'NN', 'VBZ', 'JJ', ':', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', 'CC', 'VBG', 'DT', 'NN', 'IN', 'NNP', '.']",30
natural_language_inference,62,2,Explicit Utilization of General Knowledge in Machine Reading Comprehension,"['Explicit', 'Utilization', 'of', 'General', 'Knowledge', 'in', 'Machine', 'Reading', 'Comprehension']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",9
natural_language_inference,62,4,"To bridge the gap between Machine Reading Comprehension ( MRC ) models and human beings , which is mainly reflected in the hunger for data and the robustness to noise , in this paper , we explore how to integrate the neural networks of MRC models with the general knowledge of human beings .","['To', 'bridge', 'the', 'gap', 'between', 'Machine', 'Reading', 'Comprehension', '(', 'MRC', ')', 'models', 'and', 'human', 'beings', ',', 'which', 'is', 'mainly', 'reflected', 'in', 'the', 'hunger', 'for', 'data', 'and', 'the', 'robustness', 'to', 'noise', ',', 'in', 'this', 'paper', ',', 'we', 'explore', 'how', 'to', 'integrate', 'the', 'neural', 'networks', 'of', 'MRC', 'models', 'with', 'the', 'general', 'knowledge', 'of', 'human', 'beings', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'NNS', 'CC', 'JJ', 'NNS', ',', 'WDT', 'VBZ', 'RB', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NNS', 'CC', 'DT', 'NN', 'TO', 'VB', ',', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'WRB', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'IN', 'NNP', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', '.']",54
natural_language_inference,62,41,"On the one hand , we propose a data enrichment method , which uses WordNet to extract inter-word semantic connections as general knowledge from each given passage - question pair .","['On', 'the', 'one', 'hand', ',', 'we', 'propose', 'a', 'data', 'enrichment', 'method', ',', 'which', 'uses', 'WordNet', 'to', 'extract', 'inter-word', 'semantic', 'connections', 'as', 'general', 'knowledge', 'from', 'each', 'given', 'passage', '-', 'question', 'pair', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'CD', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'NN', ',', 'WDT', 'VBZ', 'NNP', 'TO', 'VB', 'JJ', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'IN', 'DT', 'VBN', 'NN', ':', 'NN', 'NN', '.']",31
natural_language_inference,62,42,"On the other hand , we propose an end - to - end MRC model named as Knowledge Aided Reader ( KAR ) , which explicitly uses the above extracted general knowledge to assist its attention mechanisms .","['On', 'the', 'other', 'hand', ',', 'we', 'propose', 'an', 'end', '-', 'to', '-', 'end', 'MRC', 'model', 'named', 'as', 'Knowledge', 'Aided', 'Reader', '(', 'KAR', ')', ',', 'which', 'explicitly', 'uses', 'the', 'above', 'extracted', 'general', 'knowledge', 'to', 'assist', 'its', 'attention', 'mechanisms', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', ':', 'TO', ':', 'NN', 'NNP', 'NN', 'VBN', 'IN', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'WDT', 'RB', 'VBZ', 'DT', 'NN', 'VBD', 'JJ', 'NN', 'TO', 'VB', 'PRP$', 'NN', 'NNS', '.']",38
natural_language_inference,62,181,"We tokenize the MRC dataset with spa Cy 2.0.13 , manipulate WordNet 3.0 with NLTK 3.3 , and implement KAR with TensorFlow 1.11.0 .","['We', 'tokenize', 'the', 'MRC', 'dataset', 'with', 'spa', 'Cy', '2.0.13', ',', 'manipulate', 'WordNet', '3.0', 'with', 'NLTK', '3.3', ',', 'and', 'implement', 'KAR', 'with', 'TensorFlow', '1.11.0', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', 'IN', 'JJ', 'NNP', 'CD', ',', 'JJ', 'NNP', 'CD', 'IN', 'NNP', 'CD', ',', 'CC', 'JJ', 'NNP', 'IN', 'NNP', 'CD', '.']",24
natural_language_inference,62,183,"For the dense layers and the BiLSTMs , we set the dimensionality unit d to 600 .","['For', 'the', 'dense', 'layers', 'and', 'the', 'BiLSTMs', ',', 'we', 'set', 'the', 'dimensionality', 'unit', 'd', 'to', '600', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NN', 'NNS', 'CC', 'DT', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'NN', 'TO', 'CD', '.']",17
natural_language_inference,62,184,"For model optimization , we apply the Adam ( Kingma and Ba , 2014 ) optimizer with a learning rate of 0.0005 and a minibatch size of 32 .","['For', 'model', 'optimization', ',', 'we', 'apply', 'the', 'Adam', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', 'optimizer', 'with', 'a', 'learning', 'rate', 'of', '0.0005', 'and', 'a', 'minibatch', 'size', 'of', '32', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'NN', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', 'CC', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",29
natural_language_inference,62,186,"To avoid overfitting , we apply dropout to the dense layers and the BiLSTMs with a dropout rate of 0.3 .","['To', 'avoid', 'overfitting', ',', 'we', 'apply', 'dropout', 'to', 'the', 'dense', 'layers', 'and', 'the', 'BiLSTMs', 'with', 'a', 'dropout', 'rate', 'of', '0.3', '.']","['B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['TO', 'VB', 'NN', ',', 'PRP', 'VBP', 'RB', 'TO', 'DT', 'NN', 'NNS', 'CC', 'DT', 'NNP', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",21
natural_language_inference,62,187,"To boost the performance , we apply exponential moving average with a decay rate of 0.999 .","['To', 'boost', 'the', 'performance', ',', 'we', 'apply', 'exponential', 'moving', 'average', 'with', 'a', 'decay', 'rate', 'of', '0.999', '.']","['B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'VBG', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', '.']",17
natural_language_inference,62,198,"Then we conduct an ablation study by replacing the knowledge aided attention mechanisms with the mutual attention proposed by and the self attention proposed by separately , and find that the F 1 score of KAR drops by 4.2 on the development set , 7.8 on AddSent , and 9.1 on AddOneSent .","['Then', 'we', 'conduct', 'an', 'ablation', 'study', 'by', 'replacing', 'the', 'knowledge', 'aided', 'attention', 'mechanisms', 'with', 'the', 'mutual', 'attention', 'proposed', 'by', 'and', 'the', 'self', 'attention', 'proposed', 'by', 'separately', ',', 'and', 'find', 'that', 'the', 'F', '1', 'score', 'of', 'KAR', 'drops', 'by', '4.2', 'on', 'the', 'development', 'set', ',', '7.8', 'on', 'AddSent', ',', 'and', '9.1', 'on', 'AddOneSent', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O']","['RB', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'VBD', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'VBN', 'IN', 'CC', 'DT', 'NN', 'NN', 'VBN', 'IN', 'RB', ',', 'CC', 'VB', 'IN', 'DT', 'NNP', 'CD', 'NN', 'IN', 'NNP', 'NNS', 'IN', 'CD', 'IN', 'DT', 'NN', 'NN', ',', 'CD', 'IN', 'NNP', ',', 'CC', 'CD', 'IN', 'NNP', '.']",53
natural_language_inference,62,199,"Finally we find that after only one epoch of training , KAR already achieves an EM of 71.9 and an F 1 score of 80.8 on the development set , which is even better than the final performance of several strong baselines , such as DCN ( EM / F1 : 65.4 / 75.6 ) and BiDAF ( EM / F1 : 67.7 / 77.3 ) .","['Finally', 'we', 'find', 'that', 'after', 'only', 'one', 'epoch', 'of', 'training', ',', 'KAR', 'already', 'achieves', 'an', 'EM', 'of', '71.9', 'and', 'an', 'F', '1', 'score', 'of', '80.8', 'on', 'the', 'development', 'set', ',', 'which', 'is', 'even', 'better', 'than', 'the', 'final', 'performance', 'of', 'several', 'strong', 'baselines', ',', 'such', 'as', 'DCN', '(', 'EM', '/', 'F1', ':', '65.4', '/', '75.6', ')', 'and', 'BiDAF', '(', 'EM', '/', 'F1', ':', '67.7', '/', '77.3', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', 'PRP', 'VBP', 'IN', 'IN', 'RB', 'CD', 'NN', 'IN', 'NN', ',', 'NNP', 'RB', 'VBZ', 'DT', 'NNP', 'IN', 'CD', 'CC', 'DT', 'NNP', 'CD', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', 'NN', ',', 'WDT', 'VBZ', 'RB', 'JJR', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'JJ', 'NNS', ',', 'JJ', 'IN', 'NNP', '(', 'NNP', 'NNP', 'NNP', ':', 'CD', '$', 'CD', ')', 'CC', 'NNP', '(', 'NNP', 'NNP', 'NNP', ':', 'CD', '$', 'CD', ')', '.']",67
natural_language_inference,39,2,Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement,"['Pairwise', 'Word', 'Interaction', 'Modeling', 'with', 'Deep', 'Neural', 'Networks', 'for', 'Semantic', 'Similarity', 'Measurement']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",12
natural_language_inference,39,4,"Textual similarity measurement is a challenging problem , as it requires understanding the semantics of input sentences .","['Textual', 'similarity', 'measurement', 'is', 'a', 'challenging', 'problem', ',', 'as', 'it', 'requires', 'understanding', 'the', 'semantics', 'of', 'input', 'sentences', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', 'VBZ', 'DT', 'NN', 'NN', ',', 'IN', 'PRP', 'VBZ', 'VBG', 'DT', 'NNS', 'IN', 'NN', 'NNS', '.']",18
natural_language_inference,39,9,"Given two pieces of text , measuring their semantic textual similarity ( STS ) remains a fundamental problem in language research and lies at the core of many language processing tasks , including question answering , query ranking , and paraphrase generation .","['Given', 'two', 'pieces', 'of', 'text', ',', 'measuring', 'their', 'semantic', 'textual', 'similarity', '(', 'STS', ')', 'remains', 'a', 'fundamental', 'problem', 'in', 'language', 'research', 'and', 'lies', 'at', 'the', 'core', 'of', 'many', 'language', 'processing', 'tasks', ',', 'including', 'question', 'answering', ',', 'query', 'ranking', ',', 'and', 'paraphrase', 'generation', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'CD', 'NNS', 'IN', 'NN', ',', 'VBG', 'PRP$', 'JJ', 'JJ', 'NN', '(', 'NNP', ')', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NN', 'CC', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', 'NNS', ',', 'VBG', 'NN', 'NN', ',', 'NN', 'NN', ',', 'CC', 'NN', 'NN', '.']",43
natural_language_inference,39,16,"In contrast , we focus on capturing fine - grained word - level information directly .","['In', 'contrast', ',', 'we', 'focus', 'on', 'capturing', 'fine', '-', 'grained', 'word', '-', 'level', 'information', 'directly', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'IN', 'VBG', 'JJ', ':', 'VBN', 'NN', ':', 'NN', 'NN', 'RB', '.']",16
natural_language_inference,39,18,"First , instead of using sentence modeling , we propose pairwise word interaction modeling that encourages explicit word context interactions across sentences .","['First', ',', 'instead', 'of', 'using', 'sentence', 'modeling', ',', 'we', 'propose', 'pairwise', 'word', 'interaction', 'modeling', 'that', 'encourages', 'explicit', 'word', 'context', 'interactions', 'across', 'sentences', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['RB', ',', 'RB', 'IN', 'VBG', 'NN', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'NN', 'VBG', 'IN', 'VBZ', 'JJ', 'NN', 'NN', 'NNS', 'IN', 'NNS', '.']",23
natural_language_inference,39,20,"Second , based on the pairwise word interactions , we describe a novel similarity focus layer which helps the model selectively identify important word interactions depending on their importance for similarity measurement .","['Second', ',', 'based', 'on', 'the', 'pairwise', 'word', 'interactions', ',', 'we', 'describe', 'a', 'novel', 'similarity', 'focus', 'layer', 'which', 'helps', 'the', 'model', 'selectively', 'identify', 'important', 'word', 'interactions', 'depending', 'on', 'their', 'importance', 'for', 'similarity', 'measurement', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ',', 'VBN', 'IN', 'DT', 'NN', 'NN', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'RB', 'JJ', 'JJ', 'NN', 'NNS', 'VBG', 'IN', 'PRP$', 'NN', 'IN', 'NN', 'NN', '.']",33
natural_language_inference,39,155,"For the SICK and MSRVID experiments , we used 300 - dimension Glo Ve word embeddings .","['For', 'the', 'SICK', 'and', 'MSRVID', 'experiments', ',', 'we', 'used', '300', '-', 'dimension', 'Glo', 'Ve', 'word', 'embeddings', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNP', 'CC', 'NNP', 'NNS', ',', 'PRP', 'VBD', 'CD', ':', 'NN', 'NNP', 'NNP', 'NN', 'NNS', '.']",17
natural_language_inference,39,156,"For the STS2014 , WikiQA , and TrecQA experiments , we used 300 dimension PARAGRAM - SL999 embeddings from and the PARAGRAM - PHRASE embeddings from , trained on word pairs from the Paraphrase Database ( PPDB ) .","['For', 'the', 'STS2014', ',', 'WikiQA', ',', 'and', 'TrecQA', 'experiments', ',', 'we', 'used', '300', 'dimension', 'PARAGRAM', '-', 'SL999', 'embeddings', 'from', 'and', 'the', 'PARAGRAM', '-', 'PHRASE', 'embeddings', 'from', ',', 'trained', 'on', 'word', 'pairs', 'from', 'the', 'Paraphrase', 'Database', '(', 'PPDB', ')', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', 'NNS', ',', 'PRP', 'VBD', 'CD', 'NN', 'NNP', ':', 'NNP', 'NNS', 'IN', 'CC', 'DT', 'NNP', ':', 'NNP', 'NNS', 'IN', ',', 'VBN', 'IN', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NNP', '(', 'NNP', ')', '.']",39
natural_language_inference,39,161,Our timing experiments were conducted on an Intel Xeon E5 - 2680 CPU .,"['Our', 'timing', 'experiments', 'were', 'conducted', 'on', 'an', 'Intel', 'Xeon', 'E5', '-', '2680', 'CPU', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'NNS', 'VBD', 'VBN', 'IN', 'DT', 'NNP', 'NNP', 'NNP', ':', 'CD', 'NNP', '.']",14
natural_language_inference,39,162,"Due to sentence length variations , for the SICK and MSRVID data we padded the sentences to 32 words ; for the STS2014 , WikiQA , and TrecQA data , we padded the sentences to 48 words ..","['Due', 'to', 'sentence', 'length', 'variations', ',', 'for', 'the', 'SICK', 'and', 'MSRVID', 'data', 'we', 'padded', 'the', 'sentences', 'to', '32', 'words', ';', 'for', 'the', 'STS2014', ',', 'WikiQA', ',', 'and', 'TrecQA', 'data', ',', 'we', 'padded', 'the', 'sentences', 'to', '48', 'words', '..']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['JJ', 'TO', 'VB', 'NN', 'NNS', ',', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NN', 'PRP', 'VBD', 'DT', 'NNS', 'TO', 'CD', 'NNS', ':', 'IN', 'DT', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', 'NNS', ',', 'PRP', 'VBD', 'DT', 'NNS', 'TO', 'CD', 'NNS', 'NN']",38
natural_language_inference,39,176,Wiki QA Results .,"['Wiki', 'QA', 'Results', '.']","['B-n', 'I-n', 'O', 'O']","['NNP', 'NNP', 'NNP', '.']",4
natural_language_inference,39,178,"The neural network models in the table , paragraph vector ( PV ) , CNN , and PV - Cnt / CNN - Cnt with word matching features , are mostly based on sentence modeling .","['The', 'neural', 'network', 'models', 'in', 'the', 'table', ',', 'paragraph', 'vector', '(', 'PV', ')', ',', 'CNN', ',', 'and', 'PV', '-', 'Cnt', '/', 'CNN', '-', 'Cnt', 'with', 'word', 'matching', 'features', ',', 'are', 'mostly', 'based', 'on', 'sentence', 'modeling', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'NN', ',', 'NN', 'NN', '(', 'NNP', ')', ',', 'NNP', ',', 'CC', 'NNP', ':', 'NNP', 'NNP', 'NNP', ':', 'NN', 'IN', 'NN', 'NN', 'NNS', ',', 'VBP', 'RB', 'VBN', 'IN', 'NN', 'NN', '.']",36
natural_language_inference,39,179,Our model outperforms them all .,"['Our', 'model', 'outperforms', 'them', 'all', '.']","['B-n', 'I-n', 'B-n', 'O', 'O', 'O']","['PRP$', 'NN', 'VBZ', 'PRP', 'DT', '.']",6
natural_language_inference,39,190,"We found large drops when removing the context modeling component , indicating that the context information provided by the Bi - LSTMs is crucial for the following components ( e.g. , interaction modeling ) .","['We', 'found', 'large', 'drops', 'when', 'removing', 'the', 'context', 'modeling', 'component', ',', 'indicating', 'that', 'the', 'context', 'information', 'provided', 'by', 'the', 'Bi', '-', 'LSTMs', 'is', 'crucial', 'for', 'the', 'following', 'components', '(', 'e.g.', ',', 'interaction', 'modeling', ')', '.']","['O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'JJ', 'NNS', 'WRB', 'VBG', 'DT', 'NN', 'NN', 'NN', ',', 'VBG', 'IN', 'DT', 'NN', 'NN', 'VBN', 'IN', 'DT', 'NNP', ':', 'NNP', 'VBZ', 'JJ', 'IN', 'DT', 'JJ', 'NNS', '(', 'NN', ',', 'NN', 'NN', ')', '.']",35
natural_language_inference,39,191,"The use of our similarity focus layer is also beneficial , especially on the WikiQA data .","['The', 'use', 'of', 'our', 'similarity', 'focus', 'layer', 'is', 'also', 'beneficial', ',', 'especially', 'on', 'the', 'WikiQA', 'data', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'PRP$', 'NN', 'NN', 'NN', 'VBZ', 'RB', 'JJ', ',', 'RB', 'IN', 'DT', 'NNP', 'NNS', '.']",17
natural_language_inference,39,192,"When we replaced the entire similarity focus layer with a random dropout layer ( p = 0.3 ) , the dropout layer hurts accuracy ; this shows the importance of directing the model to focus on important pairwise word interactions , to better capture similarity .","['When', 'we', 'replaced', 'the', 'entire', 'similarity', 'focus', 'layer', 'with', 'a', 'random', 'dropout', 'layer', '(', 'p', '=', '0.3', ')', ',', 'the', 'dropout', 'layer', 'hurts', 'accuracy', ';', 'this', 'shows', 'the', 'importance', 'of', 'directing', 'the', 'model', 'to', 'focus', 'on', 'important', 'pairwise', 'word', 'interactions', ',', 'to', 'better', 'capture', 'similarity', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['WRB', 'PRP', 'VBD', 'DT', 'JJ', 'NN', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'NN', '(', 'JJ', 'NNP', 'CD', ')', ',', 'DT', 'NN', 'NN', 'VBZ', 'NN', ':', 'DT', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'DT', 'NN', 'TO', 'VB', 'IN', 'JJ', 'NN', 'NN', 'NNS', ',', 'TO', 'RBR', 'NN', 'NN', '.']",46
natural_language_inference,69,2,Constructing Datasets for Multi-hop Reading Comprehension Across Documents,"['Constructing', 'Datasets', 'for', 'Multi-hop', 'Reading', 'Comprehension', 'Across', 'Documents']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['VBG', 'NNS', 'IN', 'NNP', 'NNP', 'NNP', 'IN', 'NNS']",8
natural_language_inference,69,4,"Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence , paragraph , or document .","['Most', 'Reading', 'Comprehension', 'methods', 'limit', 'themselves', 'to', 'queries', 'which', 'can', 'be', 'answered', 'using', 'a', 'single', 'sentence', ',', 'paragraph', ',', 'or', 'document', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJS', 'JJ', 'NNP', 'NNS', 'VBP', 'PRP', 'TO', 'NNS', 'WDT', 'MD', 'VB', 'VBN', 'VBG', 'DT', 'JJ', 'NN', ',', 'NN', ',', 'CC', 'NN', '.']",22
natural_language_inference,69,17,Contemporary end - to - end Reading Comprehension ( RC ) methods can learn to extract the correct answer span within a given text and approach human - level performance .,"['Contemporary', 'end', '-', 'to', '-', 'end', 'Reading', 'Comprehension', '(', 'RC', ')', 'methods', 'can', 'learn', 'to', 'extract', 'the', 'correct', 'answer', 'span', 'within', 'a', 'given', 'text', 'and', 'approach', 'human', '-', 'level', 'performance', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', ':', 'TO', ':', 'NN', 'VBG', 'NNP', '(', 'NNP', ')', 'NNS', 'MD', 'VB', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'VBN', 'NN', 'CC', 'NN', 'JJ', ':', 'NN', 'NN', '.']",31
natural_language_inference,69,29,"The first , WIKIHOP , uses sets of WIKIPEDIA articles where answers to queries about specific properties of an entity can not be located in the entity 's article .","['The', 'first', ',', 'WIKIHOP', ',', 'uses', 'sets', 'of', 'WIKIPEDIA', 'articles', 'where', 'answers', 'to', 'queries', 'about', 'specific', 'properties', 'of', 'an', 'entity', 'can', 'not', 'be', 'located', 'in', 'the', 'entity', ""'s"", 'article', '.']","['O', 'O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', ',', 'NNP', ',', 'VBZ', 'NNS', 'IN', 'NNP', 'NNS', 'WRB', 'NNS', 'TO', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'MD', 'RB', 'VB', 'VBN', 'IN', 'DT', 'NN', 'POS', 'NN', '.']",30
natural_language_inference,69,30,"In the second dataset , MEDHOP , the goal is to establish drug - drug interactions based on scientific findings about drugs and proteins and their interactions , found across multiple MEDLINE abstracts .","['In', 'the', 'second', 'dataset', ',', 'MEDHOP', ',', 'the', 'goal', 'is', 'to', 'establish', 'drug', '-', 'drug', 'interactions', 'based', 'on', 'scientific', 'findings', 'about', 'drugs', 'and', 'proteins', 'and', 'their', 'interactions', ',', 'found', 'across', 'multiple', 'MEDLINE', 'abstracts', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'NNP', ',', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'NN', ':', 'NN', 'NNS', 'VBN', 'IN', 'JJ', 'NNS', 'IN', 'NNS', 'CC', 'NNS', 'CC', 'PRP$', 'NNS', ',', 'VBN', 'IN', 'JJ', 'NNP', 'NNS', '.']",34
natural_language_inference,69,31,"For both datasets we draw upon existing Knowledge Bases ( KBs ) , WIKIDATA and DRUG - BANK , as ground truth , utilizing distant supervision ) to induce the data - similar to and .","['For', 'both', 'datasets', 'we', 'draw', 'upon', 'existing', 'Knowledge', 'Bases', '(', 'KBs', ')', ',', 'WIKIDATA', 'and', 'DRUG', '-', 'BANK', ',', 'as', 'ground', 'truth', ',', 'utilizing', 'distant', 'supervision', ')', 'to', 'induce', 'the', 'data', '-', 'similar', 'to', 'and', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNS', 'PRP', 'VBP', 'IN', 'VBG', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'NNP', 'CC', 'NNP', ':', 'NNP', ',', 'IN', 'NN', 'NN', ',', 'VBG', 'JJ', 'NN', ')', 'TO', 'VB', 'DT', 'NNS', ':', 'JJ', 'TO', 'CC', '.']",36
natural_language_inference,69,213,Random Selects a random candidate ; note that the number of candidates differs between samples .,"['Random', 'Selects', 'a', 'random', 'candidate', ';', 'note', 'that', 'the', 'number', 'of', 'candidates', 'differs', 'between', 'samples', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', ':', 'CC', 'IN', 'DT', 'NN', 'IN', 'NNS', 'NNS', 'IN', 'NNS', '.']",16
natural_language_inference,69,214,Max- mention,"['Max-', 'mention']","['B-n', 'I-n']","['JJ', 'NN']",2
natural_language_inference,69,215,Predicts the most frequently mentioned candidate in the support documents,"['Predicts', 'the', 'most', 'frequently', 'mentioned', 'candidate', 'in', 'the', 'support', 'documents']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n']","['VBZ', 'DT', 'RBS', 'RB', 'VBN', 'NN', 'IN', 'DT', 'NN', 'NNS']",10
natural_language_inference,69,217,Majority - candidate - per-query - type,"['Majority', '-', 'candidate', '-', 'per-query', '-', 'type']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NN', ':', 'JJ', ':', 'NN']",7
natural_language_inference,69,218,Predicts the candidate c ?,"['Predicts', 'the', 'candidate', 'c', '?']","['B-p', 'O', 'B-n', 'O', 'O']","['VBZ', 'DT', 'NN', 'NN', '.']",5
natural_language_inference,69,219,"C q that was most frequently observed as the true answer in the training set , given the query type of q .","['C', 'q', 'that', 'was', 'most', 'frequently', 'observed', 'as', 'the', 'true', 'answer', 'in', 'the', 'training', 'set', ',', 'given', 'the', 'query', 'type', 'of', 'q', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'WDT', 'VBD', 'RBS', 'RB', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'VBN', 'DT', 'NN', 'NN', 'IN', 'NN', '.']",23
natural_language_inference,69,221,TF - IDF,"['TF', '-', 'IDF']","['B-n', 'I-n', 'I-n']","['NNP', ':', 'NN']",3
natural_language_inference,69,222,Retrieval - based models are known to be strong QA baselines if candidate answers are provided .,"['Retrieval', '-', 'based', 'models', 'are', 'known', 'to', 'be', 'strong', 'QA', 'baselines', 'if', 'candidate', 'answers', 'are', 'provided', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'O']","['NNP', ':', 'VBN', 'NNS', 'VBP', 'VBN', 'TO', 'VB', 'JJ', 'NNP', 'NNS', 'IN', 'NN', 'NNS', 'VBP', 'VBN', '.']",17
natural_language_inference,69,229,"( 1 ) Document - cue During dataset construction we observed that certain document - answer pairs appear more frequently than others , to the effect that the correct candidate is often indicated solely by the presence of certain documents in Sq .","['(', '1', ')', 'Document', '-', 'cue', 'During', 'dataset', 'construction', 'we', 'observed', 'that', 'certain', 'document', '-', 'answer', 'pairs', 'appear', 'more', 'frequently', 'than', 'others', ',', 'to', 'the', 'effect', 'that', 'the', 'correct', 'candidate', 'is', 'often', 'indicated', 'solely', 'by', 'the', 'presence', 'of', 'certain', 'documents', 'in', 'Sq', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['(', 'CD', ')', 'NNP', ':', 'NN', 'IN', 'JJ', 'NN', 'PRP', 'VBD', 'IN', 'JJ', 'NN', ':', 'NN', 'NNS', 'VBP', 'RBR', 'RB', 'IN', 'NNS', ',', 'TO', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'RB', 'VBN', 'RB', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NNP', '.']",43
natural_language_inference,69,261,"The Document - cue baseline can predict more than a third of the samples correctly , for both datasets , even after sub - sampling frequent document - answer pairs for WIKIHOP .","['The', 'Document', '-', 'cue', 'baseline', 'can', 'predict', 'more', 'than', 'a', 'third', 'of', 'the', 'samples', 'correctly', ',', 'for', 'both', 'datasets', ',', 'even', 'after', 'sub', '-', 'sampling', 'frequent', 'document', '-', 'answer', 'pairs', 'for', 'WIKIHOP', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O']","['DT', 'NNP', ':', 'NN', 'NN', 'MD', 'VB', 'JJR', 'IN', 'DT', 'JJ', 'IN', 'DT', 'NNS', 'RB', ',', 'IN', 'DT', 'NNS', ',', 'RB', 'IN', 'JJ', ':', 'VBG', 'JJ', 'NN', ':', 'NN', 'NNS', 'IN', 'NNP', '.']",33
natural_language_inference,69,265,"In the masked setup all baseline models reliant on lexical cues fail in the face of the randomized answer expressions , since the same answer option has different placeholders in different samples .","['In', 'the', 'masked', 'setup', 'all', 'baseline', 'models', 'reliant', 'on', 'lexical', 'cues', 'fail', 'in', 'the', 'face', 'of', 'the', 'randomized', 'answer', 'expressions', ',', 'since', 'the', 'same', 'answer', 'option', 'has', 'different', 'placeholders', 'in', 'different', 'samples', '.']","['B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', 'DT', 'NN', 'NNS', 'VBP', 'IN', 'JJ', 'NNS', 'VBP', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNS', ',', 'IN', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', '.']",33
natural_language_inference,69,267,Both neural RC models are able to largely retain or even improve their strong performance when answers are masked : they are able to leverage the textual context of the candidate expressions .,"['Both', 'neural', 'RC', 'models', 'are', 'able', 'to', 'largely', 'retain', 'or', 'even', 'improve', 'their', 'strong', 'performance', 'when', 'answers', 'are', 'masked', ':', 'they', 'are', 'able', 'to', 'leverage', 'the', 'textual', 'context', 'of', 'the', 'candidate', 'expressions', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NNP', 'NNS', 'VBP', 'JJ', 'TO', 'RB', 'VB', 'CC', 'RB', 'VB', 'PRP$', 'JJ', 'NN', 'WRB', 'NNS', 'VBP', 'VBN', ':', 'PRP', 'VBP', 'JJ', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NNS', '.']",33
natural_language_inference,69,269,"In contrast , for the open - domain setting of WIKIHOP , a reduction of the answer vocabulary to 100 random single - token mask expressions clearly helps the model in selecting a candidate span , compared to the multi-token candidate expressions in the unmasked setting .","['In', 'contrast', ',', 'for', 'the', 'open', '-', 'domain', 'setting', 'of', 'WIKIHOP', ',', 'a', 'reduction', 'of', 'the', 'answer', 'vocabulary', 'to', '100', 'random', 'single', '-', 'token', 'mask', 'expressions', 'clearly', 'helps', 'the', 'model', 'in', 'selecting', 'a', 'candidate', 'span', ',', 'compared', 'to', 'the', 'multi-token', 'candidate', 'expressions', 'in', 'the', 'unmasked', 'setting', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'IN', 'DT', 'JJ', ':', 'NN', 'NN', 'IN', 'NNP', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'TO', 'CD', 'NN', 'JJ', ':', 'NN', 'NN', 'NNS', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'DT', 'NN', 'NN', ',', 'VBN', 'TO', 'DT', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",47
natural_language_inference,63,2,A Multi - Stage Memory Augmented Neural Network for Machine Reading Comprehension,"['A', 'Multi', '-', 'Stage', 'Memory', 'Augmented', 'Neural', 'Network', 'for', 'Machine', 'Reading', 'Comprehension']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['DT', 'NNP', ':', 'NN', 'NN', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",12
natural_language_inference,63,4,Reading Comprehension ( RC ) of text is one of the fundamental tasks in natural language processing .,"['Reading', 'Comprehension', '(', 'RC', ')', 'of', 'text', 'is', 'one', 'of', 'the', 'fundamental', 'tasks', 'in', 'natural', 'language', 'processing', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NNP', '(', 'NNP', ')', 'IN', 'NN', 'VBZ', 'CD', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NN', '.']",18
natural_language_inference,63,5,"In recent years , several end - to - end neural network models have been proposed to solve RC tasks .","['In', 'recent', 'years', ',', 'several', 'end', '-', 'to', '-', 'end', 'neural', 'network', 'models', 'have', 'been', 'proposed', 'to', 'solve', 'RC', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O']","['IN', 'JJ', 'NNS', ',', 'JJ', 'NN', ':', 'TO', ':', 'VB', 'JJ', 'NN', 'NNS', 'VBP', 'VBN', 'VBN', 'TO', 'VB', 'NNP', 'NNS', '.']",21
natural_language_inference,63,12,"One possible way of measuring RC is by formulating it as answer span prediction style Question Answering ( QA ) task , which is finding an answer to the question based on the given document ( s ) .","['One', 'possible', 'way', 'of', 'measuring', 'RC', 'is', 'by', 'formulating', 'it', 'as', 'answer', 'span', 'prediction', 'style', 'Question', 'Answering', '(', 'QA', ')', 'task', ',', 'which', 'is', 'finding', 'an', 'answer', 'to', 'the', 'question', 'based', 'on', 'the', 'given', 'document', '(', 's', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['CD', 'JJ', 'NN', 'IN', 'VBG', 'NNP', 'VBZ', 'IN', 'VBG', 'PRP', 'IN', 'NN', 'NN', 'NN', 'NN', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', ',', 'WDT', 'VBZ', 'VBG', 'DT', 'NN', 'TO', 'DT', 'NN', 'VBN', 'IN', 'DT', 'VBN', 'NN', '(', 'NN', ')', '.']",39
natural_language_inference,63,13,"Recently , influential deep learning approaches have been proposed to solve this QA task . ; propose the attention mechanism between question and context for question - aware contextual representation .","['Recently', ',', 'influential', 'deep', 'learning', 'approaches', 'have', 'been', 'proposed', 'to', 'solve', 'this', 'QA', 'task', '.', ';', 'propose', 'the', 'attention', 'mechanism', 'between', 'question', 'and', 'context', 'for', 'question', '-', 'aware', 'contextual', 'representation', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'JJ', 'JJ', 'NN', 'NNS', 'VBP', 'VBN', 'VBN', 'TO', 'VB', 'DT', 'NNP', 'NN', '.', ':', 'VB', 'DT', 'NN', 'NN', 'IN', 'NN', 'CC', 'NN', 'IN', 'NN', ':', 'JJ', 'JJ', 'NN', '.']",31
natural_language_inference,63,24,"In this work , we build a QA model that can understand long documents by utilizing Memory Augmented Neural Networks ( MANNs ) .","['In', 'this', 'work', ',', 'we', 'build', 'a', 'QA', 'model', 'that', 'can', 'understand', 'long', 'documents', 'by', 'utilizing', 'Memory', 'Augmented', 'Neural', 'Networks', '(', 'MANNs', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', 'NN', 'WDT', 'MD', 'VB', 'JJ', 'NNS', 'IN', 'VBG', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', '.']",24
natural_language_inference,63,25,This type of neural networks decouples the memory capacity from the number of model parameters .,"['This', 'type', 'of', 'neural', 'networks', 'decouples', 'the', 'memory', 'capacity', 'from', 'the', 'number', 'of', 'model', 'parameters', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'JJ', 'NNS', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', '.']",16
natural_language_inference,63,137,We develop MAMCN using Tensorflow 1 deep learning framework and Sonnet 2 library .,"['We', 'develop', 'MAMCN', 'using', 'Tensorflow', '1', 'deep', 'learning', 'framework', 'and', 'Sonnet', '2', 'library', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', 'VBG', 'NNP', 'CD', 'JJ', 'NN', 'NN', 'CC', 'NNP', 'CD', 'NN', '.']",14
natural_language_inference,63,138,"For the word - level embedding , we tokenize the documents using NLTK toolkit and substitute words with GloVe 6B 43.16 46.90 49.28 55.83 BiDAF 40.32 45.91 44.86 50.71 hidden size is set to 200 for QUASAR - T and Triv - iaQA , and 100 for SQuAD .","['For', 'the', 'word', '-', 'level', 'embedding', ',', 'we', 'tokenize', 'the', 'documents', 'using', 'NLTK', 'toolkit', 'and', 'substitute', 'words', 'with', 'GloVe', '6B', '43.16', '46.90', '49.28', '55.83', 'BiDAF', '40.32', '45.91', '44.86', '50.71', 'hidden', 'size', 'is', 'set', 'to', '200', 'for', 'QUASAR', '-', 'T', 'and', 'Triv', '-', 'iaQA', ',', 'and', '100', 'for', 'SQuAD', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ':', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNS', 'VBG', 'NNP', 'NN', 'CC', 'NN', 'NNS', 'IN', 'NNP', 'CD', 'CD', 'CD', 'CD', 'CD', 'NNP', 'CD', 'CD', 'CD', 'CD', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'IN', 'NNP', ':', 'NN', 'CC', 'NNP', ':', 'NN', ',', 'CC', 'CD', 'IN', 'NNP', '.']",49
natural_language_inference,63,139,"In the memory controller , we use 100 x 36 size memory initialized with zeros , 4 read heads and 1 write head .","['In', 'the', 'memory', 'controller', ',', 'we', 'use', '100', 'x', '36', 'size', 'memory', 'initialized', 'with', 'zeros', ',', '4', 'read', 'heads', 'and', '1', 'write', 'head', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'NN', ',', 'PRP', 'VBP', 'CD', '$', 'CD', 'NN', 'NN', 'VBN', 'IN', 'NNS', ',', 'CD', 'JJ', 'NNS', 'CC', 'CD', 'JJ', 'NN', '.']",24
natural_language_inference,63,140,"The optimizer is AdaDelta ( Zeiler , 2012 ) with an initial learning rate of 0.5 .","['The', 'optimizer', 'is', 'AdaDelta', '(', 'Zeiler', ',', '2012', ')', 'with', 'an', 'initial', 'learning', 'rate', 'of', '0.5', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'VBZ', 'NNP', '(', 'NNP', ',', 'CD', ')', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', '.']",17
natural_language_inference,63,141,"We train our model for 12 epochs , and batch size is set to 30 .","['We', 'train', 'our', 'model', 'for', '12', 'epochs', ',', 'and', 'batch', 'size', 'is', 'set', 'to', '30', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'IN', 'CD', 'NNS', ',', 'CC', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",16
natural_language_inference,63,142,"During the training , we keep the exponential moving average of weights with 0.001 decay and use these averages at test time .","['During', 'the', 'training', ',', 'we', 'keep', 'the', 'exponential', 'moving', 'average', 'of', 'weights', 'with', '0.001', 'decay', 'and', 'use', 'these', 'averages', 'at', 'test', 'time', '.']","['B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'CD', 'NNS', 'CC', 'VB', 'DT', 'NNS', 'IN', 'NN', 'NN', '.']",23
natural_language_inference,63,147,QUASAR - T:,"['QUASAR', '-', 'T:']","['B-n', 'I-n', 'I-n']","['NNP', ':', 'NN']",3
natural_language_inference,63,150,"As described in , the baseline ( BiDAF + DNC ) results in a reasonable gain , however , our proposed memory controller gives more performance improvement .","['As', 'described', 'in', ',', 'the', 'baseline', '(', 'BiDAF', '+', 'DNC', ')', 'results', 'in', 'a', 'reasonable', 'gain', ',', 'however', ',', 'our', 'proposed', 'memory', 'controller', 'gives', 'more', 'performance', 'improvement', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'IN', ',', 'DT', 'NN', '(', 'NNP', 'NNP', 'NNP', ')', 'NNS', 'IN', 'DT', 'JJ', 'NN', ',', 'RB', ',', 'PRP$', 'VBN', 'NN', 'NN', 'VBZ', 'JJR', 'NN', 'NN', '.']",28
natural_language_inference,63,151,We achieve 68.13 EM and 70.32 F1 for short documents and 63.44 and 65.19 for long documents which are the current best results .,"['We', 'achieve', '68.13', 'EM', 'and', '70.32', 'F1', 'for', 'short', 'documents', 'and', '63.44', 'and', '65.19', 'for', 'long', 'documents', 'which', 'are', 'the', 'current', 'best', 'results', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'CD', 'NNP', 'CC', 'CD', 'NNP', 'IN', 'JJ', 'NNS', 'CC', 'CD', 'CC', 'CD', 'IN', 'JJ', 'NNS', 'WDT', 'VBP', 'DT', 'JJ', 'JJS', 'NNS', '.']",24
natural_language_inference,63,152,TriviaQA : We compare proposed model with all the previously suggested approaches as shown in .,"['TriviaQA', ':', 'We', 'compare', 'proposed', 'model', 'with', 'all', 'the', 'previously', 'suggested', 'approaches', 'as', 'shown', 'in', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', ':', 'PRP', 'VBP', 'VBN', 'NN', 'IN', 'PDT', 'DT', 'RB', 'VBN', 'NNS', 'IN', 'VBN', 'IN', '.']",16
natural_language_inference,63,156,Our model achieves the state of the art performance over the existing approaches as shown in 77.58 84.16 O - QANet 76.24 84.60 O O SAN 76.83 84.40 O O Fusion Net 75.97 83.90 O O RaSoR + TR 75.79 83.26 O - Conducter- net 74.41 82.74 O O Reinforced Mnemonic Reader 73.20 81.80 O O BiDAF + Self-attention 72.14 81.05 - O MEMEN 70.98 80.36 O - MAMCN 70.99 79.94 -r- net 71.30 79.70 - O Document Reader 70.73 79.35 O - FastQAExt 70 .,"['Our', 'model', 'achieves', 'the', 'state', 'of', 'the', 'art', 'performance', 'over', 'the', 'existing', 'approaches', 'as', 'shown', 'in', '77.58', '84.16', 'O', '-', 'QANet', '76.24', '84.60', 'O', 'O', 'SAN', '76.83', '84.40', 'O', 'O', 'Fusion', 'Net', '75.97', '83.90', 'O', 'O', 'RaSoR', '+', 'TR', '75.79', '83.26', 'O', '-', 'Conducter-', 'net', '74.41', '82.74', 'O', 'O', 'Reinforced', 'Mnemonic', 'Reader', '73.20', '81.80', 'O', 'O', 'BiDAF', '+', 'Self-attention', '72.14', '81.05', '-', 'O', 'MEMEN', '70.98', '80.36', 'O', '-', 'MAMCN', '70.99', '79.94', '-r-', 'net', '71.30', '79.70', '-', 'O', 'Document', 'Reader', '70.73', '79.35', 'O', '-', 'FastQAExt', '70', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'VBG', 'NNS', 'IN', 'VBN', 'IN', 'CD', 'CD', 'NNP', ':', 'VBD', 'CD', 'CD', 'NNP', 'NNP', 'NNP', 'CD', 'CD', 'NNP', 'NNP', 'NNP', 'NNP', 'CD', 'CD', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'CD', 'CD', 'NNP', ':', 'JJ', 'JJ', 'CD', 'CD', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'CD', 'CD', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'CD', 'CD', ':', 'NNP', 'NNP', 'CD', 'CD', 'NNP', ':', 'NNP', 'CD', 'CD', 'JJ', 'JJ', 'CD', 'CD', ':', 'NN', 'NNP', 'NNP', 'CD', 'CD', 'NNP', ':', 'NN', 'CD', '.']",86
natural_language_inference,63,161,"First , we add ELMo which is the weighted sum of hidden layers of language model with regularization as an additional feature to our word embeddings .","['First', ',', 'we', 'add', 'ELMo', 'which', 'is', 'the', 'weighted', 'sum', 'of', 'hidden', 'layers', 'of', 'language', 'model', 'with', 'regularization', 'as', 'an', 'additional', 'feature', 'to', 'our', 'word', 'embeddings', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'NNP', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NN', 'NN', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'TO', 'PRP$', 'NN', 'NNS', '.']",27
natural_language_inference,63,162,This helped our model ( MAMCN + ELMo ) to improve F1 to 85.13 and EM to 77.44 and is the best among the models only with the additional feature augmentation .,"['This', 'helped', 'our', 'model', '(', 'MAMCN', '+', 'ELMo', ')', 'to', 'improve', 'F1', 'to', '85.13', 'and', 'EM', 'to', '77.44', 'and', 'is', 'the', 'best', 'among', 'the', 'models', 'only', 'with', 'the', 'additional', 'feature', 'augmentation', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'VBD', 'PRP$', 'NN', '(', 'NNP', 'NNP', 'NNP', ')', 'TO', 'VB', 'NNP', 'TO', 'CD', 'CC', 'NNP', 'TO', 'CD', 'CC', 'VBZ', 'DT', 'JJS', 'IN', 'DT', 'NNS', 'RB', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",32
natural_language_inference,63,168,We replace all the BiGRU units with this embedding block except the controller layer in our model ( MAMCN + ELMo + DC ) .,"['We', 'replace', 'all', 'the', 'BiGRU', 'units', 'with', 'this', 'embedding', 'block', 'except', 'the', 'controller', 'layer', 'in', 'our', 'model', '(', 'MAMCN', '+', 'ELMo', '+', 'DC', ')', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VB', 'PDT', 'DT', 'NNP', 'NNS', 'IN', 'DT', 'VBG', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'PRP$', 'NN', '(', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', ')', '.']",25
natural_language_inference,63,169,"We achieve the state of the art performance , 86.73 F1 and 79.69 EM , with the help of this em-bedding block .","['We', 'achieve', 'the', 'state', 'of', 'the', 'art', 'performance', ',', '86.73', 'F1', 'and', '79.69', 'EM', ',', 'with', 'the', 'help', 'of', 'this', 'em-bedding', 'block', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'CD', 'NNP', 'CC', 'CD', 'NNP', ',', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",23
natural_language_inference,53,2,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,"['Supervised', 'Learning', 'of', 'Universal', 'Sentence', 'Representations', 'from', 'Natural', 'Language', 'Inference', 'Data']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['VBN', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'NNP']",11
natural_language_inference,53,15,"In this paper , we study the task of learning universal representations of sentences , i.e. , a sentence encoder model that is trained on a large corpus and subsequently transferred to other tasks .","['In', 'this', 'paper', ',', 'we', 'study', 'the', 'task', 'of', 'learning', 'universal', 'representations', 'of', 'sentences', ',', 'i.e.', ',', 'a', 'sentence', 'encoder', 'model', 'that', 'is', 'trained', 'on', 'a', 'large', 'corpus', 'and', 'subsequently', 'transferred', 'to', 'other', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'NNS', ',', 'FW', ',', 'DT', 'NN', 'NN', 'NN', 'WDT', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'CC', 'RB', 'VBN', 'TO', 'JJ', 'NNS', '.']",35
natural_language_inference,53,18,"Here , we investigate whether supervised learning can be leveraged instead , taking inspiration from previous results in computer vision , where many models are pretrained on the ImageNet ) before being transferred .","['Here', ',', 'we', 'investigate', 'whether', 'supervised', 'learning', 'can', 'be', 'leveraged', 'instead', ',', 'taking', 'inspiration', 'from', 'previous', 'results', 'in', 'computer', 'vision', ',', 'where', 'many', 'models', 'are', 'pretrained', 'on', 'the', 'ImageNet', ')', 'before', 'being', 'transferred', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'IN', 'JJ', 'NN', 'MD', 'VB', 'VBN', 'RB', ',', 'VBG', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NN', 'NN', ',', 'WRB', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'NNP', ')', 'IN', 'VBG', 'VBN', '.']",34
natural_language_inference,53,22,"Hence , we investigate the impact of the sentence encoding architecture on representational transferability , and compare convolutional , recurrent and even simpler word composition schemes .","['Hence', ',', 'we', 'investigate', 'the', 'impact', 'of', 'the', 'sentence', 'encoding', 'architecture', 'on', 'representational', 'transferability', ',', 'and', 'compare', 'convolutional', ',', 'recurrent', 'and', 'even', 'simpler', 'word', 'composition', 'schemes', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBG', 'NN', 'IN', 'JJ', 'NN', ',', 'CC', 'JJ', 'NN', ',', 'NN', 'CC', 'RB', 'JJR', 'NN', 'NN', 'NNS', '.']",27
natural_language_inference,53,88,"For all our models trained on SNLI , we use SGD with a learning rate of 0.1 and a weight decay of 0.99 .","['For', 'all', 'our', 'models', 'trained', 'on', 'SNLI', ',', 'we', 'use', 'SGD', 'with', 'a', 'learning', 'rate', 'of', '0.1', 'and', 'a', 'weight', 'decay', 'of', '0.99', '.']","['B-p', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'PRP$', 'NNS', 'VBN', 'IN', 'NNP', ',', 'PRP', 'VBP', 'JJ', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', 'CC', 'DT', 'JJ', 'NN', 'IN', 'CD', '.']",24
natural_language_inference,53,89,"At each epoch , we divide the learning rate by 5 if the dev accuracy decreases .","['At', 'each', 'epoch', ',', 'we', 'divide', 'the', 'learning', 'rate', 'by', '5', 'if', 'the', 'dev', 'accuracy', 'decreases', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', 'NN', 'VBZ', '.']",17
natural_language_inference,53,90,We use minibatches of size 64 and training is stopped when the learning rate goes under the threshold of 10 ?5 .,"['We', 'use', 'minibatches', 'of', 'size', '64', 'and', 'training', 'is', 'stopped', 'when', 'the', 'learning', 'rate', 'goes', 'under', 'the', 'threshold', 'of', '10', '?5', '.']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNS', 'IN', 'NN', 'CD', 'CC', 'NN', 'VBZ', 'VBN', 'WRB', 'DT', 'NN', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'CD', 'NNS', '.']",22
natural_language_inference,53,91,"For the classifier , we use a multi - layer perceptron with 1 hidden - layer of 512 hidden units .","['For', 'the', 'classifier', ',', 'we', 'use', 'a', 'multi', '-', 'layer', 'perceptron', 'with', '1', 'hidden', '-', 'layer', 'of', '512', 'hidden', 'units', '.']","['O', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', ':', 'NN', 'NN', 'IN', 'CD', 'JJ', ':', 'NN', 'IN', 'CD', 'JJ', 'NNS', '.']",21
natural_language_inference,53,92,We use opensource GloVe vectors trained on Common Crawl 840B with 300 dimensions as fixed word embeddings .,"['We', 'use', 'opensource', 'GloVe', 'vectors', 'trained', 'on', 'Common', 'Crawl', '840B', 'with', '300', 'dimensions', 'as', 'fixed', 'word', 'embeddings', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NNP', 'NNS', 'VBD', 'IN', 'NNP', 'NNP', 'CD', 'IN', 'CD', 'NNS', 'IN', 'JJ', 'NN', 'NNS', '.']",18
natural_language_inference,53,141,Architecture impact,"['Architecture', 'impact']","['B-n', 'I-n']","['NNP', 'NN']",2
natural_language_inference,53,144,The BiLSTM - 4096 with the max - pooling operation performs best on both SNLI and transfer tasks .,"['The', 'BiLSTM', '-', '4096', 'with', 'the', 'max', '-', 'pooling', 'operation', 'performs', 'best', 'on', 'both', 'SNLI', 'and', 'transfer', 'tasks', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', ':', 'CD', 'IN', 'DT', 'NN', ':', 'NN', 'NN', 'NNS', 'RBS', 'IN', 'DT', 'NNP', 'CC', 'VB', 'NNS', '.']",19
natural_language_inference,53,145,"Looking at the micro and macro averages , we see that it performs significantly better than the other models LSTM , GRU , BiGRU - last , BiLSTM - Mean , inner-attention and the hierarchical - ConvNet. also shows that better performance on the training task does not necessarily translate in better results on the transfer tasks like when comparing inner-attention and BiLSTM - Mean for instance .","['Looking', 'at', 'the', 'micro', 'and', 'macro', 'averages', ',', 'we', 'see', 'that', 'it', 'performs', 'significantly', 'better', 'than', 'the', 'other', 'models', 'LSTM', ',', 'GRU', ',', 'BiGRU', '-', 'last', ',', 'BiLSTM', '-', 'Mean', ',', 'inner-attention', 'and', 'the', 'hierarchical', '-', 'ConvNet.', 'also', 'shows', 'that', 'better', 'performance', 'on', 'the', 'training', 'task', 'does', 'not', 'necessarily', 'translate', 'in', 'better', 'results', 'on', 'the', 'transfer', 'tasks', 'like', 'when', 'comparing', 'inner-attention', 'and', 'BiLSTM', '-', 'Mean', 'for', 'instance', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'IN', 'DT', 'NN', 'CC', 'NN', 'NNS', ',', 'PRP', 'VBP', 'IN', 'PRP', 'VBZ', 'RB', 'JJR', 'IN', 'DT', 'JJ', 'NNS', 'NNP', ',', 'NNP', ',', 'NNP', ':', 'JJ', ',', 'NNP', ':', 'NN', ',', 'NN', 'CC', 'DT', 'JJ', ':', 'NN', 'RB', 'VBZ', 'IN', 'JJR', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'RB', 'RB', 'VB', 'IN', 'JJR', 'NNS', 'IN', 'DT', 'NN', 'NNS', 'IN', 'WRB', 'VBG', 'NN', 'CC', 'NNP', ':', 'NN', 'IN', 'NN', '.']",68
natural_language_inference,53,150,"For a given model , the transfer quality is also sensitive to the optimization algorithm : when training with Adam instead of SGD , we observed that the BiLSTM - max converged faster on SNLI ( 5 epochs instead of 10 ) , but obtained worse results on the transfer tasks , most likely because of the model and classifier 's increased capability to over-specialize on the training task .","['For', 'a', 'given', 'model', ',', 'the', 'transfer', 'quality', 'is', 'also', 'sensitive', 'to', 'the', 'optimization', 'algorithm', ':', 'when', 'training', 'with', 'Adam', 'instead', 'of', 'SGD', ',', 'we', 'observed', 'that', 'the', 'BiLSTM', '-', 'max', 'converged', 'faster', 'on', 'SNLI', '(', '5', 'epochs', 'instead', 'of', '10', ')', ',', 'but', 'obtained', 'worse', 'results', 'on', 'the', 'transfer', 'tasks', ',', 'most', 'likely', 'because', 'of', 'the', 'model', 'and', 'classifier', ""'s"", 'increased', 'capability', 'to', 'over-specialize', 'on', 'the', 'training', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'VBN', 'NN', ',', 'DT', 'NN', 'NN', 'VBZ', 'RB', 'JJ', 'TO', 'DT', 'NN', 'NN', ':', 'WRB', 'VBG', 'IN', 'NNP', 'RB', 'IN', 'NNP', ',', 'PRP', 'VBD', 'IN', 'DT', 'NNP', ':', 'NN', 'VBD', 'RBR', 'IN', 'NNP', '(', 'CD', 'NNS', 'RB', 'IN', 'CD', ')', ',', 'CC', 'VBD', 'JJR', 'NNS', 'IN', 'DT', 'NN', 'NNS', ',', 'JJS', 'JJ', 'IN', 'IN', 'DT', 'NN', 'CC', 'NN', 'POS', 'JJ', 'NN', 'TO', 'VB', 'IN', 'DT', 'NN', 'NN', '.']",70
natural_language_inference,53,152,Embedding size,"['Embedding', 'size']","['B-n', 'I-n']","['VBG', 'NN']",2
natural_language_inference,53,153,"Since it is easier to linearly separate in high dimension , especially with logistic regression , it is not surprising that increased embedding sizes lead to increased performance for almost all models .","['Since', 'it', 'is', 'easier', 'to', 'linearly', 'separate', 'in', 'high', 'dimension', ',', 'especially', 'with', 'logistic', 'regression', ',', 'it', 'is', 'not', 'surprising', 'that', 'increased', 'embedding', 'sizes', 'lead', 'to', 'increased', 'performance', 'for', 'almost', 'all', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'PRP', 'VBZ', 'JJR', 'TO', 'VB', 'JJ', 'IN', 'JJ', 'NN', ',', 'RB', 'IN', 'JJ', 'NN', ',', 'PRP', 'VBZ', 'RB', 'JJ', 'IN', 'VBD', 'VBG', 'JJ', 'NN', 'TO', 'VBN', 'NN', 'IN', 'RB', 'DT', 'NNS', '.']",33
natural_language_inference,53,170,Comparison with SkipThought,"['Comparison', 'with', 'SkipThought']","['B-n', 'I-n', 'I-n']","['NNP', 'IN', 'NNP']",3
natural_language_inference,53,172,"With much less data ( 570 k compared to 64M sentences ) but with high - quality supervision from the SNLI dataset , we are able to consistently outperform the results obtained by SkipThought vectors .","['With', 'much', 'less', 'data', '(', '570', 'k', 'compared', 'to', '64M', 'sentences', ')', 'but', 'with', 'high', '-', 'quality', 'supervision', 'from', 'the', 'SNLI', 'dataset', ',', 'we', 'are', 'able', 'to', 'consistently', 'outperform', 'the', 'results', 'obtained', 'by', 'SkipThought', 'vectors', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'JJ', 'JJR', 'NNS', '(', 'CD', 'NN', 'VBN', 'TO', 'CD', 'NNS', ')', 'CC', 'IN', 'JJ', ':', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', ',', 'PRP', 'VBP', 'JJ', 'TO', 'RB', 'VB', 'DT', 'NNS', 'VBN', 'IN', 'NNP', 'NNS', '.']",36
natural_language_inference,53,174,"Our BiLSTM - max trained on SNLI performs much better than released SkipThought vectors on MR , CR , MPQA , SST , MRPC - accuracy , SICK - R , SICK - E and STS14 ( see ) .","['Our', 'BiLSTM', '-', 'max', 'trained', 'on', 'SNLI', 'performs', 'much', 'better', 'than', 'released', 'SkipThought', 'vectors', 'on', 'MR', ',', 'CR', ',', 'MPQA', ',', 'SST', ',', 'MRPC', '-', 'accuracy', ',', 'SICK', '-', 'R', ',', 'SICK', '-', 'E', 'and', 'STS14', '(', 'see', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O']","['PRP$', 'NNP', ':', 'NN', 'VBD', 'IN', 'NNP', 'NNS', 'RB', 'JJR', 'IN', 'VBN', 'NNP', 'NNS', 'IN', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', ':', 'NN', ',', 'NNP', ':', 'NN', ',', 'NNP', ':', 'NN', 'CC', 'NNP', '(', 'VB', ')', '.']",40
natural_language_inference,53,175,"Except for the SUBJ dataset , it also performs better than SkipThought - LN on MR , CR and MPQA .","['Except', 'for', 'the', 'SUBJ', 'dataset', ',', 'it', 'also', 'performs', 'better', 'than', 'SkipThought', '-', 'LN', 'on', 'MR', ',', 'CR', 'and', 'MPQA', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O']","['IN', 'IN', 'DT', 'NNP', 'NN', ',', 'PRP', 'RB', 'VBZ', 'JJR', 'IN', 'NNP', ':', 'NN', 'IN', 'NNP', ',', 'NNP', 'CC', 'NNP', '.']",21
natural_language_inference,53,176,We also observe by looking at the STS14 results that the cosine metrics in our embedding space is much more semantically informative than in SkipThought embedding space ( pearson score of 0.68 compared to 0.29 and 0.44 for ST and ST - LN ) .,"['We', 'also', 'observe', 'by', 'looking', 'at', 'the', 'STS14', 'results', 'that', 'the', 'cosine', 'metrics', 'in', 'our', 'embedding', 'space', 'is', 'much', 'more', 'semantically', 'informative', 'than', 'in', 'SkipThought', 'embedding', 'space', '(', 'pearson', 'score', 'of', '0.68', 'compared', 'to', '0.29', 'and', '0.44', 'for', 'ST', 'and', 'ST', '-', 'LN', ')', '.']","['O', 'O', 'B-p', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'IN', 'VBG', 'IN', 'DT', 'NNP', 'NNS', 'IN', 'DT', 'NN', 'NNS', 'IN', 'PRP$', 'VBG', 'NN', 'VBZ', 'RB', 'RBR', 'RB', 'JJ', 'IN', 'IN', 'NNP', 'VBG', 'NN', '(', 'JJ', 'NN', 'IN', 'CD', 'VBN', 'TO', 'CD', 'CC', 'CD', 'IN', 'NNP', 'CC', 'NNP', ':', 'NN', ')', '.']",45
natural_language_inference,53,178,NLI as a supervised training set,"['NLI', 'as', 'a', 'supervised', 'training', 'set']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'IN', 'DT', 'JJ', 'NN', 'NN']",6
natural_language_inference,53,179,"Our findings indicate that our model trained on SNLI obtains much better over all results than models trained on other supervised tasks such as COCO , dictionary definitions , NMT , PPDB and SST .","['Our', 'findings', 'indicate', 'that', 'our', 'model', 'trained', 'on', 'SNLI', 'obtains', 'much', 'better', 'over', 'all', 'results', 'than', 'models', 'trained', 'on', 'other', 'supervised', 'tasks', 'such', 'as', 'COCO', ',', 'dictionary', 'definitions', ',', 'NMT', ',', 'PPDB', 'and', 'SST', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O']","['PRP$', 'NNS', 'VBP', 'IN', 'PRP$', 'NN', 'VBN', 'IN', 'NNP', 'VBZ', 'RB', 'JJR', 'IN', 'DT', 'NNS', 'IN', 'NNS', 'VBN', 'IN', 'JJ', 'JJ', 'NNS', 'JJ', 'IN', 'NNP', ',', 'JJ', 'NNS', ',', 'NNP', ',', 'NNP', 'CC', 'NNP', '.']",35
natural_language_inference,53,183,Domain adaptation on SICK tasks,"['Domain', 'adaptation', 'on', 'SICK', 'tasks']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NN', 'IN', 'NNP', 'NNS']",5
natural_language_inference,53,184,Our transfer learning approach obtains better results than previous state - of - the - art on the SICK task - can be seen as an out - domain version of SNLI - for both entailment and relatedness .,"['Our', 'transfer', 'learning', 'approach', 'obtains', 'better', 'results', 'than', 'previous', 'state', '-', 'of', '-', 'the', '-', 'art', 'on', 'the', 'SICK', 'task', '-', 'can', 'be', 'seen', 'as', 'an', 'out', '-', 'domain', 'version', 'of', 'SNLI', '-', 'for', 'both', 'entailment', 'and', 'relatedness', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'VBG', 'NN', 'NNS', 'RBR', 'NNS', 'IN', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'DT', 'NNP', 'NN', ':', 'MD', 'VB', 'VBN', 'IN', 'DT', 'IN', ':', 'NN', 'NN', 'IN', 'NNP', ':', 'IN', 'DT', 'NN', 'CC', 'NN', '.']",39
natural_language_inference,53,185,"We obtain a pearson score of 0.885 on SICK - R while obtained 0.868 , and we obtain 86.3 % test accuracy on SICK - E while previous best handengineered models obtained 84.5 % .","['We', 'obtain', 'a', 'pearson', 'score', 'of', '0.885', 'on', 'SICK', '-', 'R', 'while', 'obtained', '0.868', ',', 'and', 'we', 'obtain', '86.3', '%', 'test', 'accuracy', 'on', 'SICK', '-', 'E', 'while', 'previous', 'best', 'handengineered', 'models', 'obtained', '84.5', '%', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VB', 'DT', 'NN', 'NN', 'IN', 'CD', 'IN', 'NNP', ':', 'NN', 'IN', 'VBN', 'CD', ',', 'CC', 'PRP', 'VB', 'CD', 'NN', 'NN', 'NN', 'IN', 'NNP', ':', 'NN', 'IN', 'JJ', 'JJS', 'JJ', 'NNS', 'VBN', 'CD', 'NN', '.']",35
natural_language_inference,53,186,"We also significantly outperformed previous transfer learning approaches on SICK - E ( Bowman et al. , 2015 ) that used the parameters of an LSTM model trained on SNLI to fine - tune on SICK ( 80.8 % accuracy ) .","['We', 'also', 'significantly', 'outperformed', 'previous', 'transfer', 'learning', 'approaches', 'on', 'SICK', '-', 'E', '(', 'Bowman', 'et', 'al.', ',', '2015', ')', 'that', 'used', 'the', 'parameters', 'of', 'an', 'LSTM', 'model', 'trained', 'on', 'SNLI', 'to', 'fine', '-', 'tune', 'on', 'SICK', '(', '80.8', '%', 'accuracy', ')', '.']","['O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'RB', 'VBN', 'JJ', 'NN', 'VBG', 'NNS', 'IN', 'NNP', ':', 'NN', '(', 'NNP', 'VBZ', 'RB', ',', 'CD', ')', 'WDT', 'VBD', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'VBD', 'IN', 'NNP', 'TO', 'VB', ':', 'NN', 'IN', 'NNP', '(', 'CD', 'NN', 'NN', ')', '.']",42
natural_language_inference,53,188,Image - caption retrieval results,"['Image', '-', 'caption', 'retrieval', 'results']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NN', 'NN', 'NNS']",5
natural_language_inference,53,191,"When trained with ResNet features and 30 k more training data , the SkipThought vectors perform significantly better than the original setting , going from 33.8 to 37.9 for caption retrieval R@1 , and from 25.9 to 30.6 on image retrieval R@1 .","['When', 'trained', 'with', 'ResNet', 'features', 'and', '30', 'k', 'more', 'training', 'data', ',', 'the', 'SkipThought', 'vectors', 'perform', 'significantly', 'better', 'than', 'the', 'original', 'setting', ',', 'going', 'from', '33.8', 'to', '37.9', 'for', 'caption', 'retrieval', 'R@1', ',', 'and', 'from', '25.9', 'to', '30.6', 'on', 'image', 'retrieval', 'R@1', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['WRB', 'VBN', 'IN', 'NNP', 'NNS', 'CC', 'CD', 'NN', 'RBR', 'NN', 'NNS', ',', 'DT', 'NNP', 'NNS', 'VBP', 'RB', 'JJR', 'IN', 'DT', 'JJ', 'NN', ',', 'VBG', 'IN', 'CD', 'TO', 'CD', 'IN', 'NN', 'NN', 'NNP', ',', 'CC', 'IN', 'CD', 'TO', 'CD', 'IN', 'NN', 'NN', 'NNP', '.']",43
natural_language_inference,53,192,"Our approach pushes the results even further , from 37.9 to 42.4 on cap-tion retrieval , and 30.6 to 33.2 on image retrieval .","['Our', 'approach', 'pushes', 'the', 'results', 'even', 'further', ',', 'from', '37.9', 'to', '42.4', 'on', 'cap-tion', 'retrieval', ',', 'and', '30.6', 'to', '33.2', 'on', 'image', 'retrieval', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'DT', 'NNS', 'RB', 'RBR', ',', 'IN', 'CD', 'TO', 'CD', 'IN', 'NN', 'NN', ',', 'CC', 'CD', 'TO', 'CD', 'IN', 'NN', 'NN', '.']",24
natural_language_inference,53,195,MultiGenre NLI,"['MultiGenre', 'NLI']","['B-n', 'I-n']","['NNP', 'NNP']",2
natural_language_inference,53,199,We observe a significant boost in performance over all compared to the model trained only on SLNI .,"['We', 'observe', 'a', 'significant', 'boost', 'in', 'performance', 'over', 'all', 'compared', 'to', 'the', 'model', 'trained', 'only', 'on', 'SLNI', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'DT', 'VBN', 'TO', 'DT', 'NN', 'VBD', 'RB', 'IN', 'NNP', '.']",18
natural_language_inference,53,200,"Our model even reaches AdaSent performance on CR , suggesting that having a larger coverage for the training task helps learn even better general representations .","['Our', 'model', 'even', 'reaches', 'AdaSent', 'performance', 'on', 'CR', ',', 'suggesting', 'that', 'having', 'a', 'larger', 'coverage', 'for', 'the', 'training', 'task', 'helps', 'learn', 'even', 'better', 'general', 'representations', '.']","['B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'RB', 'VBZ', 'NNP', 'NN', 'IN', 'NNP', ',', 'VBG', 'IN', 'VBG', 'DT', 'JJR', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'VB', 'RB', 'RBR', 'JJ', 'NNS', '.']",26
natural_language_inference,54,2,Structural Embedding of Syntactic Trees for Machine Comprehension,"['Structural', 'Embedding', 'of', 'Syntactic', 'Trees', 'for', 'Machine', 'Comprehension']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['JJ', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",8
natural_language_inference,54,9,"Reading comprehension such as SQuAD or News QA requires identifying a span from a given context , which is an extension to the traditional question answering task , aiming at responding questions posed by human with natural language .","['Reading', 'comprehension', 'such', 'as', 'SQuAD', 'or', 'News', 'QA', 'requires', 'identifying', 'a', 'span', 'from', 'a', 'given', 'context', ',', 'which', 'is', 'an', 'extension', 'to', 'the', 'traditional', 'question', 'answering', 'task', ',', 'aiming', 'at', 'responding', 'questions', 'posed', 'by', 'human', 'with', 'natural', 'language', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NN', 'JJ', 'IN', 'NNP', 'CC', 'NNP', 'NNP', 'VBZ', 'VBG', 'DT', 'NN', 'IN', 'DT', 'VBN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'TO', 'DT', 'JJ', 'NN', 'VBG', 'NN', ',', 'VBG', 'IN', 'VBG', 'NNS', 'VBN', 'IN', 'NN', 'IN', 'JJ', 'NN', '.']",39
natural_language_inference,54,26,"In this paper , we propose Structural Embedding of Syntactic Trees ( SEST ) that encode syntactic information structured by constituency tree and dependency tree into neural attention models for the question answering task .","['In', 'this', 'paper', ',', 'we', 'propose', 'Structural', 'Embedding', 'of', 'Syntactic', 'Trees', '(', 'SEST', ')', 'that', 'encode', 'syntactic', 'information', 'structured', 'by', 'constituency', 'tree', 'and', 'dependency', 'tree', 'into', 'neural', 'attention', 'models', 'for', 'the', 'question', 'answering', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NNP', 'IN', 'NNP', 'NNP', '(', 'NNP', ')', 'IN', 'VBZ', 'JJ', 'NN', 'VBN', 'IN', 'NN', 'NN', 'CC', 'NN', 'VBP', 'IN', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'NN', 'VBG', 'NN', '.']",35
natural_language_inference,54,124,We run our experiments on a machine that contains a single GTX 1080 GPU with 8 GB VRAM .,"['We', 'run', 'our', 'experiments', 'on', 'a', 'machine', 'that', 'contains', 'a', 'single', 'GTX', '1080', 'GPU', 'with', '8', 'GB', 'VRAM', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'NNS', 'IN', 'DT', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'NNP', 'CD', 'NNP', 'IN', 'CD', 'NNP', 'NNP', '.']",19
natural_language_inference,54,126,"As introduced in Section 2 , we use a variable character embedding with a fixed pre-trained word embedding to serve as part of the input into the model .","['As', 'introduced', 'in', 'Section', '2', ',', 'we', 'use', 'a', 'variable', 'character', 'embedding', 'with', 'a', 'fixed', 'pre-trained', 'word', 'embedding', 'to', 'serve', 'as', 'part', 'of', 'the', 'input', 'into', 'the', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']","['IN', 'VBN', 'IN', 'NN', 'CD', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBG', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'VBG', 'TO', 'VB', 'IN', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",29
natural_language_inference,54,127,The character embedding is implemented using CNN with a one -dimensional layer consists of 100 units with a channel size of 5 .,"['The', 'character', 'embedding', 'is', 'implemented', 'using', 'CNN', 'with', 'a', 'one', '-dimensional', 'layer', 'consists', 'of', '100', 'units', 'with', 'a', 'channel', 'size', 'of', '5', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'VBG', 'NNP', 'IN', 'DT', 'CD', 'JJ', 'NN', 'VBZ', 'IN', 'CD', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",23
natural_language_inference,54,128,It has an input depth of 8 .,"['It', 'has', 'an', 'input', 'depth', 'of', '8', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",8
natural_language_inference,54,129,The max length of SQuAD is 16 which means there are a maximum 16 words in a sentence .,"['The', 'max', 'length', 'of', 'SQuAD', 'is', '16', 'which', 'means', 'there', 'are', 'a', 'maximum', '16', 'words', 'in', 'a', 'sentence', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'IN', 'NNP', 'VBZ', 'CD', 'WDT', 'VBZ', 'EX', 'VBP', 'DT', 'JJ', 'CD', 'NNS', 'IN', 'DT', 'NN', '.']",19
natural_language_inference,54,130,"The fixed word embedding has a dimension of 100 , which is provided by the GloVe data set .","['The', 'fixed', 'word', 'embedding', 'has', 'a', 'dimension', 'of', '100', ',', 'which', 'is', 'provided', 'by', 'the', 'GloVe', 'data', 'set', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'CD', ',', 'WDT', 'VBZ', 'VBN', 'IN', 'DT', 'NNP', 'NN', 'NN', '.']",19
natural_language_inference,54,133,The POS model contains syntactic information with 39 different POS tags that serve as both input and output .,"['The', 'POS', 'model', 'contains', 'syntactic', 'information', 'with', '39', 'different', 'POS', 'tags', 'that', 'serve', 'as', 'both', 'input', 'and', 'output', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'NN', 'VBZ', 'JJ', 'NN', 'IN', 'CD', 'JJ', 'NNP', 'NN', 'WDT', 'VBP', 'IN', 'DT', 'NN', 'CC', 'NN', '.']",19
natural_language_inference,54,134,For SECT and SEDT the input of the model has a size of 8 with 30 units to be output .,"['For', 'SECT', 'and', 'SEDT', 'the', 'input', 'of', 'the', 'model', 'has', 'a', 'size', 'of', '8', 'with', '30', 'units', 'to', 'be', 'output', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'NNP', 'CC', 'NNP', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'IN', 'CD', 'NNS', 'TO', 'VB', 'NN', '.']",21
natural_language_inference,54,135,"Both of them has a maximum length size that is set to be 10 and 20 respectively , which values will be further discussed in Section 4.5 .","['Both', 'of', 'them', 'has', 'a', 'maximum', 'length', 'size', 'that', 'is', 'set', 'to', 'be', '10', 'and', '20', 'respectively', ',', 'which', 'values', 'will', 'be', 'further', 'discussed', 'in', 'Section', '4.5', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'IN', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'VBN', 'TO', 'VB', 'CD', 'CC', 'CD', 'RB', ',', 'WDT', 'NNS', 'MD', 'VB', 'RB', 'VBN', 'IN', 'NNP', 'CD', '.']",28
natural_language_inference,54,139,Predictive Performance,"['Predictive', 'Performance']","['B-n', 'I-n']","['JJ', 'NN']",2
natural_language_inference,54,140,"We first compared the performance of single models between the baseline approach BiDAF and the proposed SEST approaches , including SE - POS , SECT - LSTM , SECT - CNN , SEDT - LSTM , and SEDT - CNN , on the development dataset of SQuAD .","['We', 'first', 'compared', 'the', 'performance', 'of', 'single', 'models', 'between', 'the', 'baseline', 'approach', 'BiDAF', 'and', 'the', 'proposed', 'SEST', 'approaches', ',', 'including', 'SE', '-', 'POS', ',', 'SECT', '-', 'LSTM', ',', 'SECT', '-', 'CNN', ',', 'SEDT', '-', 'LSTM', ',', 'and', 'SEDT', '-', 'CNN', ',', 'on', 'the', 'development', 'dataset', 'of', 'SQuAD', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['PRP', 'RB', 'VBN', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', 'NNP', 'CC', 'DT', 'VBN', 'NNP', 'NNS', ',', 'VBG', 'NNP', ':', 'NN', ',', 'NNP', ':', 'NNP', ',', 'NNP', ':', 'NNP', ',', 'NNP', ':', 'NNP', ',', 'CC', 'NNP', ':', 'NNP', ',', 'IN', 'DT', 'NN', 'NN', 'IN', 'NNP', '.']",48
natural_language_inference,54,145,"Another observation is that our propose models achieve higher relative improvements in EM scores than F 1 scores over the baseline methods , providing the evidence that syntactic information can accurately locate the boundaries of the answer .","['Another', 'observation', 'is', 'that', 'our', 'propose', 'models', 'achieve', 'higher', 'relative', 'improvements', 'in', 'EM', 'scores', 'than', 'F', '1', 'scores', 'over', 'the', 'baseline', 'methods', ',', 'providing', 'the', 'evidence', 'that', 'syntactic', 'information', 'can', 'accurately', 'locate', 'the', 'boundaries', 'of', 'the', 'answer', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'IN', 'PRP$', 'JJ', 'NNS', 'VBP', 'JJR', 'JJ', 'NNS', 'IN', 'NNP', 'NNS', 'IN', 'NNP', 'CD', 'NNS', 'IN', 'DT', 'NN', 'NNS', ',', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'NN', 'MD', 'RB', 'VB', 'DT', 'NNS', 'IN', 'DT', 'NN', '.']",38
natural_language_inference,54,146,"Moreover , we found that both SECT - LSTM and SEDT - LSTM have better performance than their CNN counterparts , which suggests that LSTM can more effectively preserve the syntactic information .","['Moreover', ',', 'we', 'found', 'that', 'both', 'SECT', '-', 'LSTM', 'and', 'SEDT', '-', 'LSTM', 'have', 'better', 'performance', 'than', 'their', 'CNN', 'counterparts', ',', 'which', 'suggests', 'that', 'LSTM', 'can', 'more', 'effectively', 'preserve', 'the', 'syntactic', 'information', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBD', 'IN', 'DT', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NN', 'VBP', 'JJR', 'NN', 'IN', 'PRP$', 'NNP', 'NNS', ',', 'WDT', 'VBZ', 'IN', 'NNP', 'MD', 'RBR', 'RB', 'VB', 'DT', 'JJ', 'NN', '.']",33
natural_language_inference,54,152,Contribution of Syntactic Sequence,"['Contribution', 'of', 'Syntactic', 'Sequence']","['B-n', 'I-n', 'I-n', 'I-n']","['NN', 'IN', 'JJ', 'NN']",4
natural_language_inference,54,160,From the table we see that both the ordering and the contents of the syntactic tree are important for the models to work properly : constituency and dependency trees achieved over 20 % boost on performance compared to the randomly generated ones and our proposed ordering also out - performed the random ordering .,"['From', 'the', 'table', 'we', 'see', 'that', 'both', 'the', 'ordering', 'and', 'the', 'contents', 'of', 'the', 'syntactic', 'tree', 'are', 'important', 'for', 'the', 'models', 'to', 'work', 'properly', ':', 'constituency', 'and', 'dependency', 'trees', 'achieved', 'over', '20', '%', 'boost', 'on', 'performance', 'compared', 'to', 'the', 'randomly', 'generated', 'ones', 'and', 'our', 'proposed', 'ordering', 'also', 'out', '-', 'performed', 'the', 'random', 'ordering', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'IN', 'DT', 'DT', 'NN', 'CC', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'VBP', 'JJ', 'IN', 'DT', 'NNS', 'TO', 'VB', 'RB', ':', 'NN', 'CC', 'NN', 'NNS', 'VBD', 'IN', 'CD', 'NN', 'NN', 'IN', 'NN', 'VBN', 'TO', 'DT', 'RB', 'VBN', 'NNS', 'CC', 'PRP$', 'VBN', 'NN', 'RB', 'RP', ':', 'VBD', 'DT', 'NN', 'NN', '.']",54
natural_language_inference,54,161,It also worth mentioning that the ordering of dependency trees seems to have less impact on the performance compared to that of the constituency trees .,"['It', 'also', 'worth', 'mentioning', 'that', 'the', 'ordering', 'of', 'dependency', 'trees', 'seems', 'to', 'have', 'less', 'impact', 'on', 'the', 'performance', 'compared', 'to', 'that', 'of', 'the', 'constituency', 'trees', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBD', 'VBG', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', 'VBZ', 'TO', 'VB', 'JJR', 'NN', 'IN', 'DT', 'NN', 'VBN', 'TO', 'DT', 'IN', 'DT', 'NN', 'NNS', '.']",26
natural_language_inference,54,164,Window Size Analysis,"['Window', 'Size', 'Analysis']","['B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP']",3
natural_language_inference,54,166,"In practice , we found that limiting the window size also benefits the performance of our models .","['In', 'practice', ',', 'we', 'found', 'that', 'limiting', 'the', 'window', 'size', 'also', 'benefits', 'the', 'performance', 'of', 'our', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBD', 'IN', 'VBG', 'DT', 'NN', 'NN', 'RB', 'NNS', 'DT', 'NN', 'IN', 'PRP$', 'NNS', '.']",18
natural_language_inference,54,169,In general the results illustrate that performances of the models increase with the length of the window .,"['In', 'general', 'the', 'results', 'illustrate', 'that', 'performances', 'of', 'the', 'models', 'increase', 'with', 'the', 'length', 'of', 'the', 'window', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']","['IN', 'JJ', 'DT', 'NNS', 'VBP', 'IN', 'NNS', 'IN', 'DT', 'NNS', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",18
natural_language_inference,54,171,We also observed that larger window size does not generate predictive results that is as good as the one with window size set to 10 .,"['We', 'also', 'observed', 'that', 'larger', 'window', 'size', 'does', 'not', 'generate', 'predictive', 'results', 'that', 'is', 'as', 'good', 'as', 'the', 'one', 'with', 'window', 'size', 'set', 'to', '10', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'RB', 'VBD', 'IN', 'JJR', 'NN', 'NN', 'VBZ', 'RB', 'VB', 'JJ', 'NNS', 'WDT', 'VBZ', 'RB', 'JJ', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NN', 'VBN', 'TO', 'CD', '.']",26
natural_language_inference,13,2,Multi-range Reasoning for Machine Comprehension,"['Multi-range', 'Reasoning', 'for', 'Machine', 'Comprehension']","['O', 'O', 'O', 'B-n', 'I-n']","['NN', 'VBG', 'IN', 'NNP', 'NNP']",5
natural_language_inference,13,4,"We propose MRU ( Multi - Range Reasoning Units ) , a new fast compositional encoder for machine comprehension ( MC ) .","['We', 'propose', 'MRU', '(', 'Multi', '-', 'Range', 'Reasoning', 'Units', ')', ',', 'a', 'new', 'fast', 'compositional', 'encoder', 'for', 'machine', 'comprehension', '(', 'MC', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', '(', 'NNP', ':', 'NN', 'VBG', 'NNS', ')', ',', 'DT', 'JJ', 'JJ', 'JJ', 'NN', 'IN', 'NN', 'NN', '(', 'NNP', ')', '.']",23
natural_language_inference,13,19,"While the usage of recurrent encoder is often regarded as indispensable in highly complex MC tasks , there are still several challenges and problems pertaining to it 's usage in modern MC tasks .","['While', 'the', 'usage', 'of', 'recurrent', 'encoder', 'is', 'often', 'regarded', 'as', 'indispensable', 'in', 'highly', 'complex', 'MC', 'tasks', ',', 'there', 'are', 'still', 'several', 'challenges', 'and', 'problems', 'pertaining', 'to', 'it', ""'s"", 'usage', 'in', 'modern', 'MC', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'JJ', 'IN', 'RB', 'JJ', 'NNP', 'NNS', ',', 'EX', 'VBP', 'RB', 'JJ', 'NNS', 'CC', 'NNS', 'VBG', 'TO', 'PRP', 'VBZ', 'JJ', 'IN', 'JJ', 'NNP', 'NNS', '.']",34
natural_language_inference,13,24,"To this end , we propose a new compositional encoder that can either be used in - place of standard RNN encoders or serve as a new module that is complementary to existing neural architectures .","['To', 'this', 'end', ',', 'we', 'propose', 'a', 'new', 'compositional', 'encoder', 'that', 'can', 'either', 'be', 'used', 'in', '-', 'place', 'of', 'standard', 'RNN', 'encoders', 'or', 'serve', 'as', 'a', 'new', 'module', 'that', 'is', 'complementary', 'to', 'existing', 'neural', 'architectures', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'WDT', 'MD', 'RB', 'VB', 'VBN', 'IN', ':', 'NN', 'IN', 'JJ', 'NNP', 'NNS', 'CC', 'VBP', 'IN', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'JJ', 'TO', 'VBG', 'JJ', 'NNS', '.']",36
natural_language_inference,13,25,Our proposed MRU encoders learns gating vectors via multiple contract - and - expand layers at multiple dilated resolutions .,"['Our', 'proposed', 'MRU', 'encoders', 'learns', 'gating', 'vectors', 'via', 'multiple', 'contract', '-', 'and', '-', 'expand', 'layers', 'at', 'multiple', 'dilated', 'resolutions', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'VBN', 'NNP', 'NNS', 'VBZ', 'VBG', 'NNS', 'IN', 'JJ', 'NN', ':', 'CC', ':', 'NN', 'NNS', 'IN', 'JJ', 'JJ', 'NNS', '.']",20
natural_language_inference,13,26,"Specifically , we compress the input document an arbitrary k times at multi-ranges ( e.g. , 1 , 2 , 4 , 10 , 25 ) into a neural bag - of - words ( summed ) representation .","['Specifically', ',', 'we', 'compress', 'the', 'input', 'document', 'an', 'arbitrary', 'k', 'times', 'at', 'multi-ranges', '(', 'e.g.', ',', '1', ',', '2', ',', '4', ',', '10', ',', '25', ')', 'into', 'a', 'neural', 'bag', '-', 'of', '-', 'words', '(', 'summed', ')', 'representation', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBD', 'DT', 'JJ', 'NN', 'NNS', 'IN', 'NNS', '(', 'NN', ',', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ')', 'IN', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'NNS', '(', 'VBN', ')', 'NN', '.']",39
natural_language_inference,13,27,The compact sequence is then passed through affine transformation layers and then re-expanded to the original sequence length .,"['The', 'compact', 'sequence', 'is', 'then', 'passed', 'through', 'affine', 'transformation', 'layers', 'and', 'then', 're-expanded', 'to', 'the', 'original', 'sequence', 'length', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'JJ', 'NN', 'NNS', 'CC', 'RB', 'VBD', 'TO', 'DT', 'JJ', 'NN', 'NN', '.']",19
natural_language_inference,13,28,The k document representations ( at multiple ranges and n-gram blocks ) are then combined and modeled with fully connected layers to form the final compositional gate which are applied onto the original input document .,"['The', 'k', 'document', 'representations', '(', 'at', 'multiple', 'ranges', 'and', 'n-gram', 'blocks', ')', 'are', 'then', 'combined', 'and', 'modeled', 'with', 'fully', 'connected', 'layers', 'to', 'form', 'the', 'final', 'compositional', 'gate', 'which', 'are', 'applied', 'onto', 'the', 'original', 'input', 'document', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'NNS', '(', 'IN', 'NN', 'NNS', 'CC', 'JJ', 'NNS', ')', 'VBP', 'RB', 'VBN', 'CC', 'VBN', 'IN', 'RB', 'VBN', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",36
natural_language_inference,13,153,"RACE - the key competitors are the Stanford Attention Reader ( Stanford AR ) , Gated Attention Reader ( GA ) , and Dynamic Fusion Networks ( DFN ) .","['RACE', '-', 'the', 'key', 'competitors', 'are', 'the', 'Stanford', 'Attention', 'Reader', '(', 'Stanford', 'AR', ')', ',', 'Gated', 'Attention', 'Reader', '(', 'GA', ')', ',', 'and', 'Dynamic', 'Fusion', 'Networks', '(', 'DFN', ')', '.']","['B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'DT', 'JJ', 'NNS', 'VBP', 'DT', 'NNP', 'NNP', 'NNP', '(', 'NNP', 'NNP', ')', ',', 'VBN', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'CC', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', '.']",30
natural_language_inference,13,157,Search QA - the main competitor baseline is the AMANDA model proposed by .,"['Search', 'QA', '-', 'the', 'main', 'competitor', 'baseline', 'is', 'the', 'AMANDA', 'model', 'proposed', 'by', '.']","['B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['NNP', 'NNP', ':', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'DT', 'NNP', 'NN', 'VBN', 'IN', '.']",14
natural_language_inference,13,161,NarrativeQA,['NarrativeQA'],['B-n'],['NN'],1
natural_language_inference,13,163,"We compete on the summaries setting , in which the baselines are a context - less sequence to sequence ( seq2seq ) model , ASR and BiDAF .","['We', 'compete', 'on', 'the', 'summaries', 'setting', ',', 'in', 'which', 'the', 'baselines', 'are', 'a', 'context', '-', 'less', 'sequence', 'to', 'sequence', '(', 'seq2seq', ')', 'model', ',', 'ASR', 'and', 'BiDAF', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NNS', 'VBG', ',', 'IN', 'WDT', 'DT', 'NNS', 'VBP', 'DT', 'JJ', ':', 'JJR', 'NN', 'TO', 'VB', '(', 'NN', ')', 'NN', ',', 'NNP', 'CC', 'NNP', '.']",28
natural_language_inference,13,176,We implement all models in TensorFlow .,"['We', 'implement', 'all', 'models', 'in', 'TensorFlow', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNS', 'IN', 'NNP', '.']",7
natural_language_inference,13,177,Word embeddings are initialized with 300d Glo Ve vectors and are not fine - tuned during training .,"['Word', 'embeddings', 'are', 'initialized', 'with', '300d', 'Glo', 'Ve', 'vectors', 'and', 'are', 'not', 'fine', '-', 'tuned', 'during', 'training', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O']","['NNP', 'NNS', 'VBP', 'VBN', 'IN', 'CD', 'NNP', 'NNP', 'NNS', 'CC', 'VBP', 'RB', 'JJ', ':', 'VBN', 'IN', 'NN', '.']",18
natural_language_inference,13,178,"Dropout rate is tuned amongst { 0.1 , 0.2 , 0.3 } on all layers including the embedding layer .","['Dropout', 'rate', 'is', 'tuned', 'amongst', '{', '0.1', ',', '0.2', ',', '0.3', '}', 'on', 'all', 'layers', 'including', 'the', 'embedding', 'layer', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NN', 'NN', 'VBZ', 'VBN', 'RB', '(', 'CD', ',', 'CD', ',', 'CD', ')', 'IN', 'DT', 'NNS', 'VBG', 'DT', 'NN', 'NN', '.']",20
natural_language_inference,13,185,We adopt the Adam optimizer with a learning rate of 0.0003/ 0.001/0.001 for RACE / SearchQA / NarrativeQA respectively .,"['We', 'adopt', 'the', 'Adam', 'optimizer', 'with', 'a', 'learning', 'rate', 'of', '0.0003/', '0.001/0.001', 'for', 'RACE', '/', 'SearchQA', '/', 'NarrativeQA', 'respectively', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', 'CD', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'RB', '.']",20
natural_language_inference,13,186,The batch size is set to 64/256/32 accordingly .,"['The', 'batch', 'size', 'is', 'set', 'to', '64/256/32', 'accordingly', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'RB', '.']",9
natural_language_inference,13,187,The maximum sequence lengths are 500/200/1100 respectively .,"['The', 'maximum', 'sequence', 'lengths', 'are', '500/200/1100', 'respectively', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O']","['DT', 'JJ', 'NN', 'NNS', 'VBP', 'CD', 'RB', '.']",8
natural_language_inference,13,189,All models are trained and all runtime benchmarks are based on a TitanXP GPU .,"['All', 'models', 'are', 'trained', 'and', 'all', 'runtime', 'benchmarks', 'are', 'based', 'on', 'a', 'TitanXP', 'GPU', '.']","['B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'CC', 'DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'NNP', 'NNP', '.']",15
natural_language_inference,13,190,"Overall , there is a 6 % improvement on the RACE - H dataset and 1.8 % improvement on the RACE - M dataset .","['Overall', ',', 'there', 'is', 'a', '6', '%', 'improvement', 'on', 'the', 'RACE', '-', 'H', 'dataset', 'and', '1.8', '%', 'improvement', 'on', 'the', 'RACE', '-', 'M', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', ',', 'EX', 'VBZ', 'DT', 'CD', 'NN', 'NN', 'IN', 'DT', 'NNP', ':', 'NNP', 'NN', 'CC', 'CD', 'NN', 'NN', 'IN', 'DT', 'NNP', ':', 'NNP', 'NN', '.']",25
natural_language_inference,13,194,Experimental Results on RACE,"['Experimental', 'Results', 'on', 'RACE']","['O', 'O', 'B-p', 'B-n']","['JJ', 'NNP', 'IN', 'NNP']",4
natural_language_inference,13,195,"In general , we also found that the usage of a recurrent cell is not really crucial on this dataset since ( 1 ) Sim . MRU and MRU can achieve comparable performance to each other , ( 2 ) GRU and LSTM models do not have a competitive edge and ( 3 ) Using no encoder already achieves comparable 1 performance to DFN .","['In', 'general', ',', 'we', 'also', 'found', 'that', 'the', 'usage', 'of', 'a', 'recurrent', 'cell', 'is', 'not', 'really', 'crucial', 'on', 'this', 'dataset', 'since', '(', '1', ')', 'Sim', '.', 'MRU', 'and', 'MRU', 'can', 'achieve', 'comparable', 'performance', 'to', 'each', 'other', ',', '(', '2', ')', 'GRU', 'and', 'LSTM', 'models', 'do', 'not', 'have', 'a', 'competitive', 'edge', 'and', '(', '3', ')', 'Using', 'no', 'encoder', 'already', 'achieves', 'comparable', '1', 'performance', 'to', 'DFN', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'JJ', ',', 'PRP', 'RB', 'VBD', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'RB', 'RB', 'JJ', 'IN', 'DT', 'NN', 'IN', '(', 'CD', ')', 'NNP', '.', 'NNP', 'CC', 'NNP', 'MD', 'VB', 'JJ', 'NN', 'TO', 'DT', 'JJ', ',', '(', 'CD', ')', 'NNP', 'CC', 'NNP', 'NNS', 'VBP', 'RB', 'VB', 'DT', 'JJ', 'NN', 'CC', '(', 'CD', ')', 'VBG', 'DT', 'NN', 'RB', 'VBZ', 'JJ', 'CD', 'NN', 'TO', 'NNP', '.']",65
natural_language_inference,13,196,"Finally , an ensemble of Sim . MRU models achieve state - of - the - art performance on the RACE dataset , achieving and over all score of 53.3 % . :","['Finally', ',', 'an', 'ensemble', 'of', 'Sim', '.', 'MRU', 'models', 'achieve', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'on', 'the', 'RACE', 'dataset', ',', 'achieving', 'and', 'over', 'all', 'score', 'of', '53.3', '%', '.', ':']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O']","['RB', ',', 'DT', 'NN', 'IN', 'NNP', '.', 'NNP', 'NNS', 'VBP', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', ',', 'VBG', 'CC', 'IN', 'DT', 'NN', 'IN', 'CD', 'NN', '.', ':']",33
natural_language_inference,13,198,are baselines reported by . reports our results on the Narrative QA benchmark .,"['are', 'baselines', 'reported', 'by', '.', 'reports', 'our', 'results', 'on', 'the', 'Narrative', 'QA', 'benchmark', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['VBP', 'NNS', 'VBN', 'IN', '.', 'NNS', 'PRP$', 'NNS', 'IN', 'DT', 'NNP', 'NNP', 'NN', '.']",14
natural_language_inference,13,199,"First , we observe that 300d MRU can achieve comparable performance with BiDAF .","['First', ',', 'we', 'observe', 'that', '300d', 'MRU', 'can', 'achieve', 'comparable', 'performance', 'with', 'BiDAF', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'CD', 'NNP', 'MD', 'VB', 'JJ', 'NN', 'IN', 'NNP', '.']",14
natural_language_inference,13,200,"When compared with a BiLSTM of equal output dimensions ( 150 d ) , we find that our MRU model performs competitively , with less than 1 % deprovement across all metrics .","['When', 'compared', 'with', 'a', 'BiLSTM', 'of', 'equal', 'output', 'dimensions', '(', '150', 'd', ')', ',', 'we', 'find', 'that', 'our', 'MRU', 'model', 'performs', 'competitively', ',', 'with', 'less', 'than', '1', '%', 'deprovement', 'across', 'all', 'metrics', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['WRB', 'VBN', 'IN', 'DT', 'NNP', 'IN', 'JJ', 'NN', 'NNS', '(', 'CD', 'NN', ')', ',', 'PRP', 'VBP', 'IN', 'PRP$', 'NNP', 'NN', 'NNS', 'RB', ',', 'IN', 'JJR', 'IN', 'CD', 'NN', 'NN', 'IN', 'DT', 'NNS', '.']",33
natural_language_inference,13,202,The performance of our model is significantly better than 300d LSTM model while also being significantly faster .,"['The', 'performance', 'of', 'our', 'model', 'is', 'significantly', 'better', 'than', '300d', 'LSTM', 'model', 'while', 'also', 'being', 'significantly', 'faster', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'PRP$', 'NN', 'VBZ', 'RB', 'JJR', 'IN', 'CD', 'NNP', 'NN', 'IN', 'RB', 'VBG', 'RB', 'RBR', '.']",18
natural_language_inference,13,206,"Finally , the MRU - LSTM significantly outperforms all models , including BiDAF on this dataset .","['Finally', ',', 'the', 'MRU', '-', 'LSTM', 'significantly', 'outperforms', 'all', 'models', ',', 'including', 'BiDAF', 'on', 'this', 'dataset', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['RB', ',', 'DT', 'NNP', ':', 'NNP', 'RB', 'VBZ', 'DT', 'NNS', ',', 'VBG', 'NNP', 'IN', 'DT', 'NN', '.']",17
natural_language_inference,13,207,"Performance improvement over the vanilla BiLSTM model ranges from 1 % ? 3 % across all metrics , suggesting that MRU encoders are also effective as a complementary neural building block .","['Performance', 'improvement', 'over', 'the', 'vanilla', 'BiLSTM', 'model', 'ranges', 'from', '1', '%', '?', '3', '%', 'across', 'all', 'metrics', ',', 'suggesting', 'that', 'MRU', 'encoders', 'are', 'also', 'effective', 'as', 'a', 'complementary', 'neural', 'building', 'block', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'IN', 'DT', 'NN', 'NNP', 'NN', 'VBZ', 'IN', 'CD', 'NN', '.', 'CD', 'NN', 'IN', 'DT', 'NNS', ',', 'VBG', 'IN', 'NNP', 'NNS', 'VBP', 'RB', 'JJ', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'NN', '.']",32
natural_language_inference,88,2,Long Short - Term Memory - Networks for Machine Reading,"['Long', 'Short', '-', 'Term', 'Memory', '-', 'Networks', 'for', 'Machine', 'Reading']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', 'NNP', ':', 'NNP', 'NNP', ':', 'NNS', 'IN', 'NNP', 'NNP']",10
natural_language_inference,88,42,The idea is to use multiple memory slots outside the recurrence to piece - wise store representations of the input ; read and write operations for each slot can be modeled as an attention mechanism with a recurrent controller .,"['The', 'idea', 'is', 'to', 'use', 'multiple', 'memory', 'slots', 'outside', 'the', 'recurrence', 'to', 'piece', '-', 'wise', 'store', 'representations', 'of', 'the', 'input', ';', 'read', 'and', 'write', 'operations', 'for', 'each', 'slot', 'can', 'be', 'modeled', 'as', 'an', 'attention', 'mechanism', 'with', 'a', 'recurrent', 'controller', '.']","['O', 'O', 'O', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'TO', 'VB', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'NN', 'TO', 'VB', ':', 'NN', 'NN', 'NNS', 'IN', 'DT', 'NN', ':', 'NN', 'CC', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'MD', 'VB', 'VBN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",40
natural_language_inference,88,43,We also leverage memory and attention to empower a recurrent network with stronger memorization capability and more importantly the ability to discover relations among tokens .,"['We', 'also', 'leverage', 'memory', 'and', 'attention', 'to', 'empower', 'a', 'recurrent', 'network', 'with', 'stronger', 'memorization', 'capability', 'and', 'more', 'importantly', 'the', 'ability', 'to', 'discover', 'relations', 'among', 'tokens', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'RB', 'VBP', 'NN', 'CC', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'JJR', 'NN', 'NN', 'CC', 'RBR', 'RB', 'DT', 'NN', 'TO', 'VB', 'NNS', 'IN', 'NNS', '.']",26
natural_language_inference,88,44,This is realized by inserting a memory network module in the update of a recurrent network together with attention for memory addressing .,"['This', 'is', 'realized', 'by', 'inserting', 'a', 'memory', 'network', 'module', 'in', 'the', 'update', 'of', 'a', 'recurrent', 'network', 'together', 'with', 'attention', 'for', 'memory', 'addressing', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'VBZ', 'VBN', 'IN', 'VBG', 'DT', 'NN', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'RB', 'IN', 'NN', 'IN', 'NN', 'NN', '.']",23
natural_language_inference,88,47,"The resulting model , which we term Long Short - Term Memory - Network ( LSTMN ) , is a reading simulator that can be used for sequence processing tasks .","['The', 'resulting', 'model', ',', 'which', 'we', 'term', 'Long', 'Short', '-', 'Term', 'Memory', '-', 'Network', '(', 'LSTMN', ')', ',', 'is', 'a', 'reading', 'simulator', 'that', 'can', 'be', 'used', 'for', 'sequence', 'processing', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'VBG', 'NN', ',', 'WDT', 'PRP', 'NN', 'NNP', 'NNP', ':', 'NNP', 'NNP', ':', 'NN', '(', 'NNP', ')', ',', 'VBZ', 'DT', 'NN', 'NN', 'WDT', 'MD', 'VB', 'VBN', 'IN', 'NN', 'NN', 'NNS', '.']",31
natural_language_inference,88,49,The model processes text incrementally while learning which past tokens in the memory and to what extent they relate to the current token being processed .,"['The', 'model', 'processes', 'text', 'incrementally', 'while', 'learning', 'which', 'past', 'tokens', 'in', 'the', 'memory', 'and', 'to', 'what', 'extent', 'they', 'relate', 'to', 'the', 'current', 'token', 'being', 'processed', '.']","['O', 'O', 'B-p', 'B-n', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'VBZ', 'JJ', 'RB', 'IN', 'VBG', 'WDT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CC', 'TO', 'WP', 'NN', 'PRP', 'VBP', 'TO', 'DT', 'JJ', 'NN', 'VBG', 'VBN', '.']",26
natural_language_inference,88,50,"As a result , the model induces undirected relations among tokens as an intermediate step of learning representations .","['As', 'a', 'result', ',', 'the', 'model', 'induces', 'undirected', 'relations', 'among', 'tokens', 'as', 'an', 'intermediate', 'step', 'of', 'learning', 'representations', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'DT', 'NN', ',', 'DT', 'NN', 'NNS', 'JJ', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'NNS', '.']",19
natural_language_inference,88,144,Our code is available at https://github.com/cheng6076/,"['Our', 'code', 'is', 'available', 'at', 'https://github.com/cheng6076/']","['O', 'O', 'O', 'O', 'O', 'B-n']","['PRP$', 'NN', 'VBZ', 'JJ', 'IN', 'NN']",6
natural_language_inference,88,146,Language Modeling,"['Language', 'Modeling']","['B-n', 'I-n']","['NN', 'VBG']",2
natural_language_inference,88,152,"We used stochastic gradient descent for optimization with an initial learning rate of 0.65 , which decays by a factor of 0.85 per epoch if no significant improvement has been observed on the validation set .","['We', 'used', 'stochastic', 'gradient', 'descent', 'for', 'optimization', 'with', 'an', 'initial', 'learning', 'rate', 'of', '0.65', ',', 'which', 'decays', 'by', 'a', 'factor', 'of', '0.85', 'per', 'epoch', 'if', 'no', 'significant', 'improvement', 'has', 'been', 'observed', 'on', 'the', 'validation', 'set', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'JJ', 'NN', 'NN', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', ',', 'WDT', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'CD', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",36
natural_language_inference,88,153,We renormalize the gradient if its norm is greater than 5 .,"['We', 'renormalize', 'the', 'gradient', 'if', 'its', 'norm', 'is', 'greater', 'than', '5', '.']","['O', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'VBZ', 'JJR', 'IN', 'CD', '.']",12
natural_language_inference,88,154,The mini - batch size was set to 40 .,"['The', 'mini', '-', 'batch', 'size', 'was', 'set', 'to', '40', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', ':', 'NN', 'NN', 'VBD', 'VBN', 'TO', 'CD', '.']",10
natural_language_inference,88,155,The dimensions of the word embeddings were set to 150 for all models .,"['The', 'dimensions', 'of', 'the', 'word', 'embeddings', 'were', 'set', 'to', '150', 'for', 'all', 'models', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'DT', 'NN', 'NNS', 'VBD', 'VBN', 'TO', 'CD', 'IN', 'DT', 'NNS', '.']",14
natural_language_inference,88,157,The first one is a Kneser - Ney 5 - gram language model ( KN5 ) which generally serves as a non-neural baseline for the language modeling task .,"['The', 'first', 'one', 'is', 'a', 'Kneser', '-', 'Ney', '5', '-', 'gram', 'language', 'model', '(', 'KN5', ')', 'which', 'generally', 'serves', 'as', 'a', 'non-neural', 'baseline', 'for', 'the', 'language', 'modeling', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'NNP', ':', 'NN', 'CD', ':', 'NN', 'NN', 'NN', '(', 'NNP', ')', 'WDT', 'RB', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'VBG', 'NN', '.']",29
natural_language_inference,88,160,The gated - feedback LSTM has feedback gates connecting the hidden states across multiple time steps as an adaptive control of the information flow .,"['The', 'gated', '-', 'feedback', 'LSTM', 'has', 'feedback', 'gates', 'connecting', 'the', 'hidden', 'states', 'across', 'multiple', 'time', 'steps', 'as', 'an', 'adaptive', 'control', 'of', 'the', 'information', 'flow', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', ':', 'NN', 'NNP', 'VBZ', 'VBN', 'NNS', 'VBG', 'DT', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",25
natural_language_inference,88,161,The depth - gated LSTM uses a depth gate to connect memory cells of vertically adjacent layers .,"['The', 'depth', '-', 'gated', 'LSTM', 'uses', 'a', 'depth', 'gate', 'to', 'connect', 'memory', 'cells', 'of', 'vertically', 'adjacent', 'layers', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', ':', 'VBN', 'NNP', 'VBZ', 'DT', 'NN', 'NN', 'TO', 'VB', 'NN', 'NNS', 'IN', 'RB', 'JJ', 'NNS', '.']",18
natural_language_inference,88,172,"Amongst all deep architectures , the three - layer LSTMN also performs best .","['Amongst', 'all', 'deep', 'architectures', ',', 'the', 'three', '-', 'layer', 'LSTMN', 'also', 'performs', 'best', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O']","['NNP', 'DT', 'JJ', 'NNS', ',', 'DT', 'CD', ':', 'NN', 'NNP', 'RB', 'VBZ', 'JJS', '.']",14
natural_language_inference,88,179,Sentiment Analysis,"['Sentiment', 'Analysis']","['B-n', 'I-n']","['NN', 'NN']",2
natural_language_inference,88,193,We used pretrained 300 - D Glove 840B vectors to initialize the word embeddings .,"['We', 'used', 'pretrained', '300', '-', 'D', 'Glove', '840B', 'vectors', 'to', 'initialize', 'the', 'word', 'embeddings', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBD', 'VBD', 'CD', ':', 'NNP', 'NNP', 'CD', 'NNS', 'TO', 'VB', 'DT', 'NN', 'NNS', '.']",15
natural_language_inference,88,194,"The gradient for words with Glove embeddings , was scaled by 0.35 in the first epoch after which all word embeddings were updated normally .","['The', 'gradient', 'for', 'words', 'with', 'Glove', 'embeddings', ',', 'was', 'scaled', 'by', '0.35', 'in', 'the', 'first', 'epoch', 'after', 'which', 'all', 'word', 'embeddings', 'were', 'updated', 'normally', '.']","['O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NNS', 'IN', 'NNP', 'NNS', ',', 'VBD', 'VBN', 'IN', 'CD', 'IN', 'DT', 'JJ', 'NN', 'IN', 'WDT', 'DT', 'NN', 'NNS', 'VBD', 'VBN', 'RB', '.']",25
natural_language_inference,88,195,"We used Adam ( Kingma and Ba , 2015 ) for optimization with the two momentum parameters set to 0.9 and 0.999 respectively .","['We', 'used', 'Adam', '(', 'Kingma', 'and', 'Ba', ',', '2015', ')', 'for', 'optimization', 'with', 'the', 'two', 'momentum', 'parameters', 'set', 'to', '0.9', 'and', '0.999', 'respectively', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBD', 'NNP', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'IN', 'NN', 'IN', 'DT', 'CD', 'NN', 'NNS', 'VBD', 'TO', 'CD', 'CC', 'CD', 'RB', '.']",24
natural_language_inference,88,196,The initial learning rate was set to 2E - 3 .,"['The', 'initial', 'learning', 'rate', 'was', 'set', 'to', '2E', '-', '3', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBD', 'VBN', 'TO', 'CD', ':', 'CD', '.']",11
natural_language_inference,88,197,The regularization constant was 1E - 4 and the mini-batch size was 5 .,"['The', 'regularization', 'constant', 'was', '1E', '-', '4', 'and', 'the', 'mini-batch', 'size', 'was', '5', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBD', 'CD', ':', 'CD', 'CC', 'DT', 'JJ', 'NN', 'VBD', 'CD', '.']",14
natural_language_inference,88,198,A dropout rate of 0.5 was applied to the neural network classifier .,"['A', 'dropout', 'rate', 'of', '0.5', 'was', 'applied', 'to', 'the', 'neural', 'network', 'classifier', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'IN', 'CD', 'VBD', 'VBN', 'TO', 'DT', 'JJ', 'NN', 'NN', '.']",13
natural_language_inference,88,200,"Most of these models ( including ours ) are LSTM variants ( third block in , recursive neural networks ( first block ) , or convolutional neural networks ( CNNs ; second block ) .","['Most', 'of', 'these', 'models', '(', 'including', 'ours', ')', 'are', 'LSTM', 'variants', '(', 'third', 'block', 'in', ',', 'recursive', 'neural', 'networks', '(', 'first', 'block', ')', ',', 'or', 'convolutional', 'neural', 'networks', '(', 'CNNs', ';', 'second', 'block', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJS', 'IN', 'DT', 'NNS', '(', 'VBG', 'NNS', ')', 'VBP', 'JJ', 'NNS', '(', 'JJ', 'NN', 'IN', ',', 'JJ', 'JJ', 'NNS', '(', 'JJ', 'NN', ')', ',', 'CC', 'JJ', 'JJ', 'NNS', '(', 'NNP', ':', 'JJ', 'NN', ')', '.']",35
natural_language_inference,88,203,"For comparison , we also report the performance of the paragraph vector model ( PV ; ; see , second block ) which neither operates on trees nor sequences but learns distributed document representations parameterized directly .","['For', 'comparison', ',', 'we', 'also', 'report', 'the', 'performance', 'of', 'the', 'paragraph', 'vector', 'model', '(', 'PV', ';', ';', 'see', ',', 'second', 'block', ')', 'which', 'neither', 'operates', 'on', 'trees', 'nor', 'sequences', 'but', 'learns', 'distributed', 'document', 'representations', 'parameterized', 'directly', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ',', 'PRP', 'RB', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'NN', '(', 'NNP', ':', ':', 'VB', ',', 'JJ', 'NN', ')', 'WDT', 'DT', 'VBZ', 'IN', 'NNS', 'CC', 'NNS', 'CC', 'NNS', 'VBD', 'JJ', 'NNS', 'VBN', 'RB', '.']",37
natural_language_inference,88,204,The results in show that both 1 - and 2 - layer LSTMNs outperform the LSTM baselines while achieving numbers comparable to state of the art .,"['The', 'results', 'in', 'show', 'that', 'both', '1', '-', 'and', '2', '-', 'layer', 'LSTMNs', 'outperform', 'the', 'LSTM', 'baselines', 'while', 'achieving', 'numbers', 'comparable', 'to', 'state', 'of', 'the', 'art', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'NN', 'IN', 'DT', 'CD', ':', 'CC', 'CD', ':', 'NN', 'NNP', 'VBZ', 'DT', 'NNP', 'NNS', 'IN', 'VBG', 'NNS', 'JJ', 'TO', 'NN', 'IN', 'DT', 'NN', '.']",27
natural_language_inference,88,206,On the fine - grained and binary classification tasks our 2 - layer LSTMN performs close to the best system T -. shows examples of intra-attention for sentiment words .,"['On', 'the', 'fine', '-', 'grained', 'and', 'binary', 'classification', 'tasks', 'our', '2', '-', 'layer', 'LSTMN', 'performs', 'close', 'to', 'the', 'best', 'system', 'T', '-.', 'shows', 'examples', 'of', 'intra-attention', 'for', 'sentiment', 'words', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', ':', 'VBN', 'CC', 'JJ', 'NN', 'NNS', 'PRP$', 'CD', ':', 'NN', 'NNP', 'VBZ', 'RB', 'TO', 'DT', 'JJS', 'NN', 'NNP', 'NN', 'VBZ', 'NNS', 'IN', 'NN', 'IN', 'NN', 'NNS', '.']",30
natural_language_inference,88,208,Natural Language Inference,"['Natural', 'Language', 'Inference']","['B-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NN']",3
natural_language_inference,88,220,We used pre-trained 300 - D Glove 840B vectors to initialize the word embeddings .,"['We', 'used', 'pre-trained', '300', '-', 'D', 'Glove', '840B', 'vectors', 'to', 'initialize', 'the', 'word', 'embeddings', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBD', 'JJ', 'CD', ':', 'NNP', 'NNP', 'CD', 'NNS', 'TO', 'VB', 'DT', 'NN', 'NNS', '.']",15
natural_language_inference,88,221,"Out - of - vocabulary ( OOV ) words were initialized randomly with Gaussian samples ( = 0 , ?= 1 ) .","['Out', '-', 'of', '-', 'vocabulary', '(', 'OOV', ')', 'words', 'were', 'initialized', 'randomly', 'with', 'Gaussian', 'samples', '(', '=', '0', ',', '?=', '1', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', ':', 'IN', ':', 'NN', '(', 'NNP', ')', 'NNS', 'VBD', 'VBN', 'RB', 'IN', 'JJ', 'NNS', '(', 'JJ', 'CD', ',', 'RB', 'CD', ')', '.']",23
natural_language_inference,88,222,"We only updated OOV vectors in the first epoch , after which all word embeddings were updated normally .","['We', 'only', 'updated', 'OOV', 'vectors', 'in', 'the', 'first', 'epoch', ',', 'after', 'which', 'all', 'word', 'embeddings', 'were', 'updated', 'normally', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O']","['PRP', 'RB', 'VBD', 'NNP', 'NNS', 'IN', 'DT', 'JJ', 'NN', ',', 'IN', 'WDT', 'DT', 'NN', 'NNS', 'VBD', 'VBN', 'RB', '.']",19
natural_language_inference,88,223,"The dropout rate was selected from [ 0.1 , 0.2 , 0.3 , 0.4 ] .","['The', 'dropout', 'rate', 'was', 'selected', 'from', '[', '0.1', ',', '0.2', ',', '0.3', ',', '0.4', ']', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBD', 'VBN', 'IN', 'JJ', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', 'NN', '.']",16
natural_language_inference,88,224,"We used Adam ( Kingma and Ba , 2015 ) for optimization with the two momentum parameters set to 0.9 and 0.999 respectively , and the initial learning rate set to 1E - 3 .","['We', 'used', 'Adam', '(', 'Kingma', 'and', 'Ba', ',', '2015', ')', 'for', 'optimization', 'with', 'the', 'two', 'momentum', 'parameters', 'set', 'to', '0.9', 'and', '0.999', 'respectively', ',', 'and', 'the', 'initial', 'learning', 'rate', 'set', 'to', '1E', '-', '3', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'NNP', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'IN', 'NN', 'IN', 'DT', 'CD', 'NN', 'NNS', 'VBD', 'TO', 'CD', 'CC', 'CD', 'RB', ',', 'CC', 'DT', 'JJ', 'NN', 'NN', 'VBN', 'TO', 'CD', ':', 'CD', '.']",35
natural_language_inference,88,225,The mini- batch size was set to 16 or 32 .,"['The', 'mini-', 'batch', 'size', 'was', 'set', 'to', '16', 'or', '32', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBD', 'VBN', 'TO', 'CD', 'CC', 'CD', '.']",11
natural_language_inference,88,228,"Specifically , these include a model which encodes the premise and hypothesis independently with two LSTMs , a shared LSTM ( Rocktschel et al. , 2016 ) , a word - by - word attention model , and a matching LSTM ( m LSTM ; ) .","['Specifically', ',', 'these', 'include', 'a', 'model', 'which', 'encodes', 'the', 'premise', 'and', 'hypothesis', 'independently', 'with', 'two', 'LSTMs', ',', 'a', 'shared', 'LSTM', '(', 'Rocktschel', 'et', 'al.', ',', '2016', ')', ',', 'a', 'word', '-', 'by', '-', 'word', 'attention', 'model', ',', 'and', 'a', 'matching', 'LSTM', '(', 'm', 'LSTM', ';', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'VBP', 'DT', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'CC', 'NN', 'RB', 'IN', 'CD', 'NNP', ',', 'DT', 'VBN', 'NNP', '(', 'NNP', 'RB', 'RB', ',', 'CD', ')', ',', 'DT', 'NN', ':', 'IN', ':', 'NN', 'NN', 'NN', ',', 'CC', 'DT', 'JJ', 'NNP', '(', 'JJ', 'NNP', ':', ')', '.']",47
natural_language_inference,88,230,We also compared our models with a bag - of - words baseline which averages the pre-trained embeddings for the words in each sentence and concatenates them to create features for a logistic regression classifier ( first block in ) .,"['We', 'also', 'compared', 'our', 'models', 'with', 'a', 'bag', '-', 'of', '-', 'words', 'baseline', 'which', 'averages', 'the', 'pre-trained', 'embeddings', 'for', 'the', 'words', 'in', 'each', 'sentence', 'and', 'concatenates', 'them', 'to', 'create', 'features', 'for', 'a', 'logistic', 'regression', 'classifier', '(', 'first', 'block', 'in', ')', '.']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBN', 'PRP$', 'NNS', 'IN', 'DT', 'JJ', ':', 'IN', ':', 'NNS', 'VBP', 'WDT', 'VBZ', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'CC', 'VBZ', 'PRP', 'TO', 'VB', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', '(', 'JJ', 'NN', 'IN', ')', '.']",41
natural_language_inference,88,231,LSTMNs achieve better performance compared Models,"['LSTMNs', 'achieve', 'better', 'performance', 'compared', 'Models']","['B-n', 'B-p', 'B-n', 'I-n', 'O', 'O']","['NNP', 'VBP', 'JJR', 'NN', 'VBN', 'NNP']",6
natural_language_inference,88,233,"We also observe that fusion is generally beneficial , and that deep fusion slightly improves over shallow fusion .","['We', 'also', 'observe', 'that', 'fusion', 'is', 'generally', 'beneficial', ',', 'and', 'that', 'deep', 'fusion', 'slightly', 'improves', 'over', 'shallow', 'fusion', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', 'VBZ', 'RB', 'JJ', ',', 'CC', 'IN', 'JJ', 'NN', 'RB', 'VBZ', 'RP', 'JJ', 'NN', '.']",19
natural_language_inference,88,235,"With standard training , our deep fusion yields the state - of - the - art performance in this task .","['With', 'standard', 'training', ',', 'our', 'deep', 'fusion', 'yields', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'in', 'this', 'task', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['IN', 'JJ', 'NN', ',', 'PRP$', 'JJ', 'NN', 'NNS', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'DT', 'NN', '.']",21
natural_language_inference,71,4,We present a novel recurrent neural network ( RNN ) based model that combines the remembering ability of unitary RNNs with the ability of gated RNNs to effectively forget redundant / irrelevant information in its memory .,"['We', 'present', 'a', 'novel', 'recurrent', 'neural', 'network', '(', 'RNN', ')', 'based', 'model', 'that', 'combines', 'the', 'remembering', 'ability', 'of', 'unitary', 'RNNs', 'with', 'the', 'ability', 'of', 'gated', 'RNNs', 'to', 'effectively', 'forget', 'redundant', '/', 'irrelevant', 'information', 'in', 'its', 'memory', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'JJ', 'NN', '(', 'NNP', ')', 'VBN', 'NN', 'WDT', 'VBZ', 'DT', 'VBG', 'NN', 'IN', 'JJ', 'NNP', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNP', 'TO', 'RB', 'VB', 'JJ', 'NNP', 'JJ', 'NN', 'IN', 'PRP$', 'NN', '.']",37
natural_language_inference,71,11,Recurrent Neural Networks with gating units - such as Long Short Term Memory ( LSTMs ) and Gated Recurrent Units ( GRUs ) ),"['Recurrent', 'Neural', 'Networks', 'with', 'gating', 'units', '-', 'such', 'as', 'Long', 'Short', 'Term', 'Memory', '(', 'LSTMs', ')', 'and', 'Gated', 'Recurrent', 'Units', '(', 'GRUs', ')', ')']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', 'IN', 'VBG', 'NNS', ':', 'JJ', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'CC', 'VBN', 'NNP', 'NNP', '(', 'NNP', ')', ')']",24
natural_language_inference,71,13,These works have proven the importance of gating units for Recurrent Neural Networks .,"['These', 'works', 'have', 'proven', 'the', 'importance', 'of', 'gating', 'units', 'for', 'Recurrent', 'Neural', 'Networks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'DT', 'NN', 'IN', 'VBG', 'NNS', 'IN', 'NNP', 'NNP', 'NNP', '.']",14
natural_language_inference,71,14,The main advantage of using these gated units in RNNs is primarily due to the ease of optimization of the models using them and to reduce the learning degeneracies such as vanishing gradients that can cripple conventional RNNs .,"['The', 'main', 'advantage', 'of', 'using', 'these', 'gated', 'units', 'in', 'RNNs', 'is', 'primarily', 'due', 'to', 'the', 'ease', 'of', 'optimization', 'of', 'the', 'models', 'using', 'them', 'and', 'to', 'reduce', 'the', 'learning', 'degeneracies', 'such', 'as', 'vanishing', 'gradients', 'that', 'can', 'cripple', 'conventional', 'RNNs', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'VBN', 'NNS', 'IN', 'NNP', 'VBZ', 'RB', 'JJ', 'TO', 'DT', 'NN', 'IN', 'NN', 'IN', 'DT', 'NNS', 'VBG', 'PRP', 'CC', 'TO', 'VB', 'DT', 'VBG', 'NNS', 'JJ', 'IN', 'VBG', 'NNS', 'WDT', 'MD', 'VB', 'JJ', 'NNP', '.']",39
natural_language_inference,71,29,"We propose a new architecture , the Gated Orthogonal Recurrent Unit ( GORU ) , which combines the advantages of the above two frameworks , namely ( i ) the ability to capture long term dependencies by using orthogonal matrices and ( ii ) the ability to "" forget "" by using a GRU structure .","['We', 'propose', 'a', 'new', 'architecture', ',', 'the', 'Gated', 'Orthogonal', 'Recurrent', 'Unit', '(', 'GORU', ')', ',', 'which', 'combines', 'the', 'advantages', 'of', 'the', 'above', 'two', 'frameworks', ',', 'namely', '(', 'i', ')', 'the', 'ability', 'to', 'capture', 'long', 'term', 'dependencies', 'by', 'using', 'orthogonal', 'matrices', 'and', '(', 'ii', ')', 'the', 'ability', 'to', '""', 'forget', '""', 'by', 'using', 'a', 'GRU', 'structure', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', ',', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'WDT', 'VBZ', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'CD', 'NNS', ',', 'RB', '(', 'NN', ')', 'DT', 'NN', 'TO', 'VB', 'JJ', 'NN', 'NNS', 'IN', 'VBG', 'JJ', 'NNS', 'CC', '(', 'NN', ')', 'DT', 'NN', 'TO', 'VB', 'VB', 'JJ', 'IN', 'VBG', 'DT', 'NNP', 'NN', '.']",56
natural_language_inference,71,30,"We demonstrate that GORU is able to learn long term dependencies effectively , even in complicated datasets which require a forgetting ability .","['We', 'demonstrate', 'that', 'GORU', 'is', 'able', 'to', 'learn', 'long', 'term', 'dependencies', 'effectively', ',', 'even', 'in', 'complicated', 'datasets', 'which', 'require', 'a', 'forgetting', 'ability', '.']","['O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'VBZ', 'JJ', 'TO', 'VB', 'JJ', 'NN', 'NNS', 'RB', ',', 'RB', 'IN', 'VBN', 'NNS', 'WDT', 'VBP', 'DT', 'JJ', 'NN', '.']",23
natural_language_inference,71,31,"In this work , we focus on implementation of orthogonal transition matrices which is just a subset of the unitary matrices .","['In', 'this', 'work', ',', 'we', 'focus', 'on', 'implementation', 'of', 'orthogonal', 'transition', 'matrices', 'which', 'is', 'just', 'a', 'subset', 'of', 'the', 'unitary', 'matrices', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'IN', 'NN', 'IN', 'JJ', 'NN', 'NNS', 'WDT', 'VBZ', 'RB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNS', '.']",22
natural_language_inference,71,103,"GORU is implemented in Tensorflow , available from https://github.com/jingli9111/GORU-tensorflow","['GORU', 'is', 'implemented', 'in', 'Tensorflow', ',', 'available', 'from', 'https://github.com/jingli9111/GORU-tensorflow']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']","['NNP', 'VBZ', 'VBN', 'IN', 'NNP', ',', 'JJ', 'IN', 'JJ']",9
natural_language_inference,71,105,The first task we consider is the well known Copying Memory Task .,"['The', 'first', 'task', 'we', 'consider', 'is', 'the', 'well', 'known', 'Copying', 'Memory', 'Task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'PRP', 'VBP', 'VBZ', 'DT', 'RB', 'VBN', 'NNP', 'NNP', 'NNP', '.']",13
natural_language_inference,71,112,"In this experiment , we use RMSProp optimization with a learning rate of 0.001 and a decay rate of 0.9 for all models .","['In', 'this', 'experiment', ',', 'we', 'use', 'RMSProp', 'optimization', 'with', 'a', 'learning', 'rate', 'of', '0.001', 'and', 'a', 'decay', 'rate', 'of', '0.9', 'for', 'all', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', 'CC', 'DT', 'JJ', 'NN', 'IN', 'CD', 'IN', 'DT', 'NNS', '.']",24
natural_language_inference,71,113,The batch size is set to 128 .,"['The', 'batch', 'size', 'is', 'set', 'to', '128', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",8
natural_language_inference,71,115,"Hidden state sizes are set to 128 , 100 , 90 , 512 , respectively to match total number of hidden to hidden parameters .","['Hidden', 'state', 'sizes', 'are', 'set', 'to', '128', ',', '100', ',', '90', ',', '512', ',', 'respectively', 'to', 'match', 'total', 'number', 'of', 'hidden', 'to', 'hidden', 'parameters', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ',', 'RB', 'TO', 'VB', 'JJ', 'NN', 'IN', 'JJ', 'TO', 'VB', 'NNS', '.']",25
natural_language_inference,71,120,The GORU is the only gated - system to successfully solve this task while the GRU and LSTM get stuck at the baseline as shown in .,"['The', 'GORU', 'is', 'the', 'only', 'gated', '-', 'system', 'to', 'successfully', 'solve', 'this', 'task', 'while', 'the', 'GRU', 'and', 'LSTM', 'get', 'stuck', 'at', 'the', 'baseline', 'as', 'shown', 'in', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'VBZ', 'DT', 'RB', 'VBN', ':', 'NN', 'TO', 'RB', 'VB', 'DT', 'NN', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'VB', 'VBN', 'IN', 'DT', 'NN', 'IN', 'VBN', 'IN', '.']",27
natural_language_inference,71,121,Denoise Task,"['Denoise', 'Task']","['B-n', 'I-n']","['NNP', 'NNP']",2
natural_language_inference,71,128,"Just as in the previous experiment , we use RM - SProp optimization algorithm with a learning rate of 0.01 and a decay rate of 0.9 for all models .","['Just', 'as', 'in', 'the', 'previous', 'experiment', ',', 'we', 'use', 'RM', '-', 'SProp', 'optimization', 'algorithm', 'with', 'a', 'learning', 'rate', 'of', '0.01', 'and', 'a', 'decay', 'rate', 'of', '0.9', 'for', 'all', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['RB', 'IN', 'IN', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'JJ', ':', 'NN', 'NN', 'NN', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', 'CC', 'DT', 'JJ', 'NN', 'IN', 'CD', 'IN', 'DT', 'NNS', '.']",30
natural_language_inference,71,129,The batch size is set to 128 .,"['The', 'batch', 'size', 'is', 'set', 'to', '128', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",8
natural_language_inference,71,131,"Hidden state sizes are set to 128 , 100 , 90 , 512 , respectively to match total number of hidden to hidden parameters .","['Hidden', 'state', 'sizes', 'are', 'set', 'to', '128', ',', '100', ',', '90', ',', '512', ',', 'respectively', 'to', 'match', 'total', 'number', 'of', 'hidden', 'to', 'hidden', 'parameters', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ',', 'RB', 'TO', 'VB', 'JJ', 'NN', 'IN', 'JJ', 'TO', 'VB', 'NNS', '.']",25
natural_language_inference,71,132,"EURNN get stuck at the baseline because of lacking forgetting mechanism , while GORU and GRU successfully solve the task .","['EURNN', 'get', 'stuck', 'at', 'the', 'baseline', 'because', 'of', 'lacking', 'forgetting', 'mechanism', ',', 'while', 'GORU', 'and', 'GRU', 'successfully', 'solve', 'the', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'O']","['NNP', 'VB', 'VBN', 'IN', 'DT', 'NN', 'IN', 'IN', 'VBG', 'VBG', 'NN', ',', 'IN', 'NNP', 'CC', 'NNP', 'RB', 'VB', 'DT', 'NN', '.']",21
natural_language_inference,71,136,Parenthesis Task,"['Parenthesis', 'Task']","['B-n', 'I-n']","['NN', 'NN']",2
natural_language_inference,71,142,"In our experiment , the total input length is set to 200 .","['In', 'our', 'experiment', ',', 'the', 'total', 'input', 'length', 'is', 'set', 'to', '200', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'PRP$', 'NN', ',', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",13
natural_language_inference,71,143,"We used batch size 128 and RMSProp Optimizer with a learning rate 0.001 , decay rate 0.9 on all models .","['We', 'used', 'batch', 'size', '128', 'and', 'RMSProp', 'Optimizer', 'with', 'a', 'learning', 'rate', '0.001', ',', 'decay', 'rate', '0.9', 'on', 'all', 'models', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'NN', 'NN', 'CD', 'CC', 'NNP', 'NNP', 'IN', 'DT', 'VBG', 'NN', 'CD', ',', 'NN', 'NN', 'CD', 'IN', 'DT', 'NNS', '.']",21
natural_language_inference,71,146,"The GORU is able to successfully outperform GRU , LSTM and EURNN in terms of both learning speed and final performances as shown in .","['The', 'GORU', 'is', 'able', 'to', 'successfully', 'outperform', 'GRU', ',', 'LSTM', 'and', 'EURNN', 'in', 'terms', 'of', 'both', 'learning', 'speed', 'and', 'final', 'performances', 'as', 'shown', 'in', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'VBZ', 'JJ', 'TO', 'RB', 'VB', 'NNP', ',', 'NNP', 'CC', 'NNP', 'IN', 'NNS', 'IN', 'DT', 'VBG', 'NN', 'CC', 'JJ', 'NNS', 'IN', 'VBN', 'IN', '.']",25
natural_language_inference,71,147,We also analyzed the activations of the update gates for GORU and GRU .,"['We', 'also', 'analyzed', 'the', 'activations', 'of', 'the', 'update', 'gates', 'for', 'GORU', 'and', 'GRU', '.']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBD', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NNP', 'CC', 'NNP', '.']",14
natural_language_inference,71,152,Algorithmic Task,"['Algorithmic', 'Task']","['B-n', 'I-n']","['NNP', 'NN']",2
natural_language_inference,71,156,We used batch size 50 and hidden size 128 for all models .,"['We', 'used', 'batch', 'size', '50', 'and', 'hidden', 'size', '128', 'for', 'all', 'models', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'NN', 'NN', 'CD', 'CC', 'JJ', 'NN', 'CD', 'IN', 'DT', 'NNS', '.']",13
natural_language_inference,71,157,The RNNs are trained with RMSProp optimizer with a learning rate of 0.001 and decay rate of 0.9 .,"['The', 'RNNs', 'are', 'trained', 'with', 'RMSProp', 'optimizer', 'with', 'a', 'learning', 'rate', 'of', '0.001', 'and', 'decay', 'rate', 'of', '0.9', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NNP', 'VBP', 'VBN', 'IN', 'NNP', 'NN', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', 'CC', 'JJ', 'NN', 'IN', 'CD', '.']",19
natural_language_inference,71,159,We found that the GORU performs averagely better than GRU / LSTM and EURNN .,"['We', 'found', 'that', 'the', 'GORU', 'performs', 'averagely', 'better', 'than', 'GRU', '/', 'LSTM', 'and', 'EURNN', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'IN', 'DT', 'NNP', 'NNS', 'RB', 'RBR', 'IN', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', '.']",15
natural_language_inference,27,3,COMBINING LOCAL CONVOLUTION WITH GLOBAL SELF - ATTENTION FOR READING COMPRE - HENSION,"['COMBINING', 'LOCAL', 'CONVOLUTION', 'WITH', 'GLOBAL', 'SELF', '-', 'ATTENTION', 'FOR', 'READING', 'COMPRE', '-', 'HENSION']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', ':', 'NN', 'IN', 'NNP', 'NNP', ':', 'NN']",13
natural_language_inference,27,5,Current end - to - end machine reading and question answering ( Q&A ) models are primarily based on recurrent neural networks ( RNNs ) with attention .,"['Current', 'end', '-', 'to', '-', 'end', 'machine', 'reading', 'and', 'question', 'answering', '(', 'Q&A', ')', 'models', 'are', 'primarily', 'based', 'on', 'recurrent', 'neural', 'networks', '(', 'RNNs', ')', 'with', 'attention', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', ':', 'TO', ':', 'NN', 'NN', 'NN', 'CC', 'NN', 'NN', '(', 'NNP', ')', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'JJ', 'JJ', 'NNS', '(', 'NNP', ')', 'IN', 'NN', '.']",28
natural_language_inference,27,14,There is growing interest in the tasks of machine reading comprehension and automated question answering .,"['There', 'is', 'growing', 'interest', 'in', 'the', 'tasks', 'of', 'machine', 'reading', 'comprehension', 'and', 'automated', 'question', 'answering', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['EX', 'VBZ', 'VBG', 'NN', 'IN', 'DT', 'NNS', 'IN', 'NN', 'NN', 'NN', 'CC', 'JJ', 'NN', 'NN', '.']",16
natural_language_inference,27,21,"In this paper , aiming to make the machine comprehension fast , we propose to remove the recurrent nature of these models .","['In', 'this', 'paper', ',', 'aiming', 'to', 'make', 'the', 'machine', 'comprehension', 'fast', ',', 'we', 'propose', 'to', 'remove', 'the', 'recurrent', 'nature', 'of', 'these', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'VBG', 'TO', 'VB', 'DT', 'NN', 'NN', 'RB', ',', 'PRP', 'VBP', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNS', '.']",23
natural_language_inference,27,22,We instead exclusively use convolutions and self - attentions as the building blocks of encoders that separately encodes the query and context .,"['We', 'instead', 'exclusively', 'use', 'convolutions', 'and', 'self', '-', 'attentions', 'as', 'the', 'building', 'blocks', 'of', 'encoders', 'that', 'separately', 'encodes', 'the', 'query', 'and', 'context', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'RB', 'JJ', 'NNS', 'CC', 'PRP', ':', 'NNS', 'IN', 'DT', 'NN', 'NNS', 'IN', 'NNS', 'WDT', 'RB', 'VBZ', 'DT', 'NN', 'CC', 'NN', '.']",23
natural_language_inference,27,23,Then we learn the interactions between context and question by standard attentions .,"['Then', 'we', 'learn', 'the', 'interactions', 'between', 'context', 'and', 'question', 'by', 'standard', 'attentions', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', 'PRP', 'VBP', 'DT', 'NNS', 'IN', 'NN', 'CC', 'NN', 'IN', 'JJ', 'NNS', '.']",13
natural_language_inference,27,24,The resulting representation is encoded again with our recurrency - free encoder before finally decoding to the probability of each position being the start or end of the answer span .,"['The', 'resulting', 'representation', 'is', 'encoded', 'again', 'with', 'our', 'recurrency', '-', 'free', 'encoder', 'before', 'finally', 'decoding', 'to', 'the', 'probability', 'of', 'each', 'position', 'being', 'the', 'start', 'or', 'end', 'of', 'the', 'answer', 'span', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'VBN', 'RB', 'IN', 'PRP$', 'NN', ':', 'JJ', 'NN', 'IN', 'RB', 'VBG', 'TO', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBG', 'DT', 'NN', 'CC', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",31
natural_language_inference,27,25,"We call this architecture QANet , which is shown in .","['We', 'call', 'this', 'architecture', 'QANet', ',', 'which', 'is', 'shown', 'in', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NNP', ',', 'WDT', 'VBZ', 'VBN', 'IN', '.']",11
natural_language_inference,27,182,EXPERIMENTS ON SQUAD,"['EXPERIMENTS', 'ON', 'SQUAD']","['O', 'B-p', 'B-n']","['NNS', 'NNP', 'NNP']",3
natural_language_inference,27,201,We employ two types of standard regularizations .,"['We', 'employ', 'two', 'types', 'of', 'standard', 'regularizations', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'NNS', 'IN', 'JJ', 'NNS', '.']",8
natural_language_inference,27,202,"First , we use L2 weight decay on all the trainable variables , with parameter ? = 3 10 ?7 .","['First', ',', 'we', 'use', 'L2', 'weight', 'decay', 'on', 'all', 'the', 'trainable', 'variables', ',', 'with', 'parameter', '?', '=', '3', '10', '?7', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'JJ', 'NN', 'NN', 'IN', 'PDT', 'DT', 'JJ', 'NNS', ',', 'IN', 'NN', '.', '$', 'CD', 'CD', 'NN', '.']",21
natural_language_inference,27,203,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .","['We', 'additionally', 'use', 'dropout', 'on', 'word', ',', 'character', 'embeddings', 'and', 'between', 'layers', ',', 'where', 'the', 'word', 'and', 'character', 'dropout', 'rates', 'are', '0.1', 'and', '0.05', 'respectively', ',', 'and', 'the', 'dropout', 'rate', 'between', 'every', 'two', 'layers', 'is', '0.1', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'RB', 'VBP', 'NN', 'IN', 'NN', ',', 'NN', 'NNS', 'CC', 'IN', 'NNS', ',', 'WRB', 'DT', 'NN', 'CC', 'NN', 'NN', 'NNS', 'VBP', 'CD', 'CC', 'CD', 'RB', ',', 'CC', 'DT', 'NN', 'NN', 'IN', 'DT', 'CD', 'NNS', 'VBZ', 'CD', '.']",37
natural_language_inference,27,204,"We also adopt the stochastic depth method ( layer dropout ) within each embedding or model encoder layer , where sublayer l has survival probability pl = 1 ? l L ( 1 ? p L ) where L is the last layer and p L = 0.9 .","['We', 'also', 'adopt', 'the', 'stochastic', 'depth', 'method', '(', 'layer', 'dropout', ')', 'within', 'each', 'embedding', 'or', 'model', 'encoder', 'layer', ',', 'where', 'sublayer', 'l', 'has', 'survival', 'probability', 'pl', '=', '1', '?', 'l', 'L', '(', '1', '?', 'p', 'L', ')', 'where', 'L', 'is', 'the', 'last', 'layer', 'and', 'p', 'L', '=', '0.9', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'JJ', 'NN', 'NN', '(', 'VB', 'NN', ')', 'IN', 'DT', 'VBG', 'CC', 'NN', 'NN', 'NN', ',', 'WRB', 'NN', 'NN', 'VBZ', 'VBN', 'NN', 'NN', 'VBD', 'CD', '.', 'NN', 'NNP', '(', 'CD', '.', 'NN', 'NNP', ')', 'WRB', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'CC', 'NN', 'NNP', 'VBZ', 'CD', '.']",49
natural_language_inference,27,205,"The hidden size and the convolution filter number are all 128 , the batch size is 32 , training steps are 150 K for original data , 250 K for "" data augmentation 2 "" , and 340 K for "" data augmentation 3 "" .","['The', 'hidden', 'size', 'and', 'the', 'convolution', 'filter', 'number', 'are', 'all', '128', ',', 'the', 'batch', 'size', 'is', '32', ',', 'training', 'steps', 'are', '150', 'K', 'for', 'original', 'data', ',', '250', 'K', 'for', '""', 'data', 'augmentation', '2', '""', ',', 'and', '340', 'K', 'for', '""', 'data', 'augmentation', '3', '""', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['DT', 'JJ', 'NN', 'CC', 'DT', 'NN', 'NN', 'NN', 'VBP', 'DT', 'CD', ',', 'DT', 'NN', 'NN', 'VBZ', 'CD', ',', 'VBG', 'NNS', 'VBP', 'CD', 'NNP', 'IN', 'JJ', 'NNS', ',', 'CD', 'NNP', 'IN', 'NNP', 'NNS', 'NN', 'CD', 'NN', ',', 'CC', 'CD', 'NNP', 'IN', 'NNP', 'NNS', 'NN', 'CD', 'NN', '.']",46
natural_language_inference,27,206,"The numbers of convolution layers in the embedding and modeling encoder are 4 and 2 , kernel sizes are 7 and 5 , and the block numbers for the encoders are 1 and 7 , respectively .","['The', 'numbers', 'of', 'convolution', 'layers', 'in', 'the', 'embedding', 'and', 'modeling', 'encoder', 'are', '4', 'and', '2', ',', 'kernel', 'sizes', 'are', '7', 'and', '5', ',', 'and', 'the', 'block', 'numbers', 'for', 'the', 'encoders', 'are', '1', 'and', '7', ',', 'respectively', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'NN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NN', 'NN', 'VBP', 'CD', 'CC', 'CD', ',', 'NN', 'NNS', 'VBP', 'CD', 'CC', 'CD', ',', 'CC', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NNS', 'VBP', 'CD', 'CC', 'CD', ',', 'RB', '.']",37
natural_language_inference,27,207,"We use the ADAM optimizer ( Kingma & Ba , 2014 ) with ? 1 = 0.8 , ? 2 = 0.999 , = 10 ?7 .","['We', 'use', 'the', 'ADAM', 'optimizer', '(', 'Kingma', '&', 'Ba', ',', '2014', ')', 'with', '?', '1', '=', '0.8', ',', '?', '2', '=', '0.999', ',', '=', '10', '?7', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'IN', '.', 'CD', 'NN', 'CD', ',', '.', 'CD', 'NN', 'CD', ',', 'VBD', 'CD', 'NN', '.']",27
natural_language_inference,27,208,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .","['We', 'use', 'a', 'learning', 'rate', 'warm', '-', 'up', 'scheme', 'with', 'an', 'inverse', 'exponential', 'increase', 'from', '0.0', 'to', '0.001', 'in', 'the', 'first', '1000', 'steps', ',', 'and', 'then', 'maintain', 'a', 'constant', 'learning', 'rate', 'for', 'the', 'remainder', 'of', 'training', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'NN', ':', 'RB', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'CD', 'TO', 'CD', 'IN', 'DT', 'JJ', 'CD', 'NNS', ',', 'CC', 'RB', 'VB', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', '.']",37
natural_language_inference,27,209,Exponential moving average is applied on all trainable variables with a decay rate 0.9999 .,"['Exponential', 'moving', 'average', 'is', 'applied', 'on', 'all', 'trainable', 'variables', 'with', 'a', 'decay', 'rate', '0.9999', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['JJ', 'VBG', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', 'CD', '.']",15
natural_language_inference,27,210,"Finally , we implement our model in Python using Tensorflow and carry out our experiments on an NVIDIA p 100 GPU .","['Finally', ',', 'we', 'implement', 'our', 'model', 'in', 'Python', 'using', 'Tensorflow', 'and', 'carry', 'out', 'our', 'experiments', 'on', 'an', 'NVIDIA', 'p', '100', 'GPU', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'PRP$', 'NN', 'IN', 'NNP', 'VBG', 'NNP', 'CC', 'VBP', 'RP', 'PRP$', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'CD', 'NNP', '.']",22
natural_language_inference,27,219,"As can be seen from the table , the accuracy ( EM / F1 ) performance of our model is on par with the state - of - the - art models .","['As', 'can', 'be', 'seen', 'from', 'the', 'table', ',', 'the', 'accuracy', '(', 'EM', '/', 'F1', ')', 'performance', 'of', 'our', 'model', 'is', 'on', 'par', 'with', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'MD', 'VB', 'VBN', 'IN', 'DT', 'NN', ',', 'DT', 'NN', '(', 'NNP', 'NNP', 'NNP', ')', 'NN', 'IN', 'PRP$', 'NN', 'VBZ', 'IN', 'NN', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', '.']",33
natural_language_inference,27,220,"In particular , our model trained on the original dataset outperforms all the documented results in the literature , in terms of both EM and F1 scores ( see second column of ) .","['In', 'particular', ',', 'our', 'model', 'trained', 'on', 'the', 'original', 'dataset', 'outperforms', 'all', 'the', 'documented', 'results', 'in', 'the', 'literature', ',', 'in', 'terms', 'of', 'both', 'EM', 'and', 'F1', 'scores', '(', 'see', 'second', 'column', 'of', ')', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', ',', 'PRP$', 'NN', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NNS', 'PDT', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', ',', 'IN', 'NNS', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NNS', '(', 'VB', 'JJ', 'NN', 'IN', ')', '.']",34
natural_language_inference,27,221,"When trained with the augmented data with proper sampling scheme , our model can get significant gain 1.5/1.1 on EM / F1 .","['When', 'trained', 'with', 'the', 'augmented', 'data', 'with', 'proper', 'sampling', 'scheme', ',', 'our', 'model', 'can', 'get', 'significant', 'gain', '1.5/1.1', 'on', 'EM', '/', 'F1', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['WRB', 'VBN', 'IN', 'DT', 'VBN', 'NN', 'IN', 'JJ', 'VBG', 'NN', ',', 'PRP$', 'NN', 'MD', 'VB', 'JJ', 'NN', 'CD', 'IN', 'NNP', 'NNP', 'NNP', '.']",23
natural_language_inference,27,222,"Finally , our result on the official test set is 76.2/84.6 , which significantly outperforms the best documented result 73.2/81.8 .","['Finally', ',', 'our', 'result', 'on', 'the', 'official', 'test', 'set', 'is', '76.2/84.6', ',', 'which', 'significantly', 'outperforms', 'the', 'best', 'documented', 'result', '73.2/81.8', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'CD', ',', 'WDT', 'RB', 'VBZ', 'DT', 'JJS', 'JJ', 'NN', 'CD', '.']",21
natural_language_inference,27,247,"As can be seen from the table , the use of convolutions in the encoders is crucial : both F1 and EM drop drastically by almost 3 percent if it is removed .","['As', 'can', 'be', 'seen', 'from', 'the', 'table', ',', 'the', 'use', 'of', 'convolutions', 'in', 'the', 'encoders', 'is', 'crucial', ':', 'both', 'F1', 'and', 'EM', 'drop', 'drastically', 'by', 'almost', '3', 'percent', 'if', 'it', 'is', 'removed', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'O']","['IN', 'MD', 'VB', 'VBN', 'IN', 'DT', 'NN', ',', 'DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NNS', 'VBZ', 'JJ', ':', 'DT', 'NNP', 'CC', 'NNP', 'NN', 'RB', 'IN', 'RB', 'CD', 'NN', 'IN', 'PRP', 'VBZ', 'VBN', '.']",33
natural_language_inference,27,248,Self- attention in the encoders is also a necessary component that contributes 1.4/1.3 gain of EM / F1 to the ultimate performance .,"['Self-', 'attention', 'in', 'the', 'encoders', 'is', 'also', 'a', 'necessary', 'component', 'that', 'contributes', '1.4/1.3', 'gain', 'of', 'EM', '/', 'F1', 'to', 'the', 'ultimate', 'performance', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['JJ', 'NN', 'IN', 'DT', 'NNS', 'VBZ', 'RB', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'CD', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'TO', 'DT', 'JJ', 'NN', '.']",23
natural_language_inference,27,254,"As the last block of rows in the table shows , data augmentation proves to be helpful in further boosting performance .","['As', 'the', 'last', 'block', 'of', 'rows', 'in', 'the', 'table', 'shows', ',', 'data', 'augmentation', 'proves', 'to', 'be', 'helpful', 'in', 'further', 'boosting', 'performance', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NNS', ',', 'NNS', 'NN', 'VBZ', 'TO', 'VB', 'JJ', 'IN', 'JJ', 'NN', 'NN', '.']",22
natural_language_inference,27,255,"Making the training data twice as large by adding the En - Fr - En data only ( ratio 1:1 between original training data and augmented data , as indicated by row "" data augmentation 2 ( 1:1:0 ) "" ) yields an increase in the F1 by 0.5 percent .","['Making', 'the', 'training', 'data', 'twice', 'as', 'large', 'by', 'adding', 'the', 'En', '-', 'Fr', '-', 'En', 'data', 'only', '(', 'ratio', '1:1', 'between', 'original', 'training', 'data', 'and', 'augmented', 'data', ',', 'as', 'indicated', 'by', 'row', '""', 'data', 'augmentation', '2', '(', '1:1:0', ')', '""', ')', 'yields', 'an', 'increase', 'in', 'the', 'F1', 'by', '0.5', 'percent', '.']","['B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-p', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O']","['VBG', 'DT', 'NN', 'NNS', 'RB', 'IN', 'JJ', 'IN', 'VBG', 'DT', 'NNP', ':', 'NNP', ':', 'NNP', 'VBZ', 'RB', '(', 'VB', 'CD', 'IN', 'JJ', 'NN', 'NNS', 'CC', 'VBN', 'NNS', ',', 'IN', 'VBN', 'IN', 'NN', 'NNP', 'VBZ', 'NN', 'CD', '(', 'CD', ')', 'NN', ')', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NNP', 'IN', 'CD', 'NN', '.']",51
natural_language_inference,27,259,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio between the original and augmented data during training can further boost the model performance .","['Although', 'injecting', 'more', 'data', 'beyond', '3', 'does', 'not', 'benefit', 'the', 'model', ',', 'we', 'observe', 'that', 'a', 'good', 'sampling', 'ratio', 'between', 'the', 'original', 'and', 'augmented', 'data', 'during', 'training', 'can', 'further', 'boost', 'the', 'model', 'performance', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'O']","['IN', 'VBG', 'JJR', 'NNS', 'IN', 'CD', 'VBZ', 'RB', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'CC', 'JJ', 'NNS', 'IN', 'NN', 'MD', 'RB', 'VB', 'DT', 'NN', 'NN', '.']",34
natural_language_inference,27,260,"In particular , when we increase the sampling weight of augmented data from ( 1:1:1 ) to ( 1:2:1 ) , the EM / F1 performance drops by 0.5/0.3 .","['In', 'particular', ',', 'when', 'we', 'increase', 'the', 'sampling', 'weight', 'of', 'augmented', 'data', 'from', '(', '1:1:1', ')', 'to', '(', '1:2:1', ')', ',', 'the', 'EM', '/', 'F1', 'performance', 'drops', 'by', '0.5/0.3', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-p', 'B-n', 'O']","['IN', 'JJ', ',', 'WRB', 'PRP', 'VBP', 'DT', 'VBG', 'NN', 'IN', 'VBN', 'NNS', 'IN', '(', 'CD', ')', 'TO', '(', 'CD', ')', ',', 'DT', 'NNP', 'NNP', 'NNP', 'NN', 'NNS', 'IN', 'CD', '.']",30
natural_language_inference,27,264,"Empirically , the ratio ( 3:1:1 ) yields the best performance , with 1.5/1.1 gain over the base model on EM / F1 .","['Empirically', ',', 'the', 'ratio', '(', '3:1:1', ')', 'yields', 'the', 'best', 'performance', ',', 'with', '1.5/1.1', 'gain', 'over', 'the', 'base', 'model', 'on', 'EM', '/', 'F1', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', '(', 'CD', ')', 'VBZ', 'DT', 'JJS', 'NN', ',', 'IN', 'CD', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'NNP', '.']",24
natural_language_inference,79,2,Distributed Representations of Sentences and Documents,"['Distributed', 'Representations', 'of', 'Sentences', 'and', 'Documents']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['VBN', 'NNS', 'IN', 'NNS', 'CC', 'NNS']",6
natural_language_inference,79,23,"In this paper , we propose Paragraph Vector , an unsupervised framework that learns continuous distributed vector representations for pieces of texts .","['In', 'this', 'paper', ',', 'we', 'propose', 'Paragraph', 'Vector', ',', 'an', 'unsupervised', 'framework', 'that', 'learns', 'continuous', 'distributed', 'vector', 'representations', 'for', 'pieces', 'of', 'texts', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'NNP', 'NNP', ',', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'JJ', 'VBN', 'NN', 'NNS', 'IN', 'NNS', 'IN', 'NN', '.']",23
natural_language_inference,79,25,"The name Paragraph Vector is to emphasize the fact that the method can be applied to variable - length pieces of texts , anything from a phrase or sentence to a large document .","['The', 'name', 'Paragraph', 'Vector', 'is', 'to', 'emphasize', 'the', 'fact', 'that', 'the', 'method', 'can', 'be', 'applied', 'to', 'variable', '-', 'length', 'pieces', 'of', 'texts', ',', 'anything', 'from', 'a', 'phrase', 'or', 'sentence', 'to', 'a', 'large', 'document', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NNP', 'NNP', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'MD', 'VB', 'VBN', 'TO', 'JJ', ':', 'NN', 'NNS', 'IN', 'NN', ',', 'NN', 'IN', 'DT', 'NN', 'CC', 'NN', 'TO', 'DT', 'JJ', 'NN', '.']",34
natural_language_inference,79,26,"In our model , the vector representation is trained to be useful for predicting words in a paragraph .","['In', 'our', 'model', ',', 'the', 'vector', 'representation', 'is', 'trained', 'to', 'be', 'useful', 'for', 'predicting', 'words', 'in', 'a', 'paragraph', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'O']","['IN', 'PRP$', 'NN', ',', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'JJ', 'IN', 'VBG', 'NNS', 'IN', 'DT', 'NN', '.']",19
natural_language_inference,79,27,"More precisely , we concatenate the paragraph vector with several word vectors from a paragraph and predict the following word in the given context .","['More', 'precisely', ',', 'we', 'concatenate', 'the', 'paragraph', 'vector', 'with', 'several', 'word', 'vectors', 'from', 'a', 'paragraph', 'and', 'predict', 'the', 'following', 'word', 'in', 'the', 'given', 'context', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RBR', 'RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'VBN', 'NN', '.']",25
natural_language_inference,79,28,Both word vectors and paragraph vectors are trained by the stochastic gradient descent and backpropagation .,"['Both', 'word', 'vectors', 'and', 'paragraph', 'vectors', 'are', 'trained', 'by', 'the', 'stochastic', 'gradient', 'descent', 'and', 'backpropagation', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O']","['DT', 'NN', 'NNS', 'CC', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'CC', 'NN', '.']",16
natural_language_inference,79,29,"While paragraph vectors are unique among paragraphs , the word vectors are shared .","['While', 'paragraph', 'vectors', 'are', 'unique', 'among', 'paragraphs', ',', 'the', 'word', 'vectors', 'are', 'shared', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'JJ', 'NNS', 'VBP', 'JJ', 'IN', 'NN', ',', 'DT', 'NN', 'NNS', 'VBP', 'VBN', '.']",14
natural_language_inference,79,30,"At prediction time , the paragraph vectors are inferred by fixing the word vectors and training the new paragraph vector until convergence .","['At', 'prediction', 'time', ',', 'the', 'paragraph', 'vectors', 'are', 'inferred', 'by', 'fixing', 'the', 'word', 'vectors', 'and', 'training', 'the', 'new', 'paragraph', 'vector', 'until', 'convergence', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'NN', 'NN', ',', 'DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'VBG', 'DT', 'NN', 'NNS', 'CC', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NN', '.']",23
natural_language_inference,79,163,"We learn the word vectors and paragraph vectors using 75,000 training documents ( 25,000 labeled and 50,000 unlabeled instances ) .","['We', 'learn', 'the', 'word', 'vectors', 'and', 'paragraph', 'vectors', 'using', '75,000', 'training', 'documents', '(', '25,000', 'labeled', 'and', '50,000', 'unlabeled', 'instances', ')', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NNS', 'CC', 'NN', 'NNS', 'VBG', 'CD', 'NN', 'NNS', '(', 'CD', 'VBN', 'CC', 'CD', 'JJ', 'NNS', ')', '.']",21
natural_language_inference,79,164,"The paragraph vectors for the 25,000 labeled instances are then fed through a neural network with one hidden layer with 50 units and a logistic classifier to learn to predict the sentiment .","['The', 'paragraph', 'vectors', 'for', 'the', '25,000', 'labeled', 'instances', 'are', 'then', 'fed', 'through', 'a', 'neural', 'network', 'with', 'one', 'hidden', 'layer', 'with', '50', 'units', 'and', 'a', 'logistic', 'classifier', 'to', 'learn', 'to', 'predict', 'the', 'sentiment', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['DT', 'NN', 'NNS', 'IN', 'DT', 'CD', 'JJ', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NN', 'NN', 'IN', 'CD', 'NNS', 'CC', 'DT', 'JJ', 'NN', 'TO', 'VB', 'TO', 'VB', 'DT', 'NN', '.']",33
natural_language_inference,79,169,"In particular , we cross validate the window size , and the optimal window size is 10 words .","['In', 'particular', ',', 'we', 'cross', 'validate', 'the', 'window', 'size', ',', 'and', 'the', 'optimal', 'window', 'size', 'is', '10', 'words', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'JJ', ',', 'PRP', 'VBP', 'VB', 'DT', 'NN', 'NN', ',', 'CC', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'CD', 'NNS', '.']",19
natural_language_inference,79,170,"The vector presented to the classifier is a concatenation of two vectors , one from PV - DBOW and one from PV - DM .","['The', 'vector', 'presented', 'to', 'the', 'classifier', 'is', 'a', 'concatenation', 'of', 'two', 'vectors', ',', 'one', 'from', 'PV', '-', 'DBOW', 'and', 'one', 'from', 'PV', '-', 'DM', '.']","['O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBD', 'TO', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NNS', ',', 'CD', 'IN', 'NNP', ':', 'NN', 'CC', 'CD', 'IN', 'NNP', ':', 'NN', '.']",25
natural_language_inference,79,171,"In PV - DBOW , the learned vector representations have 400 dimensions .","['In', 'PV', '-', 'DBOW', ',', 'the', 'learned', 'vector', 'representations', 'have', '400', 'dimensions', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'NNP', ':', 'NN', ',', 'DT', 'JJ', 'NN', 'NNS', 'VBP', 'CD', 'NNS', '.']",13
natural_language_inference,79,172,"In PV - DM , the learned vector representations have 400 dimensions for both words and documents .","['In', 'PV', '-', 'DM', ',', 'the', 'learned', 'vector', 'representations', 'have', '400', 'dimensions', 'for', 'both', 'words', 'and', 'documents', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', ':', 'NN', ',', 'DT', 'JJ', 'NN', 'NNS', 'VBP', 'CD', 'NNS', 'IN', 'DT', 'NNS', 'CC', 'NNS', '.']",18
natural_language_inference,79,173,"To predict the 10 - th word , we concatenate the paragraph vectors and word vectors .","['To', 'predict', 'the', '10', '-', 'th', 'word', ',', 'we', 'concatenate', 'the', 'paragraph', 'vectors', 'and', 'word', 'vectors', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'CD', ':', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NNS', 'CC', 'NN', 'NNS', '.']",17
natural_language_inference,79,174,"Special characters such as , .!?","['Special', 'characters', 'such', 'as', ',', '.!?']","['B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n']","['JJ', 'NNS', 'JJ', 'IN', ',', 'NN']",6
natural_language_inference,79,175,are treated as a normal word .,"['are', 'treated', 'as', 'a', 'normal', 'word', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['VBP', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",7
natural_language_inference,79,179,"As can be seen from the for long documents , bag - of - words models perform quite well and it is difficult to improve upon them using word vectors .","['As', 'can', 'be', 'seen', 'from', 'the', 'for', 'long', 'documents', ',', 'bag', '-', 'of', '-', 'words', 'models', 'perform', 'quite', 'well', 'and', 'it', 'is', 'difficult', 'to', 'improve', 'upon', 'them', 'using', 'word', 'vectors', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'MD', 'VB', 'VBN', 'IN', 'DT', 'IN', 'JJ', 'NNS', ',', 'SYM', ':', 'IN', ':', 'NNS', 'NNS', 'VBP', 'RB', 'RB', 'CC', 'PRP', 'VBZ', 'JJ', 'TO', 'VB', 'IN', 'PRP', 'VBG', 'NN', 'NNS', '.']",31
natural_language_inference,79,181,The combination of two models yields an improvement approximately 1.5 % in terms of error rates .,"['The', 'combination', 'of', 'two', 'models', 'yields', 'an', 'improvement', 'approximately', '1.5', '%', 'in', 'terms', 'of', 'error', 'rates', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'CD', 'NNS', 'NNS', 'DT', 'NN', 'RB', 'CD', 'NN', 'IN', 'NNS', 'IN', 'NN', 'NNS', '.']",17
natural_language_inference,79,184,The method described in this paper is the only approach that goes significantly beyond the barrier of 10 % error rate .,"['The', 'method', 'described', 'in', 'this', 'paper', 'is', 'the', 'only', 'approach', 'that', 'goes', 'significantly', 'beyond', 'the', 'barrier', 'of', '10', '%', 'error', 'rate', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBN', 'IN', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'RB', 'IN', 'DT', 'NN', 'IN', 'CD', 'NN', 'NN', 'NN', '.']",22
natural_language_inference,79,185,It achieves 7.42 % which is another 1.3 % absolute improvement ( or 15 % relative improvement ) over the best previous result of ..,"['It', 'achieves', '7.42', '%', 'which', 'is', 'another', '1.3', '%', 'absolute', 'improvement', '(', 'or', '15', '%', 'relative', 'improvement', ')', 'over', 'the', 'best', 'previous', 'result', 'of', '..']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBZ', 'CD', 'NN', 'WDT', 'VBZ', 'DT', 'CD', 'NN', 'JJ', 'NN', '(', 'CC', 'CD', 'NN', 'JJ', 'NN', ')', 'IN', 'DT', 'JJS', 'JJ', 'NN', 'IN', 'NN']",25
natural_language_inference,95,2,Multi - Passage Machine Reading Comprehension with Cross - Passage Answer Verification,"['Multi', '-', 'Passage', 'Machine', 'Reading', 'Comprehension', 'with', 'Cross', '-', 'Passage', 'Answer', 'Verification']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', ':', 'NN', 'NNP', 'NNP']",12
natural_language_inference,95,4,Machine reading comprehension ( MRC ) on real web data usually requires the machine to answer a question by analyzing multiple passages retrieved by search engine .,"['Machine', 'reading', 'comprehension', '(', 'MRC', ')', 'on', 'real', 'web', 'data', 'usually', 'requires', 'the', 'machine', 'to', 'answer', 'a', 'question', 'by', 'analyzing', 'multiple', 'passages', 'retrieved', 'by', 'search', 'engine', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'VBG', 'NN', '(', 'NNP', ')', 'IN', 'JJ', 'NNS', 'NNS', 'RB', 'VBZ', 'DT', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'VBG', 'JJ', 'NNS', 'VBN', 'IN', 'NN', 'NN', '.']",27
natural_language_inference,95,5,"Compared with MRC on a single passage , multi-passage MRC is more challenging , since we are likely to get multiple confusing answer candidates from different passages .","['Compared', 'with', 'MRC', 'on', 'a', 'single', 'passage', ',', 'multi-passage', 'MRC', 'is', 'more', 'challenging', ',', 'since', 'we', 'are', 'likely', 'to', 'get', 'multiple', 'confusing', 'answer', 'candidates', 'from', 'different', 'passages', '.']","['O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'IN', 'NNP', 'IN', 'DT', 'JJ', 'NN', ',', 'JJ', 'NNP', 'VBZ', 'RBR', 'JJ', ',', 'IN', 'PRP', 'VBP', 'JJ', 'TO', 'VB', 'JJ', 'VBG', 'NN', 'NNS', 'IN', 'JJ', 'NNS', '.']",28
natural_language_inference,95,51,"The over all framework of our model is demonstrated in , which consists of three modules .","['The', 'over', 'all', 'framework', 'of', 'our', 'model', 'is', 'demonstrated', 'in', ',', 'which', 'consists', 'of', 'three', 'modules', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'IN', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'VBZ', 'VBN', 'IN', ',', 'WDT', 'VBZ', 'IN', 'CD', 'NNS', '.']",17
natural_language_inference,95,52,"First , we follow the boundary - based MRC models to find an answer candidate for each passage by identifying the start and end position of the answer ( .","['First', ',', 'we', 'follow', 'the', 'boundary', '-', 'based', 'MRC', 'models', 'to', 'find', 'an', 'answer', 'candidate', 'for', 'each', 'passage', 'by', 'identifying', 'the', 'start', 'and', 'end', 'position', 'of', 'the', 'answer', '(', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'JJ', ':', 'VBN', 'NNP', 'NNS', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'VBG', 'DT', 'NN', 'CC', 'JJ', 'NN', 'IN', 'DT', 'NN', '(', '.']",30
natural_language_inference,95,53,"Second , we model the meanings of the answer candidates extracted from those passages and use the content scores to measure the quality of the candidates from a second perspective .","['Second', ',', 'we', 'model', 'the', 'meanings', 'of', 'the', 'answer', 'candidates', 'extracted', 'from', 'those', 'passages', 'and', 'use', 'the', 'content', 'scores', 'to', 'measure', 'the', 'quality', 'of', 'the', 'candidates', 'from', 'a', 'second', 'perspective', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O']","['JJ', ',', 'PRP', 'VBP', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NNS', 'VBD', 'IN', 'DT', 'NNS', 'CC', 'VB', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",31
natural_language_inference,95,54,"Third , we conduct the answer verification by enabling each answer candidate to attend to the other candidates based on their representations .","['Third', ',', 'we', 'conduct', 'the', 'answer', 'verification', 'by', 'enabling', 'each', 'answer', 'candidate', 'to', 'attend', 'to', 'the', 'other', 'candidates', 'based', 'on', 'their', 'representations', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['NNP', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'NN', 'TO', 'VB', 'TO', 'DT', 'JJ', 'NNS', 'VBN', 'IN', 'PRP$', 'NNS', '.']",23
natural_language_inference,95,56,"Therefore , the final answer is determined by three factors : the boundary , the content and the crosspassage answer verification .","['Therefore', ',', 'the', 'final', 'answer', 'is', 'determined', 'by', 'three', 'factors', ':', 'the', 'boundary', ',', 'the', 'content', 'and', 'the', 'crosspassage', 'answer', 'verification', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'CD', 'NNS', ':', 'DT', 'NN', ',', 'DT', 'NN', 'CC', 'DT', 'NN', 'NN', 'NN', '.']",22
natural_language_inference,95,57,"The three steps are modeled using different modules , which can be jointly trained in our end - to - end framework .","['The', 'three', 'steps', 'are', 'modeled', 'using', 'different', 'modules', ',', 'which', 'can', 'be', 'jointly', 'trained', 'in', 'our', 'end', '-', 'to', '-', 'end', 'framework', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'CD', 'NNS', 'VBP', 'VBN', 'VBG', 'JJ', 'NNS', ',', 'WDT', 'MD', 'VB', 'RB', 'VBN', 'IN', 'PRP$', 'NN', ':', 'TO', ':', 'NN', 'NN', '.']",23
natural_language_inference,95,145,"For MS - MARCO , we preprocess the corpus with the reversible tokenizer from Stanford CoreNLP and we choose the span that achieves the highest ROUGE - L score with the reference answers as the gold span for training .","['For', 'MS', '-', 'MARCO', ',', 'we', 'preprocess', 'the', 'corpus', 'with', 'the', 'reversible', 'tokenizer', 'from', 'Stanford', 'CoreNLP', 'and', 'we', 'choose', 'the', 'span', 'that', 'achieves', 'the', 'highest', 'ROUGE', '-', 'L', 'score', 'with', 'the', 'reference', 'answers', 'as', 'the', 'gold', 'span', 'for', 'training', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ':', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'CC', 'PRP', 'VBP', 'DT', 'NN', 'WDT', 'VBZ', 'DT', 'JJS', 'NNP', ':', 'NNP', 'NN', 'IN', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'NN', '.']",40
natural_language_inference,95,146,We employ the 300 - D pre-trained Glove embeddings and keep it fixed during training .,"['We', 'employ', 'the', '300', '-', 'D', 'pre-trained', 'Glove', 'embeddings', 'and', 'keep', 'it', 'fixed', 'during', 'training', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'CD', ':', 'NNP', 'JJ', 'NNP', 'NNS', 'CC', 'VB', 'PRP', 'JJ', 'IN', 'NN', '.']",16
natural_language_inference,95,147,The character embeddings are randomly initialized with its dimension as 30 .,"['The', 'character', 'embeddings', 'are', 'randomly', 'initialized', 'with', 'its', 'dimension', 'as', '30', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'PRP$', 'NN', 'IN', 'CD', '.']",12
natural_language_inference,95,161,Results on DuReader,"['Results', 'on', 'DuReader']","['O', 'B-p', 'B-n']","['NNS', 'IN', 'NNP']",3
natural_language_inference,95,165,We can see that this paragraph ranking can boost the BiDAF baseline significantly .,"['We', 'can', 'see', 'that', 'this', 'paragraph', 'ranking', 'can', 'boost', 'the', 'BiDAF', 'baseline', 'significantly', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O']","['PRP', 'MD', 'VB', 'IN', 'DT', 'NN', 'NN', 'MD', 'VB', 'DT', 'NNP', 'NN', 'RB', '.']",14
natural_language_inference,95,166,"Finally , we implement our system based on this new strategy , and our system ( single model ) achieves further improvement by a large margin .","['Finally', ',', 'we', 'implement', 'our', 'system', 'based', 'on', 'this', 'new', 'strategy', ',', 'and', 'our', 'system', '(', 'single', 'model', ')', 'achieves', 'further', 'improvement', 'by', 'a', 'large', 'margin', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'PRP$', 'NN', 'VBN', 'IN', 'DT', 'JJ', 'NN', ',', 'CC', 'PRP$', 'NN', '(', 'JJ', 'NN', ')', 'VBZ', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",27
natural_language_inference,95,186,"From , we can see that the answer verification makes a great contribution to the over all improvement , which confirms our hypothesis that cross - passage answer verification is useful for the multi-passage MRC .","['From', ',', 'we', 'can', 'see', 'that', 'the', 'answer', 'verification', 'makes', 'a', 'great', 'contribution', 'to', 'the', 'over', 'all', 'improvement', ',', 'which', 'confirms', 'our', 'hypothesis', 'that', 'cross', '-', 'passage', 'answer', 'verification', 'is', 'useful', 'for', 'the', 'multi-passage', 'MRC', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', ',', 'PRP', 'MD', 'VB', 'IN', 'DT', 'JJR', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'TO', 'DT', 'IN', 'DT', 'NN', ',', 'WDT', 'VBZ', 'PRP$', 'NN', 'IN', 'NN', ':', 'NN', 'NN', 'NN', 'VBZ', 'JJ', 'IN', 'DT', 'JJ', 'NNP', '.']",36
natural_language_inference,95,187,"For the ablation of the content model , we analyze that it will not only affect the content score itself , but also violate the verification model since the content probabilities are necessary for the answer representation , which will be further analyzed in Section 4.3 .","['For', 'the', 'ablation', 'of', 'the', 'content', 'model', ',', 'we', 'analyze', 'that', 'it', 'will', 'not', 'only', 'affect', 'the', 'content', 'score', 'itself', ',', 'but', 'also', 'violate', 'the', 'verification', 'model', 'since', 'the', 'content', 'probabilities', 'are', 'necessary', 'for', 'the', 'answer', 'representation', ',', 'which', 'will', 'be', 'further', 'analyzed', 'in', 'Section', '4.3', '.']","['B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'PRP', 'VBP', 'IN', 'PRP', 'MD', 'RB', 'RB', 'VB', 'DT', 'NN', 'NN', 'PRP', ',', 'CC', 'RB', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NNS', 'VBP', 'JJ', 'IN', 'DT', 'NN', 'NN', ',', 'WDT', 'MD', 'VB', 'JJ', 'VBN', 'IN', 'NN', 'CD', '.']",47
natural_language_inference,95,188,"Another discovery is that jointly training the three models can provide great benefits , which shows that the three tasks are actually closely related and can boost each other with shared representations at bottom layers .","['Another', 'discovery', 'is', 'that', 'jointly', 'training', 'the', 'three', 'models', 'can', 'provide', 'great', 'benefits', ',', 'which', 'shows', 'that', 'the', 'three', 'tasks', 'are', 'actually', 'closely', 'related', 'and', 'can', 'boost', 'each', 'other', 'with', 'shared', 'representations', 'at', 'bottom', 'layers', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'RB', 'VBG', 'DT', 'CD', 'NNS', 'MD', 'VB', 'JJ', 'NNS', ',', 'WDT', 'VBZ', 'IN', 'DT', 'CD', 'NNS', 'VBP', 'RB', 'RB', 'JJ', 'CC', 'MD', 'VB', 'DT', 'JJ', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', '.']",36
natural_language_inference,95,189,"At last , comparing our method with the baseline , we achieve an improvement of nearly 3 points without the yes / no classification .","['At', 'last', ',', 'comparing', 'our', 'method', 'with', 'the', 'baseline', ',', 'we', 'achieve', 'an', 'improvement', 'of', 'nearly', '3', 'points', 'without', 'the', 'yes', '/', 'no', 'classification', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', ',', 'VBG', 'PRP$', 'NN', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'RB', 'CD', 'NNS', 'IN', 'DT', 'NNS', 'VBP', 'DT', 'NN', '.']",25
natural_language_inference,74,2,Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference,"['Discourse', 'Marker', 'Augmented', 'Network', 'with', 'Reinforcement', 'Learning', 'for', 'Natural', 'Language', 'Inference']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",11
natural_language_inference,74,4,"Natural Language Inference ( NLI ) , also known as Recognizing Textual Entailment ( RTE ) , is one of the most important problems in natural language processing .","['Natural', 'Language', 'Inference', '(', 'NLI', ')', ',', 'also', 'known', 'as', 'Recognizing', 'Textual', 'Entailment', '(', 'RTE', ')', ',', 'is', 'one', 'of', 'the', 'most', 'important', 'problems', 'in', 'natural', 'language', 'processing', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'RB', 'VBN', 'IN', 'VBG', 'JJ', 'NNP', '(', 'NNP', ')', ',', 'VBZ', 'CD', 'IN', 'DT', 'RBS', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NN', '.']",29
natural_language_inference,74,6,"While current approaches mostly focus on the interaction architectures of the sentences , in this paper , we propose to transfer knowledge from some important discourse markers to augment the quality of the NLI model .","['While', 'current', 'approaches', 'mostly', 'focus', 'on', 'the', 'interaction', 'architectures', 'of', 'the', 'sentences', ',', 'in', 'this', 'paper', ',', 'we', 'propose', 'to', 'transfer', 'knowledge', 'from', 'some', 'important', 'discourse', 'markers', 'to', 'augment', 'the', 'quality', 'of', 'the', 'NLI', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O']","['IN', 'JJ', 'NNS', 'RB', 'VBP', 'IN', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NNS', ',', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'TO', 'VB', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNS', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",36
natural_language_inference,74,34,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'Discourse', 'Marker', 'Augmented', 'Network', 'for', 'natural', 'language', 'inference', ',', 'where', 'we', 'transfer', 'the', 'knowledge', 'from', 'the', 'existing', 'supervised', 'task', ':', 'Discourse', 'Marker', 'Prediction', '(', 'DMP', ')', ',', 'to', 'an', 'integrated', 'NLI', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'JJ', 'NN', 'NN', ',', 'WRB', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'VBG', 'JJ', 'NN', ':', 'NN', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'TO', 'DT', 'VBN', 'NNP', 'NN', '.']",40
natural_language_inference,74,35,We first propose a sentence encoder model that learns the representations of the sentences from the DMP task and then inject the encoder to the NLI network .,"['We', 'first', 'propose', 'a', 'sentence', 'encoder', 'model', 'that', 'learns', 'the', 'representations', 'of', 'the', 'sentences', 'from', 'the', 'DMP', 'task', 'and', 'then', 'inject', 'the', 'encoder', 'to', 'the', 'NLI', 'network', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBD', 'DT', 'NN', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'CC', 'RB', 'VB', 'DT', 'NN', 'TO', 'DT', 'NNP', 'NN', '.']",28
natural_language_inference,74,37,"In consideration of that different confidence level of the final labels should be discriminated , we employ reinforcement learning with a reward defined by the uniformity extent of the original labels to train the model .","['In', 'consideration', 'of', 'that', 'different', 'confidence', 'level', 'of', 'the', 'final', 'labels', 'should', 'be', 'discriminated', ',', 'we', 'employ', 'reinforcement', 'learning', 'with', 'a', 'reward', 'defined', 'by', 'the', 'uniformity', 'extent', 'of', 'the', 'original', 'labels', 'to', 'train', 'the', 'model', '.']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['IN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'MD', 'VB', 'VBN', ',', 'PRP', 'VBP', 'JJ', 'VBG', 'IN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'NN', '.']",36
natural_language_inference,74,153,We use the Stanford CoreNLP toolkit to tokenize the words and generate POS and NER tags .,"['We', 'use', 'the', 'Stanford', 'CoreNLP', 'toolkit', 'to', 'tokenize', 'the', 'words', 'and', 'generate', 'POS', 'and', 'NER', 'tags', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NNP', 'NN', 'TO', 'VB', 'DT', 'NNS', 'CC', 'VB', 'NNP', 'CC', 'NNP', 'NNS', '.']",17
natural_language_inference,74,154,"The word embeddings are initialized by 300d Glove , the dimensions of POS and NER embeddings are 30 and 10 .","['The', 'word', 'embeddings', 'are', 'initialized', 'by', '300d', 'Glove', ',', 'the', 'dimensions', 'of', 'POS', 'and', 'NER', 'embeddings', 'are', '30', 'and', '10', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'CD', 'NNP', ',', 'DT', 'NNS', 'IN', 'NNP', 'CC', 'NNP', 'NNS', 'VBP', 'CD', 'CC', 'CD', '.']",21
natural_language_inference,74,156,We apply Tensorflow r 1.3 as our neural network framework .,"['We', 'apply', 'Tensorflow', 'r', '1.3', 'as', 'our', 'neural', 'network', 'framework', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'CD', 'IN', 'PRP$', 'JJ', 'NN', 'NN', '.']",11
natural_language_inference,74,157,"We set the hidden size as 300 for all the LSTM layers and apply dropout between layers with an initial ratio of 0.9 , the decay rate as 0.97 for every 5000 step .","['We', 'set', 'the', 'hidden', 'size', 'as', '300', 'for', 'all', 'the', 'LSTM', 'layers', 'and', 'apply', 'dropout', 'between', 'layers', 'with', 'an', 'initial', 'ratio', 'of', '0.9', ',', 'the', 'decay', 'rate', 'as', '0.97', 'for', 'every', '5000', 'step', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'CD', 'IN', 'PDT', 'DT', 'NNP', 'NNS', 'CC', 'VB', 'NN', 'IN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', ',', 'DT', 'NN', 'NN', 'IN', 'CD', 'IN', 'DT', 'CD', 'NN', '.']",34
natural_language_inference,74,158,We use the AdaDelta for optimization as described in with ? as 0.95 and as 1 e - 8 .,"['We', 'use', 'the', 'AdaDelta', 'for', 'optimization', 'as', 'described', 'in', 'with', '?', 'as', '0.95', 'and', 'as', '1', 'e', '-', '8', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'IN', 'NN', 'IN', 'VBN', 'IN', 'IN', '.', 'IN', 'CD', 'CC', 'IN', 'CD', 'SYM', ':', 'CD', '.']",20
natural_language_inference,74,159,We set our batch size as 36 and the initial learning rate as 0.6 .,"['We', 'set', 'our', 'batch', 'size', 'as', '36', 'and', 'the', 'initial', 'learning', 'rate', 'as', '0.6', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBD', 'PRP$', 'NN', 'NN', 'IN', 'CD', 'CC', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', '.']",15
natural_language_inference,74,162,"For DMP task , we use stochastic gradient descent with initial learning rate as 0.1 , and we anneal by half each time the validation accuracy is lower than the previous epoch .","['For', 'DMP', 'task', ',', 'we', 'use', 'stochastic', 'gradient', 'descent', 'with', 'initial', 'learning', 'rate', 'as', '0.1', ',', 'and', 'we', 'anneal', 'by', 'half', 'each', 'time', 'the', 'validation', 'accuracy', 'is', 'lower', 'than', 'the', 'previous', 'epoch', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NNP', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'VBG', 'NN', 'IN', 'CD', ',', 'CC', 'PRP', 'VBP', 'IN', 'PDT', 'DT', 'NN', 'DT', 'NN', 'NN', 'VBZ', 'JJR', 'IN', 'DT', 'JJ', 'NN', '.']",33
natural_language_inference,74,163,"The number of epochs is set to be 10 , and the feedforward dropout rate is 0.2 .","['The', 'number', 'of', 'epochs', 'is', 'set', 'to', 'be', '10', ',', 'and', 'the', 'feedforward', 'dropout', 'rate', 'is', '0.2', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O']","['DT', 'NN', 'IN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'CD', ',', 'CC', 'DT', 'NN', 'NN', 'NN', 'VBZ', 'CD', '.']",18
natural_language_inference,74,169,"Obviously , the performance of most of the integrated methods are better than the sentence encoding based models above .","['Obviously', ',', 'the', 'performance', 'of', 'most', 'of', 'the', 'integrated', 'methods', 'are', 'better', 'than', 'the', 'sentence', 'encoding', 'based', 'models', 'above', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['RB', ',', 'DT', 'NN', 'IN', 'JJS', 'IN', 'DT', 'VBN', 'NNS', 'VBP', 'JJR', 'IN', 'DT', 'NN', 'VBG', 'VBN', 'NNS', 'IN', '.']",20
natural_language_inference,74,172,"The performance of our model achieves 89.6 % on SNLI , 80.3 % on matched MultiNLI and 79.4 % on mismatched MultiNLI , which are all state - of - the - art results .","['The', 'performance', 'of', 'our', 'model', 'achieves', '89.6', '%', 'on', 'SNLI', ',', '80.3', '%', 'on', 'matched', 'MultiNLI', 'and', '79.4', '%', 'on', 'mismatched', 'MultiNLI', ',', 'which', 'are', 'all', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'PRP$', 'NN', 'VBZ', 'CD', 'NN', 'IN', 'NNP', ',', 'CD', 'NN', 'IN', 'VBN', 'NNP', 'CC', 'CD', 'NN', 'IN', 'JJ', 'NNP', ',', 'WDT', 'VBP', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', '.']",35
natural_language_inference,74,174,"As shown in , we conduct an ablation experiment on SNLI development dataset to evaluate the individual contribution of each component of our model .","['As', 'shown', 'in', ',', 'we', 'conduct', 'an', 'ablation', 'experiment', 'on', 'SNLI', 'development', 'dataset', 'to', 'evaluate', 'the', 'individual', 'contribution', 'of', 'each', 'component', 'of', 'our', 'model', '.']","['O', 'O', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBN', 'IN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'NNP', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'PRP$', 'NN', '.']",25
natural_language_inference,74,176,"The result is obviously not satisfactory , which indicates that only using sentence embedding from discourse markers to predict the answer is not ideal in large - scale datasets .","['The', 'result', 'is', 'obviously', 'not', 'satisfactory', ',', 'which', 'indicates', 'that', 'only', 'using', 'sentence', 'embedding', 'from', 'discourse', 'markers', 'to', 'predict', 'the', 'answer', 'is', 'not', 'ideal', 'in', 'large', '-', 'scale', 'datasets', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'RB', 'RB', 'JJ', ',', 'WDT', 'VBZ', 'IN', 'RB', 'VBG', 'NN', 'VBG', 'IN', 'NN', 'NNS', 'TO', 'VB', 'DT', 'NN', 'VBZ', 'RB', 'JJ', 'IN', 'JJ', ':', 'NN', 'NNS', '.']",30
natural_language_inference,74,177,"We then remove the sentence encoder model , which means we do n't use the knowledge transferred from the DMP task and thus the representations r p and r hare set to be zero vectors in the equation ( 6 ) and the equation .","['We', 'then', 'remove', 'the', 'sentence', 'encoder', 'model', ',', 'which', 'means', 'we', 'do', ""n't"", 'use', 'the', 'knowledge', 'transferred', 'from', 'the', 'DMP', 'task', 'and', 'thus', 'the', 'representations', 'r', 'p', 'and', 'r', 'hare', 'set', 'to', 'be', 'zero', 'vectors', 'in', 'the', 'equation', '(', '6', ')', 'and', 'the', 'equation', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VB', 'DT', 'NN', 'NN', 'NN', ',', 'WDT', 'VBZ', 'PRP', 'VBP', 'RB', 'VB', 'DT', 'NN', 'VBD', 'IN', 'DT', 'NNP', 'NN', 'CC', 'RB', 'DT', 'NNS', 'VBP', 'NN', 'CC', 'NN', 'NN', 'VBN', 'TO', 'VB', 'CD', 'NNS', 'IN', 'DT', 'NN', '(', 'CD', ')', 'CC', 'DT', 'NN', '.']",45
natural_language_inference,74,178,"We observe that the performance drops significantly to 87 . 24 % , which is nearly 1.5 % to our DMAN model , which indicates that the discourse markers have deep connections with the logical relations between two sentences they links .","['We', 'observe', 'that', 'the', 'performance', 'drops', 'significantly', 'to', '87', '.', '24', '%', ',', 'which', 'is', 'nearly', '1.5', '%', 'to', 'our', 'DMAN', 'model', ',', 'which', 'indicates', 'that', 'the', 'discourse', 'markers', 'have', 'deep', 'connections', 'with', 'the', 'logical', 'relations', 'between', 'two', 'sentences', 'they', 'links', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'TO', 'CD', '.', 'CD', 'NN', ',', 'WDT', 'VBZ', 'RB', 'CD', 'NN', 'TO', 'PRP$', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'IN', 'DT', 'NN', 'NNS', 'VBP', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'CD', 'NNS', 'PRP', 'VBP', '.']",42
natural_language_inference,74,181,"we remove the character - level embedding and the POS and NER features , the performance drops a lot .","['we', 'remove', 'the', 'character', '-', 'level', 'embedding', 'and', 'the', 'POS', 'and', 'NER', 'features', ',', 'the', 'performance', 'drops', 'a', 'lot', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', ':', 'NN', 'NN', 'CC', 'DT', 'NNP', 'CC', 'NNP', 'NNS', ',', 'DT', 'NN', 'VBZ', 'DT', 'NN', '.']",20
natural_language_inference,74,183,The exact match feature also demonstrates its effectiveness in the ablation result .,"['The', 'exact', 'match', 'feature', 'also', 'demonstrates', 'its', 'effectiveness', 'in', 'the', 'ablation', 'result', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'RB', 'VBZ', 'PRP$', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",13
natural_language_inference,74,184,"Finally , we ablate the reinforcement learning part , in other words , we only use the original loss function to optimize the model ( set ? = 1 ) .","['Finally', ',', 'we', 'ablate', 'the', 'reinforcement', 'learning', 'part', ',', 'in', 'other', 'words', ',', 'we', 'only', 'use', 'the', 'original', 'loss', 'function', 'to', 'optimize', 'the', 'model', '(', 'set', '?', '=', '1', ')', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBG', 'NN', ',', 'IN', 'JJ', 'NNS', ',', 'PRP', 'RB', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', '(', 'VBN', '.', 'JJ', 'CD', ')', '.']",31
natural_language_inference,74,185,"The result drops about 0.5 % , which proves that it is helpful to utilize all the information from the annotators .","['The', 'result', 'drops', 'about', '0.5', '%', ',', 'which', 'proves', 'that', 'it', 'is', 'helpful', 'to', 'utilize', 'all', 'the', 'information', 'from', 'the', 'annotators', '.']","['O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'IN', 'CD', 'NN', ',', 'WDT', 'VBZ', 'IN', 'PRP', 'VBZ', 'JJ', 'TO', 'VB', 'PDT', 'DT', 'NN', 'IN', 'DT', 'NNS', '.']",22
natural_language_inference,78,2,"Compare , Compress and Propagate : Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference","['Compare', ',', 'Compress', 'and', 'Propagate', ':', 'Enhancing', 'Neural', 'Architectures', 'with', 'Alignment', 'Factorization', 'for', 'Natural', 'Language', 'Inference']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', ',', 'NNP', 'CC', 'NNP', ':', 'JJ', 'JJ', 'NNS', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",16
natural_language_inference,78,4,This paper presents a new deep learning architecture for Natural Language Inference ( NLI ) .,"['This', 'paper', 'presents', 'a', 'new', 'deep', 'learning', 'architecture', 'for', 'Natural', 'Language', 'Inference', '(', 'NLI', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', '.']",16
natural_language_inference,78,14,"More concretely , given a premise and hypothesis , NLI aims to detect whether the latter entails or contradicts the former .","['More', 'concretely', ',', 'given', 'a', 'premise', 'and', 'hypothesis', ',', 'NLI', 'aims', 'to', 'detect', 'whether', 'the', 'latter', 'entails', 'or', 'contradicts', 'the', 'former', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RBR', 'RB', ',', 'VBN', 'DT', 'NN', 'CC', 'NN', ',', 'NNP', 'VBZ', 'TO', 'VB', 'IN', 'DT', 'JJ', 'NNS', 'CC', 'VBZ', 'DT', 'JJ', '.']",22
natural_language_inference,78,26,There are several new novel components in our work .,"['There', 'are', 'several', 'new', 'novel', 'components', 'in', 'our', 'work', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['EX', 'VBP', 'JJ', 'JJ', 'JJ', 'NNS', 'IN', 'PRP$', 'NN', '.']",10
natural_language_inference,78,27,"Firstly , we propose a compare , compress and propagate ( Com Prop ) architecture where compressed alignment features are propagated to upper layers ( such as a RNN - based encoder ) for enhancing representation learning .","['Firstly', ',', 'we', 'propose', 'a', 'compare', ',', 'compress', 'and', 'propagate', '(', 'Com', 'Prop', ')', 'architecture', 'where', 'compressed', 'alignment', 'features', 'are', 'propagated', 'to', 'upper', 'layers', '(', 'such', 'as', 'a', 'RNN', '-', 'based', 'encoder', ')', 'for', 'enhancing', 'representation', 'learning', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', ',', 'NN', 'CC', 'NN', '(', 'NNP', 'NNP', ')', 'NN', 'WRB', 'VBN', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'JJ', 'NNS', '(', 'JJ', 'IN', 'DT', 'NNP', ':', 'VBN', 'NN', ')', 'IN', 'VBG', 'NN', 'NN', '.']",38
natural_language_inference,78,28,"Secondly , in order to achieve an efficient propagation of alignment features , we propose alignment factorization layers to reduce each alignment vector to a single scalar valued feature .","['Secondly', ',', 'in', 'order', 'to', 'achieve', 'an', 'efficient', 'propagation', 'of', 'alignment', 'features', ',', 'we', 'propose', 'alignment', 'factorization', 'layers', 'to', 'reduce', 'each', 'alignment', 'vector', 'to', 'a', 'single', 'scalar', 'valued', 'feature', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'IN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', ',', 'PRP', 'VBP', 'JJ', 'NN', 'NNS', 'TO', 'VB', 'DT', 'NN', 'NN', 'TO', 'DT', 'JJ', 'NN', 'VBN', 'NN', '.']",30
natural_language_inference,78,29,"Each scalar valued feature is used to augment the base word representation , allowing the subsequent RNN encoder layers to benefit from not only global but also cross sentence information .","['Each', 'scalar', 'valued', 'feature', 'is', 'used', 'to', 'augment', 'the', 'base', 'word', 'representation', ',', 'allowing', 'the', 'subsequent', 'RNN', 'encoder', 'layers', 'to', 'benefit', 'from', 'not', 'only', 'global', 'but', 'also', 'cross', 'sentence', 'information', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NN', 'NN', 'NN', ',', 'VBG', 'DT', 'JJ', 'NNP', 'NN', 'NNS', 'TO', 'VB', 'IN', 'RB', 'RB', 'JJ', 'CC', 'RB', 'JJ', 'NN', 'NN', '.']",31
natural_language_inference,78,192,We implement our model in TensorFlow and train them on Nvidia P100 GPUs .,"['We', 'implement', 'our', 'model', 'in', 'TensorFlow', 'and', 'train', 'them', 'on', 'Nvidia', 'P100', 'GPUs', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'IN', 'NNP', 'CC', 'VB', 'PRP', 'IN', 'NNP', 'NNP', 'NNP', '.']",14
natural_language_inference,78,193,"We use the Adam optimizer ( Kingma and Ba , 2014 ) with an initial learning rate of 0.0003 .","['We', 'use', 'the', 'Adam', 'optimizer', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', 'with', 'an', 'initial', 'learning', 'rate', 'of', '0.0003', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', '.']",20
natural_language_inference,78,194,L2 regularization is set to 10 ?6 .,"['L2', 'regularization', 'is', 'set', 'to', '10', '?6', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['NNP', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'NNS', '.']",8
natural_language_inference,78,195,"Dropout with a keep probability of 0.8 is applied after each fullyconnected , recurrent or highway layer .","['Dropout', 'with', 'a', 'keep', 'probability', 'of', '0.8', 'is', 'applied', 'after', 'each', 'fullyconnected', ',', 'recurrent', 'or', 'highway', 'layer', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', 'VBZ', 'VBN', 'IN', 'DT', 'VBN', ',', 'NN', 'CC', 'NN', 'NN', '.']",18
natural_language_inference,78,196,"The batch size is tuned amongst { 128 , 256 , 512 } .","['The', 'batch', 'size', 'is', 'tuned', 'amongst', '{', '128', ',', '256', ',', '512', '}', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'RB', '(', 'CD', ',', 'CD', ',', 'CD', ')', '.']",14
natural_language_inference,78,197,"The number of latent factors k for the factorization layer is tuned amongst { 5 , 10 , 50 , 100 , 150 } .","['The', 'number', 'of', 'latent', 'factors', 'k', 'for', 'the', 'factorization', 'layer', 'is', 'tuned', 'amongst', '{', '5', ',', '10', ',', '50', ',', '100', ',', '150', '}', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'JJ', 'NNS', 'VBP', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'RB', '(', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ')', '.']",25
natural_language_inference,78,198,The size of the hidden layers of the highway network layers are set to 300 .,"['The', 'size', 'of', 'the', 'hidden', 'layers', 'of', 'the', 'highway', 'network', 'layers', 'are', 'set', 'to', '300', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'CD', '.']",16
natural_language_inference,78,199,All parameters are initialized with xavier initialization .,"['All', 'parameters', 'are', 'initialized', 'with', 'xavier', 'initialization', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'JJR', 'NN', '.']",8
natural_language_inference,78,200,Word embeddings are preloaded with 300d Glo Ve embeddings and fixed during training .,"['Word', 'embeddings', 'are', 'preloaded', 'with', '300d', 'Glo', 'Ve', 'embeddings', 'and', 'fixed', 'during', 'training', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['NNP', 'NNS', 'VBP', 'VBN', 'IN', 'CD', 'NNP', 'NNP', 'NNS', 'CC', 'VBN', 'IN', 'NN', '.']",14
natural_language_inference,78,201,Sequence lengths are padded to batch - wise maximum .,"['Sequence', 'lengths', 'are', 'padded', 'to', 'batch', '-', 'wise', 'maximum', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNS', 'VBP', 'VBN', 'TO', 'VB', ':', 'NN', 'NN', '.']",10
natural_language_inference,78,202,The batch order is ( randomly ) sorted within buckets following .,"['The', 'batch', 'order', 'is', '(', 'randomly', ')', 'sorted', 'within', 'buckets', 'following', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O']","['DT', 'NN', 'NN', 'VBZ', '(', 'RB', ')', 'VBD', 'IN', 'NNS', 'VBG', '.']",12
natural_language_inference,78,204,Table 1 reports our results on the SNLI benchmark .,"['Table', '1', 'reports', 'our', 'results', 'on', 'the', 'SNLI', 'benchmark', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['JJ', 'CD', 'NNS', 'PRP$', 'NNS', 'IN', 'DT', 'NNP', 'NN', '.']",10
natural_language_inference,78,205,"On the cross sentence ( single model setting ) , the performance of our proposed CAFE model is extremely competitive .","['On', 'the', 'cross', 'sentence', '(', 'single', 'model', 'setting', ')', ',', 'the', 'performance', 'of', 'our', 'proposed', 'CAFE', 'model', 'is', 'extremely', 'competitive', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'NN', '(', 'JJ', 'NN', 'VBG', ')', ',', 'DT', 'NN', 'IN', 'PRP$', 'VBN', 'NNP', 'NN', 'VBZ', 'RB', 'JJ', '.']",21
natural_language_inference,78,207,CAFE obtains,"['CAFE', 'obtains']","['B-n', 'B-p']","['NNP', 'NNS']",2
natural_language_inference,78,208,"88.5 % accuracy on the SNLI test set , an extremely competitive score on the extremely popular benchmark .","['88.5', '%', 'accuracy', 'on', 'the', 'SNLI', 'test', 'set', ',', 'an', 'extremely', 'competitive', 'score', 'on', 'the', 'extremely', 'popular', 'benchmark', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['CD', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NN', ',', 'DT', 'RB', 'JJ', 'NN', 'IN', 'DT', 'RB', 'JJ', 'NN', '.']",19
natural_language_inference,78,210,"For example , CAFE also achieves 88.3 % and 88.1 % test accuracy with only 3.5 M and 1.5 M parameters","['For', 'example', ',', 'CAFE', 'also', 'achieves', '88.3', '%', 'and', '88.1', '%', 'test', 'accuracy', 'with', 'only', '3.5', 'M', 'and', '1.5', 'M', 'parameters']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['IN', 'NN', ',', 'NNP', 'RB', 'VBZ', 'CD', 'NN', 'CC', 'CD', 'NN', 'NN', 'NN', 'IN', 'RB', 'CD', 'NNP', 'CC', 'CD', 'NNP', 'NNS']",21
natural_language_inference,78,214,"Due to resource constraints , we did not train CAFE + ELMo ensembles but a single run ( and single model ) of CAFE + ELMo already achieves 89.0 score on SNLI .","['Due', 'to', 'resource', 'constraints', ',', 'we', 'did', 'not', 'train', 'CAFE', '+', 'ELMo', 'ensembles', 'but', 'a', 'single', 'run', '(', 'and', 'single', 'model', ')', 'of', 'CAFE', '+', 'ELMo', 'already', 'achieves', '89.0', 'score', 'on', 'SNLI', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['JJ', 'TO', 'VB', 'NNS', ',', 'PRP', 'VBD', 'RB', 'VB', 'NNP', 'NNP', 'NNP', 'NNS', 'CC', 'DT', 'JJ', 'NN', '(', 'CC', 'JJ', 'NN', ')', 'IN', 'NNP', 'NNP', 'NNP', 'RB', 'VBZ', 'CD', 'NN', 'IN', 'NNP', '.']",33
natural_language_inference,78,216,This outperforms the state - of - theart ESIM and DIIN models with only a fraction of the parameter cost .,"['This', 'outperforms', 'the', 'state', '-', 'of', '-', 'theart', 'ESIM', 'and', 'DIIN', 'models', 'with', 'only', 'a', 'fraction', 'of', 'the', 'parameter', 'cost', '.']","['O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'NN', 'NNP', 'CC', 'NNP', 'NNS', 'IN', 'RB', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",21
natural_language_inference,78,218,"Moreover , our lightweight adaptation achieves 87.7 % with only 750K parameters , which makes it extremely performant amongst models having the same amount of parameters such as the decomposable attention model ( 86.8 % ) .","['Moreover', ',', 'our', 'lightweight', 'adaptation', 'achieves', '87.7', '%', 'with', 'only', '750K', 'parameters', ',', 'which', 'makes', 'it', 'extremely', 'performant', 'amongst', 'models', 'having', 'the', 'same', 'amount', 'of', 'parameters', 'such', 'as', 'the', 'decomposable', 'attention', 'model', '(', '86.8', '%', ')', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP$', 'JJ', 'NN', 'VBZ', 'CD', 'NN', 'IN', 'RB', 'CD', 'NNS', ',', 'WDT', 'VBZ', 'PRP', 'RB', 'JJ', 'NN', 'NNS', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'JJ', 'IN', 'DT', 'JJ', 'NN', 'NN', '(', 'CD', 'NN', ')', '.']",37
natural_language_inference,78,219,"Finally , an ensemble of 5 CAFE models achieves 89.3 % test accuracy , the best test scores on the SNLI benchmark to date 3 .","['Finally', ',', 'an', 'ensemble', 'of', '5', 'CAFE', 'models', 'achieves', '89.3', '%', 'test', 'accuracy', ',', 'the', 'best', 'test', 'scores', 'on', 'the', 'SNLI', 'benchmark', 'to', 'date', '3', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'DT', 'NN', 'IN', 'CD', 'NNP', 'NNS', 'VBZ', 'CD', 'NN', 'NN', 'NN', ',', 'DT', 'JJS', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'TO', 'NN', 'CD', '.']",26
natural_language_inference,78,228,"On MultiNLI , CAFE significantly outperforms ESIM , a strong state - of - the - art model on both settings .","['On', 'MultiNLI', ',', 'CAFE', 'significantly', 'outperforms', 'ESIM', ',', 'a', 'strong', 'state', '-', 'of', '-', 'the', '-', 'art', 'model', 'on', 'both', 'settings', '.']","['O', 'B-n', 'O', 'B-n', 'B-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ',', 'NNP', 'RB', 'VBZ', 'NNP', ',', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'DT', 'NNS', '.']",22
natural_language_inference,78,229,We also outperform the ESIM + Read model .,"['We', 'also', 'outperform', 'the', 'ESIM', '+', 'Read', 'model', '.']","['O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NNP', 'NNP', 'NNP', 'NN', '.']",9
natural_language_inference,78,230,An ensemble of CAFE models achieve competitive re-sult on the MultiNLI dataset .,"['An', 'ensemble', 'of', 'CAFE', 'models', 'achieve', 'competitive', 're-sult', 'on', 'the', 'MultiNLI', 'dataset', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'NNP', 'NNS', 'VBP', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",13
natural_language_inference,78,231,"On SciTail , our proposed CAFE model achieves state - of - the - art performance .","['On', 'SciTail', ',', 'our', 'proposed', 'CAFE', 'model', 'achieves', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', '.']","['B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', ',', 'PRP$', 'VBN', 'NNP', 'NN', 'NNS', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', '.']",17
natural_language_inference,78,232,The performance gain over strong baselines such as DecompAtt and ESIM are ?,"['The', 'performance', 'gain', 'over', 'strong', 'baselines', 'such', 'as', 'DecompAtt', 'and', 'ESIM', 'are', '?']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'B-p', 'O']","['DT', 'NN', 'NN', 'IN', 'JJ', 'NNS', 'JJ', 'IN', 'NNP', 'CC', 'NNP', 'VBP', '.']",13
natural_language_inference,78,233,10 % ? 13 % in terms of accuracy .,"['10', '%', '?', '13', '%', 'in', 'terms', 'of', 'accuracy', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['CD', 'NN', '.', 'CD', 'NN', 'IN', 'NNS', 'IN', 'NN', '.']",10
natural_language_inference,78,234,"CAFE also outperforms DGEM , which uses a graph - based attention for improved performance , by a significant margin of 5 % .","['CAFE', 'also', 'outperforms', 'DGEM', ',', 'which', 'uses', 'a', 'graph', '-', 'based', 'attention', 'for', 'improved', 'performance', ',', 'by', 'a', 'significant', 'margin', 'of', '5', '%', '.']","['B-n', 'O', 'B-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'RB', 'VBZ', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NN', ':', 'VBN', 'NN', 'IN', 'JJ', 'NN', ',', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NN', '.']",24
natural_language_inference,78,239,The 1 - layer linear setting performs the best and is therefore reported in .,"['The', '1', '-', 'layer', 'linear', 'setting', 'performs', 'the', 'best', 'and', 'is', 'therefore', 'reported', 'in', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'CD', ':', 'NN', 'JJ', 'NN', 'VBZ', 'DT', 'JJS', 'CC', 'VBZ', 'RB', 'VBN', 'IN', '.']",15
natural_language_inference,78,240,Using ReLU seems to be worse than nonlinear FC layers .,"['Using', 'ReLU', 'seems', 'to', 'be', 'worse', 'than', 'nonlinear', 'FC', 'layers', '.']","['B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['VBG', 'NNP', 'VBZ', 'TO', 'VB', 'JJR', 'IN', 'JJ', 'NNP', 'NNS', '.']",11
natural_language_inference,78,243,"In , we explore the utility of using character and syntactic embeddings , which we found to have helped CAFE marginally .","['In', ',', 'we', 'explore', 'the', 'utility', 'of', 'using', 'character', 'and', 'syntactic', 'embeddings', ',', 'which', 'we', 'found', 'to', 'have', 'helped', 'CAFE', 'marginally', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-n', 'O', 'O']","['IN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'NN', 'CC', 'JJ', 'NNS', ',', 'WDT', 'PRP', 'VBD', 'TO', 'VB', 'VBN', 'NNP', 'RB', '.']",22
natural_language_inference,78,244,"In ( 4 ) , we remove the inter-attention alignment features , which naturally impact the model performance significantly .","['In', '(', '4', ')', ',', 'we', 'remove', 'the', 'inter-attention', 'alignment', 'features', ',', 'which', 'naturally', 'impact', 'the', 'model', 'performance', 'significantly', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'O']","['IN', '(', 'CD', ')', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'NNS', ',', 'WDT', 'RB', 'VBP', 'DT', 'NN', 'NN', 'RB', '.']",20
natural_language_inference,78,246,We observe that both highway layers have marginally helped the over all performance .,"['We', 'observe', 'that', 'both', 'highway', 'layers', 'have', 'marginally', 'helped', 'the', 'over', 'all', 'performance', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'DT', 'IN', 'DT', 'NN', '.']",14
natural_language_inference,78,248,We observe that the Sub and Concat compositions were more important than the Mul composition .,"['We', 'observe', 'that', 'the', 'Sub', 'and', 'Concat', 'compositions', 'were', 'more', 'important', 'than', 'the', 'Mul', 'composition', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NNS', 'VBD', 'RBR', 'JJ', 'IN', 'DT', 'NNP', 'NN', '.']",16
natural_language_inference,78,250,"Finally , in ( 10 ) , we replace the LSTM encoder with a BiLSTM , observing that adding bi-directionality did not improve performance for our model .","['Finally', ',', 'in', '(', '10', ')', ',', 'we', 'replace', 'the', 'LSTM', 'encoder', 'with', 'a', 'BiLSTM', ',', 'observing', 'that', 'adding', 'bi-directionality', 'did', 'not', 'improve', 'performance', 'for', 'our', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'IN', '(', 'CD', ')', ',', 'PRP', 'VB', 'DT', 'NNP', 'NN', 'IN', 'DT', 'NNP', ',', 'VBG', 'IN', 'VBG', 'NN', 'VBD', 'RB', 'VB', 'NN', 'IN', 'PRP$', 'NN', '.']",28
natural_language_inference,93,2,Gated Self - Matching Networks for Reading Comprehension and Question Answering,"['Gated', 'Self', '-', 'Matching', 'Networks', 'for', 'Reading', 'Comprehension', 'and', 'Question', 'Answering']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['VBN', 'NNP', ':', 'VBG', 'NNS', 'IN', 'VBG', 'NNP', 'CC', 'NNP', 'NNP']",11
natural_language_inference,93,4,"In this paper , we present the gated selfmatching networks for reading comprehension style question answering , which aims to answer questions from a given passage .","['In', 'this', 'paper', ',', 'we', 'present', 'the', 'gated', 'selfmatching', 'networks', 'for', 'reading', 'comprehension', 'style', 'question', 'answering', ',', 'which', 'aims', 'to', 'answer', 'questions', 'from', 'a', 'given', 'passage', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NNS', 'IN', 'VBG', 'NN', 'NN', 'NN', 'NN', ',', 'WDT', 'VBZ', 'TO', 'VB', 'NNS', 'IN', 'DT', 'VBN', 'NN', '.']",27
natural_language_inference,93,22,"Inspired by , we introduce a gated self - matching network , illustrated in , an end - to - end neural network model for reading comprehension and question answering .","['Inspired', 'by', ',', 'we', 'introduce', 'a', 'gated', 'self', '-', 'matching', 'network', ',', 'illustrated', 'in', ',', 'an', 'end', '-', 'to', '-', 'end', 'neural', 'network', 'model', 'for', 'reading', 'comprehension', 'and', 'question', 'answering', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', ',', 'PRP', 'VBP', 'DT', 'VBN', 'NN', ':', 'NN', 'NN', ',', 'VBN', 'IN', ',', 'DT', 'NN', ':', 'TO', ':', 'VB', 'JJ', 'NN', 'NN', 'IN', 'VBG', 'NN', 'CC', 'NN', 'NN', '.']",31
natural_language_inference,93,23,Our model consists of four parts :,"['Our', 'model', 'consists', 'of', 'four', 'parts', ':']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'IN', 'CD', 'NNS', ':']",7
natural_language_inference,93,24,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .","['1', ')', 'the', 'recurrent', 'network', 'encoder', 'to', 'build', 'representation', 'for', 'questions', 'and', 'passages', 'separately', ',', '2', ')', 'the', 'gated', 'matching', 'layer', 'to', 'match', 'the', 'question', 'and', 'passage', ',', '3', ')', 'the', 'self', '-', 'matching', 'layer', 'to', 'aggregate', 'information', 'from', 'the', 'whole', 'passage', ',', 'and', '4', ')', 'the', 'pointernetwork', 'based', 'answer', 'boundary', 'prediction', 'layer', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', ')', 'DT', 'NN', 'NN', 'NN', 'TO', 'VB', 'NN', 'IN', 'NNS', 'CC', 'NNS', 'RB', ',', 'CD', ')', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'CC', 'NN', ',', 'CD', ')', 'DT', 'NN', ':', 'VBG', 'NN', 'TO', 'VB', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'CC', 'CD', ')', 'DT', 'NN', 'VBN', 'IN', 'JJ', 'NN', 'NN', '.']",54
natural_language_inference,93,108,We use the tokenizer from Stanford CoreNLP to preprocess each passage and question .,"['We', 'use', 'the', 'tokenizer', 'from', 'Stanford', 'CoreNLP', 'to', 'preprocess', 'each', 'passage', 'and', 'question', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'TO', 'VB', 'DT', 'NN', 'CC', 'NN', '.']",14
natural_language_inference,93,109,The Gated Recurrent Unit variant of LSTM is used throughout our model .,"['The', 'Gated', 'Recurrent', 'Unit', 'variant', 'of', 'LSTM', 'is', 'used', 'throughout', 'our', 'model', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NNP', 'NNP', 'NNP', 'NN', 'IN', 'NNP', 'VBZ', 'VBN', 'IN', 'PRP$', 'NN', '.']",13
natural_language_inference,93,110,"For word embedding , we use pretrained case - sensitive GloVe embeddings 2 ( Pennington et al. , 2014 ) for both questions and passages , and it is fixed during training ; We use zero vectors to represent all out - of - vocab words .","['For', 'word', 'embedding', ',', 'we', 'use', 'pretrained', 'case', '-', 'sensitive', 'GloVe', 'embeddings', '2', '(', 'Pennington', 'et', 'al.', ',', '2014', ')', 'for', 'both', 'questions', 'and', 'passages', ',', 'and', 'it', 'is', 'fixed', 'during', 'training', ';', 'We', 'use', 'zero', 'vectors', 'to', 'represent', 'all', 'out', '-', 'of', '-', 'vocab', 'words', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', ':', 'JJ', 'NNP', 'NNS', 'CD', '(', 'NNP', 'VBZ', 'RB', ',', 'CD', ')', 'IN', 'DT', 'NNS', 'CC', 'NNS', ',', 'CC', 'PRP', 'VBZ', 'VBN', 'IN', 'NN', ':', 'PRP', 'VBP', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'IN', ':', 'IN', ':', 'NN', 'NNS', '.']",47
natural_language_inference,93,111,"We utilize 1 layer of bi-directional GRU to compute character - level embeddings and 3 layers of bi-directional GRU to encode questions and passages , the gated attention - based recurrent network for question and passage matching is also encoded bidirectionally in our experiment .","['We', 'utilize', '1', 'layer', 'of', 'bi-directional', 'GRU', 'to', 'compute', 'character', '-', 'level', 'embeddings', 'and', '3', 'layers', 'of', 'bi-directional', 'GRU', 'to', 'encode', 'questions', 'and', 'passages', ',', 'the', 'gated', 'attention', '-', 'based', 'recurrent', 'network', 'for', 'question', 'and', 'passage', 'matching', 'is', 'also', 'encoded', 'bidirectionally', 'in', 'our', 'experiment', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'CD', 'NN', 'IN', 'JJ', 'NNP', 'TO', 'VB', 'NN', ':', 'NN', 'NNS', 'CC', 'CD', 'NNS', 'IN', 'JJ', 'NNP', 'TO', 'VB', 'NNS', 'CC', 'NNS', ',', 'DT', 'JJ', 'NN', ':', 'VBN', 'NN', 'NN', 'IN', 'NN', 'CC', 'NN', 'NN', 'VBZ', 'RB', 'VBN', 'RB', 'IN', 'PRP$', 'NN', '.']",45
natural_language_inference,93,112,The hidden vector length is set to 75 for all layers .,"['The', 'hidden', 'vector', 'length', 'is', 'set', 'to', '75', 'for', 'all', 'layers', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'IN', 'DT', 'NNS', '.']",12
natural_language_inference,93,113,The hidden size used to compute attention scores is also 75 .,"['The', 'hidden', 'size', 'used', 'to', 'compute', 'attention', 'scores', 'is', 'also', '75', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['DT', 'JJ', 'NN', 'VBN', 'TO', 'VB', 'NN', 'NNS', 'VBZ', 'RB', 'CD', '.']",12
natural_language_inference,93,114,We also apply dropout between layers with a dropout rate of 0.2 .,"['We', 'also', 'apply', 'dropout', 'between', 'layers', 'with', 'a', 'dropout', 'rate', 'of', '0.2', '.']","['O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'RB', 'VBP', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",13
natural_language_inference,93,115,"The model is optimized with AdaDelta ( Zeiler , 2012 ) with an initial learning rate of 1 .","['The', 'model', 'is', 'optimized', 'with', 'AdaDelta', '(', 'Zeiler', ',', '2012', ')', 'with', 'an', 'initial', 'learning', 'rate', 'of', '1', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'IN', 'NNP', '(', 'NNP', ',', 'CD', ')', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', '.']",19
natural_language_inference,93,116,The ? and used in AdaDelta are 0.95 and 1e ? 6 respectively .,"['The', '?', 'and', 'used', 'in', 'AdaDelta', 'are', '0.95', 'and', '1e', '?', '6', 'respectively', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['DT', '.', 'CC', 'VBD', 'IN', 'NNP', 'VBP', 'CD', 'CC', 'CD', '.', 'CD', 'RB', '.']",14
natural_language_inference,93,132,attention - based recurrent network ( GARNN ) and self - matching attention mechanism positively contribute to the final results of gated self - matching networks .,"['attention', '-', 'based', 'recurrent', 'network', '(', 'GARNN', ')', 'and', 'self', '-', 'matching', 'attention', 'mechanism', 'positively', 'contribute', 'to', 'the', 'final', 'results', 'of', 'gated', 'self', '-', 'matching', 'networks', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', ':', 'VBN', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'PRP', ':', 'VBG', 'NN', 'NN', 'RB', 'JJ', 'TO', 'DT', 'JJ', 'NNS', 'IN', 'JJ', 'NN', ':', 'NN', 'NNS', '.']",27
natural_language_inference,93,133,"Removing self - matching results in 3.5 point EM drop , which reveals that information in the passage plays an important role .","['Removing', 'self', '-', 'matching', 'results', 'in', '3.5', 'point', 'EM', 'drop', ',', 'which', 'reveals', 'that', 'information', 'in', 'the', 'passage', 'plays', 'an', 'important', 'role', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'PRP', ':', 'VBG', 'NNS', 'IN', 'CD', 'NN', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'IN', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', '.']",23
natural_language_inference,93,134,Characterlevel embeddings contribute towards the model 's performance since it can better handle out - ofvocab or rare words .,"['Characterlevel', 'embeddings', 'contribute', 'towards', 'the', 'model', ""'s"", 'performance', 'since', 'it', 'can', 'better', 'handle', 'out', '-', 'ofvocab', 'or', 'rare', 'words', '.']","['B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNS', 'VBP', 'IN', 'DT', 'NN', 'POS', 'NN', 'IN', 'PRP', 'MD', 'VB', 'VB', 'RP', ':', 'NN', 'CC', 'JJ', 'NNS', '.']",20
natural_language_inference,93,137,Character - level embeddings are not utilized .,"['Character', '-', 'level', 'embeddings', 'are', 'not', 'utilized', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O']","['NNP', ':', 'NN', 'NNS', 'VBP', 'RB', 'JJ', '.']",8
natural_language_inference,93,138,"As shown in , the gate introduced in question and passage matching layer is helpful for both GRU and LSTM on the SQuAD dataset .","['As', 'shown', 'in', ',', 'the', 'gate', 'introduced', 'in', 'question', 'and', 'passage', 'matching', 'layer', 'is', 'helpful', 'for', 'both', 'GRU', 'and', 'LSTM', 'on', 'the', 'SQuAD', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBN', 'IN', ',', 'DT', 'NN', 'VBN', 'IN', 'NN', 'CC', 'NN', 'JJ', 'NN', 'VBZ', 'JJ', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'IN', 'DT', 'NNP', 'NN', '.']",25
natural_language_inference,83,2,Story Comprehension for Predicting What Happens Next,"['Story', 'Comprehension', 'for', 'Predicting', 'What', 'Happens', 'Next']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'IN', 'VBG', 'WP', 'VBZ', 'JJ']",7
natural_language_inference,83,4,"Automatic story comprehension is a fundamental challenge in Natural Language Understanding , and can enable computers to learn about social norms , human behavior and commonsense .","['Automatic', 'story', 'comprehension', 'is', 'a', 'fundamental', 'challenge', 'in', 'Natural', 'Language', 'Understanding', ',', 'and', 'can', 'enable', 'computers', 'to', 'learn', 'about', 'social', 'norms', ',', 'human', 'behavior', 'and', 'commonsense', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'NNP', ',', 'CC', 'MD', 'VB', 'NNS', 'TO', 'VB', 'IN', 'JJ', 'NNS', ',', 'JJ', 'NN', 'CC', 'NN', '.']",27
natural_language_inference,83,14,"For these reasons , automatically understanding stories is an interesting but challenging task for Computational Linguists .","['For', 'these', 'reasons', ',', 'automatically', 'understanding', 'stories', 'is', 'an', 'interesting', 'but', 'challenging', 'task', 'for', 'Computational', 'Linguists', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNS', ',', 'RB', 'JJ', 'NNS', 'VBZ', 'DT', 'JJ', 'CC', 'VBG', 'NN', 'IN', 'JJ', 'NNS', '.']",17
natural_language_inference,83,29,"Recently , introduced the story - cloze task for testing this ability , albeit without the aspect of language generation .","['Recently', ',', 'introduced', 'the', 'story', '-', 'cloze', 'task', 'for', 'testing', 'this', 'ability', ',', 'albeit', 'without', 'the', 'aspect', 'of', 'language', 'generation', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'VBD', 'DT', 'NN', ':', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', ',', 'IN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', '.']",21
natural_language_inference,83,35,"In this paper we explore three semantic aspects of story understanding : ( i ) the sequence of events described in the story , ( ii ) the evolution of sentiment and emotional trajectories , and ( iii ) topical consistency .","['In', 'this', 'paper', 'we', 'explore', 'three', 'semantic', 'aspects', 'of', 'story', 'understanding', ':', '(', 'i', ')', 'the', 'sequence', 'of', 'events', 'described', 'in', 'the', 'story', ',', '(', 'ii', ')', 'the', 'evolution', 'of', 'sentiment', 'and', 'emotional', 'trajectories', ',', 'and', '(', 'iii', ')', 'topical', 'consistency', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'CD', 'JJ', 'NNS', 'IN', 'NN', 'NN', ':', '(', 'NN', ')', 'DT', 'NN', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ',', '(', 'NN', ')', 'DT', 'NN', 'IN', 'NN', 'CC', 'JJ', 'NNS', ',', 'CC', '(', 'NN', ')', 'JJ', 'NN', '.']",42
natural_language_inference,83,36,"The first aspect is motivated from approaches in semantic script induction , and evaluates if events described in an ending - alternative are likely to occur within the sequence of events described in the preceding context .","['The', 'first', 'aspect', 'is', 'motivated', 'from', 'approaches', 'in', 'semantic', 'script', 'induction', ',', 'and', 'evaluates', 'if', 'events', 'described', 'in', 'an', 'ending', '-', 'alternative', 'are', 'likely', 'to', 'occur', 'within', 'the', 'sequence', 'of', 'events', 'described', 'in', 'the', 'preceding', 'context', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'NNS', 'IN', 'JJ', 'NN', 'NN', ',', 'CC', 'VBZ', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'VBG', ':', 'NN', 'VBP', 'JJ', 'TO', 'VB', 'IN', 'DT', 'NN', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'VBG', 'NN', '.']",37
natural_language_inference,83,42,Our model captures this by evaluating if the sentiment described in an ending option makes sense considering the context of the story .,"['Our', 'model', 'captures', 'this', 'by', 'evaluating', 'if', 'the', 'sentiment', 'described', 'in', 'an', 'ending', 'option', 'makes', 'sense', 'considering', 'the', 'context', 'of', 'the', 'story', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']","['PRP$', 'NN', 'VBZ', 'DT', 'IN', 'VBG', 'IN', 'DT', 'NN', 'VBD', 'IN', 'DT', 'VBG', 'NN', 'VBZ', 'NN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",23
natural_language_inference,83,45,Our model accounts for that by analyzing if the topic of an ending option is consistent with the preceding context .,"['Our', 'model', 'accounts', 'for', 'that', 'by', 'analyzing', 'if', 'the', 'topic', 'of', 'an', 'ending', 'option', 'is', 'consistent', 'with', 'the', 'preceding', 'context', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'NNS', 'IN', 'DT', 'IN', 'VBG', 'IN', 'DT', 'NN', 'IN', 'DT', 'VBG', 'NN', 'VBZ', 'JJ', 'IN', 'DT', 'VBG', 'NN', '.']",21
natural_language_inference,83,46,We present a log - linear model that is used to weigh the various aspects of the story using a hidden variable .,"['We', 'present', 'a', 'log', '-', 'linear', 'model', 'that', 'is', 'used', 'to', 'weigh', 'the', 'various', 'aspects', 'of', 'the', 'story', 'using', 'a', 'hidden', 'variable', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', ':', 'JJ', 'NN', 'WDT', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'VBG', 'DT', 'JJ', 'NN', '.']",23
natural_language_inference,83,173,DSSM : It trains two deep neural networks to project the context and the ending - options into the same vector space .,"['DSSM', ':', 'It', 'trains', 'two', 'deep', 'neural', 'networks', 'to', 'project', 'the', 'context', 'and', 'the', 'ending', '-', 'options', 'into', 'the', 'same', 'vector', 'space', '.']","['B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NN', ':', 'PRP', 'VBZ', 'CD', 'JJ', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'NN', 'CC', 'DT', 'VBG', ':', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",23
natural_language_inference,83,175,Msap :,"['Msap', ':']","['B-n', 'O']","['NN', ':']",2
natural_language_inference,83,177,It trains a logistic regression based on stylistic and languagemodel based features .,"['It', 'trains', 'a', 'logistic', 'regression', 'based', 'on', 'stylistic', 'and', 'languagemodel', 'based', 'features', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'DT', 'JJ', 'NN', 'VBN', 'IN', 'JJ', 'CC', 'NN', 'VBN', 'NNS', '.']",13
natural_language_inference,83,178,LR : Our next baseline is a simple logistic regression model which is agnostic to the fact that there are multiple types of aspects .,"['LR', ':', 'Our', 'next', 'baseline', 'is', 'a', 'simple', 'logistic', 'regression', 'model', 'which', 'is', 'agnostic', 'to', 'the', 'fact', 'that', 'there', 'are', 'multiple', 'types', 'of', 'aspects', '.']","['B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', ':', 'PRP$', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'JJ', 'TO', 'DT', 'NN', 'IN', 'EX', 'VBP', 'JJ', 'NNS', 'IN', 'NNS', '.']",25
natural_language_inference,83,180,Majority Vote :,"['Majority', 'Vote', ':']","['B-n', 'I-n', 'O']","['NNP', 'NNP', ':']",3
natural_language_inference,83,181,"This ensemble method uses the features extracted for each of the K = 3 aspects , to train K separate logistic regression models .","['This', 'ensemble', 'method', 'uses', 'the', 'features', 'extracted', 'for', 'each', 'of', 'the', 'K', '=', '3', 'aspects', ',', 'to', 'train', 'K', 'separate', 'logistic', 'regression', 'models', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'NNS', 'VBD', 'IN', 'DT', 'IN', 'DT', 'NNP', 'NNP', 'CD', 'NNS', ',', 'TO', 'VB', 'NNP', 'JJ', 'JJ', 'NN', 'NNS', '.']",24
natural_language_inference,83,183,Soft Voting :,"['Soft', 'Voting', ':']","['B-n', 'I-n', 'O']","['JJ', 'NN', ':']",3
natural_language_inference,83,184,This baseline also learns K different aspect - specific classifiers .,"['This', 'baseline', 'also', 'learns', 'K', 'different', 'aspect', '-', 'specific', 'classifiers', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'RB', 'VBZ', 'NNP', 'JJ', 'NN', ':', 'JJ', 'NNS', '.']",11
natural_language_inference,83,189,Aspect - aware Ensemble :,"['Aspect', '-', 'aware', 'Ensemble', ':']","['B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'JJ', 'JJ', ':']",5
natural_language_inference,83,190,"Like the voting methods , this baseline also trains K different aspectspecific classifiers .","['Like', 'the', 'voting', 'methods', ',', 'this', 'baseline', 'also', 'trains', 'K', 'different', 'aspectspecific', 'classifiers', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'NNS', ',', 'DT', 'NN', 'RB', 'VBZ', 'NNP', 'JJ', 'NN', 'NNS', '.']",14
natural_language_inference,83,197,shows the performance of a logistic regression model trained using all the features ( All ) and then using individual feature - groups .,"['shows', 'the', 'performance', 'of', 'a', 'logistic', 'regression', 'model', 'trained', 'using', 'all', 'the', 'features', '(', 'All', ')', 'and', 'then', 'using', 'individual', 'feature', '-', 'groups', '.']","['B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['VBZ', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'VBD', 'VBG', 'PDT', 'DT', 'NNS', '(', 'DT', ')', 'CC', 'RB', 'VBG', 'JJ', 'NN', ':', 'NNS', '.']",24
natural_language_inference,83,198,"We can see that the features extracted from the aspect analyzing the event - sequence have the strongest predictive power , followed by those characterizing Sentiment - trajectory .","['We', 'can', 'see', 'that', 'the', 'features', 'extracted', 'from', 'the', 'aspect', 'analyzing', 'the', 'event', '-', 'sequence', 'have', 'the', 'strongest', 'predictive', 'power', ',', 'followed', 'by', 'those', 'characterizing', 'Sentiment', '-', 'trajectory', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'IN', 'DT', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'VBG', 'DT', 'NN', ':', 'NN', 'VBP', 'DT', 'JJS', 'JJ', 'NN', ',', 'VBN', 'IN', 'DT', 'VBG', 'NNP', ':', 'NN', '.']",29
natural_language_inference,83,199,The features measuring top - ical consistency result in lowest accuracy but they still perform better than random on the task .,"['The', 'features', 'measuring', 'top', '-', 'ical', 'consistency', 'result', 'in', 'lowest', 'accuracy', 'but', 'they', 'still', 'perform', 'better', 'than', 'random', 'on', 'the', 'task', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'VBG', 'JJ', ':', 'JJ', 'NN', 'NN', 'IN', 'JJS', 'NN', 'CC', 'PRP', 'RB', 'VBP', 'JJR', 'IN', 'NN', 'IN', 'DT', 'NN', '.']",22
natural_language_inference,56,2,Recurrent Relational Networks,"['Recurrent', 'Relational', 'Networks']","['B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP']",3
natural_language_inference,56,5,"We introduce the recurrent relational network , a general purpose module that operates on a graph representation of objects .","['We', 'introduce', 'the', 'recurrent', 'relational', 'network', ',', 'a', 'general', 'purpose', 'module', 'that', 'operates', 'on', 'a', 'graph', 'representation', 'of', 'objects', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'JJ', 'NN', ',', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', '.']",20
natural_language_inference,56,25,"Toward generally realizing the ability to methodically reason about objects and their interactions over many steps , this paper introduces a composite function , the recurrent relational network .","['Toward', 'generally', 'realizing', 'the', 'ability', 'to', 'methodically', 'reason', 'about', 'objects', 'and', 'their', 'interactions', 'over', 'many', 'steps', ',', 'this', 'paper', 'introduces', 'a', 'composite', 'function', ',', 'the', 'recurrent', 'relational', 'network', '.']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'RB', 'VBG', 'DT', 'NN', 'TO', 'RB', 'NN', 'IN', 'NNS', 'CC', 'PRP$', 'NNS', 'IN', 'JJ', 'NNS', ',', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', ',', 'DT', 'NN', 'JJ', 'NN', '.']",29
natural_language_inference,56,26,It serves as a modular component for many - step relational reasoning in end - to - end differentiable learning systems .,"['It', 'serves', 'as', 'a', 'modular', 'component', 'for', 'many', '-', 'step', 'relational', 'reasoning', 'in', 'end', '-', 'to', '-', 'end', 'differentiable', 'learning', 'systems', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', ':', 'NN', 'JJ', 'NN', 'IN', 'NN', ':', 'TO', ':', 'NN', 'JJ', 'VBG', 'NNS', '.']",22
natural_language_inference,56,27,"It encodes the inductive biases that 1 ) objects exists in the world 2 ) they can be sufficiently described by properties 3 ) properties can changeover time 4 ) objects can affect each other and 5 ) given the properties , the effects object have on each other is invariant to time .","['It', 'encodes', 'the', 'inductive', 'biases', 'that', '1', ')', 'objects', 'exists', 'in', 'the', 'world', '2', ')', 'they', 'can', 'be', 'sufficiently', 'described', 'by', 'properties', '3', ')', 'properties', 'can', 'changeover', 'time', '4', ')', 'objects', 'can', 'affect', 'each', 'other', 'and', '5', ')', 'given', 'the', 'properties', ',', 'the', 'effects', 'object', 'have', 'on', 'each', 'other', 'is', 'invariant', 'to', 'time', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBZ', 'DT', 'JJ', 'NNS', 'IN', 'CD', ')', 'NNS', 'VBZ', 'IN', 'DT', 'NN', 'CD', ')', 'PRP', 'MD', 'VB', 'RB', 'VBN', 'IN', 'NNS', 'CD', ')', 'NNS', 'MD', 'VB', 'NN', 'CD', ')', 'NNS', 'MD', 'VB', 'DT', 'JJ', 'CC', 'CD', ')', 'VBN', 'DT', 'NNS', ',', 'DT', 'NNS', 'VBP', 'VBP', 'IN', 'DT', 'JJ', 'VBZ', 'JJ', 'TO', 'NN', '.']",54
natural_language_inference,56,28,"An important insight from the work of is to decompose a function for relational reasoning into two components or "" modules "" :","['An', 'important', 'insight', 'from', 'the', 'work', 'of', 'is', 'to', 'decompose', 'a', 'function', 'for', 'relational', 'reasoning', 'into', 'two', 'components', 'or', '""', 'modules', '""', ':']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'IN', 'JJ', 'VBG', 'IN', 'CD', 'NNS', 'CC', 'JJ', 'NNS', 'NNS', ':']",23
natural_language_inference,56,29,"a perceptual front - end , which is tasked to recognize objects in the raw input and represent them as vectors , and a relational reasoning module , which uses the representation to reason about the objects and their interactions .","['a', 'perceptual', 'front', '-', 'end', ',', 'which', 'is', 'tasked', 'to', 'recognize', 'objects', 'in', 'the', 'raw', 'input', 'and', 'represent', 'them', 'as', 'vectors', ',', 'and', 'a', 'relational', 'reasoning', 'module', ',', 'which', 'uses', 'the', 'representation', 'to', 'reason', 'about', 'the', 'objects', 'and', 'their', 'interactions', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', ':', 'NN', ',', 'WDT', 'VBZ', 'VBN', 'TO', 'VB', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'CC', 'VB', 'PRP', 'IN', 'NNS', ',', 'CC', 'DT', 'JJ', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'TO', 'NN', 'IN', 'DT', 'NNS', 'CC', 'PRP$', 'NNS', '.']",41
natural_language_inference,56,30,Both modules are trained jointly end - to - end .,"['Both', 'modules', 'are', 'trained', 'jointly', 'end', '-', 'to', '-', 'end', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'RB', 'VB', ':', 'TO', ':', 'NN', '.']",11
natural_language_inference,56,31,"In computer science parlance , the relational reasoning module implements an interface : it operates on a graph of nodes and directed edges , where the nodes are represented by real valued vectors , and is differentiable .","['In', 'computer', 'science', 'parlance', ',', 'the', 'relational', 'reasoning', 'module', 'implements', 'an', 'interface', ':', 'it', 'operates', 'on', 'a', 'graph', 'of', 'nodes', 'and', 'directed', 'edges', ',', 'where', 'the', 'nodes', 'are', 'represented', 'by', 'real', 'valued', 'vectors', ',', 'and', 'is', 'differentiable', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O']","['IN', 'NN', 'NN', 'NN', ',', 'DT', 'JJ', 'NN', 'NN', 'NNS', 'DT', 'NN', ':', 'PRP', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'NNS', 'CC', 'VBD', 'NNS', ',', 'WRB', 'DT', 'NNS', 'VBP', 'VBN', 'IN', 'JJ', 'VBN', 'NNS', ',', 'CC', 'VBZ', 'JJ', '.']",38
natural_language_inference,56,42,Recurrent Relational Networks,"['Recurrent', 'Relational', 'Networks']","['B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP']",3
natural_language_inference,56,81,Code to reproduce all experiments can be found at github.com/rasmusbergpalm/recurrent-relationalnetworks. designed as a set of prerequisite tasks for reasoning .,"['Code', 'to', 'reproduce', 'all', 'experiments', 'can', 'be', 'found', 'at', 'github.com/rasmusbergpalm/recurrent-relationalnetworks.', 'designed', 'as', 'a', 'set', 'of', 'prerequisite', 'tasks', 'for', 'reasoning', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'TO', 'VB', 'DT', 'NNS', 'MD', 'VB', 'VBN', 'IN', 'JJ', 'VBN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'VBG', '.']",20
natural_language_inference,56,90,bAbI question - answering tasks,"['bAbI', 'question', '-', 'answering', 'tasks']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['JJ', 'NN', ':', 'NN', 'NNS']",5
natural_language_inference,56,104,"Surprisingly , we find that we only need a single step of relational reasoning to solve all the bAbI tasks .","['Surprisingly', ',', 'we', 'find', 'that', 'we', 'only', 'need', 'a', 'single', 'step', 'of', 'relational', 'reasoning', 'to', 'solve', 'all', 'the', 'bAbI', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'IN', 'PRP', 'RB', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'TO', 'VB', 'PDT', 'DT', 'NN', 'NNS', '.']",21
natural_language_inference,56,108,"Regardless , it appears multiple steps of relational reasoning are not important for the bAbI dataset .","['Regardless', ',', 'it', 'appears', 'multiple', 'steps', 'of', 'relational', 'reasoning', 'are', 'not', 'important', 'for', 'the', 'bAbI', 'dataset', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBZ', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'VBP', 'RB', 'JJ', 'IN', 'DT', 'NN', 'NN', '.']",17
natural_language_inference,56,109,Pretty - CLEVR,"['Pretty', '-', 'CLEVR']","['B-n', 'I-n', 'I-n']","['NNP', ':', 'NN']",3
natural_language_inference,56,140,"Mirroring the results from the "" Sort - of - CLEVR "" dataset the MLP perfectly solves the non-relational questions , but struggle with even single jump questions and seem to lower bound the performance of the relational networks .","['Mirroring', 'the', 'results', 'from', 'the', '""', 'Sort', '-', 'of', '-', 'CLEVR', '""', 'dataset', 'the', 'MLP', 'perfectly', 'solves', 'the', 'non-relational', 'questions', ',', 'but', 'struggle', 'with', 'even', 'single', 'jump', 'questions', 'and', 'seem', 'to', 'lower', 'bound', 'the', 'performance', 'of', 'the', 'relational', 'networks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'NNP', ':', 'IN', ':', 'NNP', 'NNP', 'VBD', 'DT', 'NNP', 'RB', 'VBZ', 'DT', 'JJ', 'NNS', ',', 'CC', 'VBP', 'IN', 'RB', 'JJ', 'NN', 'NNS', 'CC', 'VBP', 'TO', 'VB', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNS', '.']",40
natural_language_inference,56,141,"The relational network solves the non-relational questions as well as the ones requiring a single jump , but the accuracy sharply drops off with more jumps .","['The', 'relational', 'network', 'solves', 'the', 'non-relational', 'questions', 'as', 'well', 'as', 'the', 'ones', 'requiring', 'a', 'single', 'jump', ',', 'but', 'the', 'accuracy', 'sharply', 'drops', 'off', 'with', 'more', 'jumps', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NNS', 'RB', 'RB', 'IN', 'DT', 'NNS', 'VBG', 'DT', 'JJ', 'NN', ',', 'CC', 'DT', 'NN', 'RB', 'VBZ', 'IN', 'IN', 'JJR', 'NNS', '.']",27
natural_language_inference,56,145,Sudoku,['Sudoku'],['B-n'],['NN'],1
natural_language_inference,56,153,Our network learns to solve 94.1 % of even the hardest 17 - givens Sudokus after 32 steps .,"['Our', 'network', 'learns', 'to', 'solve', '94.1', '%', 'of', 'even', 'the', 'hardest', '17', '-', 'givens', 'Sudokus', 'after', '32', 'steps', '.']","['B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'TO', 'VB', 'CD', 'NN', 'IN', 'RB', 'DT', 'JJS', 'CD', ':', 'NNS', 'NNP', 'IN', 'CD', 'NNS', '.']",19
natural_language_inference,56,159,"See figure 4 . We can see that even simple Sudokus with 33 givens require upwards of 10 steps of relational reasoning , whereas the harder 17 givens continue to improve even after 32 steps .","['See', 'figure', '4', '.', 'We', 'can', 'see', 'that', 'even', 'simple', 'Sudokus', 'with', '33', 'givens', 'require', 'upwards', 'of', '10', 'steps', 'of', 'relational', 'reasoning', ',', 'whereas', 'the', 'harder', '17', 'givens', 'continue', 'to', 'improve', 'even', 'after', '32', 'steps', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['VB', 'NN', 'CD', '.', 'PRP', 'MD', 'VB', 'IN', 'RB', 'JJ', 'NNP', 'IN', 'CD', 'NNS', 'VBP', 'NNS', 'IN', 'CD', 'NNS', 'IN', 'JJ', 'NN', ',', 'IN', 'DT', 'NN', 'CD', 'NNS', 'VBP', 'TO', 'VB', 'RB', 'IN', 'CD', 'NNS', '.']",36
natural_language_inference,56,162,At 64 steps the accuracy for the 17 givens puzzles increases to 96.6 % .,"['At', '64', 'steps', 'the', 'accuracy', 'for', 'the', '17', 'givens', 'puzzles', 'increases', 'to', '96.6', '%', '.']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'CD', 'NNS', 'DT', 'NN', 'IN', 'DT', 'CD', 'NNS', 'NNS', 'NNS', 'TO', 'CD', 'NN', '.']",15
natural_language_inference,56,172,"Our network outperforms loopy belief propagation , with parallel and random messages passing updates .","['Our', 'network', 'outperforms', 'loopy', 'belief', 'propagation', ',', 'with', 'parallel', 'and', 'random', 'messages', 'passing', 'updates', '.']","['O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP$', 'NN', 'NNS', 'VBP', 'JJ', 'NN', ',', 'IN', 'JJ', 'CC', 'JJ', 'NNS', 'VBG', 'NNS', '.']",15
natural_language_inference,56,173,"It also outperforms a version of loopy belief propagation modified specifically for solving Sudokus that uses 250 steps , Sinkhorn balancing every two steps and iteratively picks the most probable digit .","['It', 'also', 'outperforms', 'a', 'version', 'of', 'loopy', 'belief', 'propagation', 'modified', 'specifically', 'for', 'solving', 'Sudokus', 'that', 'uses', '250', 'steps', ',', 'Sinkhorn', 'balancing', 'every', 'two', 'steps', 'and', 'iteratively', 'picks', 'the', 'most', 'probable', 'digit', '.']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', 'VBD', 'RB', 'IN', 'VBG', 'NNP', 'WDT', 'VBZ', 'CD', 'NNS', ',', 'NNP', 'VBG', 'DT', 'CD', 'NNS', 'CC', 'RB', 'VBZ', 'DT', 'RBS', 'JJ', 'NN', '.']",32
natural_language_inference,56,176,"Finally we outperform Park which treats the Sudoku as a 9x9 image , uses 10 convolutional layers , iteratively picks the most probable digit , and evaluate on easier Sudokus with 24 - 36 givens .","['Finally', 'we', 'outperform', 'Park', 'which', 'treats', 'the', 'Sudoku', 'as', 'a', '9x9', 'image', ',', 'uses', '10', 'convolutional', 'layers', ',', 'iteratively', 'picks', 'the', 'most', 'probable', 'digit', ',', 'and', 'evaluate', 'on', 'easier', 'Sudokus', 'with', '24', '-', '36', 'givens', '.']","['O', 'O', 'B-n', 'B-p', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'PRP', 'VBP', 'NNP', 'WDT', 'VBZ', 'DT', 'NNP', 'IN', 'DT', 'CD', 'NN', ',', 'VBZ', 'CD', 'JJ', 'NNS', ',', 'RB', 'VBZ', 'DT', 'RBS', 'JJ', 'NN', ',', 'CC', 'VB', 'IN', 'JJR', 'NNP', 'IN', 'CD', ':', 'CD', 'NNS', '.']",36
natural_language_inference,50,2,Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,"['Hyperbolic', 'Representation', 'Learning', 'for', 'Fast', 'and', 'Efficient', 'Neural', 'Question', 'Answering']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'IN', 'NNP', 'CC', 'NNP', 'NNP', 'NNP', 'NNP']",10
natural_language_inference,50,24,"In this paper , we propose an extremely simple neural ranking model for question answering that achieves highly competitive results on several benchmarks with only a fraction of the runtime and only 40K - 90 K parameters ( as opposed to millions ) .","['In', 'this', 'paper', ',', 'we', 'propose', 'an', 'extremely', 'simple', 'neural', 'ranking', 'model', 'for', 'question', 'answering', 'that', 'achieves', 'highly', 'competitive', 'results', 'on', 'several', 'benchmarks', 'with', 'only', 'a', 'fraction', 'of', 'the', 'runtime', 'and', 'only', '40K', '-', '90', 'K', 'parameters', '(', 'as', 'opposed', 'to', 'millions', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'RB', 'JJ', 'JJ', 'VBG', 'NN', 'IN', 'NN', 'VBG', 'WDT', 'VBZ', 'RB', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'RB', 'DT', 'NN', 'IN', 'DT', 'NN', 'CC', 'RB', 'CD', ':', 'CD', 'NNP', 'NNS', '(', 'IN', 'VBN', 'TO', 'NNS', ')', '.']",44
natural_language_inference,50,25,Our neural ranking models the relationships between QA pairs in Hyperbolic space instead of Euclidean space .,"['Our', 'neural', 'ranking', 'models', 'the', 'relationships', 'between', 'QA', 'pairs', 'in', 'Hyperbolic', 'space', 'instead', 'of', 'Euclidean', 'space', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP$', 'JJ', 'NN', 'NNS', 'DT', 'NNS', 'IN', 'NNP', 'NNS', 'IN', 'NNP', 'NN', 'RB', 'IN', 'JJ', 'NN', '.']",17
natural_language_inference,50,26,Hyperbolic space is an embedding space with a constant negative curvature in which the distance towards the border is increasing exponentially .,"['Hyperbolic', 'space', 'is', 'an', 'embedding', 'space', 'with', 'a', 'constant', 'negative', 'curvature', 'in', 'which', 'the', 'distance', 'towards', 'the', 'border', 'is', 'increasing', 'exponentially', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['JJ', 'NN', 'VBZ', 'DT', 'VBG', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'WDT', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'VBG', 'RB', '.']",22
natural_language_inference,50,187,YahooCQA,['YahooCQA'],['B-n'],['NN'],1
natural_language_inference,50,188,- The key competitors of this dataset are the Neural Tensor LSTM ( NTN - LSTM ) and HD - LSTM from Tay et al.,"['-', 'The', 'key', 'competitors', 'of', 'this', 'dataset', 'are', 'the', 'Neural', 'Tensor', 'LSTM', '(', 'NTN', '-', 'LSTM', ')', 'and', 'HD', '-', 'LSTM', 'from', 'Tay', 'et', 'al.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n']","[':', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'DT', 'NNP', 'NNP', 'NNP', '(', 'NNP', ':', 'NNP', ')', 'CC', 'NNP', ':', 'NN', 'IN', 'NNP', 'FW', 'NN']",25
natural_language_inference,50,189,"along with their implementation of the Convolutional Neural Tensor Network , vanilla CNN model , and the Okapi BM - 25 benchmark .","['along', 'with', 'their', 'implementation', 'of', 'the', 'Convolutional', 'Neural', 'Tensor', 'Network', ',', 'vanilla', 'CNN', 'model', ',', 'and', 'the', 'Okapi', 'BM', '-', '25', 'benchmark', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'IN', 'PRP$', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', ',', 'NN', 'NNP', 'NN', ',', 'CC', 'DT', 'NNP', 'NNP', ':', 'CD', 'NN', '.']",23
natural_language_inference,50,190,"Additionally , we also report our own implementations of QA - BiLSTM , QA - CNN , AP - BiLSTM and AP - CNN on this dataset based on our experimental setup . WikiQA","['Additionally', ',', 'we', 'also', 'report', 'our', 'own', 'implementations', 'of', 'QA', '-', 'BiLSTM', ',', 'QA', '-', 'CNN', ',', 'AP', '-', 'BiLSTM', 'and', 'AP', '-', 'CNN', 'on', 'this', 'dataset', 'based', 'on', 'our', 'experimental', 'setup', '.', 'WikiQA']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']","['RB', ',', 'PRP', 'RB', 'VBP', 'PRP$', 'JJ', 'NNS', 'IN', 'NNP', ':', 'NNP', ',', 'NNP', ':', 'NNP', ',', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NN', 'IN', 'DT', 'NN', 'VBN', 'IN', 'PRP$', 'JJ', 'NN', '.', 'VB']",34
natural_language_inference,50,191,"- The key competitors of this dataset are the Paragraph Vector ( PV ) and PV + Cnt models of Le and Mikolv , CNN + Cnt model from Yu et al. and LCLR ( Yih et al . ) .","['-', 'The', 'key', 'competitors', 'of', 'this', 'dataset', 'are', 'the', 'Paragraph', 'Vector', '(', 'PV', ')', 'and', 'PV', '+', 'Cnt', 'models', 'of', 'Le', 'and', 'Mikolv', ',', 'CNN', '+', 'Cnt', 'model', 'from', 'Yu', 'et', 'al.', 'and', 'LCLR', '(', 'Yih', 'et', 'al', '.', ')', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","[':', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'DT', 'NNP', 'NNP', '(', 'NNP', ')', 'CC', 'NNP', 'NNP', 'NNP', 'NNS', 'IN', 'NNP', 'CC', 'NNP', ',', 'NNP', 'NNP', 'NNP', 'NN', 'IN', 'NNP', 'CC', 'NN', 'CC', 'NNP', '(', 'NNP', 'RB', 'RB', '.', ')', '.']",41
natural_language_inference,50,194,"For the clean version of this dataset , we also compare with AP - CNN and QA - BiLSTM / CNN .","['For', 'the', 'clean', 'version', 'of', 'this', 'dataset', ',', 'we', 'also', 'compare', 'with', 'AP', '-', 'CNN', 'and', 'QA', '-', 'BiLSTM', '/', 'CNN', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', ',', 'PRP', 'RB', 'VBP', 'IN', 'NNP', ':', 'NN', 'CC', 'NNP', ':', 'NNP', 'NNP', 'NNP', '.']",22
natural_language_inference,50,210,Hyper QA is implemented in Tensor - Flow .,"['Hyper', 'QA', 'is', 'implemented', 'in', 'Tensor', '-', 'Flow', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'VBZ', 'VBN', 'IN', 'NNP', ':', 'NN', '.']",9
natural_language_inference,50,211,"We adopt the AdaGrad optimizer with initial learning rate tuned amongst { 0.2 , 0.1 , 0.05 , 0.01 } .","['We', 'adopt', 'the', 'AdaGrad', 'optimizer', 'with', 'initial', 'learning', 'rate', 'tuned', 'amongst', '{', '0.2', ',', '0.1', ',', '0.05', ',', '0.01', '}', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', 'IN', 'JJ', 'VBG', 'NN', 'VBD', 'RB', '(', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ')', '.']",21
natural_language_inference,50,212,"The batch size is tuned amongst { 50 , 100 , 200 } .","['The', 'batch', 'size', 'is', 'tuned', 'amongst', '{', '50', ',', '100', ',', '200', '}', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'RB', '(', 'CD', ',', 'CD', ',', 'CD', ')', '.']",14
natural_language_inference,50,213,Models are trained for 25 epochs and the model parameters are saved each time the performance on the validation set is topped .,"['Models', 'are', 'trained', 'for', '25', 'epochs', 'and', 'the', 'model', 'parameters', 'are', 'saved', 'each', 'time', 'the', 'performance', 'on', 'the', 'validation', 'set', 'is', 'topped', '.']","['B-n', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['NNS', 'VBP', 'VBN', 'IN', 'CD', 'NNS', 'CC', 'DT', 'NN', 'NNS', 'VBP', 'VBN', 'DT', 'NN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'VBN', '.']",23
natural_language_inference,50,214,"The dimension of the projection layer is tuned amongst { 100 , 200 , 300 , 400 } .","['The', 'dimension', 'of', 'the', 'projection', 'layer', 'is', 'tuned', 'amongst', '{', '100', ',', '200', ',', '300', ',', '400', '}', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'RB', '(', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ')', '.']",19
natural_language_inference,50,215,"L2 regularization is tuned amongst { 0.001 , 0.0001 , 0.00001 }.","['L2', 'regularization', 'is', 'tuned', 'amongst', '{', '0.001', ',', '0.0001', ',', '0.00001', '}.']","['B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NN', 'VBZ', 'VBN', 'RB', '(', 'CD', ',', 'CD', ',', 'CD', 'NN']",12
natural_language_inference,50,216,The negative sampling rate is tuned from 2 to 8 .,"['The', 'negative', 'sampling', 'rate', 'is', 'tuned', 'from', '2', 'to', '8', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', 'TO', 'CD', '.']",11
natural_language_inference,50,227,reports the experimental results on SemEvalCQA .,"['reports', 'the', 'experimental', 'results', 'on', 'SemEvalCQA', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'O']","['NNS', 'DT', 'JJ', 'NNS', 'IN', 'NNP', '.']",7
natural_language_inference,50,228,Our proposed approach achieves highly competitive performance on this dataset .,"['Our', 'proposed', 'approach', 'achieves', 'highly', 'competitive', 'performance', 'on', 'this', 'dataset', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP$', 'VBN', 'NN', 'VBZ', 'RB', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",11
natural_language_inference,50,229,"Specifically , we have obtained the best P@1 performance over all , outperforming the state - of - the - art AI - CNN model by 3 % in terms of P@1 .","['Specifically', ',', 'we', 'have', 'obtained', 'the', 'best', 'P@1', 'performance', 'over', 'all', ',', 'outperforming', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'AI', '-', 'CNN', 'model', 'by', '3', '%', 'in', 'terms', 'of', 'P@1', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['RB', ',', 'PRP', 'VBP', 'VBN', 'DT', 'JJS', 'JJ', 'NN', 'IN', 'DT', ',', 'VBG', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNP', ':', 'NNP', 'NN', 'IN', 'CD', 'NN', 'IN', 'NNS', 'IN', 'NNP', '.']",33
natural_language_inference,50,230,The performance of our model on MAP is marginally short from the best performing model .,"['The', 'performance', 'of', 'our', 'model', 'on', 'MAP', 'is', 'marginally', 'short', 'from', 'the', 'best', 'performing', 'model', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'PRP$', 'NN', 'IN', 'NNP', 'VBZ', 'RB', 'JJ', 'IN', 'DT', 'JJS', 'NN', 'NN', '.']",16
natural_language_inference,50,239,reports the results on TrecQA ( raw ) .,"['reports', 'the', 'results', 'on', 'TrecQA', '(', 'raw', ')', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'DT', 'NNS', 'IN', 'NNP', '(', 'JJ', ')', '.']",9
natural_language_inference,50,240,Hyper QA achieves very competitive performance on both MAP and MRR metrics .,"['Hyper', 'QA', 'achieves', 'very', 'competitive', 'performance', 'on', 'both', 'MAP', 'and', 'MRR', 'metrics', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'VBZ', 'RB', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NNS', '.']",13
natural_language_inference,50,241,"Specifically , Hyper QA outperforms the basic CNN model of ( S&M ) by 2 % ? 3 % in terms of MAP / MRR .","['Specifically', ',', 'Hyper', 'QA', 'outperforms', 'the', 'basic', 'CNN', 'model', 'of', '(', 'S&M', ')', 'by', '2', '%', '?', '3', '%', 'in', 'terms', 'of', 'MAP', '/', 'MRR', '.']","['O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NNP', 'NN', 'IN', '(', 'NNP', ')', 'IN', 'CD', 'NN', '.', 'CD', 'NN', 'IN', 'NNS', 'IN', 'NNP', 'NNP', 'NNP', '.']",26
natural_language_inference,50,247,"Similarly , reports the results on TrecQA ( clean ) .","['Similarly', ',', 'reports', 'the', 'results', 'on', 'TrecQA', '(', 'clean', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'VBZ', 'DT', 'NNS', 'IN', 'NNP', '(', 'JJ', ')', '.']",11
natural_language_inference,50,248,"Similarly , Hyper QA also outperforms MP - CNN , AP - CNN and QA - CNN .","['Similarly', ',', 'Hyper', 'QA', 'also', 'outperforms', 'MP', '-', 'CNN', ',', 'AP', '-', 'CNN', 'and', 'QA', '-', 'CNN', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'NNP', 'NNP', 'RB', 'VBZ', 'NNP', ':', 'NNP', ',', 'NNP', ':', 'NN', 'CC', 'NNP', ':', 'NN', '.']",18
natural_language_inference,99,4,"Machine Comprehension ( MC ) is a challenging task in Natural Language Processing field , which aims to guide the machine to comprehend a passage and answer the given question .","['Machine', 'Comprehension', '(', 'MC', ')', 'is', 'a', 'challenging', 'task', 'in', 'Natural', 'Language', 'Processing', 'field', ',', 'which', 'aims', 'to', 'guide', 'the', 'machine', 'to', 'comprehend', 'a', 'passage', 'and', 'answer', 'the', 'given', 'question', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NNP', '(', 'NNP', ')', 'VBZ', 'DT', 'VBG', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'TO', 'VB', 'DT', 'NN', 'CC', 'VB', 'DT', 'VBN', 'NN', '.']",31
natural_language_inference,99,5,"Many existing approaches on MC task are suffering the inefficiency in some bottlenecks , such as insufficient lexical understanding , complex question - passage interaction , incorrect answer extraction and soon .","['Many', 'existing', 'approaches', 'on', 'MC', 'task', 'are', 'suffering', 'the', 'inefficiency', 'in', 'some', 'bottlenecks', ',', 'such', 'as', 'insufficient', 'lexical', 'understanding', ',', 'complex', 'question', '-', 'passage', 'interaction', ',', 'incorrect', 'answer', 'extraction', 'and', 'soon', '.']","['O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'VBG', 'NNS', 'IN', 'NNP', 'NN', 'VBP', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NNS', ',', 'JJ', 'IN', 'JJ', 'JJ', 'NN', ',', 'JJ', 'NN', ':', 'NN', 'NN', ',', 'JJ', 'NN', 'NN', 'CC', 'RB', '.']",32
natural_language_inference,99,13,Recently machine comprehension task accumulates much concern among NLP researchers .,"['Recently', 'machine', 'comprehension', 'task', 'accumulates', 'much', 'concern', 'among', 'NLP', 'researchers', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'NN', 'NN', 'NN', 'VBZ', 'JJ', 'NN', 'IN', 'NNP', 'NNS', '.']",11
natural_language_inference,99,28,"In this paper , we propose the novel framework named Smarnet with the hope that it can become as smart as humans .","['In', 'this', 'paper', ',', 'we', 'propose', 'the', 'novel', 'framework', 'named', 'Smarnet', 'with', 'the', 'hope', 'that', 'it', 'can', 'become', 'as', 'smart', 'as', 'humans', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBN', 'NNP', 'IN', 'DT', 'NN', 'IN', 'PRP', 'MD', 'VB', 'RB', 'JJ', 'IN', 'NNS', '.']",23
natural_language_inference,99,30,"Specifically , we first introduce the Smarnet framework that exploits fine - grained word understanding with various attribution discriminations , like humans recite words with corresponding properties .","['Specifically', ',', 'we', 'first', 'introduce', 'the', 'Smarnet', 'framework', 'that', 'exploits', 'fine', '-', 'grained', 'word', 'understanding', 'with', 'various', 'attribution', 'discriminations', ',', 'like', 'humans', 'recite', 'words', 'with', 'corresponding', 'properties', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'RB', 'VBP', 'DT', 'NNP', 'NN', 'WDT', 'VBZ', 'JJ', ':', 'VBN', 'NN', 'VBG', 'IN', 'JJ', 'NN', 'NNS', ',', 'IN', 'NNS', 'JJ', 'NNS', 'IN', 'VBG', 'NNS', '.']",28
natural_language_inference,99,31,We then develop the interactive attention with memory network to mimic human reading procedure .,"['We', 'then', 'develop', 'the', 'interactive', 'attention', 'with', 'memory', 'network', 'to', 'mimic', 'human', 'reading', 'procedure', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NN', 'TO', 'VB', 'JJ', 'NN', 'NN', '.']",15
natural_language_inference,99,32,We also add a checking layer on the answer refining in order to ensure the accuracy .,"['We', 'also', 'add', 'a', 'checking', 'layer', 'on', 'the', 'answer', 'refining', 'in', 'order', 'to', 'ensure', 'the', 'accuracy', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'VBG', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'NN', 'TO', 'VB', 'DT', 'NN', '.']",17
natural_language_inference,99,190,"We preprocess each passage and question using the library of nltk and exploit the popular pretrained word embedding GloVe with 100 - dimensional vectors ( Pennington , Socher , and Manning 2014 ) for both questions and passages .","['We', 'preprocess', 'each', 'passage', 'and', 'question', 'using', 'the', 'library', 'of', 'nltk', 'and', 'exploit', 'the', 'popular', 'pretrained', 'word', 'embedding', 'GloVe', 'with', '100', '-', 'dimensional', 'vectors', '(', 'Pennington', ',', 'Socher', ',', 'and', 'Manning', '2014', ')', 'for', 'both', 'questions', 'and', 'passages', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'CC', 'NN', 'VBG', 'DT', 'NN', 'IN', 'NN', 'CC', 'VB', 'DT', 'JJ', 'VBD', 'NN', 'VBG', 'NNP', 'IN', 'CD', ':', 'JJ', 'NNS', '(', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', 'CD', ')', 'IN', 'DT', 'NNS', 'CC', 'NNS', '.']",39
natural_language_inference,99,191,The size of char - level embedding is also set as 100 - dimensional and is obtained by CNN filters under the instruction of ( Kim 2014 ) .,"['The', 'size', 'of', 'char', '-', 'level', 'embedding', 'is', 'also', 'set', 'as', '100', '-', 'dimensional', 'and', 'is', 'obtained', 'by', 'CNN', 'filters', 'under', 'the', 'instruction', 'of', '(', 'Kim', '2014', ')', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'JJ', ':', 'NN', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'CD', ':', 'NN', 'CC', 'VBZ', 'VBN', 'IN', 'NNP', 'NNS', 'IN', 'DT', 'NN', 'IN', '(', 'NNP', 'CD', ')', '.']",29
natural_language_inference,99,193,We adopt the AdaDelta ( Zeiler 2012 ) optimizer for training with an initial learning rate of 0.0005 .,"['We', 'adopt', 'the', 'AdaDelta', '(', 'Zeiler', '2012', ')', 'optimizer', 'for', 'training', 'with', 'an', 'initial', 'learning', 'rate', 'of', '0.0005', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', '(', 'NNP', 'CD', ')', 'NN', 'IN', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', '.']",19
natural_language_inference,99,194,The batch size is set to be 48 for both the SQuAD and TriviaQA datasets .,"['The', 'batch', 'size', 'is', 'set', 'to', 'be', '48', 'for', 'both', 'the', 'SQuAD', 'and', 'TriviaQA', 'datasets', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'CD', 'IN', 'DT', 'DT', 'NNP', 'CC', 'NNP', 'NNS', '.']",16
natural_language_inference,99,195,We also apply dropout ( Srivastava et al. 2014 ) between layers with a dropout rate of 0.2 .,"['We', 'also', 'apply', 'dropout', '(', 'Srivastava', 'et', 'al.', '2014', ')', 'between', 'layers', 'with', 'a', 'dropout', 'rate', 'of', '0.2', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'RB', 'VBP', 'NN', '(', 'NNP', 'RB', 'RB', 'CD', ')', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",19
natural_language_inference,99,196,"For the multi-hop reasoning , we set the number of hops as 2 which is imitating human reading procedure on skimming and scanning .","['For', 'the', 'multi-hop', 'reasoning', ',', 'we', 'set', 'the', 'number', 'of', 'hops', 'as', '2', 'which', 'is', 'imitating', 'human', 'reading', 'procedure', 'on', 'skimming', 'and', 'scanning', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NNS', 'IN', 'CD', 'WDT', 'VBZ', 'VBG', 'JJ', 'NN', 'NN', 'IN', 'NN', 'CC', 'NN', '.']",24
natural_language_inference,99,197,"During training , we set the moving averages of all weights as the exponential decay rate of 0.999 .","['During', 'training', ',', 'we', 'set', 'the', 'moving', 'averages', 'of', 'all', 'weights', 'as', 'the', 'exponential', 'decay', 'rate', 'of', '0.999', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'DT', 'VBG', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', '.']",19
natural_language_inference,99,209,"From the tables 1 and 2 we can see our single model achieves an EM score of 71.415 % and a F1 score of 80.160 % and the ensemble model improves to EM 75.989 % and F1 83. 475 % , which are both only after the r-net method at the time of submission .","['From', 'the', 'tables', '1', 'and', '2', 'we', 'can', 'see', 'our', 'single', 'model', 'achieves', 'an', 'EM', 'score', 'of', '71.415', '%', 'and', 'a', 'F1', 'score', 'of', '80.160', '%', 'and', 'the', 'ensemble', 'model', 'improves', 'to', 'EM', '75.989', '%', 'and', 'F1', '83.', '475', '%', ',', 'which', 'are', 'both', 'only', 'after', 'the', 'r-net', 'method', 'at', 'the', 'time', 'of', 'submission', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNS', 'CD', 'CC', 'CD', 'PRP', 'MD', 'VB', 'PRP$', 'JJ', 'NN', 'VBZ', 'DT', 'NNP', 'NN', 'IN', 'CD', 'NN', 'CC', 'DT', 'NNP', 'NN', 'IN', 'CD', 'NN', 'CC', 'DT', 'JJ', 'NN', 'VBZ', 'TO', 'VB', 'CD', 'NN', 'CC', 'NNP', 'CD', 'CD', 'NN', ',', 'WDT', 'VBP', 'DT', 'RB', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', '.']",55
natural_language_inference,99,211,We also compare our models on the recently proposed dataset Trivia QA. shows the performance comparison on the test set of Trivia QA .,"['We', 'also', 'compare', 'our', 'models', 'on', 'the', 'recently', 'proposed', 'dataset', 'Trivia', 'QA.', 'shows', 'the', 'performance', 'comparison', 'on', 'the', 'test', 'set', 'of', 'Trivia', 'QA', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'PRP$', 'NNS', 'IN', 'DT', 'RB', 'VBN', 'NN', 'NNP', 'NNP', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'NNP', 'NNP', '.']",24
natural_language_inference,99,212,We can see our Smarnet model outperforms the other baselines on both wikipedia domain and web domain .,"['We', 'can', 'see', 'our', 'Smarnet', 'model', 'outperforms', 'the', 'other', 'baselines', 'on', 'both', 'wikipedia', 'domain', 'and', 'web', 'domain', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'PRP$', 'NNP', 'NN', 'VBZ', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'CC', 'NN', 'NN', '.']",18
natural_language_inference,99,216,"We see the full features integration obtain the best performance , which demonstrates the necessity of combining all the features into consideration .","['We', 'see', 'the', 'full', 'features', 'integration', 'obtain', 'the', 'best', 'performance', ',', 'which', 'demonstrates', 'the', 'necessity', 'of', 'combining', 'all', 'the', 'features', 'into', 'consideration', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NNS', 'NN', 'VB', 'DT', 'JJS', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'PDT', 'DT', 'NNS', 'IN', 'NN', '.']",23
natural_language_inference,99,217,"Among all the feature ablations , the Part - Of - Speech , Exact Match , Qtype features drop much more than the other features , which shows the importance of these three features .","['Among', 'all', 'the', 'feature', 'ablations', ',', 'the', 'Part', '-', 'Of', '-', 'Speech', ',', 'Exact', 'Match', ',', 'Qtype', 'features', 'drop', 'much', 'more', 'than', 'the', 'other', 'features', ',', 'which', 'shows', 'the', 'importance', 'of', 'these', 'three', 'features', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PDT', 'DT', 'NN', 'NNS', ',', 'DT', 'NNP', ':', 'IN', ':', 'NN', ',', 'NNP', 'NNP', ',', 'NNP', 'VBZ', 'VB', 'RB', 'JJR', 'IN', 'DT', 'JJ', 'NNS', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'CD', 'NNS', '.']",35
natural_language_inference,99,219,"As for the final ablation of POS and NER , we can see the performance decays over 3 % point , which clearly proves the usefulness of the comprehensive lexical information .","['As', 'for', 'the', 'final', 'ablation', 'of', 'POS', 'and', 'NER', ',', 'we', 'can', 'see', 'the', 'performance', 'decays', 'over', '3', '%', 'point', ',', 'which', 'clearly', 'proves', 'the', 'usefulness', 'of', 'the', 'comprehensive', 'lexical', 'information', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'CC', 'NNP', ',', 'PRP', 'MD', 'VB', 'DT', 'NN', 'VBZ', 'IN', 'CD', 'NN', 'NN', ',', 'WDT', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NN', '.']",32
natural_language_inference,99,221,"We first replace our input gate mechanism into simplified feature concatenation strategy , the performance drops nearly 2.3 % on the EM score , which proves the effectiveness of our proposed dynamic input gating mechanism .","['We', 'first', 'replace', 'our', 'input', 'gate', 'mechanism', 'into', 'simplified', 'feature', 'concatenation', 'strategy', ',', 'the', 'performance', 'drops', 'nearly', '2.3', '%', 'on', 'the', 'EM', 'score', ',', 'which', 'proves', 'the', 'effectiveness', 'of', 'our', 'proposed', 'dynamic', 'input', 'gating', 'mechanism', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VB', 'PRP$', 'NN', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NN', 'NN', ',', 'DT', 'NN', 'VBD', 'RB', 'CD', 'NN', 'IN', 'DT', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'PRP$', 'VBN', 'JJ', 'NN', 'VBG', 'NN', '.']",36
natural_language_inference,99,223,The result proves that our modification of employing question influence on the passage encoding can boost the result up to 1.3 % on the EM score .,"['The', 'result', 'proves', 'that', 'our', 'modification', 'of', 'employing', 'question', 'influence', 'on', 'the', 'passage', 'encoding', 'can', 'boost', 'the', 'result', 'up', 'to', '1.3', '%', 'on', 'the', 'EM', 'score', '.']","['O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'IN', 'PRP$', 'NN', 'IN', 'VBG', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'MD', 'VB', 'DT', 'NN', 'RB', 'TO', 'CD', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",27
natural_language_inference,34,2,Dynamically Fused Graph Network for Multi-hop Reasoning,"['Dynamically', 'Fused', 'Graph', 'Network', 'for', 'Multi-hop', 'Reasoning']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['RB', 'VBN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",7
natural_language_inference,34,4,Text - based question answering ( TBQA ) has been studied extensively in recent years .,"['Text', '-', 'based', 'question', 'answering', '(', 'TBQA', ')', 'has', 'been', 'studied', 'extensively', 'in', 'recent', 'years', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'VBN', 'NN', 'VBG', '(', 'NNP', ')', 'VBZ', 'VBN', 'VBN', 'RB', 'IN', 'JJ', 'NNS', '.']",16
natural_language_inference,34,13,Question answering ( QA ) has been a popular topic in natural language processing .,"['Question', 'answering', '(', 'QA', ')', 'has', 'been', 'a', 'popular', 'topic', 'in', 'natural', 'language', 'processing', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'VBG', '(', 'NNP', ')', 'VBZ', 'VBN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",15
natural_language_inference,34,14,QA provides a quantifiable way to evaluate an NLP system 's capability on language understanding and reasoning .,"['QA', 'provides', 'a', 'quantifiable', 'way', 'to', 'evaluate', 'an', 'NLP', 'system', ""'s"", 'capability', 'on', 'language', 'understanding', 'and', 'reasoning', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NNP', 'NN', 'POS', 'NN', 'IN', 'NN', 'NN', 'CC', 'NN', '.']",18
natural_language_inference,34,50,"In this paper , we propose Dynamically Fused Graph Network ( DFGN ) , a novel method to address the aforementioned concerns for multi-hop text - based QA .","['In', 'this', 'paper', ',', 'we', 'propose', 'Dynamically', 'Fused', 'Graph', 'Network', '(', 'DFGN', ')', ',', 'a', 'novel', 'method', 'to', 'address', 'the', 'aforementioned', 'concerns', 'for', 'multi-hop', 'text', '-', 'based', 'QA', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'RB', 'VBN', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'IN', 'NN', 'NN', ':', 'VBN', 'NNP', '.']",29
natural_language_inference,34,51,"For the first challenge , DFGN constructs a dynamic entity graph based on entity mentions in the query and documents .","['For', 'the', 'first', 'challenge', ',', 'DFGN', 'constructs', 'a', 'dynamic', 'entity', 'graph', 'based', 'on', 'entity', 'mentions', 'in', 'the', 'query', 'and', 'documents', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'NNS', 'VBN', 'IN', 'NN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NNS', '.']",21
natural_language_inference,34,52,This process iterates in multiple rounds to achieve multihop reasoning .,"['This', 'process', 'iterates', 'in', 'multiple', 'rounds', 'to', 'achieve', 'multihop', 'reasoning', '.']","['O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'IN', 'JJ', 'NNS', 'TO', 'VB', 'NN', 'NN', '.']",11
natural_language_inference,34,53,"In each round , DFGN generates and reasons on a dynamic graph , where irrelevant entities are masked out while only reasoning sources are preserved , via a mask prediction module .","['In', 'each', 'round', ',', 'DFGN', 'generates', 'and', 'reasons', 'on', 'a', 'dynamic', 'graph', ',', 'where', 'irrelevant', 'entities', 'are', 'masked', 'out', 'while', 'only', 'reasoning', 'sources', 'are', 'preserved', ',', 'via', 'a', 'mask', 'prediction', 'module', '.']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'NNP', 'VBZ', 'CC', 'NNS', 'IN', 'DT', 'JJ', 'NN', ',', 'WRB', 'JJ', 'NNS', 'VBP', 'VBN', 'RP', 'IN', 'RB', 'VBG', 'NNS', 'VBP', 'VBN', ',', 'IN', 'DT', 'NN', 'NN', 'NN', '.']",32
natural_language_inference,34,56,"To solve the second challenge , we propose a fusion process in DFGN to solve the unrestricted QA challenge .","['To', 'solve', 'the', 'second', 'challenge', ',', 'we', 'propose', 'a', 'fusion', 'process', 'in', 'DFGN', 'to', 'solve', 'the', 'unrestricted', 'QA', 'challenge', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'NNP', 'TO', 'VB', 'DT', 'JJ', 'NNP', 'NN', '.']",20
natural_language_inference,34,57,"We not only aggregate information from documents to the entity graph ( doc2 graph ) , but also propagate the information of the entity graph back to document representations ( graph2doc ) .","['We', 'not', 'only', 'aggregate', 'information', 'from', 'documents', 'to', 'the', 'entity', 'graph', '(', 'doc2', 'graph', ')', ',', 'but', 'also', 'propagate', 'the', 'information', 'of', 'the', 'entity', 'graph', 'back', 'to', 'document', 'representations', '(', 'graph2doc', ')', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'RB', 'JJ', 'NN', 'IN', 'NNS', 'TO', 'DT', 'NN', 'NN', '(', 'JJ', 'NN', ')', ',', 'CC', 'RB', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'TO', 'NN', 'NNS', '(', 'NN', ')', '.']",33
natural_language_inference,34,58,"The fusion process is iteratively performed at each hop through the document tokens and entities , and the final resulting answer is then obtained from document tokens .","['The', 'fusion', 'process', 'is', 'iteratively', 'performed', 'at', 'each', 'hop', 'through', 'the', 'document', 'tokens', 'and', 'entities', ',', 'and', 'the', 'final', 'resulting', 'answer', 'is', 'then', 'obtained', 'from', 'document', 'tokens', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NNS', 'CC', 'NNS', ',', 'CC', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'NN', 'NNS', '.']",28
natural_language_inference,34,59,"The fusion process of doc2 graph and graph2doc along with the dynamic entity graph jointly improve the interaction between the information of documents and the entity graph , leading to a less noisy entity graph and thus more accurate answers .","['The', 'fusion', 'process', 'of', 'doc2', 'graph', 'and', 'graph2doc', 'along', 'with', 'the', 'dynamic', 'entity', 'graph', 'jointly', 'improve', 'the', 'interaction', 'between', 'the', 'information', 'of', 'documents', 'and', 'the', 'entity', 'graph', ',', 'leading', 'to', 'a', 'less', 'noisy', 'entity', 'graph', 'and', 'thus', 'more', 'accurate', 'answers', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'IN', 'NN', 'NN', 'CC', 'NN', 'IN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'RB', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNS', 'CC', 'DT', 'NN', 'NN', ',', 'VBG', 'TO', 'DT', 'JJR', 'JJ', 'NN', 'NN', 'CC', 'RB', 'JJR', 'JJ', 'NNS', '.']",41
natural_language_inference,34,209,"In paragraph selection stage , we use the uncased version of BERT Tokenizer to tokenize all passages and questions .","['In', 'paragraph', 'selection', 'stage', ',', 'we', 'use', 'the', 'uncased', 'version', 'of', 'BERT', 'Tokenizer', 'to', 'tokenize', 'all', 'passages', 'and', 'questions', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'TO', 'VB', 'DT', 'NNS', 'CC', 'NNS', '.']",20
natural_language_inference,34,210,The encoding vectors of sentence pairs are generated from a pre-trained BERT model .,"['The', 'encoding', 'vectors', 'of', 'sentence', 'pairs', 'are', 'generated', 'from', 'a', 'pre-trained', 'BERT', 'model', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'VBG', 'NNS', 'IN', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NNP', 'NN', '.']",14
natural_language_inference,34,211,We set a relatively low threshold during selection to keep a high recall ( 97 % ) and a reasonable precision ( 69 % ) on supporting facts .,"['We', 'set', 'a', 'relatively', 'low', 'threshold', 'during', 'selection', 'to', 'keep', 'a', 'high', 'recall', '(', '97', '%', ')', 'and', 'a', 'reasonable', 'precision', '(', '69', '%', ')', 'on', 'supporting', 'facts', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'RB', 'JJ', 'NN', 'IN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', '(', 'CD', 'NN', ')', 'CC', 'DT', 'JJ', 'NN', '(', 'CD', 'NN', ')', 'IN', 'VBG', 'NNS', '.']",29
natural_language_inference,34,212,"In graph construction stage , we use a pretrained NER model from Stanford CoreNLP Toolkits 1 to extract named entities .","['In', 'graph', 'construction', 'stage', ',', 'we', 'use', 'a', 'pretrained', 'NER', 'model', 'from', 'Stanford', 'CoreNLP', 'Toolkits', '1', 'to', 'extract', 'named', 'entities', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'JJ', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNP', 'NN', 'IN', 'NNP', 'NNP', 'VBZ', 'CD', 'TO', 'VB', 'JJ', 'NNS', '.']",21
natural_language_inference,34,213,The maximum number of entities in a graph is set to be 40 .,"['The', 'maximum', 'number', 'of', 'entities', 'in', 'a', 'graph', 'is', 'set', 'to', 'be', '40', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'CD', '.']",14
natural_language_inference,34,214,Each entity node in the entity graphs has an average degree of 3.52 .,"['Each', 'entity', 'node', 'in', 'the', 'entity', 'graphs', 'has', 'an', 'average', 'degree', 'of', '3.52', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'RB', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'CD', '.']",14
natural_language_inference,34,215,"In the encoding stage , we also use a pre-trained BERT model as the encoder , thus d 1 is 768 .","['In', 'the', 'encoding', 'stage', ',', 'we', 'also', 'use', 'a', 'pre-trained', 'BERT', 'model', 'as', 'the', 'encoder', ',', 'thus', 'd', '1', 'is', '768', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NN', 'NN', ',', 'PRP', 'RB', 'VBP', 'DT', 'JJ', 'NNP', 'NN', 'IN', 'DT', 'NN', ',', 'RB', 'JJ', 'CD', 'VBZ', 'CD', '.']",22
natural_language_inference,34,216,All the hidden state dimensions d 2 are set to 300 .,"['All', 'the', 'hidden', 'state', 'dimensions', 'd', '2', 'are', 'set', 'to', '300', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['PDT', 'DT', 'JJ', 'NN', 'NNS', 'VBP', 'CD', 'VBP', 'VBN', 'TO', 'CD', '.']",12
natural_language_inference,34,217,We set the dropout rate for all hidden units of LSTM and dynamic graph attention to 0.3 and 0.5 respectively .,"['We', 'set', 'the', 'dropout', 'rate', 'for', 'all', 'hidden', 'units', 'of', 'LSTM', 'and', 'dynamic', 'graph', 'attention', 'to', '0.3', 'and', '0.5', 'respectively', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NNP', 'CC', 'JJ', 'NN', 'NN', 'TO', 'CD', 'CC', 'CD', 'RB', '.']",21
natural_language_inference,34,218,"For optimization , we use Adam Optimizer with an initial learning rate of 1 e ?4 .","['For', 'optimization', ',', 'we', 'use', 'Adam', 'Optimizer', 'with', 'an', 'initial', 'learning', 'rate', 'of', '1', 'e', '?4', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'NN', 'NN', '.']",17
natural_language_inference,34,222,We first present a comparison between baseline models and our DFGN 2 . shows the performance of different models in the private test set of Hotpot QA .,"['We', 'first', 'present', 'a', 'comparison', 'between', 'baseline', 'models', 'and', 'our', 'DFGN', '2', '.', 'shows', 'the', 'performance', 'of', 'different', 'models', 'in', 'the', 'private', 'test', 'set', 'of', 'Hotpot', 'QA', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBD', 'DT', 'NN', 'IN', 'NN', 'NNS', 'CC', 'PRP$', 'NNP', 'CD', '.', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'NNP', '.']",28
natural_language_inference,34,223,From the table we can see that our model achieves the second best result on the leaderboard now 3 ( on March 1st ) .,"['From', 'the', 'table', 'we', 'can', 'see', 'that', 'our', 'model', 'achieves', 'the', 'second', 'best', 'result', 'on', 'the', 'leaderboard', 'now', '3', '(', 'on', 'March', '1st', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O']","['IN', 'DT', 'NN', 'PRP', 'MD', 'VB', 'IN', 'PRP$', 'NN', 'VBZ', 'DT', 'JJ', 'RBS', 'NN', 'IN', 'DT', 'NN', 'RB', 'CD', '(', 'IN', 'NNP', 'CD', ')', '.']",25
natural_language_inference,34,224,"Besides , the answer performance and the joint performance of our model are competitive against state - of - the - art unpublished models .","['Besides', ',', 'the', 'answer', 'performance', 'and', 'the', 'joint', 'performance', 'of', 'our', 'model', 'are', 'competitive', 'against', 'state', '-', 'of', '-', 'the', '-', 'art', 'unpublished', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', ',', 'DT', 'NN', 'NN', 'CC', 'DT', 'JJ', 'NN', 'IN', 'PRP$', 'NN', 'VBP', 'JJ', 'IN', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'JJ', 'NNS', '.']",25
natural_language_inference,34,227,The results show that our model achieves a 1.5 % gain in the joint F1 - score with the entity graph built from a better entity recognizer .,"['The', 'results', 'show', 'that', 'our', 'model', 'achieves', 'a', '1.5', '%', 'gain', 'in', 'the', 'joint', 'F1', '-', 'score', 'with', 'the', 'entity', 'graph', 'built', 'from', 'a', 'better', 'entity', 'recognizer', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'IN', 'PRP$', 'NN', 'VBZ', 'DT', 'CD', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNP', ':', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBN', 'IN', 'DT', 'JJR', 'NN', 'NN', '.']",28
natural_language_inference,34,230,The ablation results of QA performances in the development set of Hotpot QA are shown in .,"['The', 'ablation', 'results', 'of', 'QA', 'performances', 'in', 'the', 'development', 'set', 'of', 'Hotpot', 'QA', 'are', 'shown', 'in', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NNS', 'IN', 'NNP', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'NNP', 'NNP', 'VBP', 'VBN', 'IN', '.']",17
natural_language_inference,34,231,From the table we can see that each of our model components can provide from 1 % to 2 % relative gain over the QA performance .,"['From', 'the', 'table', 'we', 'can', 'see', 'that', 'each', 'of', 'our', 'model', 'components', 'can', 'provide', 'from', '1', '%', 'to', '2', '%', 'relative', 'gain', 'over', 'the', 'QA', 'performance', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'PRP', 'MD', 'VB', 'IN', 'DT', 'IN', 'PRP$', 'NN', 'NNS', 'MD', 'VB', 'IN', 'CD', 'NN', 'TO', 'CD', 'NN', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",27
natural_language_inference,34,232,"Particularly , using a 1 - layer fusion block leads to an obvious performance loss , which implies the significance of performing multi-hop reasoning in Hotpot QA .","['Particularly', ',', 'using', 'a', '1', '-', 'layer', 'fusion', 'block', 'leads', 'to', 'an', 'obvious', 'performance', 'loss', ',', 'which', 'implies', 'the', 'significance', 'of', 'performing', 'multi-hop', 'reasoning', 'in', 'Hotpot', 'QA', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'VBG', 'DT', 'CD', ':', 'NN', 'NN', 'NN', 'VBZ', 'TO', 'DT', 'JJ', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'JJ', 'NN', 'IN', 'NNP', 'NNP', '.']",28
natural_language_inference,34,233,"Besides , the dataset abla-tion results show that our model is not very sensitive to the noisy paragraphs comparing with the baseline model which can achieve a more than 5 % performance gain in the "" gold paragraphs only "" and "" supporting facts only "" settings .","['Besides', ',', 'the', 'dataset', 'abla-tion', 'results', 'show', 'that', 'our', 'model', 'is', 'not', 'very', 'sensitive', 'to', 'the', 'noisy', 'paragraphs', 'comparing', 'with', 'the', 'baseline', 'model', 'which', 'can', 'achieve', 'a', 'more', 'than', '5', '%', 'performance', 'gain', 'in', 'the', '""', 'gold', 'paragraphs', 'only', '""', 'and', '""', 'supporting', 'facts', 'only', '""', 'settings', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['IN', ',', 'DT', 'NN', 'NN', 'NNS', 'VBP', 'IN', 'PRP$', 'NN', 'VBZ', 'RB', 'RB', 'JJ', 'TO', 'DT', 'JJ', 'NN', 'VBG', 'IN', 'DT', 'NN', 'NN', 'WDT', 'MD', 'VB', 'DT', 'JJR', 'IN', 'CD', 'NN', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NN', 'RB', 'NNP', 'CC', 'NNP', 'VBG', 'NNS', 'RB', 'JJ', 'NNS', '.']",48
natural_language_inference,37,2,Simple and Effective Multi - Paragraph Reading Comprehension,"['Simple', 'and', 'Effective', 'Multi', '-', 'Paragraph', 'Reading', 'Comprehension']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NN', 'CC', 'NNP', 'NNP', ':', 'NN', 'NNP', 'NNP']",8
natural_language_inference,37,4,We consider the problem of adapting neural paragraph - level question answering models to the case where entire documents are given as input .,"['We', 'consider', 'the', 'problem', 'of', 'adapting', 'neural', 'paragraph', '-', 'level', 'question', 'answering', 'models', 'to', 'the', 'case', 'where', 'entire', 'documents', 'are', 'given', 'as', 'input', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'JJ', 'NN', ':', 'NN', 'NN', 'VBG', 'NNS', 'TO', 'DT', 'NN', 'WRB', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'NN', '.']",24
natural_language_inference,37,14,The recent success of neural models at answering questions given a related paragraph suggests neural models have the potential to be a key part of a solution to this problem .,"['The', 'recent', 'success', 'of', 'neural', 'models', 'at', 'answering', 'questions', 'given', 'a', 'related', 'paragraph', 'suggests', 'neural', 'models', 'have', 'the', 'potential', 'to', 'be', 'a', 'key', 'part', 'of', 'a', 'solution', 'to', 'this', 'problem', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'VBG', 'NNS', 'VBN', 'DT', 'JJ', 'NN', 'VBZ', 'JJ', 'NNS', 'VBP', 'DT', 'JJ', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'TO', 'DT', 'NN', '.']",31
natural_language_inference,37,21,In this paper we start by proposing an improved pipelined method which achieves state - of - the - art results .,"['In', 'this', 'paper', 'we', 'start', 'by', 'proposing', 'an', 'improved', 'pipelined', 'method', 'which', 'achieves', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'IN', 'VBG', 'DT', 'VBN', 'VBD', 'NN', 'WDT', 'VBZ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', '.']",22
natural_language_inference,37,22,"Then we introduce a method for training models to produce accurate per-paragraph confidence scores , and we show how combining this method with multiple paragraph selection further increases performance .","['Then', 'we', 'introduce', 'a', 'method', 'for', 'training', 'models', 'to', 'produce', 'accurate', 'per-paragraph', 'confidence', 'scores', ',', 'and', 'we', 'show', 'how', 'combining', 'this', 'method', 'with', 'multiple', 'paragraph', 'selection', 'further', 'increases', 'performance', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'NNS', 'TO', 'VB', 'JJ', 'JJ', 'NN', 'NNS', ',', 'CC', 'PRP', 'VBP', 'WRB', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', 'JJ', 'NNS', 'NN', '.']",30
natural_language_inference,37,24,We propose a TF - IDF heuristic to select which paragraphs to train and test on .,"['We', 'propose', 'a', 'TF', '-', 'IDF', 'heuristic', 'to', 'select', 'which', 'paragraphs', 'to', 'train', 'and', 'test', 'on', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', ':', 'NNP', 'JJ', 'TO', 'VB', 'WDT', 'NN', 'TO', 'VB', 'CC', 'VB', 'IN', '.']",17
natural_language_inference,37,26,"To handle the noise this creates , we use a summed objective function that marginalizes the model 's output over all locations the answer text occurs .","['To', 'handle', 'the', 'noise', 'this', 'creates', ',', 'we', 'use', 'a', 'summed', 'objective', 'function', 'that', 'marginalizes', 'the', 'model', ""'s"", 'output', 'over', 'all', 'locations', 'the', 'answer', 'text', 'occurs', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O']","['TO', 'VB', 'DT', 'NN', 'DT', 'VBZ', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'POS', 'NN', 'IN', 'DT', 'NNS', 'DT', 'NN', 'NN', 'VBZ', '.']",27
natural_language_inference,37,31,"We resolve these problems by sampling paragraphs from the context documents , including paragraphs that do not contain an answer , to train on .","['We', 'resolve', 'these', 'problems', 'by', 'sampling', 'paragraphs', 'from', 'the', 'context', 'documents', ',', 'including', 'paragraphs', 'that', 'do', 'not', 'contain', 'an', 'answer', ',', 'to', 'train', 'on', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNS', 'IN', 'VBG', 'NN', 'IN', 'DT', 'NN', 'NNS', ',', 'VBG', 'NN', 'WDT', 'VBP', 'RB', 'VB', 'DT', 'NN', ',', 'TO', 'VB', 'IN', '.']",25
natural_language_inference,37,32,"We then use a shared - normalization objective where paragraphs are processed independently , but the probability of an answer candidate is marginalized over all paragraphs sampled from the same document .","['We', 'then', 'use', 'a', 'shared', '-', 'normalization', 'objective', 'where', 'paragraphs', 'are', 'processed', 'independently', ',', 'but', 'the', 'probability', 'of', 'an', 'answer', 'candidate', 'is', 'marginalized', 'over', 'all', 'paragraphs', 'sampled', 'from', 'the', 'same', 'document', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'VBN', ':', 'NN', 'NN', 'WRB', 'NN', 'VBP', 'VBN', 'RB', ',', 'CC', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NNS', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",32
natural_language_inference,37,164,"We train the model with the Adadelta optimizer ( Zeiler , 2012 ) with a batch size 60 for Triv - ia QA and 45 for SQuAD .","['We', 'train', 'the', 'model', 'with', 'the', 'Adadelta', 'optimizer', '(', 'Zeiler', ',', '2012', ')', 'with', 'a', 'batch', 'size', '60', 'for', 'Triv', '-', 'ia', 'QA', 'and', '45', 'for', 'SQuAD', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', '(', 'NNP', ',', 'CD', ')', 'IN', 'DT', 'NN', 'NN', 'CD', 'IN', 'NNP', ':', 'NN', 'NNP', 'CC', 'CD', 'IN', 'NNP', '.']",28
natural_language_inference,37,168,The Glo Ve 300 dimensional word vectors released by are used for word embeddings .,"['The', 'Glo', 'Ve', '300', 'dimensional', 'word', 'vectors', 'released', 'by', 'are', 'used', 'for', 'word', 'embeddings', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NNP', 'NNP', 'CD', 'JJ', 'NN', 'NNS', 'VBN', 'IN', 'VBP', 'VBN', 'IN', 'NN', 'NNS', '.']",15
natural_language_inference,37,169,"On SQuAD , we use a dimensionality of size 100 for the GRUs and of size 200 for the linear layers employed after each attention mechanism .","['On', 'SQuAD', ',', 'we', 'use', 'a', 'dimensionality', 'of', 'size', '100', 'for', 'the', 'GRUs', 'and', 'of', 'size', '200', 'for', 'the', 'linear', 'layers', 'employed', 'after', 'each', 'attention', 'mechanism', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'CD', 'IN', 'DT', 'NNP', 'CC', 'IN', 'NN', 'CD', 'IN', 'DT', 'JJ', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",27
natural_language_inference,37,170,"We find for TriviaQA , likely because there is more data , using a larger dimensionality of 140 for each GRU and 280 for the linear layers is beneficial .","['We', 'find', 'for', 'TriviaQA', ',', 'likely', 'because', 'there', 'is', 'more', 'data', ',', 'using', 'a', 'larger', 'dimensionality', 'of', '140', 'for', 'each', 'GRU', 'and', '280', 'for', 'the', 'linear', 'layers', 'is', 'beneficial', '.']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['PRP', 'VBP', 'IN', 'NNP', ',', 'RB', 'IN', 'EX', 'VBZ', 'JJR', 'NNS', ',', 'VBG', 'DT', 'JJR', 'NN', 'IN', 'CD', 'IN', 'DT', 'NNP', 'CC', 'CD', 'IN', 'DT', 'JJ', 'NNS', 'VBZ', 'JJ', '.']",30
natural_language_inference,37,171,"During training , we maintain an exponential moving average of the weights with a decay rate of 0.999 .","['During', 'training', ',', 'we', 'maintain', 'an', 'exponential', 'moving', 'average', 'of', 'the', 'weights', 'with', 'a', 'decay', 'rate', 'of', '0.999', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', '.']",19
natural_language_inference,37,174,Trivia QA Web,"['Trivia', 'QA', 'Web']","['B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP']",3
natural_language_inference,37,180,We find both TF - IDF ranking and the sum objective to be effective ; even without changing the model we achieve state - of - the - art results .,"['We', 'find', 'both', 'TF', '-', 'IDF', 'ranking', 'and', 'the', 'sum', 'objective', 'to', 'be', 'effective', ';', 'even', 'without', 'changing', 'the', 'model', 'we', 'achieve', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NNP', ':', 'NNP', 'NN', 'CC', 'DT', 'NN', 'NN', 'TO', 'VB', 'JJ', ':', 'RB', 'IN', 'VBG', 'DT', 'NN', 'PRP', 'VBP', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', '.']",31
natural_language_inference,37,181,Using our refined model increases the gain by another 4 points .,"['Using', 'our', 'refined', 'model', 'increases', 'the', 'gain', 'by', 'another', '4', 'points', '.']","['B-p', 'O', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['VBG', 'PRP$', 'JJ', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'CD', 'NNS', '.']",12
natural_language_inference,37,186,"The shared - norm , merge , and no-answer training methods improve the model 's ability to utilize more text , with the shared - norm method being significantly ahead of the others on the verified set and tied with the merge approach on the general set .","['The', 'shared', '-', 'norm', ',', 'merge', ',', 'and', 'no-answer', 'training', 'methods', 'improve', 'the', 'model', ""'s"", 'ability', 'to', 'utilize', 'more', 'text', ',', 'with', 'the', 'shared', '-', 'norm', 'method', 'being', 'significantly', 'ahead', 'of', 'the', 'others', 'on', 'the', 'verified', 'set', 'and', 'tied', 'with', 'the', 'merge', 'approach', 'on', 'the', 'general', 'set', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'VBN', ':', 'NN', ',', 'NN', ',', 'CC', 'JJ', 'NN', 'NNS', 'VBP', 'DT', 'NN', 'POS', 'NN', 'TO', 'VB', 'JJR', 'NN', ',', 'IN', 'DT', 'VBN', ':', 'NN', 'NN', 'VBG', 'RB', 'RB', 'IN', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'CC', 'VBN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",48
natural_language_inference,37,187,Trivia QA Unfiltered,"['Trivia', 'QA', 'Unfiltered']","['B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'VBD']",3
natural_language_inference,37,192,"Note the base model starts to lose performance as more paragraphs are used , showing that errors are being caused by the model being overly confident in incorrect extractions . :","['Note', 'the', 'base', 'model', 'starts', 'to', 'lose', 'performance', 'as', 'more', 'paragraphs', 'are', 'used', ',', 'showing', 'that', 'errors', 'are', 'being', 'caused', 'by', 'the', 'model', 'being', 'overly', 'confident', 'in', 'incorrect', 'extractions', '.', ':']","['O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VB', 'DT', 'NN', 'NN', 'VBZ', 'TO', 'VB', 'NN', 'IN', 'JJR', 'NN', 'VBP', 'VBN', ',', 'VBG', 'IN', 'NNS', 'VBP', 'VBG', 'VBN', 'IN', 'DT', 'NN', 'VBG', 'JJ', 'NN', 'IN', 'JJ', 'NNS', '.', ':']",31
natural_language_inference,37,196,SQuAD,['SQuAD'],['B-n'],['NN'],1
natural_language_inference,37,210,"While all our approaches had some benefit , the shared - norm model is the strongest , and is the only one to not lose performance as large numbers of paragraphs are used .","['While', 'all', 'our', 'approaches', 'had', 'some', 'benefit', ',', 'the', 'shared', '-', 'norm', 'model', 'is', 'the', 'strongest', ',', 'and', 'is', 'the', 'only', 'one', 'to', 'not', 'lose', 'performance', 'as', 'large', 'numbers', 'of', 'paragraphs', 'are', 'used', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'PRP$', 'NNS', 'VBD', 'DT', 'NN', ',', 'DT', 'VBN', ':', 'NN', 'NN', 'VBZ', 'DT', 'JJS', ',', 'CC', 'VBZ', 'DT', 'RB', 'NN', 'TO', 'RB', 'VB', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NN', 'VBP', 'VBN', '.']",34
natural_language_inference,37,216,"Our paragraph - level model is competitive on this task , and our variations to handle the multi-paragraph setting only cause a minor loss of performance .","['Our', 'paragraph', '-', 'level', 'model', 'is', 'competitive', 'on', 'this', 'task', ',', 'and', 'our', 'variations', 'to', 'handle', 'the', 'multi-paragraph', 'setting', 'only', 'cause', 'a', 'minor', 'loss', 'of', 'performance', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP$', 'NN', ':', 'NN', 'NN', 'VBZ', 'JJ', 'IN', 'DT', 'NN', ',', 'CC', 'PRP$', 'NNS', 'TO', 'VB', 'DT', 'NN', 'VBG', 'RB', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', '.']",27
natural_language_inference,37,219,The base model starts to drop in performance once more than two paragraphs are used .,"['The', 'base', 'model', 'starts', 'to', 'drop', 'in', 'performance', 'once', 'more', 'than', 'two', 'paragraphs', 'are', 'used', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'TO', 'VB', 'IN', 'NN', 'RB', 'JJR', 'IN', 'CD', 'NNS', 'VBP', 'VBN', '.']",16
natural_language_inference,37,220,"However , the shared - norm approach is able to reach a peak performance of 72.37 F1 and 64.08 EM given 15 paragraphs .","['However', ',', 'the', 'shared', '-', 'norm', 'approach', 'is', 'able', 'to', 'reach', 'a', 'peak', 'performance', 'of', '72.37', 'F1', 'and', '64.08', 'EM', 'given', '15', 'paragraphs', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'DT', 'VBN', ':', 'NN', 'NN', 'VBZ', 'JJ', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NNP', 'CC', 'CD', 'NNP', 'VBN', 'CD', 'NN', '.']",24
natural_language_inference,84,2,LEARNING TO COMPUTE WORD EMBEDDINGS ON THE FLY,"['LEARNING', 'TO', 'COMPUTE', 'WORD', 'EMBEDDINGS', 'ON', 'THE', 'FLY']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",8
natural_language_inference,84,12,"Learning representations for rare words is a well - known challenge of natural language understanding , since the standard end - to - end supervised learning methods require many occurrences of each word to generalize well .","['Learning', 'representations', 'for', 'rare', 'words', 'is', 'a', 'well', '-', 'known', 'challenge', 'of', 'natural', 'language', 'understanding', ',', 'since', 'the', 'standard', 'end', '-', 'to', '-', 'end', 'supervised', 'learning', 'methods', 'require', 'many', 'occurrences', 'of', 'each', 'word', 'to', 'generalize', 'well', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NNS', 'IN', 'JJ', 'NNS', 'VBZ', 'DT', 'RB', ':', 'VBN', 'NN', 'IN', 'JJ', 'NN', 'NN', ',', 'IN', 'DT', 'JJ', 'NN', ':', 'TO', ':', 'NN', 'VBD', 'JJ', 'NNS', 'VBP', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'TO', 'VB', 'RB', '.']",37
natural_language_inference,84,20,"In this paper we propose a new method for computing embeddings "" on the fly "" , which jointly addresses the large vocabulary problem and the paucity of data for learning representations in the long tail of the Zipfian distribution .","['In', 'this', 'paper', 'we', 'propose', 'a', 'new', 'method', 'for', 'computing', 'embeddings', '""', 'on', 'the', 'fly', '""', ',', 'which', 'jointly', 'addresses', 'the', 'large', 'vocabulary', 'problem', 'and', 'the', 'paucity', 'of', 'data', 'for', 'learning', 'representations', 'in', 'the', 'long', 'tail', 'of', 'the', 'Zipfian', 'distribution', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'NNS', 'NN', 'IN', 'DT', 'NN', 'NNP', ',', 'WDT', 'RB', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'CC', 'DT', 'NN', 'IN', 'NNS', 'IN', 'VBG', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",41
natural_language_inference,84,21,"This method , which we illustrate in , can be summarized as follows : instead of directly learning separate representations for all words in a potentially unbounded vocabulary , we train a network to predict the representations of words based on auxiliary data .","['This', 'method', ',', 'which', 'we', 'illustrate', 'in', ',', 'can', 'be', 'summarized', 'as', 'follows', ':', 'instead', 'of', 'directly', 'learning', 'separate', 'representations', 'for', 'all', 'words', 'in', 'a', 'potentially', 'unbounded', 'vocabulary', ',', 'we', 'train', 'a', 'network', 'to', 'predict', 'the', 'representations', 'of', 'words', 'based', 'on', 'auxiliary', 'data', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', ',', 'WDT', 'PRP', 'VBP', 'IN', ',', 'MD', 'VB', 'VBN', 'IN', 'VBZ', ':', 'RB', 'IN', 'RB', 'VBG', 'JJ', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'DT', 'RB', 'JJ', 'JJ', ',', 'PRP', 'VBP', 'DT', 'NN', 'TO', 'VB', 'DT', 'NNS', 'IN', 'NNS', 'VBN', 'IN', 'JJ', 'NNS', '.']",44
natural_language_inference,84,25,Several sources of auxiliary data can be used simultaneously as input to a neural network that will compute a combined representation .,"['Several', 'sources', 'of', 'auxiliary', 'data', 'can', 'be', 'used', 'simultaneously', 'as', 'input', 'to', 'a', 'neural', 'network', 'that', 'will', 'compute', 'a', 'combined', 'representation', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['JJ', 'NNS', 'IN', 'JJ', 'NNS', 'MD', 'VB', 'VBN', 'RB', 'IN', 'NN', 'TO', 'DT', 'JJ', 'NN', 'WDT', 'MD', 'VB', 'DT', 'JJ', 'NN', '.']",22
natural_language_inference,84,26,"These representations can then be used for out - of - vocabulary words , or combined with withinvocabulary word embeddings directly trained on the task of interest or pretrained from an external data source .","['These', 'representations', 'can', 'then', 'be', 'used', 'for', 'out', '-', 'of', '-', 'vocabulary', 'words', ',', 'or', 'combined', 'with', 'withinvocabulary', 'word', 'embeddings', 'directly', 'trained', 'on', 'the', 'task', 'of', 'interest', 'or', 'pretrained', 'from', 'an', 'external', 'data', 'source', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'MD', 'RB', 'VB', 'VBN', 'IN', 'IN', ':', 'IN', ':', 'JJ', 'NNS', ',', 'CC', 'VBN', 'IN', 'JJ', 'NN', 'NNS', 'RB', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NN', 'CC', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",35
natural_language_inference,84,27,"Importantly , the auxiliary data encoders are trained jointly with the objective , ensuring the preservation of semantic alignment with representations of within - vocabulary words .","['Importantly', ',', 'the', 'auxiliary', 'data', 'encoders', 'are', 'trained', 'jointly', 'with', 'the', 'objective', ',', 'ensuring', 'the', 'preservation', 'of', 'semantic', 'alignment', 'with', 'representations', 'of', 'within', '-', 'vocabulary', 'words', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'JJ', 'NN', 'NNS', 'VBP', 'VBN', 'RB', 'IN', 'DT', 'NN', ',', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'IN', ':', 'JJ', 'NNS', '.']",27
natural_language_inference,84,99,QUESTION ANSWERING,"['QUESTION', 'ANSWERING']","['B-n', 'I-n']","['NN', 'NN']",2
natural_language_inference,84,129,Looking at the results one can see that adding any external information results in a significant improvement over the baseline model ( B ) ( 3.7 - 10.5 points ) .,"['Looking', 'at', 'the', 'results', 'one', 'can', 'see', 'that', 'adding', 'any', 'external', 'information', 'results', 'in', 'a', 'significant', 'improvement', 'over', 'the', 'baseline', 'model', '(', 'B', ')', '(', '3.7', '-', '10.5', 'points', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['VBG', 'IN', 'DT', 'NNS', 'CD', 'MD', 'VB', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '(', 'NNP', ')', '(', 'CD', ':', 'CD', 'NNS', ')', '.']",31
natural_language_inference,84,130,"When the dictionary alone is used , mean pooling ( D3 ) performs similarly to LSTM ( D4 ) .","['When', 'the', 'dictionary', 'alone', 'is', 'used', ',', 'mean', 'pooling', '(', 'D3', ')', 'performs', 'similarly', 'to', 'LSTM', '(', 'D4', ')', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['WRB', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', ',', 'JJ', 'NN', '(', 'NNP', ')', 'VBZ', 'RB', 'TO', 'NNP', '(', 'NNP', ')', '.']",20
natural_language_inference,84,133,"We found that adding the spelling ( S ) helps more than adding a dictionary ( D ) ( 3 points difference ) , possibly due to relatively lower coverage of our dictionary .","['We', 'found', 'that', 'adding', 'the', 'spelling', '(', 'S', ')', 'helps', 'more', 'than', 'adding', 'a', 'dictionary', '(', 'D', ')', '(', '3', 'points', 'difference', ')', ',', 'possibly', 'due', 'to', 'relatively', 'lower', 'coverage', 'of', 'our', 'dictionary', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'IN', 'VBG', 'DT', 'NN', '(', 'NNP', ')', 'VBZ', 'JJR', 'IN', 'VBG', 'DT', 'JJ', '(', 'NNP', ')', '(', 'CD', 'NNS', 'NN', ')', ',', 'RB', 'JJ', 'TO', 'RB', 'JJR', 'NN', 'IN', 'PRP$', 'NN', '.']",34
natural_language_inference,84,134,"However , the model that uses both ( SD ) has a 1.1 point advantage over the model that uses just the spelling ( S ) , demonstrating that combining several forms of auxiliary data allows the model to exploit the complementary information they provide .","['However', ',', 'the', 'model', 'that', 'uses', 'both', '(', 'SD', ')', 'has', 'a', '1.1', 'point', 'advantage', 'over', 'the', 'model', 'that', 'uses', 'just', 'the', 'spelling', '(', 'S', ')', ',', 'demonstrating', 'that', 'combining', 'several', 'forms', 'of', 'auxiliary', 'data', 'allows', 'the', 'model', 'to', 'exploit', 'the', 'complementary', 'information', 'they', 'provide', '.']","['O', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'DT', 'NN', 'WDT', 'VBZ', 'DT', '(', 'NNP', ')', 'VBZ', 'DT', 'CD', 'NN', 'NN', 'IN', 'DT', 'NN', 'WDT', 'VBZ', 'RB', 'DT', 'NN', '(', 'NNP', ')', ',', 'VBG', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', 'VBZ', 'DT', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'PRP', 'VBP', '.']",46
natural_language_inference,84,135,"The model with GLoVe embeddings ( G ) is still ahead with a 1.1 point margin , but the gap has been shrunk .","['The', 'model', 'with', 'GLoVe', 'embeddings', '(', 'G', ')', 'is', 'still', 'ahead', 'with', 'a', '1.1', 'point', 'margin', ',', 'but', 'the', 'gap', 'has', 'been', 'shrunk', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'NNP', 'NNS', '(', 'NNP', ')', 'VBZ', 'RB', 'RB', 'IN', 'DT', 'CD', 'NN', 'NN', ',', 'CC', 'DT', 'NN', 'VBZ', 'VBN', 'VBN', '.']",24
natural_language_inference,84,155,ENTAILMENT PREDICTION,"['ENTAILMENT', 'PREDICTION']","['B-n', 'I-n']","['NNP', 'NNP']",2
natural_language_inference,84,172,"Compared to the SQuAD results , an important difference is that spelling was not as useful on SNLI and MultiNLI .","['Compared', 'to', 'the', 'SQuAD', 'results', ',', 'an', 'important', 'difference', 'is', 'that', 'spelling', 'was', 'not', 'as', 'useful', 'on', 'SNLI', 'and', 'MultiNLI', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['VBN', 'TO', 'DT', 'NNP', 'NNS', ',', 'DT', 'JJ', 'NN', 'VBZ', 'IN', 'VBG', 'VBD', 'RB', 'RB', 'JJ', 'IN', 'NNP', 'CC', 'NNP', '.']",21
natural_language_inference,84,173,"We also note that we tried using fixed random embeddings for OOV words as proposed by , and that this method did not bring a significant advantage over the baseline .","['We', 'also', 'note', 'that', 'we', 'tried', 'using', 'fixed', 'random', 'embeddings', 'for', 'OOV', 'words', 'as', 'proposed', 'by', ',', 'and', 'that', 'this', 'method', 'did', 'not', 'bring', 'a', 'significant', 'advantage', 'over', 'the', 'baseline', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['PRP', 'RB', 'VBP', 'IN', 'PRP', 'VBD', 'VBG', 'VBN', 'JJ', 'NNS', 'IN', 'NNP', 'NNS', 'IN', 'VBN', 'IN', ',', 'CC', 'IN', 'DT', 'NN', 'VBD', 'RB', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",31
natural_language_inference,84,178,"shows that , as expected , dictionary - enabled models significantly outperform baseline models for sentences containing rare words .","['shows', 'that', ',', 'as', 'expected', ',', 'dictionary', '-', 'enabled', 'models', 'significantly', 'outperform', 'baseline', 'models', 'for', 'sentences', 'containing', 'rare', 'words', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['NNS', 'WDT', ',', 'IN', 'VBN', ',', 'JJ', ':', 'JJ', 'NNS', 'RB', 'VBP', 'NN', 'NNS', 'IN', 'NNS', 'VBG', 'JJ', 'NNS', '.']",20
natural_language_inference,84,179,LANGUAGE MODELLING,"['LANGUAGE', 'MODELLING']","['B-n', 'I-n']","['NN', 'NN']",2
natural_language_inference,84,199,"Similarly to our other experiments , using external information to compute embeddings of unknown words helps in all cases .","['Similarly', 'to', 'our', 'other', 'experiments', ',', 'using', 'external', 'information', 'to', 'compute', 'embeddings', 'of', 'unknown', 'words', 'helps', 'in', 'all', 'cases', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', 'TO', 'PRP$', 'JJ', 'NNS', ',', 'VBG', 'JJ', 'NN', 'TO', 'VB', 'NNS', 'IN', 'JJ', 'NNS', 'VBZ', 'IN', 'DT', 'NNS', '.']",20
natural_language_inference,84,201,"We note that lemma + lowercase performs worse than any model with the dictionary , which suggests that dictionary definitions are used in a non-trivial way .","['We', 'note', 'that', 'lemma', '+', 'lowercase', 'performs', 'worse', 'than', 'any', 'model', 'with', 'the', 'dictionary', ',', 'which', 'suggests', 'that', 'dictionary', 'definitions', 'are', 'used', 'in', 'a', 'non-trivial', 'way', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'NN', 'NNP', 'NN', 'NNS', 'JJR', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'WDT', 'VBZ', 'IN', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",27
natural_language_inference,84,202,Adding spelling consistently helps more than adding dictionary definitions .,"['Adding', 'spelling', 'consistently', 'helps', 'more', 'than', 'adding', 'dictionary', 'definitions', '.']","['B-p', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-p', 'B-n', 'I-n', 'O']","['VBG', 'VBG', 'RB', 'VBZ', 'JJR', 'IN', 'VBG', 'JJ', 'NNS', '.']",10
natural_language_inference,84,204,"Using both dictionary and spelling is consistently slightly better than using just spelling , and the improvement is more pronounced in the restricted setting .","['Using', 'both', 'dictionary', 'and', 'spelling', 'is', 'consistently', 'slightly', 'better', 'than', 'using', 'just', 'spelling', ',', 'and', 'the', 'improvement', 'is', 'more', 'pronounced', 'in', 'the', 'restricted', 'setting', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'DT', 'JJ', 'CC', 'NN', 'VBZ', 'RB', 'RB', 'JJR', 'IN', 'VBG', 'RB', 'NN', ',', 'CC', 'DT', 'NN', 'VBZ', 'RBR', 'JJ', 'IN', 'DT', 'JJ', 'NN', '.']",25
natural_language_inference,84,205,Using Glo Ve embeddings results in the best perplexity .,"['Using', 'Glo', 'Ve', 'embeddings', 'results', 'in', 'the', 'best', 'perplexity', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['VBG', 'NNP', 'NNP', 'NNS', 'NNS', 'IN', 'DT', 'JJS', 'NN', '.']",10
natural_language_inference,49,2,Published as a conference paper at ICLR 2018 NATURAL LANGUAGE INFERENCE OVER INTERACTION SPACE,"['Published', 'as', 'a', 'conference', 'paper', 'at', 'ICLR', '2018', 'NATURAL', 'LANGUAGE', 'INFERENCE', 'OVER', 'INTERACTION', 'SPACE']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['VBN', 'IN', 'DT', 'NN', 'NN', 'IN', 'NNP', 'CD', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",14
natural_language_inference,49,10,"Natural Language Inference ( NLI also known as recognizing textual entiailment , or RTE ) task requires one to determine whether the logical relationship between two sentences is among entailment ( if the premise is true , then the hypothesis must be true ) , contradiction ( if the premise is true , then the hypothesis must be false ) and neutral ( neither entailment nor contradiction ) .","['Natural', 'Language', 'Inference', '(', 'NLI', 'also', 'known', 'as', 'recognizing', 'textual', 'entiailment', ',', 'or', 'RTE', ')', 'task', 'requires', 'one', 'to', 'determine', 'whether', 'the', 'logical', 'relationship', 'between', 'two', 'sentences', 'is', 'among', 'entailment', '(', 'if', 'the', 'premise', 'is', 'true', ',', 'then', 'the', 'hypothesis', 'must', 'be', 'true', ')', ',', 'contradiction', '(', 'if', 'the', 'premise', 'is', 'true', ',', 'then', 'the', 'hypothesis', 'must', 'be', 'false', ')', 'and', 'neutral', '(', 'neither', 'entailment', 'nor', 'contradiction', ')', '.']","['O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', '(', 'NNP', 'RB', 'VBN', 'IN', 'VBG', 'JJ', 'NN', ',', 'CC', 'NNP', ')', 'NN', 'VBZ', 'CD', 'TO', 'VB', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NNS', 'VBZ', 'IN', 'JJ', '(', 'IN', 'DT', 'NN', 'VBZ', 'JJ', ',', 'RB', 'DT', 'NN', 'MD', 'VB', 'JJ', ')', ',', 'NN', '(', 'IN', 'DT', 'NN', 'VBZ', 'JJ', ',', 'RB', 'DT', 'NN', 'MD', 'VB', 'JJ', ')', 'CC', 'JJ', '(', 'DT', 'NN', 'CC', 'NN', ')', '.']",69
natural_language_inference,49,23,"In this work , we push the multi-head attention to a extreme by building a word - by - word dimension - wise alignment tensor which we call interaction tensor .","['In', 'this', 'work', ',', 'we', 'push', 'the', 'multi-head', 'attention', 'to', 'a', 'extreme', 'by', 'building', 'a', 'word', '-', 'by', '-', 'word', 'dimension', '-', 'wise', 'alignment', 'tensor', 'which', 'we', 'call', 'interaction', 'tensor', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'TO', 'DT', 'JJ', 'IN', 'VBG', 'DT', 'NN', ':', 'IN', ':', 'NN', 'NN', ':', 'NN', 'NN', 'NN', 'WDT', 'PRP', 'VBP', 'NN', 'NN', '.']",31
natural_language_inference,49,24,The interaction tensor encodes the high - order alignment relationship between sentences pair .,"['The', 'interaction', 'tensor', 'encodes', 'the', 'high', '-', 'order', 'alignment', 'relationship', 'between', 'sentences', 'pair', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'DT', 'JJ', ':', 'NN', 'NN', 'NN', 'IN', 'NNS', 'NN', '.']",14
natural_language_inference,49,26,We dub the general framework as Interactive Inference Network ( IIN ) .,"['We', 'dub', 'the', 'general', 'framework', 'as', 'Interactive', 'Inference', 'Network', '(', 'IIN', ')', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', '.']",13
natural_language_inference,49,137,We implement our algorithm with Tensorflow framework .,"['We', 'implement', 'our', 'algorithm', 'with', 'Tensorflow', 'framework', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'IN', 'NNP', 'NN', '.']",8
natural_language_inference,49,138,"An Adadelta optimizer ( Zeiler , 2012 ) with ? as 0.95 and as 1e ? 8 is used to optimize all the trainable weights .","['An', 'Adadelta', 'optimizer', '(', 'Zeiler', ',', '2012', ')', 'with', '?', 'as', '0.95', 'and', 'as', '1e', '?', '8', 'is', 'used', 'to', 'optimize', 'all', 'the', 'trainable', 'weights', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'NN', '(', 'NNP', ',', 'CD', ')', 'IN', '.', 'IN', 'CD', 'CC', 'IN', 'CD', '.', 'CD', 'VBZ', 'VBN', 'TO', 'VB', 'PDT', 'DT', 'JJ', 'NNS', '.']",26
natural_language_inference,49,139,The initial learning rate is set to 0.5 and batch size to 70 .,"['The', 'initial', 'learning', 'rate', 'is', 'set', 'to', '0.5', 'and', 'batch', 'size', 'to', '70', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'CC', 'VB', 'NN', 'TO', 'CD', '.']",14
natural_language_inference,49,140,"When the model does not improve best in domain performance for 30,000 steps , an SGD optimizer with learning rate of 3e ? 4 is used to help model to find a better local optimum .","['When', 'the', 'model', 'does', 'not', 'improve', 'best', 'in', 'domain', 'performance', 'for', '30,000', 'steps', ',', 'an', 'SGD', 'optimizer', 'with', 'learning', 'rate', 'of', '3e', '?', '4', 'is', 'used', 'to', 'help', 'model', 'to', 'find', 'a', 'better', 'local', 'optimum', '.']","['O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['WRB', 'DT', 'NN', 'VBZ', 'RB', 'VB', 'JJS', 'IN', 'NN', 'NN', 'IN', 'CD', 'NNS', ',', 'DT', 'NNP', 'NN', 'IN', 'VBG', 'NN', 'IN', 'CD', '.', 'CD', 'VBZ', 'VBN', 'TO', 'VB', 'VB', 'TO', 'VB', 'DT', 'JJR', 'JJ', 'NN', '.']",36
natural_language_inference,49,141,Dropout layers are applied before all linear layers and after word - embedding layer .,"['Dropout', 'layers', 'are', 'applied', 'before', 'all', 'linear', 'layers', 'and', 'after', 'word', '-', 'embedding', 'layer', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NNS', 'CC', 'IN', 'NN', ':', 'VBG', 'NN', '.']",15
natural_language_inference,49,142,"We use an exponential decayed keep rate during training , where the initial keep rate is 1.0 and the decay rate is 0.977 for every 10,000 step .","['We', 'use', 'an', 'exponential', 'decayed', 'keep', 'rate', 'during', 'training', ',', 'where', 'the', 'initial', 'keep', 'rate', 'is', '1.0', 'and', 'the', 'decay', 'rate', 'is', '0.977', 'for', 'every', '10,000', 'step', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'JJ', 'VB', 'NN', 'IN', 'NN', ',', 'WRB', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'CD', 'CC', 'DT', 'NN', 'NN', 'VBZ', 'CD', 'IN', 'DT', 'CD', 'NN', '.']",28
natural_language_inference,49,143,We initialize our word embeddings with pre-trained 300D Glo Ve 840B vectors while the out - of - vocabulary word are randomly initialized with uniform distribution .,"['We', 'initialize', 'our', 'word', 'embeddings', 'with', 'pre-trained', '300D', 'Glo', 'Ve', '840B', 'vectors', 'while', 'the', 'out', '-', 'of', '-', 'vocabulary', 'word', 'are', 'randomly', 'initialized', 'with', 'uniform', 'distribution', '.']","['O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'VBZ', 'IN', 'JJ', 'CD', 'NNP', 'NNP', 'CD', 'NNS', 'IN', 'DT', 'RP', ':', 'IN', ':', 'JJ', 'NN', 'VBP', 'RB', 'VBN', 'IN', 'JJ', 'NN', '.']",27
natural_language_inference,49,144,The character embeddings are randomly initialized with 100D .,"['The', 'character', 'embeddings', 'are', 'randomly', 'initialized', 'with', '100D', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'CD', '.']",9
natural_language_inference,49,145,We crop or pad each token to have 16 characters .,"['We', 'crop', 'or', 'pad', 'each', 'token', 'to', 'have', '16', 'characters', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'NN', 'CC', 'VB', 'DT', 'NN', 'TO', 'VB', 'CD', 'NNS', '.']",11
natural_language_inference,49,146,The 1D convolution kernel size for character embedding is 5 .,"['The', '1D', 'convolution', 'kernel', 'size', 'for', 'character', 'embedding', 'is', '5', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'CD', 'NN', 'NNS', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'CD', '.']",11
natural_language_inference,49,147,"All weights are constraint by L2 regularization , and the L2 regularization at step t is calculated as follows :","['All', 'weights', 'are', 'constraint', 'by', 'L2', 'regularization', ',', 'and', 'the', 'L2', 'regularization', 'at', 'step', 't', 'is', 'calculated', 'as', 'follows', ':']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'NN', ',', 'CC', 'DT', 'NNP', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'VBZ', ':']",20
natural_language_inference,49,152,The first scale down ratio ?,"['The', 'first', 'scale', 'down', 'ratio', '?']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'RP', 'NN', '.']",6
natural_language_inference,49,153,in feature extraction layer is set to 0.3 and transitional scale down ratio ?,"['in', 'feature', 'extraction', 'layer', 'is', 'set', 'to', '0.3', 'and', 'transitional', 'scale', 'down', 'ratio', '?']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'CC', 'JJ', 'NN', 'RP', 'NN', '.']",14
natural_language_inference,49,154,is set to 0.5 .,"['is', 'set', 'to', '0.5', '.']","['O', 'B-p', 'I-p', 'B-n', 'O']","['VBZ', 'VBN', 'TO', 'CD', '.']",5
natural_language_inference,49,155,"The sequence length is set as a hard cutoff on all experiments : 48 for MultiNLI , 32 for SNLI and 24 for Quora Question Pair Dataset .","['The', 'sequence', 'length', 'is', 'set', 'as', 'a', 'hard', 'cutoff', 'on', 'all', 'experiments', ':', '48', 'for', 'MultiNLI', ',', '32', 'for', 'SNLI', 'and', '24', 'for', 'Quora', 'Question', 'Pair', 'Dataset', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNS', ':', 'CD', 'IN', 'NNP', ',', 'CD', 'IN', 'NNP', 'CC', 'CD', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', '.']",28
natural_language_inference,49,159,EXPERIMENT ON MULTINLI,"['EXPERIMENT', 'ON', 'MULTINLI']","['O', 'O', 'B-n']","['NNP', 'NNP', 'NNP']",3
natural_language_inference,49,164,"Our approach , without using any recurrent structure , achieves the new state - of - the - art performance of 80.0 % , exceeding current state - of - the - art performance by more than 5 % .","['Our', 'approach', ',', 'without', 'using', 'any', 'recurrent', 'structure', ',', 'achieves', 'the', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'of', '80.0', '%', ',', 'exceeding', 'current', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'by', 'more', 'than', '5', '%', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', ',', 'IN', 'VBG', 'DT', 'NN', 'NN', ',', 'VBZ', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'CD', 'NN', ',', 'VBG', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'JJR', 'IN', 'CD', 'NN', '.']",40
natural_language_inference,49,165,"Unlike the observation from , we find the out - of - domain test performance is consistently lower than in - domain test performance .","['Unlike', 'the', 'observation', 'from', ',', 'we', 'find', 'the', 'out', '-', 'of', '-', 'domain', 'test', 'performance', 'is', 'consistently', 'lower', 'than', 'in', '-', 'domain', 'test', 'performance', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'IN', ',', 'PRP', 'VBP', 'DT', 'RP', ':', 'IN', ':', 'NN', 'NN', 'NN', 'VBZ', 'RB', 'JJR', 'IN', 'IN', ':', 'NN', 'NN', 'NN', '.']",25
natural_language_inference,49,167,EXPERIMENT ON SNLI,"['EXPERIMENT', 'ON', 'SNLI']","['O', 'O', 'B-n']","['NNP', 'NNP', 'NNP']",3
natural_language_inference,49,175,"We show our model , DIIN , achieves state - of - the - art performance on the competitive leaderboard .","['We', 'show', 'our', 'model', ',', 'DIIN', ',', 'achieves', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'on', 'the', 'competitive', 'leaderboard', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'NN', ',', 'NNP', ',', 'VBZ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",21
natural_language_inference,49,179,EXPERIMENT ON QUORA QUESTION PAIR DATASET,"['EXPERIMENT', 'ON', 'QUORA', 'QUESTION', 'PAIR', 'DATASET']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",6
natural_language_inference,49,182,"BIMPM models different perspective of matching between sentence pair on both direction , then aggregates matching vector with LSTM .","['BIMPM', 'models', 'different', 'perspective', 'of', 'matching', 'between', 'sentence', 'pair', 'on', 'both', 'direction', ',', 'then', 'aggregates', 'matching', 'vector', 'with', 'LSTM', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', 'NNS', 'JJ', 'NN', 'IN', 'VBG', 'IN', 'NN', 'NN', 'IN', 'DT', 'NN', ',', 'RB', 'VBZ', 'VBG', 'NN', 'IN', 'NNP', '.']",20
natural_language_inference,49,183,DECATT word and DECATT char uses automatically collected in - domain paraphrase data to noisy pretrain n-gram word embedding and ngram subword embedding correspondingly on decomposable attention model proposed by .,"['DECATT', 'word', 'and', 'DECATT', 'char', 'uses', 'automatically', 'collected', 'in', '-', 'domain', 'paraphrase', 'data', 'to', 'noisy', 'pretrain', 'n-gram', 'word', 'embedding', 'and', 'ngram', 'subword', 'embedding', 'correspondingly', 'on', 'decomposable', 'attention', 'model', 'proposed', 'by', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['NNP', 'NN', 'CC', 'NNP', 'VBP', 'VBZ', 'RB', 'VBN', 'IN', ':', 'NN', 'NN', 'NNS', 'TO', 'VB', 'VB', 'JJ', 'NN', 'NN', 'CC', 'JJ', 'NN', 'VBG', 'RB', 'IN', 'JJ', 'NN', 'NN', 'VBN', 'IN', '.']",31
natural_language_inference,49,192,"After removing the exact match binary feature , we find the performance degrade to 78.2 on matched score on development set and 78.0 on mismatched score .","['After', 'removing', 'the', 'exact', 'match', 'binary', 'feature', ',', 'we', 'find', 'the', 'performance', 'degrade', 'to', '78.2', 'on', 'matched', 'score', 'on', 'development', 'set', 'and', '78.0', 'on', 'mismatched', 'score', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'VBG', 'DT', 'JJ', 'NN', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'CD', 'IN', 'VBN', 'NN', 'IN', 'NN', 'VBN', 'CC', 'CD', 'IN', 'VBN', 'NN', '.']",27
natural_language_inference,49,197,We obtain 73.2 for matched score and 73.6 on mismatched data .,"['We', 'obtain', '73.2', 'for', 'matched', 'score', 'and', '73.6', 'on', 'mismatched', 'data', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VB', 'CD', 'IN', 'VBN', 'NN', 'CC', 'CD', 'IN', 'VBN', 'NNS', '.']",12
natural_language_inference,49,200,"If we remove encoding layer completely , then we 'll obtain a 73.5 for matched score and 73.2 for mismatched score .","['If', 'we', 'remove', 'encoding', 'layer', 'completely', ',', 'then', 'we', ""'ll"", 'obtain', 'a', '73.5', 'for', 'matched', 'score', 'and', '73.2', 'for', 'mismatched', 'score', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'PRP', 'VBP', 'VBG', 'NN', 'RB', ',', 'RB', 'PRP', 'MD', 'VB', 'DT', 'CD', 'IN', 'VBN', 'NN', 'CC', 'CD', 'IN', 'VBN', 'NN', '.']",22
natural_language_inference,49,201,The result demonstrate the feature extraction layer have powerful capability to capture the semantic feature .,"['The', 'result', 'demonstrate', 'the', 'feature', 'extraction', 'layer', 'have', 'powerful', 'capability', 'to', 'capture', 'the', 'semantic', 'feature', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VB', 'DT', 'NN', 'NN', 'NN', 'VBP', 'JJ', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', '.']",16
natural_language_inference,49,202,"In experiment 5 , we remove both self - attention and fuse gate , thus retaining only highway network .","['In', 'experiment', '5', ',', 'we', 'remove', 'both', 'self', '-', 'attention', 'and', 'fuse', 'gate', ',', 'thus', 'retaining', 'only', 'highway', 'network', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'JJ', 'CD', ',', 'PRP', 'VBP', 'DT', 'PRP', ':', 'NN', 'CC', 'NN', 'NN', ',', 'RB', 'VBG', 'RB', 'NN', 'NN', '.']",20
natural_language_inference,49,203,The result improves to 77.7 and 77.3 respectively on matched and mismatched development set .,"['The', 'result', 'improves', 'to', '77.7', 'and', '77.3', 'respectively', 'on', 'matched', 'and', 'mismatched', 'development', 'set', '.']","['O', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'TO', 'CD', 'CC', 'CD', 'RB', 'IN', 'NNS', 'CC', 'VBD', 'NN', 'NN', '.']",15
natural_language_inference,49,204,"However , in experiment 6 , when we only remove fuse gate , to our surprise , the performance degrade to 73.5 for matched score and 73.8 for mismatched .","['However', ',', 'in', 'experiment', '6', ',', 'when', 'we', 'only', 'remove', 'fuse', 'gate', ',', 'to', 'our', 'surprise', ',', 'the', 'performance', 'degrade', 'to', '73.5', 'for', 'matched', 'score', 'and', '73.8', 'for', 'mismatched', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O']","['RB', ',', 'IN', 'JJ', 'CD', ',', 'WRB', 'PRP', 'RB', 'VB', 'NN', 'NN', ',', 'TO', 'PRP$', 'NN', ',', 'DT', 'NN', 'NN', 'TO', 'CD', 'IN', 'VBN', 'NN', 'CC', 'CD', 'IN', 'VBN', '.']",30
natural_language_inference,49,205,"On the other hand , if we use the addition of the representation after highway network and the representation after self - attention as skip connection as in experiment 7 , the performance increase to 77.3 and 76.3 .","['On', 'the', 'other', 'hand', ',', 'if', 'we', 'use', 'the', 'addition', 'of', 'the', 'representation', 'after', 'highway', 'network', 'and', 'the', 'representation', 'after', 'self', '-', 'attention', 'as', 'skip', 'connection', 'as', 'in', 'experiment', '7', ',', 'the', 'performance', 'increase', 'to', '77.3', 'and', '76.3', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'IN', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', 'CC', 'DT', 'NN', 'IN', 'PRP', ':', 'NN', 'IN', 'JJ', 'NN', 'IN', 'IN', 'NN', 'CD', ',', 'DT', 'NN', 'NN', 'TO', 'CD', 'CC', 'CD', '.']",39
natural_language_inference,45,2,Published as a conference paper at ICLR 2017 WORDS OR CHARACTERS ? FINE - GRAINED GATING FOR READING COMPREHENSION,"['Published', 'as', 'a', 'conference', 'paper', 'at', 'ICLR', '2017', 'WORDS', 'OR', 'CHARACTERS', '?', 'FINE', '-', 'GRAINED', 'GATING', 'FOR', 'READING', 'COMPREHENSION']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['VBN', 'IN', 'DT', 'NN', 'NN', 'IN', 'NNP', 'CD', 'NNP', 'NNP', 'NNP', '.', 'NNP', ':', 'NN', 'NN', 'IN', 'NNP', 'NNP']",19
natural_language_inference,45,28,"In this work , we present a fine - grained gating mechanism to combine the word - level and characterlevel representations .","['In', 'this', 'work', ',', 'we', 'present', 'a', 'fine', '-', 'grained', 'gating', 'mechanism', 'to', 'combine', 'the', 'word', '-', 'level', 'and', 'characterlevel', 'representations', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', ':', 'VBD', 'VBG', 'NN', 'TO', 'VB', 'DT', 'NN', ':', 'NN', 'CC', 'JJ', 'NNS', '.']",22
natural_language_inference,45,29,We compute a vector gate as a linear projection of the token features followed 1 Code is available at https://github.com/kimiyoung/fg-gating 1 ar Xiv: 1611.01724v2 [ cs.CL ] 11 Sep 2017,"['We', 'compute', 'a', 'vector', 'gate', 'as', 'a', 'linear', 'projection', 'of', 'the', 'token', 'features', 'followed', '1', 'Code', 'is', 'available', 'at', 'https://github.com/kimiyoung/fg-gating', '1', 'ar', 'Xiv:', '1611.01724v2', '[', 'cs.CL', ']', '11', 'Sep', '2017']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'VBD', 'CD', 'NNP', 'VBZ', 'JJ', 'IN', 'JJ', 'CD', 'NN', 'NNP', 'CD', 'NNP', 'NN', 'VBD', 'CD', 'NNP', 'CD']",30
natural_language_inference,45,31,We then multiplicatively apply the gate to the character - level and wordlevel representations .,"['We', 'then', 'multiplicatively', 'apply', 'the', 'gate', 'to', 'the', 'character', '-', 'level', 'and', 'wordlevel', 'representations', '.']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'RB', 'VB', 'DT', 'NN', 'TO', 'DT', 'NN', ':', 'NN', 'CC', 'JJ', 'NNS', '.']",15
natural_language_inference,45,32,Each dimension of the gate controls how much information is flowed from the word - level and character - level representations respectively .,"['Each', 'dimension', 'of', 'the', 'gate', 'controls', 'how', 'much', 'information', 'is', 'flowed', 'from', 'the', 'word', '-', 'level', 'and', 'character', '-', 'level', 'representations', 'respectively', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'WRB', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', ':', 'NN', 'CC', 'NN', ':', 'NN', 'NNS', 'RB', '.']",23
natural_language_inference,45,33,"We use named entity tags , part - ofspeech tags , document frequencies , and word - level representations as the features for token properties which determine the gate .","['We', 'use', 'named', 'entity', 'tags', ',', 'part', '-', 'ofspeech', 'tags', ',', 'document', 'frequencies', ',', 'and', 'word', '-', 'level', 'representations', 'as', 'the', 'features', 'for', 'token', 'properties', 'which', 'determine', 'the', 'gate', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'VBN', 'NN', 'NNS', ',', 'NN', ':', 'NN', 'NNS', ',', 'NN', 'NNS', ',', 'CC', 'NN', ':', 'NN', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'NN', 'NNS', 'WDT', 'VBP', 'DT', 'NN', '.']",30
natural_language_inference,45,34,"More generally , our fine - grained gating mechanism can be used to model multiple levels of structure in language , including words , characters , phrases , sentences and paragraphs .","['More', 'generally', ',', 'our', 'fine', '-', 'grained', 'gating', 'mechanism', 'can', 'be', 'used', 'to', 'model', 'multiple', 'levels', 'of', 'structure', 'in', 'language', ',', 'including', 'words', ',', 'characters', ',', 'phrases', ',', 'sentences', 'and', 'paragraphs', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O']","['RBR', 'RB', ',', 'PRP$', 'JJ', ':', 'VBD', 'VBG', 'NN', 'MD', 'VB', 'VBN', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'NN', 'IN', 'NN', ',', 'VBG', 'NNS', ',', 'NNS', ',', 'NNS', ',', 'NNS', 'CC', 'NN', '.']",32
natural_language_inference,100,4,"As an alternative to question answering methods based on feature engineering , deep learning approaches such as convolutional neural networks ( CNNs ) and Long Short - Term Memory Models ( LSTMs ) have recently been proposed for semantic matching of questions and answers .","['As', 'an', 'alternative', 'to', 'question', 'answering', 'methods', 'based', 'on', 'feature', 'engineering', ',', 'deep', 'learning', 'approaches', 'such', 'as', 'convolutional', 'neural', 'networks', '(', 'CNNs', ')', 'and', 'Long', 'Short', '-', 'Term', 'Memory', 'Models', '(', 'LSTMs', ')', 'have', 'recently', 'been', 'proposed', 'for', 'semantic', 'matching', 'of', 'questions', 'and', 'answers', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'TO', 'NN', 'VBG', 'NNS', 'VBN', 'IN', 'NN', 'NN', ',', 'JJ', 'VBG', 'NNS', 'JJ', 'IN', 'JJ', 'JJ', 'NNS', '(', 'NNP', ')', 'CC', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'VBP', 'RB', 'VBN', 'VBN', 'IN', 'JJ', 'NN', 'IN', 'NNS', 'CC', 'NNS', '.']",45
natural_language_inference,100,12,"Question answering ( QA ) , which returns exact answers as either short facts or long passages to natural language questions issued by users , is a challenging task and plays a central role in the next generation of advanced web search .","['Question', 'answering', '(', 'QA', ')', ',', 'which', 'returns', 'exact', 'answers', 'as', 'either', 'short', 'facts', 'or', 'long', 'passages', 'to', 'natural', 'language', 'questions', 'issued', 'by', 'users', ',', 'is', 'a', 'challenging', 'task', 'and', 'plays', 'a', 'central', 'role', 'in', 'the', 'next', 'generation', 'of', 'advanced', 'web', 'search', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'VBG', '(', 'NNP', ')', ',', 'WDT', 'VBZ', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NNS', 'CC', 'JJ', 'NNS', 'TO', 'JJ', 'NN', 'NNS', 'VBN', 'IN', 'NNS', ',', 'VBZ', 'DT', 'JJ', 'NN', 'CC', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'JJ', 'NN', '.']",43
natural_language_inference,100,13,"Many of current QA systems use a learning to rank approach that encodes question / answer pairs with complex linguistic features including lexical , syntactic and semantic features .","['Many', 'of', 'current', 'QA', 'systems', 'use', 'a', 'learning', 'to', 'rank', 'approach', 'that', 'encodes', 'question', '/', 'answer', 'pairs', 'with', 'complex', 'linguistic', 'features', 'including', 'lexical', ',', 'syntactic', 'and', 'semantic', 'features', '.']","['O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'IN', 'JJ', 'NNP', 'NNS', 'VBP', 'DT', 'NN', 'TO', 'VB', 'NN', 'WDT', 'VBZ', 'NN', 'NNP', 'NN', 'NNS', 'IN', 'JJ', 'JJ', 'NNS', 'VBG', 'JJ', ',', 'JJ', 'CC', 'JJ', 'NNS', '.']",29
natural_language_inference,100,48,"To handle these issues in the existing deep learning architectures for ranking answers , we propose an attention based neural matching model ( a NMM ) .","['To', 'handle', 'these', 'issues', 'in', 'the', 'existing', 'deep', 'learning', 'architectures', 'for', 'ranking', 'answers', ',', 'we', 'propose', 'an', 'attention', 'based', 'neural', 'matching', 'model', '(', 'a', 'NMM', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NNS', 'IN', 'DT', 'VBG', 'JJ', 'NN', 'NNS', 'IN', 'VBG', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBN', 'JJ', 'VBG', 'NN', '(', 'DT', 'NNP', ')', '.']",27
natural_language_inference,100,51,Deep neural network with value - shared weights :,"['Deep', 'neural', 'network', 'with', 'value', '-', 'shared', 'weights', ':']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'JJ', 'NN', 'IN', 'NN', ':', 'VBN', 'NNS', ':']",9
natural_language_inference,100,52,"We introduce a novel value - shared weighting scheme in deep neural networks as a counterpart of the position - shared weighting scheme in CNNs , based on the idea that semantic matching between a question and answer is mainly about the ( semantic similarity ) value regularities rather than spatial regularities .","['We', 'introduce', 'a', 'novel', 'value', '-', 'shared', 'weighting', 'scheme', 'in', 'deep', 'neural', 'networks', 'as', 'a', 'counterpart', 'of', 'the', 'position', '-', 'shared', 'weighting', 'scheme', 'in', 'CNNs', ',', 'based', 'on', 'the', 'idea', 'that', 'semantic', 'matching', 'between', 'a', 'question', 'and', 'answer', 'is', 'mainly', 'about', 'the', '(', 'semantic', 'similarity', ')', 'value', 'regularities', 'rather', 'than', 'spatial', 'regularities', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', ':', 'VBN', 'VBG', 'NN', 'IN', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', ':', 'VBN', 'VBG', 'NN', 'IN', 'NNP', ',', 'VBN', 'IN', 'DT', 'NN', 'WDT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'CC', 'NN', 'VBZ', 'RB', 'IN', 'DT', '(', 'JJ', 'NN', ')', 'NN', 'VBZ', 'RB', 'IN', 'JJ', 'NNS', '.']",53
natural_language_inference,100,53,Incorporate attention scheme over question terms :,"['Incorporate', 'attention', 'scheme', 'over', 'question', 'terms', ':']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NN', 'NN', 'IN', 'NN', 'NNS', ':']",7
natural_language_inference,100,54,"We incorporate the attention scheme over the question terms using a gating function , so that we can explicitly discriminate the question term importance .","['We', 'incorporate', 'the', 'attention', 'scheme', 'over', 'the', 'question', 'terms', 'using', 'a', 'gating', 'function', ',', 'so', 'that', 'we', 'can', 'explicitly', 'discriminate', 'the', 'question', 'term', 'importance', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NNS', 'VBG', 'DT', 'NN', 'NN', ',', 'IN', 'IN', 'PRP', 'MD', 'RB', 'VB', 'DT', 'NN', 'NN', 'NN', '.']",25
natural_language_inference,100,248,"For the setting of hyper - parameters , we set the number of bins as 600 , word embedding dimension as 700 for a NNM - 1 , the number of bins as 200 , word embedding dimension as 700 for a NNM - 2 after we tune hyper - parameters on the provided DEV set of TREC QA data .","['For', 'the', 'setting', 'of', 'hyper', '-', 'parameters', ',', 'we', 'set', 'the', 'number', 'of', 'bins', 'as', '600', ',', 'word', 'embedding', 'dimension', 'as', '700', 'for', 'a', 'NNM', '-', '1', ',', 'the', 'number', 'of', 'bins', 'as', '200', ',', 'word', 'embedding', 'dimension', 'as', '700', 'for', 'a', 'NNM', '-', '2', 'after', 'we', 'tune', 'hyper', '-', 'parameters', 'on', 'the', 'provided', 'DEV', 'set', 'of', 'TREC', 'QA', 'data', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'IN', 'JJ', ':', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NNS', 'IN', 'CD', ',', 'NN', 'VBG', 'NN', 'IN', 'CD', 'IN', 'DT', 'NNP', ':', 'CD', ',', 'DT', 'NN', 'IN', 'NNS', 'IN', 'CD', ',', 'NN', 'VBG', 'NN', 'IN', 'CD', 'IN', 'DT', 'NNP', ':', 'CD', 'IN', 'PRP', 'VBP', 'JJ', ':', 'NNS', 'IN', 'DT', 'JJ', 'NNP', 'NN', 'IN', 'NNP', 'NNP', 'NNS', '.']",61
natural_language_inference,100,341,We can see a NMM trained with TRAIN - ALL set beats all the previous state - of - the art systems including both methods using feature engineering and deep learning models .,"['We', 'can', 'see', 'a', 'NMM', 'trained', 'with', 'TRAIN', '-', 'ALL', 'set', 'beats', 'all', 'the', 'previous', 'state', '-', 'of', '-', 'the', 'art', 'systems', 'including', 'both', 'methods', 'using', 'feature', 'engineering', 'and', 'deep', 'learning', 'models', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'DT', 'NNP', 'VBN', 'IN', 'NNP', ':', 'DT', 'VBN', 'NNS', 'PDT', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', 'JJ', 'NNS', 'VBG', 'DT', 'NNS', 'VBG', 'NN', 'NN', 'CC', 'JJ', 'NN', 'NNS', '.']",33
natural_language_inference,100,343,"Furthermore , even without combining additional features , a NMM still performs well for answer ranking , showing significant improvements over previous deep learning model with no additional features and linguistic feature engineering methods .","['Furthermore', ',', 'even', 'without', 'combining', 'additional', 'features', ',', 'a', 'NMM', 'still', 'performs', 'well', 'for', 'answer', 'ranking', ',', 'showing', 'significant', 'improvements', 'over', 'previous', 'deep', 'learning', 'model', 'with', 'no', 'additional', 'features', 'and', 'linguistic', 'feature', 'engineering', 'methods', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'RB', 'IN', 'VBG', 'JJ', 'NNS', ',', 'DT', 'NNP', 'RB', 'VBZ', 'RB', 'IN', 'NN', 'NN', ',', 'VBG', 'JJ', 'NNS', 'IN', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'CC', 'JJ', 'NN', 'NN', 'NNS', '.']",35
natural_language_inference,44,2,Efficient and Robust Question Answering from Minimal Context over Documents,"['Efficient', 'and', 'Robust', 'Question', 'Answering', 'from', 'Minimal', 'Context', 'over', 'Documents']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NN', 'CC', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'NNS']",10
natural_language_inference,44,4,Neural models for question answering ( QA ) over documents have achieved significant performance improvements .,"['Neural', 'models', 'for', 'question', 'answering', '(', 'QA', ')', 'over', 'documents', 'have', 'achieved', 'significant', 'performance', 'improvements', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNS', 'IN', 'NN', 'NN', '(', 'NNP', ')', 'IN', 'NNS', 'VBP', 'VBN', 'JJ', 'NN', 'NNS', '.']",16
natural_language_inference,44,8,"Inspired by this observation , we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model .","['Inspired', 'by', 'this', 'observation', ',', 'we', 'propose', 'a', 'simple', 'sentence', 'selector', 'to', 'select', 'the', 'minimal', 'set', 'of', 'sentences', 'to', 'feed', 'into', 'the', 'QA', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O']","['VBN', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'TO', 'VB', 'IN', 'DT', 'NNP', 'NN', '.']",25
natural_language_inference,44,20,"In this paper , we aim to develop a QA system that is scalable to large documents as well as robust to adversarial inputs .","['In', 'this', 'paper', ',', 'we', 'aim', 'to', 'develop', 'a', 'QA', 'system', 'that', 'is', 'scalable', 'to', 'large', 'documents', 'as', 'well', 'as', 'robust', 'to', 'adversarial', 'inputs', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'TO', 'VB', 'DT', 'NNP', 'NN', 'WDT', 'VBZ', 'JJ', 'TO', 'JJ', 'NNS', 'RB', 'RB', 'IN', 'NN', 'TO', 'JJ', 'NNS', '.']",25
natural_language_inference,44,21,"First , we study the context required to answer the question by sampling examples in the dataset and carefully analyzing them .","['First', ',', 'we', 'study', 'the', 'context', 'required', 'to', 'answer', 'the', 'question', 'by', 'sampling', 'examples', 'in', 'the', 'dataset', 'and', 'carefully', 'analyzing', 'them', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'VBG', 'NNS', 'IN', 'DT', 'NN', 'CC', 'RB', 'VBG', 'PRP', '.']",22
natural_language_inference,44,24,"Second , inspired by this observation , we propose a sentence selector to select the minimal set of sentences to give to the QA model in order to answer the question .","['Second', ',', 'inspired', 'by', 'this', 'observation', ',', 'we', 'propose', 'a', 'sentence', 'selector', 'to', 'select', 'the', 'minimal', 'set', 'of', 'sentences', 'to', 'give', 'to', 'the', 'QA', 'model', 'in', 'order', 'to', 'answer', 'the', 'question', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['JJ', ',', 'VBN', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'TO', 'VB', 'TO', 'DT', 'NNP', 'NN', 'IN', 'NN', 'TO', 'VB', 'DT', 'NN', '.']",32
natural_language_inference,44,25,"Since the minimum number of sentences depends on the question , our sentence selector chooses a different number of sentences for each question , in contrast with previous models that select a fixed number of sentences .","['Since', 'the', 'minimum', 'number', 'of', 'sentences', 'depends', 'on', 'the', 'question', ',', 'our', 'sentence', 'selector', 'chooses', 'a', 'different', 'number', 'of', 'sentences', 'for', 'each', 'question', ',', 'in', 'contrast', 'with', 'previous', 'models', 'that', 'select', 'a', 'fixed', 'number', 'of', 'sentences', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'NNS', 'IN', 'DT', 'NN', ',', 'PRP$', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', ',', 'IN', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NNS', '.']",37
natural_language_inference,44,26,"Our sentence selector leverages three simple techniques - weight transfer , data modification and score normalization , which we show to be highly effective on the task of sentence selection .","['Our', 'sentence', 'selector', 'leverages', 'three', 'simple', 'techniques', '-', 'weight', 'transfer', ',', 'data', 'modification', 'and', 'score', 'normalization', ',', 'which', 'we', 'show', 'to', 'be', 'highly', 'effective', 'on', 'the', 'task', 'of', 'sentence', 'selection', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'NN', 'NNS', 'CD', 'NN', 'NNS', ':', 'NN', 'NN', ',', 'NNS', 'NN', 'CC', 'NN', 'NN', ',', 'WDT', 'PRP', 'VBP', 'TO', 'VB', 'RB', 'JJ', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', '.']",31
natural_language_inference,44,110,"Results shows results in the task of sentence selection on SQuAD and New s QA . First , our selector outperforms TF - IDF method and the previous state - of - the - art by large margin ( up to 2.9 % MAP ) .","['Results', 'shows', 'results', 'in', 'the', 'task', 'of', 'sentence', 'selection', 'on', 'SQuAD', 'and', 'New', 's', 'QA', '.', 'First', ',', 'our', 'selector', 'outperforms', 'TF', '-', 'IDF', 'method', 'and', 'the', 'previous', 'state', '-', 'of', '-', 'the', '-', 'art', 'by', 'large', 'margin', '(', 'up', 'to', '2.9', '%', 'MAP', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O']","['NNS', 'VBZ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'NN', 'NNP', '.', 'NNP', ',', 'PRP$', 'NN', 'VBZ', 'NNP', ':', 'NNP', 'NN', 'CC', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'JJ', 'NN', '(', 'IN', 'TO', 'CD', 'NN', 'NNP', ')', '.']",46
natural_language_inference,44,111,"Second , our three training techniques - weight transfer , data modification and score normalization - improve performance by up to 5.6 % MAP .","['Second', ',', 'our', 'three', 'training', 'techniques', '-', 'weight', 'transfer', ',', 'data', 'modification', 'and', 'score', 'normalization', '-', 'improve', 'performance', 'by', 'up', 'to', '5.6', '%', 'MAP', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', ',', 'PRP$', 'CD', 'VBG', 'NNS', ':', 'NN', 'NN', ',', 'NNS', 'NN', 'CC', 'NN', 'NN', ':', 'VB', 'NN', 'IN', 'IN', 'TO', 'CD', 'NN', 'NNP', '.']",25
natural_language_inference,44,112,"Finally , our Dyn method achieves higher accuracy with less sentences than the Top k method .","['Finally', ',', 'our', 'Dyn', 'method', 'achieves', 'higher', 'accuracy', 'with', 'less', 'sentences', 'than', 'the', 'Top', 'k', 'method', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'NNP', 'NN', 'VBZ', 'JJR', 'NN', 'IN', 'JJR', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'NN', '.']",17
natural_language_inference,44,116,"On News QA , Top 4 achieves 92.5 accuracy , whereas Dyn achieves 94.6 accuracy with 3.9 sentences per example .","['On', 'News', 'QA', ',', 'Top', '4', 'achieves', '92.5', 'accuracy', ',', 'whereas', 'Dyn', 'achieves', '94.6', 'accuracy', 'with', '3.9', 'sentences', 'per', 'example', '.']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', 'NNP', ',', 'NNP', 'CD', 'VBZ', 'CD', 'NN', ',', 'WP', 'NNP', 'VBZ', 'CD', 'NN', 'IN', 'CD', 'NNS', 'IN', 'NN', '.']",21
natural_language_inference,44,121,"On SQuAD , S - Reader achieves 6.7 training and 3.6 inference speedup on SQuAD , and 15.0 training and 6.9 inference speedup on News QA .","['On', 'SQuAD', ',', 'S', '-', 'Reader', 'achieves', '6.7', 'training', 'and', '3.6', 'inference', 'speedup', 'on', 'SQuAD', ',', 'and', '15.0', 'training', 'and', '6.9', 'inference', 'speedup', 'on', 'News', 'QA', '.']","['O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'O']","['IN', 'NNP', ',', 'NNP', ':', 'NN', 'VBZ', 'CD', 'NN', 'CC', 'CD', 'NN', 'NN', 'IN', 'NNP', ',', 'CC', 'CD', 'NN', 'CC', 'CD', 'NN', 'NN', 'IN', 'NNP', 'NNP', '.']",27
natural_language_inference,44,153,Trivia QA and SQuAD - Open,"['Trivia', 'QA', 'and', 'SQuAD', '-', 'Open']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', 'CC', 'NNP', ':', 'JJ']",6
natural_language_inference,44,171,We compare with the results from the sentences selected by TF - IDF method and our selector ( Dyn ) .,"['We', 'compare', 'with', 'the', 'results', 'from', 'the', 'sentences', 'selected', 'by', 'TF', '-', 'IDF', 'method', 'and', 'our', 'selector', '(', 'Dyn', ')', '.']","['O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNS', 'VBN', 'IN', 'NNP', ':', 'NNP', 'NN', 'CC', 'PRP$', 'NN', '(', 'NNP', ')', '.']",21
natural_language_inference,44,172,We also compare with published Rank1 - 3 models .,"['We', 'also', 'compare', 'with', 'published', 'Rank1', '-', '3', 'models', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'IN', 'VBN', 'NNP', ':', 'CD', 'NNS', '.']",10
natural_language_inference,44,173,"Results shows results on Trivia QA ( Wikipedia ) and SQuAD - Open. First , MINI - MAL obtains higher F1 and EM over FULL , with the inference speedup of up to 13.8 .","['Results', 'shows', 'results', 'on', 'Trivia', 'QA', '(', 'Wikipedia', ')', 'and', 'SQuAD', '-', 'Open.', 'First', ',', 'MINI', '-', 'MAL', 'obtains', 'higher', 'F1', 'and', 'EM', 'over', 'FULL', ',', 'with', 'the', 'inference', 'speedup', 'of', 'up', 'to', '13.8', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNS', 'VBZ', 'NNS', 'IN', 'NNP', 'NNP', '(', 'NNP', ')', 'CC', 'NNP', ':', 'NNP', 'NNP', ',', 'NNP', ':', 'NN', 'VBZ', 'JJR', 'NNP', 'CC', 'NNP', 'IN', 'NNP', ',', 'IN', 'DT', 'NN', 'NN', 'IN', 'IN', 'TO', 'CD', '.']",35
natural_language_inference,44,174,"Second , the model with our sentence selector with Dyn achieves higher F1 and EM over the model with TF - IDF selector .","['Second', ',', 'the', 'model', 'with', 'our', 'sentence', 'selector', 'with', 'Dyn', 'achieves', 'higher', 'F1', 'and', 'EM', 'over', 'the', 'model', 'with', 'TF', '-', 'IDF', 'selector', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', ',', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'NN', 'IN', 'NNP', 'VBZ', 'JJR', 'NNP', 'CC', 'NNP', 'IN', 'DT', 'NN', 'IN', 'NNP', ':', 'NNP', 'NN', '.']",24
natural_language_inference,44,176,"Third , we outperforms the published state - of - the - art on both dataset .","['Third', ',', 'we', 'outperforms', 'the', 'published', 'state', '-', 'of', '-', 'the', '-', 'art', 'on', 'both', 'dataset', '.']","['O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ',', 'PRP', 'VBP', 'DT', 'VBN', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'DT', 'NN', '.']",17
natural_language_inference,44,177,SQuAD - Adversarial,"['SQuAD', '-', 'Adversarial']","['B-n', 'I-n', 'I-n']","['NNP', ':', 'NN']",3
natural_language_inference,44,181,"Results shows that MINIMAL outperforms FULL , achieving the new state - of - the - art by large margin ( + 11.1 and + 11.5 F1 on AddSent and Add OneSent , respectively ) .","['Results', 'shows', 'that', 'MINIMAL', 'outperforms', 'FULL', ',', 'achieving', 'the', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'by', 'large', 'margin', '(', '+', '11.1', 'and', '+', '11.5', 'F1', 'on', 'AddSent', 'and', 'Add', 'OneSent', ',', 'respectively', ')', '.']","['O', 'B-p', 'O', 'B-n', 'B-n', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NNS', 'VBZ', 'IN', 'NNP', 'VBZ', 'NNP', ',', 'VBG', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'JJ', 'NN', '(', 'JJ', 'CD', 'CC', '$', 'CD', 'NNP', 'IN', 'NNP', 'CC', 'NNP', 'NNP', ',', 'RB', ')', '.']",36
natural_language_inference,58,2,ReasoNet : Learning to Stop Reading in Machine Comprehension,"['ReasoNet', ':', 'Learning', 'to', 'Stop', 'Reading', 'in', 'Machine', 'Comprehension']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NN', ':', 'NN', 'TO', 'VB', 'NNP', 'IN', 'NNP', 'NNP']",9
natural_language_inference,58,33,"With this motivation , we propose a novel neural network architecture called Reasoning Network ( ReasoNet ) .","['With', 'this', 'motivation', ',', 'we', 'propose', 'a', 'novel', 'neural', 'network', 'architecture', 'called', 'Reasoning', 'Network', '(', 'ReasoNet', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'VBN', 'NNP', 'NNP', '(', 'NNP', ')', '.']",18
natural_language_inference,58,35,"With a question in mind , ReasoNets read a document repeatedly , each time focusing on di erent parts of the document until a satisfying answer is found or formed .","['With', 'a', 'question', 'in', 'mind', ',', 'ReasoNets', 'read', 'a', 'document', 'repeatedly', ',', 'each', 'time', 'focusing', 'on', 'di', 'erent', 'parts', 'of', 'the', 'document', 'until', 'a', 'satisfying', 'answer', 'is', 'found', 'or', 'formed', '.']","['B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'IN', 'NN', ',', 'NNP', 'VBD', 'DT', 'NN', 'RB', ',', 'DT', 'NN', 'VBG', 'IN', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'VBG', 'NN', 'VBZ', 'VBN', 'CC', 'VBN', '.']",31
natural_language_inference,58,37,"Moreover , unlike previous approaches using xed number of hops or iterations , ReasoNets introduce a termination state in the inference .","['Moreover', ',', 'unlike', 'previous', 'approaches', 'using', 'xed', 'number', 'of', 'hops', 'or', 'iterations', ',', 'ReasoNets', 'introduce', 'a', 'termination', 'state', 'in', 'the', 'inference', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['RB', ',', 'IN', 'JJ', 'NNS', 'VBG', 'JJ', 'NN', 'IN', 'NNS', 'CC', 'NNS', ',', 'NNPS', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', '.']",22
natural_language_inference,58,42,"Motivated by , we tackle this challenge by proposing a reinforcement learning approach , which utilizes an instance - dependent reward baseline , to successfully train ReasoNets .","['Motivated', 'by', ',', 'we', 'tackle', 'this', 'challenge', 'by', 'proposing', 'a', 'reinforcement', 'learning', 'approach', ',', 'which', 'utilizes', 'an', 'instance', '-', 'dependent', 'reward', 'baseline', ',', 'to', 'successfully', 'train', 'ReasoNets', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['VBN', 'IN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'DT', 'NN', 'VBG', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', ':', 'JJ', 'NN', 'NN', ',', 'TO', 'RB', 'VB', 'NNS', '.']",28
natural_language_inference,58,146,CNN and Daily Mail Datasets,"['CNN', 'and', 'Daily', 'Mail', 'Datasets']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'CC', 'NNP', 'NNP', 'NNS']",5
natural_language_inference,58,149,"Vocab Size : For training our ReasoNet , we keep the most frequent | V | = 101 k words ( not including 584 entities and 1 placeholder marker ) in the CNN dataset , and | V | = 151 k words ( not including 530 entities and 1 placeholder marker ) in the Daily Mail dataset .","['Vocab', 'Size', ':', 'For', 'training', 'our', 'ReasoNet', ',', 'we', 'keep', 'the', 'most', 'frequent', '|', 'V', '|', '=', '101', 'k', 'words', '(', 'not', 'including', '584', 'entities', 'and', '1', 'placeholder', 'marker', ')', 'in', 'the', 'CNN', 'dataset', ',', 'and', '|', 'V', '|', '=', '151', 'k', 'words', '(', 'not', 'including', '530', 'entities', 'and', '1', 'placeholder', 'marker', ')', 'in', 'the', 'Daily', 'Mail', 'dataset', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', ':', 'IN', 'VBG', 'PRP$', 'NN', ',', 'PRP', 'VBP', 'DT', 'RBS', 'JJ', 'NN', 'NNP', 'NNP', 'VBZ', 'CD', 'NN', 'NNS', '(', 'RB', 'VBG', 'CD', 'NNS', 'CC', 'CD', 'NN', 'NN', ')', 'IN', 'DT', 'NNP', 'NN', ',', 'CC', 'NNP', 'NNP', 'NNP', 'VBZ', 'CD', 'NN', 'NNS', '(', 'RB', 'VBG', 'CD', 'NNS', 'CC', 'CD', 'NN', 'NN', ')', 'IN', 'DT', 'NNP', 'NNP', 'NN', '.']",59
natural_language_inference,58,150,Embedding Layer :,"['Embedding', 'Layer', ':']","['B-n', 'I-n', 'O']","['VBG', 'NN', ':']",3
natural_language_inference,58,151,"We choose 300 - dimensional word embeddings , and use the 300 - dimensional pretrained Glove word embeddings for initialization .","['We', 'choose', '300', '-', 'dimensional', 'word', 'embeddings', ',', 'and', 'use', 'the', '300', '-', 'dimensional', 'pretrained', 'Glove', 'word', 'embeddings', 'for', 'initialization', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'CD', ':', 'JJ', 'NN', 'NNS', ',', 'CC', 'VB', 'DT', 'CD', ':', 'NN', 'VBD', 'NNP', 'NN', 'NNS', 'IN', 'NN', '.']",21
natural_language_inference,58,152,We also apply dropout with probability 0.2 to the embedding layer .,"['We', 'also', 'apply', 'dropout', 'with', 'probability', '0.2', 'to', 'the', 'embedding', 'layer', '.']","['O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'NN', 'IN', 'NN', 'CD', 'TO', 'DT', 'VBG', 'NN', '.']",12
natural_language_inference,58,173,"We use ADAM optimizer for parameter optimization with an initial learning rate of 0.0005 , ? 1 = 0.9 and ? 2 = 0.999 ;","['We', 'use', 'ADAM', 'optimizer', 'for', 'parameter', 'optimization', 'with', 'an', 'initial', 'learning', 'rate', 'of', '0.0005', ',', '?', '1', '=', '0.9', 'and', '?', '2', '=', '0.999', ';']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', ',', '.', 'CD', 'JJ', 'CD', 'CC', '.', 'CD', 'JJ', 'CD', ':']",25
natural_language_inference,58,174,The absolute value of gradient on each parameter is clipped within 0.001 .,"['The', 'absolute', 'value', 'of', 'gradient', 'on', 'each', 'parameter', 'is', 'clipped', 'within', '0.001', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'CD', '.']",13
natural_language_inference,58,175,The batch size is 64 for both CNN and Daily Mail datasets .,"['The', 'batch', 'size', 'is', '64', 'for', 'both', 'CNN', 'and', 'Daily', 'Mail', 'datasets', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'CD', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NNP', 'NNS', '.']",13
natural_language_inference,58,179,Models are trained on GTX TitanX 12 GB .,"['Models', 'are', 'trained', 'on', 'GTX', 'TitanX', '12', 'GB', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'VBP', 'VBN', 'IN', 'NNP', 'NNP', 'CD', 'NNP', '.']",9
natural_language_inference,58,197,"Comparing with the AS Reader , ReasoNet shows the signi cant improvement by capturing multi-turn reasoning in the paragraph .","['Comparing', 'with', 'the', 'AS', 'Reader', ',', 'ReasoNet', 'shows', 'the', 'signi', 'cant', 'improvement', 'by', 'capturing', 'multi-turn', 'reasoning', 'in', 'the', 'paragraph', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']","['VBG', 'IN', 'DT', 'NNP', 'NNP', ',', 'NNP', 'VBZ', 'DT', 'NN', 'NN', 'NN', 'IN', 'VBG', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",20
natural_language_inference,58,198,"Iterative Attention Reader , EpiReader and GA Reader are the three multi-turn reasoning models with xed reasoning steps .","['Iterative', 'Attention', 'Reader', ',', 'EpiReader', 'and', 'GA', 'Reader', 'are', 'the', 'three', 'multi-turn', 'reasoning', 'models', 'with', 'xed', 'reasoning', 'steps', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['JJ', 'NNP', 'NNP', ',', 'NNP', 'CC', 'NNP', 'NNP', 'VBP', 'DT', 'CD', 'NN', 'VBG', 'NNS', 'IN', 'JJ', 'VBG', 'NNS', '.']",19
natural_language_inference,58,199,ReasoNet also outperforms all of them by integrating termination gate in the model which allows di erent reasoning steps for di erent test cases .,"['ReasoNet', 'also', 'outperforms', 'all', 'of', 'them', 'by', 'integrating', 'termination', 'gate', 'in', 'the', 'model', 'which', 'allows', 'di', 'erent', 'reasoning', 'steps', 'for', 'di', 'erent', 'test', 'cases', '.']","['B-n', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'RB', 'VBZ', 'DT', 'IN', 'PRP', 'IN', 'VBG', 'NN', 'NN', 'IN', 'DT', 'NN', 'WDT', 'VBZ', 'FW', 'JJ', 'VBG', 'NNS', 'IN', 'JJ', 'JJ', 'NN', 'NNS', '.']",25
natural_language_inference,58,201,ReasoNet obtains comparable results with AoA Reader on CNN test set .,"['ReasoNet', 'obtains', 'comparable', 'results', 'with', 'AoA', 'Reader', 'on', 'CNN', 'test', 'set', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'JJ', 'NNS', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NN', 'NN', '.']",12
natural_language_inference,58,210,SQuAD Dataset,"['SQuAD', 'Dataset']","['B-n', 'I-n']","['NNP', 'NNP']",2
natural_language_inference,58,217,Vocab Size :,"['Vocab', 'Size', ':']","['B-n', 'I-n', 'O']","['NNP', 'NN', ':']",3
natural_language_inference,58,218,"We use the python NLTK tokenizer 6 to preprocess passages and questions , and obtain about 100K words in the vocabulary .","['We', 'use', 'the', 'python', 'NLTK', 'tokenizer', '6', 'to', 'preprocess', 'passages', 'and', 'questions', ',', 'and', 'obtain', 'about', '100K', 'words', 'in', 'the', 'vocabulary', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NNP', 'VBZ', 'CD', 'TO', 'VB', 'NNS', 'CC', 'NNS', ',', 'CC', 'VB', 'IN', 'CD', 'NNS', 'IN', 'DT', 'NN', '.']",22
natural_language_inference,58,219,Embedding Layer : We use the 100 - dimensional pretrained Glove vectors as word embeddings .,"['Embedding', 'Layer', ':', 'We', 'use', 'the', '100', '-', 'dimensional', 'pretrained', 'Glove', 'vectors', 'as', 'word', 'embeddings', '.']","['B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['VBG', 'NN', ':', 'PRP', 'VBP', 'DT', 'CD', ':', 'NN', 'VBD', 'NNP', 'NNS', 'IN', 'NN', 'NNS', '.']",16
natural_language_inference,58,247,The maximum reasoning step T max is set to 10 in SQuAD experiments .,"['The', 'maximum', 'reasoning', 'step', 'T', 'max', 'is', 'set', 'to', '10', 'in', 'SQuAD', 'experiments', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'JJ', 'NN', 'NNP', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'IN', 'NNP', 'NNS', '.']",14
natural_language_inference,58,248,We use AdaDelta optimizer for parameter optimization with an initial learning rate of 0.5 and a batch size Results :,"['We', 'use', 'AdaDelta', 'optimizer', 'for', 'parameter', 'optimization', 'with', 'an', 'initial', 'learning', 'rate', 'of', '0.5', 'and', 'a', 'batch', 'size', 'Results', ':']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'CC', 'DT', 'NN', 'NN', 'NNS', ':']",20
natural_language_inference,58,254,"In , we demonstrate that ReasoNet outperforms all existing published approaches .","['In', ',', 'we', 'demonstrate', 'that', 'ReasoNet', 'outperforms', 'all', 'existing', 'published', 'approaches', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', ',', 'PRP', 'VBP', 'IN', 'NNP', 'VBZ', 'DT', 'VBG', 'VBN', 'NNS', '.']",12
natural_language_inference,58,255,"While we compare ReasoNet with BiDAF , ReasoNet exceeds BiDAF both in single model and ensemble model cases .","['While', 'we', 'compare', 'ReasoNet', 'with', 'BiDAF', ',', 'ReasoNet', 'exceeds', 'BiDAF', 'both', 'in', 'single', 'model', 'and', 'ensemble', 'model', 'cases', '.']","['O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'PRP', 'VBP', 'JJ', 'IN', 'NNP', ',', 'NNP', 'VBZ', 'NNP', 'DT', 'IN', 'JJ', 'NN', 'CC', 'JJ', 'NN', 'NNS', '.']",19
natural_language_inference,58,257,"In the bottom part of , we compare ReasoNet with all unpublished methods at the time of this submission , ReasoNet holds the second position in all the competing approaches in the SQuAD leaderboard .","['In', 'the', 'bottom', 'part', 'of', ',', 'we', 'compare', 'ReasoNet', 'with', 'all', 'unpublished', 'methods', 'at', 'the', 'time', 'of', 'this', 'submission', ',', 'ReasoNet', 'holds', 'the', 'second', 'position', 'in', 'all', 'the', 'competing', 'approaches', 'in', 'the', 'SQuAD', 'leaderboard', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'IN', ',', 'PRP', 'VBP', 'JJ', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'PDT', 'DT', 'VBG', 'NNS', 'IN', 'DT', 'NNP', 'NN', '.']",35
natural_language_inference,58,258,Graph Reachability,"['Graph', 'Reachability']","['B-n', 'I-n']","['NNP', 'NNP']",2
natural_language_inference,58,283,Embedding Layer,"['Embedding', 'Layer']","['B-n', 'I-n']","['VBG', 'NNP']",2
natural_language_inference,58,284,We use a 100 - dimensional embedding vector for each symbol in the query and graph description .,"['We', 'use', 'a', '100', '-', 'dimensional', 'embedding', 'vector', 'for', 'each', 'symbol', 'in', 'the', 'query', 'and', 'graph', 'description', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'CD', ':', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'CC', 'JJ', 'NN', '.']",18
natural_language_inference,58,300,"The maximum reasoning step T max is set to 15 and 25 for the small graph and large graph dataset , respectively .","['The', 'maximum', 'reasoning', 'step', 'T', 'max', 'is', 'set', 'to', '15', 'and', '25', 'for', 'the', 'small', 'graph', 'and', 'large', 'graph', 'dataset', ',', 'respectively', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['DT', 'JJ', 'JJ', 'NN', 'NNP', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'CC', 'CD', 'IN', 'DT', 'JJ', 'NN', 'CC', 'JJ', 'NN', 'NN', ',', 'RB', '.']",23
natural_language_inference,58,301,We use AdaDelta optimizer for parameter optimization with an initial learning rate of 0.5 and a batch size of 32 .,"['We', 'use', 'AdaDelta', 'optimizer', 'for', 'parameter', 'optimization', 'with', 'an', 'initial', 'learning', 'rate', 'of', '0.5', 'and', 'a', 'batch', 'size', 'of', '32', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'CC', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",21
natural_language_inference,58,307,"Deep LSTM Reader achieves 90.92 % and 71.55 % accuracy in the small and large graph dataset , respectively , which indicates the graph reachibility task is not trivial .","['Deep', 'LSTM', 'Reader', 'achieves', '90.92', '%', 'and', '71.55', '%', 'accuracy', 'in', 'the', 'small', 'and', 'large', 'graph', 'dataset', ',', 'respectively', ',', 'which', 'indicates', 'the', 'graph', 'reachibility', 'task', 'is', 'not', 'trivial', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', 'VBZ', 'CD', 'NN', 'CC', 'CD', 'NN', 'NN', 'IN', 'DT', 'JJ', 'CC', 'JJ', 'NN', 'NN', ',', 'RB', ',', 'WDT', 'VBZ', 'DT', 'NN', 'NN', 'NN', 'VBZ', 'RB', 'JJ', '.']",30
natural_language_inference,51,2,Neural Stored - program Memory,"['Neural', 'Stored', '-', 'program', 'Memory']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', ':', 'NN', 'NN']",5
natural_language_inference,51,6,"In this paper , we introduce a new memory to store weights for the controller , analogous to the stored - program memory in modern computer architectures .","['In', 'this', 'paper', ',', 'we', 'introduce', 'a', 'new', 'memory', 'to', 'store', 'weights', 'for', 'the', 'controller', ',', 'analogous', 'to', 'the', 'stored', '-', 'program', 'memory', 'in', 'modern', 'computer', 'architectures', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'TO', 'VB', 'NNS', 'IN', 'DT', 'NN', ',', 'JJ', 'TO', 'DT', 'JJ', ':', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NNS', '.']",28
natural_language_inference,51,21,Our goal is to advance a step further towards UTM by coupling a MANN with an external program memory .,"['Our', 'goal', 'is', 'to', 'advance', 'a', 'step', 'further', 'towards', 'UTM', 'by', 'coupling', 'a', 'MANN', 'with', 'an', 'external', 'program', 'memory', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'RBR', 'NNS', 'NNP', 'IN', 'VBG', 'DT', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",20
natural_language_inference,51,22,"The program memory co-exists with the data memory in the MANN , providing more flexibility , reuseability and modularity in learning complicated tasks .","['The', 'program', 'memory', 'co-exists', 'with', 'the', 'data', 'memory', 'in', 'the', 'MANN', ',', 'providing', 'more', 'flexibility', ',', 'reuseability', 'and', 'modularity', 'in', 'learning', 'complicated', 'tasks', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'NNS', 'IN', 'DT', 'NNS', 'NN', 'IN', 'DT', 'NNP', ',', 'VBG', 'JJR', 'NN', ',', 'NN', 'CC', 'NN', 'IN', 'VBG', 'JJ', 'NNS', '.']",24
natural_language_inference,51,23,"The program memory stores the weights of the MANN 's controller network , which are retrieved quickly via a key - value attention mechanism across timesteps yet updated slowly via backpropagation .","['The', 'program', 'memory', 'stores', 'the', 'weights', 'of', 'the', 'MANN', ""'s"", 'controller', 'network', ',', 'which', 'are', 'retrieved', 'quickly', 'via', 'a', 'key', '-', 'value', 'attention', 'mechanism', 'across', 'timesteps', 'yet', 'updated', 'slowly', 'via', 'backpropagation', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'NNS', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'POS', 'NN', 'NN', ',', 'WDT', 'VBP', 'VBN', 'RB', 'IN', 'DT', 'JJ', ':', 'NN', 'NN', 'NN', 'IN', 'NNS', 'RB', 'VBN', 'RB', 'IN', 'NN', '.']",32
natural_language_inference,51,24,"By introducing a meta network to moderate the operations of the program memory , our model , henceforth referred to as Neural Stored - program Memory ( NSM ) , can learn to switch the programs / weights in the controller network appropriately , adapting to different functionalities aligning with different parts of a sequential task , or different tasks in continual and few - shot learning .","['By', 'introducing', 'a', 'meta', 'network', 'to', 'moderate', 'the', 'operations', 'of', 'the', 'program', 'memory', ',', 'our', 'model', ',', 'henceforth', 'referred', 'to', 'as', 'Neural', 'Stored', '-', 'program', 'Memory', '(', 'NSM', ')', ',', 'can', 'learn', 'to', 'switch', 'the', 'programs', '/', 'weights', 'in', 'the', 'controller', 'network', 'appropriately', ',', 'adapting', 'to', 'different', 'functionalities', 'aligning', 'with', 'different', 'parts', 'of', 'a', 'sequential', 'task', ',', 'or', 'different', 'tasks', 'in', 'continual', 'and', 'few', '-', 'shot', 'learning', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBG', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', ',', 'PRP$', 'NN', ',', 'NN', 'VBD', 'TO', 'IN', 'NNP', 'NNP', ':', 'NN', 'NN', '(', 'NNP', ')', ',', 'MD', 'VB', 'TO', 'VB', 'DT', 'NNS', 'VBP', 'NNS', 'IN', 'DT', 'NN', 'NN', 'RB', ',', 'VBG', 'TO', 'JJ', 'NNS', 'VBG', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', ',', 'CC', 'JJ', 'NNS', 'IN', 'JJ', 'CC', 'JJ', ':', 'NN', 'NN', '.']",68
natural_language_inference,51,148,"Except for the Copy task , which is too simple , other tasks observe convergence speed improvement of NUTM over that of NTM , thereby validating the benefit of using two programs across timesteps even for the single task setting .","['Except', 'for', 'the', 'Copy', 'task', ',', 'which', 'is', 'too', 'simple', ',', 'other', 'tasks', 'observe', 'convergence', 'speed', 'improvement', 'of', 'NUTM', 'over', 'that', 'of', 'NTM', ',', 'thereby', 'validating', 'the', 'benefit', 'of', 'using', 'two', 'programs', 'across', 'timesteps', 'even', 'for', 'the', 'single', 'task', 'setting', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'IN', 'DT', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'RB', 'JJ', ',', 'JJ', 'NNS', 'VBP', 'NN', 'NN', 'NN', 'IN', 'NNP', 'IN', 'DT', 'IN', 'NNP', ',', 'RB', 'VBG', 'DT', 'NN', 'IN', 'VBG', 'CD', 'NNS', 'IN', 'NNS', 'RB', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",41
natural_language_inference,51,149,NUTM requires fewer training samples to converge and it generalizes better to unseen sequences that are longer than training sequences .,"['NUTM', 'requires', 'fewer', 'training', 'samples', 'to', 'converge', 'and', 'it', 'generalizes', 'better', 'to', 'unseen', 'sequences', 'that', 'are', 'longer', 'than', 'training', 'sequences', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'JJR', 'NN', 'NNS', 'TO', 'VB', 'CC', 'PRP', 'VBZ', 'RBR', 'TO', 'JJ', 'NNS', 'WDT', 'VBP', 'JJR', 'IN', 'NN', 'NNS', '.']",21
natural_language_inference,51,159,"We run the task with three additional baselines : NUTM using direct attention ( DA ) , NUTM using key - value without regularization ( KV ) , NUTM using fixed , uniform program distribution ( UP ) and a vanilla NTM with 2 memory heads ( h = 2 ) .","['We', 'run', 'the', 'task', 'with', 'three', 'additional', 'baselines', ':', 'NUTM', 'using', 'direct', 'attention', '(', 'DA', ')', ',', 'NUTM', 'using', 'key', '-', 'value', 'without', 'regularization', '(', 'KV', ')', ',', 'NUTM', 'using', 'fixed', ',', 'uniform', 'program', 'distribution', '(', 'UP', ')', 'and', 'a', 'vanilla', 'NTM', 'with', '2', 'memory', 'heads', '(', 'h', '=', '2', ')', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'CD', 'JJ', 'NNS', ':', 'NNP', 'VBG', 'JJ', 'NN', '(', 'NNP', ')', ',', 'NNP', 'VBG', 'JJ', ':', 'NN', 'IN', 'NN', '(', 'NNP', ')', ',', 'NNP', 'VBG', 'VBN', ',', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'DT', 'NN', 'NNP', 'IN', 'CD', 'NN', 'NNS', '(', 'JJ', 'NNP', 'CD', ')', '.']",52
natural_language_inference,51,163,The results demonstrate that DA exhibits fast yet shallow convergence .,"['The', 'results', 'demonstrate', 'that', 'DA', 'exhibits', 'fast', 'yet', 'shallow', 'convergence', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'IN', 'NNP', 'VBZ', 'RB', 'RB', 'JJ', 'NN', '.']",11
natural_language_inference,51,164,"It tends to fall into local minima , which finally fails to reach zero loss .","['It', 'tends', 'to', 'fall', 'into', 'local', 'minima', ',', 'which', 'finally', 'fails', 'to', 'reach', 'zero', 'loss', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBZ', 'TO', 'VB', 'IN', 'JJ', 'NN', ',', 'WDT', 'RB', 'VBZ', 'TO', 'VB', 'CD', 'NN', '.']",16
natural_language_inference,51,165,Key- value attention helps NUTM converge completely with fewer iterations .,"['Key-', 'value', 'attention', 'helps', 'NUTM', 'converge', 'completely', 'with', 'fewer', 'iterations', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'NN', 'NN', 'VBZ', 'NNP', 'NN', 'RB', 'IN', 'JJR', 'NNS', '.']",11
natural_language_inference,51,166,The performance is further improved with the proposed regularization loss .,"['The', 'performance', 'is', 'further', 'improved', 'with', 'the', 'proposed', 'regularization', 'loss', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'JJ', 'VBN', 'IN', 'DT', 'VBN', 'NN', 'NN', '.']",11
natural_language_inference,51,167,UP underperforms NUTM as it lacks dynamic programs .,"['UP', 'underperforms', 'NUTM', 'as', 'it', 'lacks', 'dynamic', 'programs', '.']","['B-n', 'B-n', 'B-n', 'B-p', 'O', 'B-n', 'B-n', 'I-n', 'O']","['IN', 'NNS', 'NNP', 'IN', 'PRP', 'VBZ', 'JJ', 'NNS', '.']",9
natural_language_inference,51,168,"The NTM with 2 heads shows slightly better convergence compared to the NTM , yet obviously underperforms NUTM ( p = 2 ) with 1 head and fewer parameters .","['The', 'NTM', 'with', '2', 'heads', 'shows', 'slightly', 'better', 'convergence', 'compared', 'to', 'the', 'NTM', ',', 'yet', 'obviously', 'underperforms', 'NUTM', '(', 'p', '=', '2', ')', 'with', '1', 'head', 'and', 'fewer', 'parameters', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'IN', 'CD', 'NNS', 'VBZ', 'RB', 'JJR', 'NN', 'VBN', 'TO', 'DT', 'NNP', ',', 'RB', 'RB', 'JJ', 'NNP', '(', 'JJ', 'NNP', 'CD', ')', 'IN', 'CD', 'NN', 'CC', 'JJR', 'NNS', '.']",30
natural_language_inference,57,2,ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE,"['ORDER', '-', 'EMBEDDINGS', 'OF', 'IMAGES', 'AND', 'LANGUAGE']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NN', 'IN', 'NNP', 'NNP', 'NNP']",7
natural_language_inference,57,4,"Hypernymy , textual entailment , and image captioning can be seen as special cases of a single visual - semantic hierarchy over words , sentences , and images .","['Hypernymy', ',', 'textual', 'entailment', ',', 'and', 'image', 'captioning', 'can', 'be', 'seen', 'as', 'special', 'cases', 'of', 'a', 'single', 'visual', '-', 'semantic', 'hierarchy', 'over', 'words', ',', 'sentences', ',', 'and', 'images', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ',', 'JJ', 'NN', ',', 'CC', 'NN', 'NN', 'MD', 'VB', 'VBN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'JJ', ':', 'JJ', 'NN', 'IN', 'NNS', ',', 'NNS', ',', 'CC', 'NNS', '.']",29
natural_language_inference,57,15,"In fact , all three relations can be seen as special cases of a partial order over images and language , illustrated in , which we refer to as the visualsemantic hierarchy .","['In', 'fact', ',', 'all', 'three', 'relations', 'can', 'be', 'seen', 'as', 'special', 'cases', 'of', 'a', 'partial', 'order', 'over', 'images', 'and', 'language', ',', 'illustrated', 'in', ',', 'which', 'we', 'refer', 'to', 'as', 'the', 'visualsemantic', 'hierarchy', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'DT', 'CD', 'NNS', 'MD', 'VB', 'VBN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'CC', 'NN', ',', 'VBN', 'IN', ',', 'WDT', 'PRP', 'VBP', 'TO', 'IN', 'DT', 'JJ', 'NN', '.']",33
natural_language_inference,57,26,"In contrast , we propose to exploit the partial order structure of the visual - semantic hierarchy by learning a mapping which is not distance - preserving but order - preserving between the visualsemantic hierarchy and a partial order over the embedding space .","['In', 'contrast', ',', 'we', 'propose', 'to', 'exploit', 'the', 'partial', 'order', 'structure', 'of', 'the', 'visual', '-', 'semantic', 'hierarchy', 'by', 'learning', 'a', 'mapping', 'which', 'is', 'not', 'distance', '-', 'preserving', 'but', 'order', '-', 'preserving', 'between', 'the', 'visualsemantic', 'hierarchy', 'and', 'a', 'partial', 'order', 'over', 'the', 'embedding', 'space', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', ':', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'NN', 'WDT', 'VBZ', 'RB', 'JJ', ':', 'NN', 'CC', 'NN', ':', 'NN', 'IN', 'DT', 'JJ', 'NN', 'CC', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",44
natural_language_inference,57,27,We call embeddings learned in this way order- embeddings .,"['We', 'call', 'embeddings', 'learned', 'in', 'this', 'way', 'order-', 'embeddings', '.']","['O', 'B-p', 'B-n', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'JJ', 'NNS', '.']",10
natural_language_inference,57,77,HYPERNYM PREDICTION,"['HYPERNYM', 'PREDICTION']","['B-n', 'I-n']","['NNP', 'NNP']",2
natural_language_inference,57,96,"To train the model , we use the standard pairwise ranking objective from Eq. ( 5 ) .","['To', 'train', 'the', 'model', ',', 'we', 'use', 'the', 'standard', 'pairwise', 'ranking', 'objective', 'from', 'Eq.', '(', '5', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBG', 'JJ', 'IN', 'NNP', '(', 'CD', ')', '.']",18
natural_language_inference,57,97,"We sample minibatches of 128 random image - caption pairs , and draw all contrastive terms from the minibatch , giving us 127 contrastive images for each caption and captions for each image .","['We', 'sample', 'minibatches', 'of', '128', 'random', 'image', '-', 'caption', 'pairs', ',', 'and', 'draw', 'all', 'contrastive', 'terms', 'from', 'the', 'minibatch', ',', 'giving', 'us', '127', 'contrastive', 'images', 'for', 'each', 'caption', 'and', 'captions', 'for', 'each', 'image', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'NNS', 'IN', 'CD', 'JJ', 'NN', ':', 'NN', 'NNS', ',', 'CC', 'VB', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', ',', 'VBG', 'PRP', 'CD', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NNS', 'IN', 'DT', 'NN', '.']",34
natural_language_inference,57,98,"We train for 15 - 30 epochs using the Adam optimizer with learning rate 0.001 , and early stopping on the validation set .","['We', 'train', 'for', '15', '-', '30', 'epochs', 'using', 'the', 'Adam', 'optimizer', 'with', 'learning', 'rate', '0.001', ',', 'and', 'early', 'stopping', 'on', 'the', 'validation', 'set', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'CD', ':', 'CD', 'NN', 'VBG', 'DT', 'NNP', 'NN', 'IN', 'VBG', 'NN', 'CD', ',', 'CC', 'RB', 'VBG', 'IN', 'DT', 'NN', 'NN', '.']",24
natural_language_inference,57,99,"We set the dimension of the embedding space and the GRU hidden state N to 1024 , the dimension of the learned word embeddings to 300 , and the margin ? to 0.05 .","['We', 'set', 'the', 'dimension', 'of', 'the', 'embedding', 'space', 'and', 'the', 'GRU', 'hidden', 'state', 'N', 'to', '1024', ',', 'the', 'dimension', 'of', 'the', 'learned', 'word', 'embeddings', 'to', '300', ',', 'and', 'the', 'margin', '?', 'to', '0.05', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'VBG', 'NN', 'CC', 'DT', 'NNP', 'NN', 'NN', 'NNP', 'TO', 'CD', ',', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNS', 'TO', 'CD', ',', 'CC', 'DT', 'NN', '.', 'TO', 'CD', '.']",34
natural_language_inference,57,101,"For consistency with and to mitigate overfitting , we constrain the caption and image embeddings to have unit L2 norm .","['For', 'consistency', 'with', 'and', 'to', 'mitigate', 'overfitting', ',', 'we', 'constrain', 'the', 'caption', 'and', 'image', 'embeddings', 'to', 'have', 'unit', 'L2', 'norm', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'IN', 'CC', 'TO', 'VB', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'CC', 'NN', 'NNS', 'TO', 'VB', 'NN', 'NNP', 'NN', '.']",21
natural_language_inference,57,115,We see that order- embeddings outperform the skipthought baseline despite not using external text corpora .,"['We', 'see', 'that', 'order-', 'embeddings', 'outperform', 'the', 'skipthought', 'baseline', 'despite', 'not', 'using', 'external', 'text', 'corpora', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'RB', 'VBG', 'JJ', 'NN', 'NN', '.']",16
natural_language_inference,57,144,TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE,"['TEXTUAL', 'ENTAILMENT', '/', 'NATURAL', 'LANGUAGE', 'INFERENCE']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",6
natural_language_inference,57,149,"Just as for caption - image ranking , we set the dimensions of the embedding space and GRU hidden state to be 1024 , the dimension of the word embeddings to be 300 , and constrain the embeddings to have unit L2 norm .","['Just', 'as', 'for', 'caption', '-', 'image', 'ranking', ',', 'we', 'set', 'the', 'dimensions', 'of', 'the', 'embedding', 'space', 'and', 'GRU', 'hidden', 'state', 'to', 'be', '1024', ',', 'the', 'dimension', 'of', 'the', 'word', 'embeddings', 'to', 'be', '300', ',', 'and', 'constrain', 'the', 'embeddings', 'to', 'have', 'unit', 'L2', 'norm', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', 'IN', 'IN', 'NN', ':', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNS', 'IN', 'DT', 'VBG', 'NN', 'CC', 'NNP', 'VBP', 'NN', 'TO', 'VB', 'CD', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'CD', ',', 'CC', 'VB', 'DT', 'NNS', 'TO', 'VB', 'NN', 'NNP', 'NN', '.']",44
natural_language_inference,57,150,We train for 10 epochs with batches of 128 sentence pairs .,"['We', 'train', 'for', '10', 'epochs', 'with', 'batches', 'of', '128', 'sentence', 'pairs', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'CD', 'NNS', 'IN', 'NNS', 'IN', 'CD', 'NN', 'NNS', '.']",12
natural_language_inference,57,151,We use the Adam optimizer with learning rate 0.001 and early stopping on the validation set .,"['We', 'use', 'the', 'Adam', 'optimizer', 'with', 'learning', 'rate', '0.001', 'and', 'early', 'stopping', 'on', 'the', 'validation', 'set', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', 'IN', 'VBG', 'NN', 'CD', 'CC', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",17
natural_language_inference,8,2,Deep Learning for Answer Sentence Selection,"['Deep', 'Learning', 'for', 'Answer', 'Sentence', 'Selection']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",6
natural_language_inference,8,36,"In this paper , we show that a neural network - based sentence model can be applied to the task of answer sentence selection .","['In', 'this', 'paper', ',', 'we', 'show', 'that', 'a', 'neural', 'network', '-', 'based', 'sentence', 'model', 'can', 'be', 'applied', 'to', 'the', 'task', 'of', 'answer', 'sentence', 'selection', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'IN', 'DT', 'JJ', 'NN', ':', 'VBN', 'NN', 'NN', 'MD', 'VB', 'VBN', 'TO', 'DT', 'NN', 'IN', 'JJR', 'NN', 'NN', '.']",25
natural_language_inference,8,37,"We construct two distributional sentence models ; first a bag - of - words model , and second , a bigram model based on a convolutional neural network .","['We', 'construct', 'two', 'distributional', 'sentence', 'models', ';', 'first', 'a', 'bag', '-', 'of', '-', 'words', 'model', ',', 'and', 'second', ',', 'a', 'bigram', 'model', 'based', 'on', 'a', 'convolutional', 'neural', 'network', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'JJ', 'NN', 'NNS', ':', 'RB', 'DT', 'NN', ':', 'IN', ':', 'NNS', 'NN', ',', 'CC', 'JJ', ',', 'DT', 'NN', 'NN', 'VBN', 'IN', 'DT', 'JJ', 'JJ', 'NN', '.']",29
natural_language_inference,8,38,"Assuming a set of pre-trained semantic word embeddings , we train a supervised model to learn a semantic matching between question and answer pairs .","['Assuming', 'a', 'set', 'of', 'pre-trained', 'semantic', 'word', 'embeddings', ',', 'we', 'train', 'a', 'supervised', 'model', 'to', 'learn', 'a', 'semantic', 'matching', 'between', 'question', 'and', 'answer', 'pairs', '.']","['B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'DT', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'CC', 'NN', 'NNS', '.']",25
natural_language_inference,8,40,"We also present an enhanced version of this model , which combines the signal of the distributed matching algorithm with two simple word matching features .","['We', 'also', 'present', 'an', 'enhanced', 'version', 'of', 'this', 'model', ',', 'which', 'combines', 'the', 'signal', 'of', 'the', 'distributed', 'matching', 'algorithm', 'with', 'two', 'simple', 'word', 'matching', 'features', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'JJ', 'NN', 'NN', 'NNS', '.']",26
natural_language_inference,8,150,We used word embeddings ( d = 50 ) that were computed using Collobert and Weston 's neural language model and provided by Turian et al ..,"['We', 'used', 'word', 'embeddings', '(', 'd', '=', '50', ')', 'that', 'were', 'computed', 'using', 'Collobert', 'and', 'Weston', ""'s"", 'neural', 'language', 'model', 'and', 'provided', 'by', 'Turian', 'et', 'al', '..']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'NN', 'NNS', '(', 'VB', 'RB', 'CD', ')', 'WDT', 'VBD', 'VBN', 'VBG', 'NNP', 'CC', 'NNP', 'POS', 'JJ', 'NN', 'NN', 'CC', 'VBN', 'IN', 'JJ', 'NN', 'NN', 'NN']",27
natural_language_inference,8,152,"The other model weights were randomly intitialised using a Gaussian distribution ( = 0 , ? = 0.01 ) .","['The', 'other', 'model', 'weights', 'were', 'randomly', 'intitialised', 'using', 'a', 'Gaussian', 'distribution', '(', '=', '0', ',', '?', '=', '0.01', ')', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NNS', 'VBD', 'RB', 'VBN', 'VBG', 'DT', 'JJ', 'NN', '(', 'JJ', 'CD', ',', '.', '$', 'CD', ')', '.']",20
natural_language_inference,8,153,All hyperparameters were optimised via grid search on the MAP score on the development data .,"['All', 'hyperparameters', 'were', 'optimised', 'via', 'grid', 'search', 'on', 'the', 'MAP', 'score', 'on', 'the', 'development', 'data', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBD', 'VBN', 'IN', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'DT', 'NN', 'NNS', '.']",16
natural_language_inference,8,154,We use the AdaGrad algorithm for training .,"['We', 'use', 'the', 'AdaGrad', 'algorithm', 'for', 'training', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', 'IN', 'NN', '.']",8
natural_language_inference,8,159,"L - BFGS was used to train the logistic regression classifier , with L2 regulariser of 0.01 .","['L', '-', 'BFGS', 'was', 'used', 'to', 'train', 'the', 'logistic', 'regression', 'classifier', ',', 'with', 'L2', 'regulariser', 'of', '0.01', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', ':', 'NNP', 'VBD', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', ',', 'IN', 'NNP', 'NN', 'IN', 'CD', '.']",18
natural_language_inference,8,161,"As can be seen , the bigram model performs better than the unigram model and the addition of the IDF - weighted word count features significantly improve performance for both models by 10 % - 15 % .","['As', 'can', 'be', 'seen', ',', 'the', 'bigram', 'model', 'performs', 'better', 'than', 'the', 'unigram', 'model', 'and', 'the', 'addition', 'of', 'the', 'IDF', '-', 'weighted', 'word', 'count', 'features', 'significantly', 'improve', 'performance', 'for', 'both', 'models', 'by', '10', '%', '-', '15', '%', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'MD', 'VB', 'VBN', ',', 'DT', 'NN', 'NN', 'NNS', 'RBR', 'IN', 'DT', 'JJ', 'NN', 'CC', 'DT', 'NN', 'IN', 'DT', 'NNP', ':', 'VBD', 'NN', 'NN', 'VBZ', 'RB', 'VB', 'NN', 'IN', 'DT', 'NNS', 'IN', 'CD', 'NN', ':', 'CD', 'NN', '.']",38
natural_language_inference,8,169,"As can be seen in , our best models ( bigram + count ) outperform all baselines and prior work on MAP and are very close to the best model proposed by Yih et al. on MRR .","['As', 'can', 'be', 'seen', 'in', ',', 'our', 'best', 'models', '(', 'bigram', '+', 'count', ')', 'outperform', 'all', 'baselines', 'and', 'prior', 'work', 'on', 'MAP', 'and', 'are', 'very', 'close', 'to', 'the', 'best', 'model', 'proposed', 'by', 'Yih', 'et', 'al.', 'on', 'MRR', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'MD', 'VB', 'VBN', 'IN', ',', 'PRP$', 'JJS', 'NNS', '(', 'JJ', 'NNP', 'NN', ')', 'IN', 'DT', 'NNS', 'CC', 'JJ', 'NN', 'IN', 'NNP', 'CC', 'VBP', 'RB', 'RB', 'TO', 'DT', 'JJS', 'NN', 'VBN', 'IN', 'NNP', 'FW', 'NN', 'IN', 'NNP', '.']",38
natural_language_inference,9,2,Published as a conference paper at ICLR 2017 QUERY - REDUCTION NETWORKS FOR QUESTION ANSWERING,"['Published', 'as', 'a', 'conference', 'paper', 'at', 'ICLR', '2017', 'QUERY', '-', 'REDUCTION', 'NETWORKS', 'FOR', 'QUESTION', 'ANSWERING']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['VBN', 'IN', 'DT', 'NN', 'NN', 'IN', 'NNP', 'CD', 'NNP', ':', 'NN', 'NNP', 'NNP', 'NNP', 'NNP']",15
natural_language_inference,9,4,"In this paper , we study the problem of question answering when reasoning over multiple facts is required .","['In', 'this', 'paper', ',', 'we', 'study', 'the', 'problem', 'of', 'question', 'answering', 'when', 'reasoning', 'over', 'multiple', 'facts', 'is', 'required', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'VBG', 'WRB', 'VBG', 'IN', 'JJ', 'NNS', 'VBZ', 'VBN', '.']",19
natural_language_inference,9,24,"Our proposed model , Query - Reduction Network 1 ( QRN ) , is a single recurrent unit that addresses the long - term dependency problem of most RNN - based models by simplifying the recurrent update , while taking the advantage of RNN 's capability to model sequential data ) .","['Our', 'proposed', 'model', ',', 'Query', '-', 'Reduction', 'Network', '1', '(', 'QRN', ')', ',', 'is', 'a', 'single', 'recurrent', 'unit', 'that', 'addresses', 'the', 'long', '-', 'term', 'dependency', 'problem', 'of', 'most', 'RNN', '-', 'based', 'models', 'by', 'simplifying', 'the', 'recurrent', 'update', ',', 'while', 'taking', 'the', 'advantage', 'of', 'RNN', ""'s"", 'capability', 'to', 'model', 'sequential', 'data', ')', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O']","['PRP$', 'VBN', 'NN', ',', 'NNP', ':', 'NN', 'NNP', 'CD', '(', 'NNP', ')', ',', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', ':', 'NN', 'NN', 'NN', 'IN', 'JJS', 'NNP', ':', 'VBN', 'NNS', 'IN', 'VBG', 'DT', 'NN', 'NN', ',', 'IN', 'VBG', 'DT', 'NN', 'IN', 'NNP', 'POS', 'NN', 'TO', 'VB', 'JJ', 'NNS', ')', '.']",52
natural_language_inference,9,25,"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time .","['QRN', 'considers', 'the', 'context', 'sentences', 'as', 'a', 'sequence', 'of', 'state', '-', 'changing', 'triggers', ',', 'and', 'transforms', '(', 'reduces', ')', 'the', 'original', 'query', 'to', 'a', 'more', 'informed', 'query', 'as', 'it', 'observes', 'each', 'trigger', 'through', 'time', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NN', ':', 'VBG', 'NNS', ',', 'CC', 'NNS', '(', 'NNS', ')', 'DT', 'JJ', 'NN', 'TO', 'DT', 'RBR', 'JJ', 'NN', 'IN', 'PRP', 'VBZ', 'DT', 'NN', 'IN', 'NN', '.']",35
natural_language_inference,9,33,"Compared to memory - based approaches , QRN can better encodes locality information because it does not use a global memory access controller ( circle nodes in ) , and the query updates are performed locally .","['Compared', 'to', 'memory', '-', 'based', 'approaches', ',', 'QRN', 'can', 'better', 'encodes', 'locality', 'information', 'because', 'it', 'does', 'not', 'use', 'a', 'global', 'memory', 'access', 'controller', '(', 'circle', 'nodes', 'in', ')', ',', 'and', 'the', 'query', 'updates', 'are', 'performed', 'locally', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O']","['VBN', 'TO', 'VB', ':', 'VBN', 'NNS', ',', 'NNP', 'MD', 'VB', 'NNS', 'JJ', 'NN', 'IN', 'PRP', 'VBZ', 'RB', 'VB', 'DT', 'JJ', 'NN', 'NN', 'NN', '(', 'NN', 'NNS', 'IN', ')', ',', 'CC', 'DT', 'NN', 'NNS', 'VBP', 'VBN', 'RB', '.']",37
natural_language_inference,9,204,We withhold 10 % of the training for development .,"['We', 'withhold', '10', '%', 'of', 'the', 'training', 'for', 'development', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'CD', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', '.']",10
natural_language_inference,9,205,We use the hidden state size of 50 by deafult .,"['We', 'use', 'the', 'hidden', 'state', 'size', 'of', '50', 'by', 'deafult', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'IN', 'NN', '.']",11
natural_language_inference,9,206,"Batch sizes of 32 for bAbI story - based QA 1k , bAb I dialog and DSTC2 dialog , and 128 for bAbI QA 10 k are used .","['Batch', 'sizes', 'of', '32', 'for', 'bAbI', 'story', '-', 'based', 'QA', '1k', ',', 'bAb', 'I', 'dialog', 'and', 'DSTC2', 'dialog', ',', 'and', '128', 'for', 'bAbI', 'QA', '10', 'k', 'are', 'used', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['NNP', 'NNS', 'IN', 'CD', 'IN', 'NN', 'NN', ':', 'VBN', 'NNP', 'CD', ',', 'NN', 'PRP', 'VBP', 'CC', 'NNP', 'NN', ',', 'CC', 'CD', 'IN', 'NN', 'NNP', 'CD', 'NN', 'VBP', 'VBN', '.']",29
natural_language_inference,9,207,The weights in the input and output modules are initialized with zero mean and the standard deviation of 1 / ? d.,"['The', 'weights', 'in', 'the', 'input', 'and', 'output', 'modules', 'are', 'initialized', 'with', 'zero', 'mean', 'and', 'the', 'standard', 'deviation', 'of', '1', '/', '?', 'd.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['DT', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'CD', 'NN', 'CC', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NN', '.', 'NN']",22
natural_language_inference,9,209,Forget bias of 2.5 is used for update gates ( no bias for reset gates ) .,"['Forget', 'bias', 'of', '2.5', 'is', 'used', 'for', 'update', 'gates', '(', 'no', 'bias', 'for', 'reset', 'gates', ')', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'IN', 'CD', 'VBZ', 'VBN', 'IN', 'JJ', 'NNS', '(', 'DT', 'NN', 'IN', 'NN', 'NNS', ')', '.']",17
natural_language_inference,9,210,L2 weight decay of 0.001 ( 0.0005 for QA 10 k ) is used for all weights .,"['L2', 'weight', 'decay', 'of', '0.001', '(', '0.0005', 'for', 'QA', '10', 'k', ')', 'is', 'used', 'for', 'all', 'weights', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['NNP', 'VBD', 'NN', 'IN', 'CD', '(', 'CD', 'IN', 'NNP', 'CD', 'NN', ')', 'VBZ', 'VBN', 'IN', 'DT', 'NNS', '.']",18
natural_language_inference,9,211,The loss function is the cross entropy between v and the one - hot vector of the true answer .,"['The', 'loss', 'function', 'is', 'the', 'cross', 'entropy', 'between', 'v', 'and', 'the', 'one', '-', 'hot', 'vector', 'of', 'the', 'true', 'answer', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'NN', 'CC', 'DT', 'CD', ':', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",20
natural_language_inference,9,212,"The loss is minimized by stochastic gradient descent for maximally 500 epochs , but training is early stopped if the loss on the development data does not decrease for 50 epochs .","['The', 'loss', 'is', 'minimized', 'by', 'stochastic', 'gradient', 'descent', 'for', 'maximally', '500', 'epochs', ',', 'but', 'training', 'is', 'early', 'stopped', 'if', 'the', 'loss', 'on', 'the', 'development', 'data', 'does', 'not', 'decrease', 'for', '50', 'epochs', '.']","['O', 'B-n', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', 'NN', 'NN', 'IN', 'RB', 'CD', 'NNS', ',', 'CC', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'RB', 'VB', 'IN', 'CD', 'NNS', '.']",32
natural_language_inference,9,213,The learning rate is controlled by AdaGrad with the initial learning rate of 0.5 ( 0.1 for QA 10 k ) .,"['The', 'learning', 'rate', 'is', 'controlled', 'by', 'AdaGrad', 'with', 'the', 'initial', 'learning', 'rate', 'of', '0.5', '(', '0.1', 'for', 'QA', '10', 'k', ')', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'NNP', 'IN', 'DT', 'JJ', 'VBG', 'NN', 'IN', 'CD', '(', 'CD', 'IN', 'NNP', 'CD', 'NN', ')', '.']",22
natural_language_inference,9,214,"Since the model is sensitive to the weight initialization , we repeat each training procedure 10 times ( 50 times for 10 k ) with the new random initialization of the weights and report the result on the test data with the lowest loss on the development data .","['Since', 'the', 'model', 'is', 'sensitive', 'to', 'the', 'weight', 'initialization', ',', 'we', 'repeat', 'each', 'training', 'procedure', '10', 'times', '(', '50', 'times', 'for', '10', 'k', ')', 'with', 'the', 'new', 'random', 'initialization', 'of', 'the', 'weights', 'and', 'report', 'the', 'result', 'on', 'the', 'test', 'data', 'with', 'the', 'lowest', 'loss', 'on', 'the', 'development', 'data', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'VBZ', 'JJ', 'TO', 'DT', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'CD', 'NNS', '(', 'CD', 'NNS', 'IN', 'CD', 'NNS', ')', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'DT', 'NNS', 'CC', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NN', 'NNS', '.']",49
natural_language_inference,9,217,"These include LSTM , End - to - end Memory Networks ( N2N ) , Dynamic Memory Networks ( DMN + ) , Gated End - to - end Memory Networks ( GMe m N2N ) , and Differentiable Neural Computer ( DNC ) .","['These', 'include', 'LSTM', ',', 'End', '-', 'to', '-', 'end', 'Memory', 'Networks', '(', 'N2N', ')', ',', 'Dynamic', 'Memory', 'Networks', '(', 'DMN', '+', ')', ',', 'Gated', 'End', '-', 'to', '-', 'end', 'Memory', 'Networks', '(', 'GMe', 'm', 'N2N', ')', ',', 'and', 'Differentiable', 'Neural', 'Computer', '(', 'DNC', ')', '.']","['O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBP', 'NNP', ',', 'NNP', ':', 'TO', ':', 'VB', 'NN', 'NNP', '(', 'NNP', ')', ',', 'NNP', 'NNP', 'NNP', '(', 'NNP', 'NNP', ')', ',', 'VBD', 'NNP', ':', 'TO', ':', 'VB', 'NN', 'NNP', '(', 'NNP', 'NNP', 'NNP', ')', ',', 'CC', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', '.']",45
natural_language_inference,9,220,"In 1 k data , QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ) outperforms all other models by a large margin ( 2.8 + % ) .","['In', '1', 'k', 'data', ',', 'QRN', ""'s"", ""'"", '2', ""r'"", '(', '2', 'layers', '+', 'reset', 'gate', '+', 'd', '=', '50', ')', 'outperforms', 'all', 'other', 'models', 'by', 'a', 'large', 'margin', '(', '2.8', '+', '%', ')', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'CD', 'NN', 'NNS', ',', 'NNP', 'POS', 'POS', 'CD', 'NN', '(', 'CD', 'NNS', 'JJ', 'JJ', 'NN', 'NNP', 'NN', 'NNP', 'CD', ')', 'NNS', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', '(', 'CD', 'NNP', 'NN', ')', '.']",35
natural_language_inference,9,221,"In 10 k dataset , the average accuracy of QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model outperforms all previous models by a large margin ( 2.5 + % ) , achieving a nearly perfect score of 99.7 % .","['In', '10', 'k', 'dataset', ',', 'the', 'average', 'accuracy', 'of', 'QRN', ""'s"", ""'"", '6r200', ""'"", '(', '6', 'layers', '+', 'reset', 'gate', '+', 'd', '=', '200', ')', 'model', 'outperforms', 'all', 'previous', 'models', 'by', 'a', 'large', 'margin', '(', '2.5', '+', '%', ')', ',', 'achieving', 'a', 'nearly', 'perfect', 'score', 'of', '99.7', '%', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'CD', 'NN', 'NN', ',', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'POS', 'POS', 'CD', 'POS', '(', 'CD', 'NNS', 'JJ', 'JJ', 'NN', 'NNP', 'NN', 'NNP', 'CD', ')', 'NN', 'VBZ', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', '(', 'CD', 'NNP', 'NN', ')', ',', 'VBG', 'DT', 'RB', 'JJ', 'NN', 'IN', 'CD', 'NN', '.']",49
natural_language_inference,9,225,QRN outperforms previous work by a large margin ( 2.0 + % ) in every comparison .,"['QRN', 'outperforms', 'previous', 'work', 'by', 'a', 'large', 'margin', '(', '2.0', '+', '%', ')', 'in', 'every', 'comparison', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NN', 'NNS', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', '(', 'CD', 'NNP', 'NN', ')', 'IN', 'DT', 'NN', '.']",17
natural_language_inference,9,229,"According to the ablation results , we infer that : ( a ) When the number of layers is only one , the model lacks reasoning capability .","['According', 'to', 'the', 'ablation', 'results', ',', 'we', 'infer', 'that', ':', '(', 'a', ')', 'When', 'the', 'number', 'of', 'layers', 'is', 'only', 'one', ',', 'the', 'model', 'lacks', 'reasoning', 'capability', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['VBG', 'TO', 'DT', 'NN', 'NNS', ',', 'PRP', 'VBP', 'IN', ':', '(', 'DT', ')', 'WRB', 'DT', 'NN', 'IN', 'NNS', 'VBZ', 'RB', 'CD', ',', 'DT', 'NN', 'VBZ', 'VBG', 'NN', '.']",28
natural_language_inference,9,230,"In the case of 1 k dataset , when there are too many layers ( 6 ) , it seems correctly training the model becomes increasingly difficult .","['In', 'the', 'case', 'of', '1', 'k', 'dataset', ',', 'when', 'there', 'are', 'too', 'many', 'layers', '(', '6', ')', ',', 'it', 'seems', 'correctly', 'training', 'the', 'model', 'becomes', 'increasingly', 'difficult', '.']","['B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'IN', 'CD', 'NN', 'NN', ',', 'WRB', 'EX', 'VBP', 'RB', 'JJ', 'NNS', '(', 'CD', ')', ',', 'PRP', 'VBZ', 'RB', 'VBG', 'DT', 'NN', 'VBZ', 'RB', 'JJ', '.']",28
natural_language_inference,9,231,"In the case of 10 k dataset , many layers ( 6 ) and hidden dimensions ( 200 ) helps reasoning , most notably in difficult task such as task 16 .","['In', 'the', 'case', 'of', '10', 'k', 'dataset', ',', 'many', 'layers', '(', '6', ')', 'and', 'hidden', 'dimensions', '(', '200', ')', 'helps', 'reasoning', ',', 'most', 'notably', 'in', 'difficult', 'task', 'such', 'as', 'task', '16', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'IN', 'CD', 'NN', 'NN', ',', 'JJ', 'NNS', '(', 'CD', ')', 'CC', 'JJ', 'NNS', '(', 'CD', ')', 'VBZ', 'VBG', ',', 'JJS', 'RB', 'IN', 'JJ', 'NN', 'JJ', 'IN', 'NN', 'CD', '.']",32
natural_language_inference,9,232,( b ) Adding the reset gate helps .,"['(', 'b', ')', 'Adding', 'the', 'reset', 'gate', 'helps', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O']","['(', 'NN', ')', 'VBG', 'DT', 'NN', 'NN', 'VBZ', '.']",9
natural_language_inference,9,233,"( c ) Including vector gates hurts in 1 k datasets , as the model either overfits to the training data or converges to local minima .","['(', 'c', ')', 'Including', 'vector', 'gates', 'hurts', 'in', '1', 'k', 'datasets', ',', 'as', 'the', 'model', 'either', 'overfits', 'to', 'the', 'training', 'data', 'or', 'converges', 'to', 'local', 'minima', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'NN', ')', 'VBG', 'NN', 'NNS', 'VBZ', 'IN', 'CD', 'NN', 'NNS', ',', 'IN', 'DT', 'NN', 'CC', 'NNS', 'TO', 'DT', 'NN', 'NNS', 'CC', 'NNS', 'TO', 'JJ', 'NN', '.']",27
natural_language_inference,9,234,"On the other hand , vector gates in bAbI story - based QA 10 k dataset sometimes help .","['On', 'the', 'other', 'hand', ',', 'vector', 'gates', 'in', 'bAbI', 'story', '-', 'based', 'QA', '10', 'k', 'dataset', 'sometimes', 'help', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'NN', 'NNS', 'IN', 'JJ', 'NN', ':', 'VBN', 'NNP', 'CD', 'NN', 'NN', 'RB', 'NN', '.']",19
natural_language_inference,9,235,"( d ) Increasing the dimension of the hidden state to 100 in the dialog 's Task 6 ( DSTC2 ) helps , while there is not much improvement in the dialog 's Task 1 - 5 .","['(', 'd', ')', 'Increasing', 'the', 'dimension', 'of', 'the', 'hidden', 'state', 'to', '100', 'in', 'the', 'dialog', ""'s"", 'Task', '6', '(', 'DSTC2', ')', 'helps', ',', 'while', 'there', 'is', 'not', 'much', 'improvement', 'in', 'the', 'dialog', ""'s"", 'Task', '1', '-', '5', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'NN', ')', 'VBG', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'TO', 'CD', 'IN', 'DT', 'NN', 'POS', 'NNP', 'CD', '(', 'NNP', ')', 'VBZ', ',', 'IN', 'EX', 'VBZ', 'RB', 'JJ', 'NN', 'IN', 'DT', 'NN', 'POS', 'NNP', 'CD', ':', 'CD', '.']",38
natural_language_inference,9,236,It can be hypothesized that a larger hidden state is required for real data . Parallelization .,"['It', 'can', 'be', 'hypothesized', 'that', 'a', 'larger', 'hidden', 'state', 'is', 'required', 'for', 'real', 'data', '.', 'Parallelization', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O']","['PRP', 'MD', 'VB', 'VBN', 'IN', 'DT', 'JJR', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', 'NNS', '.', 'NN', '.']",17
natural_language_inference,25,2,Contextualized Word Representations for Reading Comprehension,"['Contextualized', 'Word', 'Representations', 'for', 'Reading', 'Comprehension']","['O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",6
natural_language_inference,25,8,Reading comprehension ( RC ) is a high - level task in natural language understanding that requires reading a document and answering questions about its content .,"['Reading', 'comprehension', '(', 'RC', ')', 'is', 'a', 'high', '-', 'level', 'task', 'in', 'natural', 'language', 'understanding', 'that', 'requires', 'reading', 'a', 'document', 'and', 'answering', 'questions', 'about', 'its', 'content', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NN', '(', 'NNP', ')', 'VBZ', 'DT', 'JJ', ':', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'VBG', 'DT', 'NN', 'CC', 'VBG', 'NNS', 'IN', 'PRP$', 'NN', '.']",27
natural_language_inference,25,9,"RC has attracted substantial attention over the last few years with the advent of large annotated datasets , computing resources , and neural network models and optimization procedures .","['RC', 'has', 'attracted', 'substantial', 'attention', 'over', 'the', 'last', 'few', 'years', 'with', 'the', 'advent', 'of', 'large', 'annotated', 'datasets', ',', 'computing', 'resources', ',', 'and', 'neural', 'network', 'models', 'and', 'optimization', 'procedures', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'VBN', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'JJ', 'JJ', 'NNS', ',', 'VBG', 'NNS', ',', 'CC', 'JJ', 'NN', 'NNS', 'CC', 'NN', 'NNS', '.']",29
natural_language_inference,25,14,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a module that produces token embeddings by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .","['To', 'illustrate', 'this', 'idea', ',', 'we', 'take', 'a', 'model', 'that', 'carries', 'out', 'only', 'basic', 'question', '-', 'document', 'interaction', 'and', 'prepend', 'to', 'it', 'a', 'module', 'that', 'produces', 'token', 'embeddings', 'by', 'explicitly', 'gating', 'between', 'contextual', 'and', 'non-contextual', 'representations', '(', 'for', 'both', 'the', 'document', 'and', 'question', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'WDT', 'VBZ', 'RP', 'RB', 'JJ', 'NN', ':', 'NN', 'NN', 'CC', 'NN', 'TO', 'PRP', 'DT', 'NN', 'WDT', 'VBZ', 'JJ', 'NNS', 'IN', 'RB', 'VBG', 'IN', 'JJ', 'CC', 'JJ', 'NNS', '(', 'IN', 'DT', 'DT', 'NN', 'CC', 'NN', ')', '.']",45
natural_language_inference,25,16,"Motivated by these findings , we turn to a semisupervised setting in which we leverage a language model , pre-trained on large amounts of data , as a sequence encoder which forcibly facilitates context utilization .","['Motivated', 'by', 'these', 'findings', ',', 'we', 'turn', 'to', 'a', 'semisupervised', 'setting', 'in', 'which', 'we', 'leverage', 'a', 'language', 'model', ',', 'pre-trained', 'on', 'large', 'amounts', 'of', 'data', ',', 'as', 'a', 'sequence', 'encoder', 'which', 'forcibly', 'facilitates', 'context', 'utilization', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'NNS', ',', 'PRP', 'VBP', 'TO', 'DT', 'JJ', 'NN', 'IN', 'WDT', 'PRP', 'VBP', 'DT', 'NN', 'NN', ',', 'JJ', 'IN', 'JJ', 'NNS', 'IN', 'NNS', ',', 'IN', 'DT', 'NN', 'NN', 'WDT', 'RB', 'VBZ', 'JJ', 'NN', '.']",36
natural_language_inference,25,68,"In we compare these two variants over the development set and observe superior performance by the contextual one , illustrating the benefit of contextualization and specifically per-sequence contextualization which is done separately for the question and for the passage .","['In', 'we', 'compare', 'these', 'two', 'variants', 'over', 'the', 'development', 'set', 'and', 'observe', 'superior', 'performance', 'by', 'the', 'contextual', 'one', ',', 'illustrating', 'the', 'benefit', 'of', 'contextualization', 'and', 'specifically', 'per-sequence', 'contextualization', 'which', 'is', 'done', 'separately', 'for', 'the', 'question', 'and', 'for', 'the', 'passage', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PRP', 'VBP', 'DT', 'CD', 'NNS', 'IN', 'DT', 'NN', 'NN', 'CC', 'VB', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'CD', ',', 'VBG', 'DT', 'NN', 'IN', 'NN', 'CC', 'RB', 'NN', 'NN', 'WDT', 'VBZ', 'VBN', 'RB', 'IN', 'DT', 'NN', 'CC', 'IN', 'DT', 'NN', '.']",40
natural_language_inference,25,76,"On average , the less frequent a word - type is , the smaller are its gate activations , i.e. , the reembedded representation of a rare word places less weight on its fixed word - embedding and more on its contextual representation , compared to a common word .","['On', 'average', ',', 'the', 'less', 'frequent', 'a', 'word', '-', 'type', 'is', ',', 'the', 'smaller', 'are', 'its', 'gate', 'activations', ',', 'i.e.', ',', 'the', 'reembedded', 'representation', 'of', 'a', 'rare', 'word', 'places', 'less', 'weight', 'on', 'its', 'fixed', 'word', '-', 'embedding', 'and', 'more', 'on', 'its', 'contextual', 'representation', ',', 'compared', 'to', 'a', 'common', 'word', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ',', 'DT', 'FW', 'JJ', 'DT', 'NN', ':', 'NN', 'VBZ', ',', 'DT', 'JJR', 'VBP', 'PRP$', 'JJ', 'NNS', ',', 'NN', ',', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNS', 'CC', 'NN', 'IN', 'PRP$', 'VBN', 'NN', ':', 'NN', 'CC', 'JJR', 'IN', 'PRP$', 'JJ', 'NN', ',', 'VBN', 'TO', 'DT', 'JJ', 'NN', '.']",50
natural_language_inference,25,80,Supplementing the calculation of token reembeddings with the hidden states of a strong language model proves to be highly effective .,"['Supplementing', 'the', 'calculation', 'of', 'token', 'reembeddings', 'with', 'the', 'hidden', 'states', 'of', 'a', 'strong', 'language', 'model', 'proves', 'to', 'be', 'highly', 'effective', '.']","['B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['VBG', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'TO', 'VB', 'RB', 'JJ', '.']",21
natural_language_inference,25,83,"Overall , we observe a significant improvement with all three configurations , effectively showing the benefit of training a QA model in a semisupervised fashion with a large language model .","['Overall', ',', 'we', 'observe', 'a', 'significant', 'improvement', 'with', 'all', 'three', 'configurations', ',', 'effectively', 'showing', 'the', 'benefit', 'of', 'training', 'a', 'QA', 'model', 'in', 'a', 'semisupervised', 'fashion', 'with', 'a', 'large', 'language', 'model', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['JJ', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'CD', 'NNS', ',', 'RB', 'VBG', 'DT', 'NN', 'IN', 'VBG', 'DT', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",31
natural_language_inference,25,84,"Besides a crosscutting boost in results , we note that the performance due to utilizing the LM hidden states of the first LSTM layer significantly surpasses the other two variants .","['Besides', 'a', 'crosscutting', 'boost', 'in', 'results', ',', 'we', 'note', 'that', 'the', 'performance', 'due', 'to', 'utilizing', 'the', 'LM', 'hidden', 'states', 'of', 'the', 'first', 'LSTM', 'layer', 'significantly', 'surpasses', 'the', 'other', 'two', 'variants', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'VBG', 'NN', 'IN', 'NNS', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'JJ', 'TO', 'VBG', 'DT', 'NNP', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NNP', 'NN', 'RB', 'VBZ', 'DT', 'JJ', 'CD', 'NNS', '.']",31
natural_language_inference,25,90,We use pre-trained GloVe embeddings of dimension d w = 300 and produce character - based word representations via dc = 100 convolutional filters over character embeddings as in .,"['We', 'use', 'pre-trained', 'GloVe', 'embeddings', 'of', 'dimension', 'd', 'w', '=', '300', 'and', 'produce', 'character', '-', 'based', 'word', 'representations', 'via', 'dc', '=', '100', 'convolutional', 'filters', 'over', 'character', 'embeddings', 'as', 'in', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', 'NNP', 'NNS', 'IN', 'NN', 'NN', 'VBD', 'JJ', 'CD', 'CC', 'VB', 'JJR', ':', 'VBN', 'NN', 'NNS', 'IN', 'NN', '$', 'CD', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'IN', '.']",30
natural_language_inference,98,2,Recurrent Neural Network - Based Sentence Encoder with Gated Attention for Natural Language Inference,"['Recurrent', 'Neural', 'Network', '-', 'Based', 'Sentence', 'Encoder', 'with', 'Gated', 'Attention', 'for', 'Natural', 'Language', 'Inference']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', ':', 'VBD', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",14
natural_language_inference,98,14,"Task aims to evaluate language understanding models for sentence representation with natural language inference ( NLI ) tasks , where a sentence is represented as a fixedlength vector .","['Task', 'aims', 'to', 'evaluate', 'language', 'understanding', 'models', 'for', 'sentence', 'representation', 'with', 'natural', 'language', 'inference', '(', 'NLI', ')', 'tasks', ',', 'where', 'a', 'sentence', 'is', 'represented', 'as', 'a', 'fixedlength', 'vector', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NNS', 'TO', 'VB', 'NN', 'JJ', 'NNS', 'IN', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'NNS', ',', 'WRB', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",29
natural_language_inference,98,16,"Specifically , NLI is concerned with determining whether a hypothesis sentence h can be inferred from a premise sentence p.","['Specifically', ',', 'NLI', 'is', 'concerned', 'with', 'determining', 'whether', 'a', 'hypothesis', 'sentence', 'h', 'can', 'be', 'inferred', 'from', 'a', 'premise', 'sentence', 'p.']","['O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'NNP', 'VBZ', 'VBN', 'IN', 'VBG', 'IN', 'DT', 'NN', 'NN', 'NN', 'MD', 'VB', 'VBN', 'IN', 'DT', 'NN', 'NN', 'NN']",20
natural_language_inference,98,29,"We present here the proposed natural language inference networks which are composed of the following major components : word embedding , sequence encoder , composition layer , and the toplayer classifier .","['We', 'present', 'here', 'the', 'proposed', 'natural', 'language', 'inference', 'networks', 'which', 'are', 'composed', 'of', 'the', 'following', 'major', 'components', ':', 'word', 'embedding', ',', 'sequence', 'encoder', ',', 'composition', 'layer', ',', 'and', 'the', 'toplayer', 'classifier', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'RB', 'DT', 'VBN', 'JJ', 'NN', 'NN', 'NNS', 'WDT', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'JJ', 'NNS', ':', 'NN', 'NN', ',', 'NN', 'NN', ',', 'NN', 'NN', ',', 'CC', 'DT', 'NN', 'NN', '.']",32
natural_language_inference,98,31,Word Embedding,"['Word', 'Embedding']","['B-n', 'I-n']","['NNP', 'NNP']",2
natural_language_inference,98,33,We concatenate embeddings learned at two different levels to represent each word in the sentence : the character composition and holistic word - level embedding .,"['We', 'concatenate', 'embeddings', 'learned', 'at', 'two', 'different', 'levels', 'to', 'represent', 'each', 'word', 'in', 'the', 'sentence', ':', 'the', 'character', 'composition', 'and', 'holistic', 'word', '-', 'level', 'embedding', '.']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNS', 'VBN', 'IN', 'CD', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', ':', 'DT', 'NN', 'NN', 'CC', 'JJ', 'NN', ':', 'NN', 'NN', '.']",26
natural_language_inference,98,40,Sequence Encoder,"['Sequence', 'Encoder']","['B-n', 'I-n']","['NN', 'NN']",2
natural_language_inference,98,41,"To represent words and their context in a premise and hypothesis , sentence pairs are fed into sentence encoders to obtain hidden vectors ( h p and h h ) .","['To', 'represent', 'words', 'and', 'their', 'context', 'in', 'a', 'premise', 'and', 'hypothesis', ',', 'sentence', 'pairs', 'are', 'fed', 'into', 'sentence', 'encoders', 'to', 'obtain', 'hidden', 'vectors', '(', 'h', 'p', 'and', 'h', 'h', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'NNS', 'CC', 'PRP$', 'NN', 'IN', 'DT', 'NN', 'CC', 'NN', ',', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'NN', 'NNS', 'TO', 'VB', 'JJ', 'NNS', '(', 'JJ', 'NN', 'CC', 'NN', 'NN', ')', '.']",31
natural_language_inference,98,42,We use stacked bidirectional LSTMs ( BiL - STM ) as the encoders .,"['We', 'use', 'stacked', 'bidirectional', 'LSTMs', '(', 'BiL', '-', 'STM', ')', 'as', 'the', 'encoders', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'JJ', 'NNP', '(', 'NNP', ':', 'NN', ')', 'IN', 'DT', 'NNS', '.']",14
natural_language_inference,98,53,Composition Layer,"['Composition', 'Layer']","['B-n', 'I-n']","['NNP', 'NNP']",2
natural_language_inference,98,54,"To transform sentences into fixed - length vector representations and reason using those representations , we need to compose the hidden vectors obtained by the sequence encoder layer ( h p and h h ) .","['To', 'transform', 'sentences', 'into', 'fixed', '-', 'length', 'vector', 'representations', 'and', 'reason', 'using', 'those', 'representations', ',', 'we', 'need', 'to', 'compose', 'the', 'hidden', 'vectors', 'obtained', 'by', 'the', 'sequence', 'encoder', 'layer', '(', 'h', 'p', 'and', 'h', 'h', ')', '.']","['B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'NNS', 'IN', 'VBN', ':', 'NN', 'NN', 'NNS', 'CC', 'NN', 'VBG', 'DT', 'NNS', ',', 'PRP', 'VBP', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'NN', 'NN', '(', 'JJ', 'NN', 'CC', 'NN', 'NN', ')', '.']",36
natural_language_inference,98,55,We propose intra-sentence gated - attention to obtain a fixed - length vector .,"['We', 'propose', 'intra-sentence', 'gated', '-', 'attention', 'to', 'obtain', 'a', 'fixed', '-', 'length', 'vector', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'VBN', ':', 'NN', 'TO', 'VB', 'DT', 'JJ', ':', 'NN', 'NN', '.']",14
natural_language_inference,98,67,Top - layer Classifier,"['Top', '-', 'layer', 'Classifier']","['B-n', 'I-n', 'I-n', 'I-n']","['JJ', ':', 'NN', 'NNP']",4
natural_language_inference,98,68,Our inference model feeds the resulting vectors obtained above to the final classifier to determine the over all inference relationship .,"['Our', 'inference', 'model', 'feeds', 'the', 'resulting', 'vectors', 'obtained', 'above', 'to', 'the', 'final', 'classifier', 'to', 'determine', 'the', 'over', 'all', 'inference', 'relationship', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'NN', 'VBZ', 'DT', 'VBG', 'NNS', 'VBN', 'IN', 'TO', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'IN', 'DT', 'NN', 'NN', '.']",21
natural_language_inference,98,82,"To help replicate our results , we publish our code at https : //github.com/lukecq1231/enc_nli","['To', 'help', 'replicate', 'our', 'results', ',', 'we', 'publish', 'our', 'code', 'at', 'https', ':', '//github.com/lukecq1231/enc_nli']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['TO', 'VB', 'VB', 'PRP$', 'NNS', ',', 'PRP', 'VBP', 'PRP$', 'NN', 'IN', 'NN', ':', 'NN']",14
natural_language_inference,98,84,"We use the Adam ( Kingma and Ba , 2014 ) for optimization .","['We', 'use', 'the', 'Adam', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', 'for', 'optimization', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'IN', 'NN', '.']",14
natural_language_inference,98,85,"Stacked BiLSTM has 3 layers , and all hidden states of BiLSTMs and MLP have 600 dimensions .","['Stacked', 'BiLSTM', 'has', '3', 'layers', ',', 'and', 'all', 'hidden', 'states', 'of', 'BiLSTMs', 'and', 'MLP', 'have', '600', 'dimensions', '.']","['B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['VBN', 'NNP', 'VBZ', 'CD', 'NNS', ',', 'CC', 'DT', 'JJ', 'NNS', 'IN', 'NNP', 'CC', 'NNP', 'VBP', 'CD', 'NNS', '.']",18
natural_language_inference,98,86,"The character embedding has 15 dimensions , and CNN filters length is [ 1 , 3 , 5 ] , each of those is 100 dimensions .","['The', 'character', 'embedding', 'has', '15', 'dimensions', ',', 'and', 'CNN', 'filters', 'length', 'is', '[', '1', ',', '3', ',', '5', ']', ',', 'each', 'of', 'those', 'is', '100', 'dimensions', '.']","['O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'CD', 'NNS', ',', 'CC', 'NNP', 'NNS', 'VBP', 'VBZ', 'JJ', 'CD', ',', 'CD', ',', 'CD', 'NN', ',', 'DT', 'IN', 'DT', 'VBZ', 'CD', 'NNS', '.']",27
natural_language_inference,98,87,We use pretrained GloVe - 840B - 300D vectors as our word - level embeddings and fix these embeddings during the training process .,"['We', 'use', 'pretrained', 'GloVe', '-', '840B', '-', '300D', 'vectors', 'as', 'our', 'word', '-', 'level', 'embeddings', 'and', 'fix', 'these', 'embeddings', 'during', 'the', 'training', 'process', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'VBN', 'NNP', ':', 'CD', ':', 'CD', 'NNS', 'IN', 'PRP$', 'NN', ':', 'NN', 'NNS', 'CC', 'VB', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",24
natural_language_inference,98,88,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,"['Out', '-', 'of', '-', 'vocabulary', '(', 'OOV', ')', 'words', 'are', 'initialized', 'randomly', 'with', 'Gaussian', 'samples', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', ':', 'IN', ':', 'NN', '(', 'NNP', ')', 'NNS', 'VBP', 'VBN', 'RB', 'IN', 'JJ', 'NNS', '.']",16
natural_language_inference,98,95,"In addition , we also use our implementation of ESIM , which achieves an accuracy of 76.8 % in the in - domain test set , and 75.8 % in the cross - domain test set , which presents the state - of - the - art results .","['In', 'addition', ',', 'we', 'also', 'use', 'our', 'implementation', 'of', 'ESIM', ',', 'which', 'achieves', 'an', 'accuracy', 'of', '76.8', '%', 'in', 'the', 'in', '-', 'domain', 'test', 'set', ',', 'and', '75.8', '%', 'in', 'the', 'cross', '-', 'domain', 'test', 'set', ',', 'which', 'presents', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'RB', 'VBP', 'PRP$', 'NN', 'IN', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', 'IN', 'DT', 'IN', ':', 'NN', 'NN', 'NN', ',', 'CC', 'CD', 'NN', 'IN', 'DT', 'NN', ':', 'NN', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', '.']",49
natural_language_inference,98,96,"After removing the cross - sentence attention and adding our gated - attention model , we achieve accuracies of 73.5 % and 73.6 % , which ranks first in the cross - domain test set and ranks second in the in - domain test set among the single models .","['After', 'removing', 'the', 'cross', '-', 'sentence', 'attention', 'and', 'adding', 'our', 'gated', '-', 'attention', 'model', ',', 'we', 'achieve', 'accuracies', 'of', '73.5', '%', 'and', '73.6', '%', ',', 'which', 'ranks', 'first', 'in', 'the', 'cross', '-', 'domain', 'test', 'set', 'and', 'ranks', 'second', 'in', 'the', 'in', '-', 'domain', 'test', 'set', 'among', 'the', 'single', 'models', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'VBG', 'DT', 'NN', ':', 'NN', 'NN', 'CC', 'VBG', 'PRP$', 'VBN', ':', 'NN', 'NN', ',', 'PRP', 'VBP', 'NNS', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', ',', 'WDT', 'VBZ', 'RB', 'IN', 'DT', 'NN', ':', 'NN', 'NN', 'NN', 'CC', 'NNS', 'VBP', 'IN', 'DT', 'IN', ':', 'NN', 'NN', 'VBN', 'IN', 'DT', 'JJ', 'NNS', '.']",50
natural_language_inference,98,97,"When ensembling our models , we obtain accuracies 74.9 % and 74.9 % , which ranks first in both test sets .","['When', 'ensembling', 'our', 'models', ',', 'we', 'obtain', 'accuracies', '74.9', '%', 'and', '74.9', '%', ',', 'which', 'ranks', 'first', 'in', 'both', 'test', 'sets', '.']","['B-p', 'B-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'B-p', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['WRB', 'VBG', 'PRP$', 'NNS', ',', 'PRP', 'VB', 'NNS', 'CD', 'NN', 'CC', 'CD', 'NN', ',', 'WDT', 'VBZ', 'RB', 'IN', 'DT', 'NN', 'NNS', '.']",22
natural_language_inference,98,102,"If we remove the gated - attention , the accuracies drop to 72.8 % and 73.6 % .","['If', 'we', 'remove', 'the', 'gated', '-', 'attention', ',', 'the', 'accuracies', 'drop', 'to', '72.8', '%', 'and', '73.6', '%', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'PRP', 'VBP', 'DT', 'JJ', ':', 'NN', ',', 'DT', 'NNS', 'NN', 'TO', 'CD', 'NN', 'CC', 'CD', 'NN', '.']",18
natural_language_inference,98,103,"If we remove charactercomposition vector , the accuracies drop to 72.9 % and 73.5 % .","['If', 'we', 'remove', 'charactercomposition', 'vector', ',', 'the', 'accuracies', 'drop', 'to', '72.9', '%', 'and', '73.5', '%', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'PRP', 'VBP', 'NN', 'NN', ',', 'DT', 'NNS', 'NN', 'TO', 'CD', 'NN', 'CC', 'CD', 'NN', '.']",16
natural_language_inference,98,104,"If we remove word - level embedding , the accuracies drop to 65.6 % and 66.0 % .","['If', 'we', 'remove', 'word', '-', 'level', 'embedding', ',', 'the', 'accuracies', 'drop', 'to', '65.6', '%', 'and', '66.0', '%', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'PRP', 'VBP', 'NN', ':', 'NN', 'NN', ',', 'DT', 'NNS', 'NN', 'TO', 'CD', 'NN', 'CC', 'CD', 'NN', '.']",18
natural_language_inference,90,2,A Decomposable Attention Model for Natural Language Inference,"['A', 'Decomposable', 'Attention', 'Model', 'for', 'Natural', 'Language', 'Inference']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['DT', 'JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",8
natural_language_inference,90,9,Natural language inference ( NLI ) refers to the problem of determining entailment and contradiction relationships between a premise and a hypothesis .,"['Natural', 'language', 'inference', '(', 'NLI', ')', 'refers', 'to', 'the', 'problem', 'of', 'determining', 'entailment', 'and', 'contradiction', 'relationships', 'between', 'a', 'premise', 'and', 'a', 'hypothesis', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', '(', 'NNP', ')', 'NNS', 'TO', 'DT', 'NN', 'IN', 'VBG', 'NN', 'CC', 'NN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'DT', 'NN', '.']",23
natural_language_inference,90,10,NLI is a central problem in language understanding ) and recently the large SNLI corpus of 570K sentence pairs was created for this task .,"['NLI', 'is', 'a', 'central', 'problem', 'in', 'language', 'understanding', ')', 'and', 'recently', 'the', 'large', 'SNLI', 'corpus', 'of', '570K', 'sentence', 'pairs', 'was', 'created', 'for', 'this', 'task', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NN', ')', 'CC', 'RB', 'DT', 'JJ', 'NNP', 'NN', 'IN', 'CD', 'NN', 'NN', 'VBD', 'VBN', 'IN', 'DT', 'NN', '.']",25
natural_language_inference,90,24,"In contrast to existing approaches , our approach only relies on alignment and is fully computationally decomposable with respect to the input text .","['In', 'contrast', 'to', 'existing', 'approaches', ',', 'our', 'approach', 'only', 'relies', 'on', 'alignment', 'and', 'is', 'fully', 'computationally', 'decomposable', 'with', 'respect', 'to', 'the', 'input', 'text', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'VBG', 'NNS', ',', 'PRP$', 'NN', 'RB', 'VBZ', 'IN', 'NN', 'CC', 'VBZ', 'RB', 'RB', 'JJ', 'IN', 'NN', 'TO', 'DT', 'NN', 'NN', '.']",24
natural_language_inference,90,26,"Given two sentences , where each word is repre-sented by an embedding vector , we first create a soft alignment matrix using neural attention .","['Given', 'two', 'sentences', ',', 'where', 'each', 'word', 'is', 'repre-sented', 'by', 'an', 'embedding', 'vector', ',', 'we', 'first', 'create', 'a', 'soft', 'alignment', 'matrix', 'using', 'neural', 'attention', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['VBN', 'CD', 'NNS', ',', 'WRB', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'NN', ',', 'PRP', 'RB', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'VBG', 'JJ', 'NN', '.']",25
natural_language_inference,90,27,We then use the ( soft ) alignment to decompose the task into subproblems that are solved separately .,"['We', 'then', 'use', 'the', '(', 'soft', ')', 'alignment', 'to', 'decompose', 'the', 'task', 'into', 'subproblems', 'that', 'are', 'solved', 'separately', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', '(', 'JJ', ')', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNS', 'WDT', 'VBP', 'VBN', 'RB', '.']",19
natural_language_inference,90,28,"Finally , the results of these subproblems are merged to produce the final classification .","['Finally', ',', 'the', 'results', 'of', 'these', 'subproblems', 'are', 'merged', 'to', 'produce', 'the', 'final', 'classification', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'DT', 'NNS', 'IN', 'DT', 'NNS', 'VBP', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NN', '.']",15
natural_language_inference,90,29,"In addition , we optionally apply intra-sentence attention to endow the model with a richer encoding of substructures prior to the alignment step .","['In', 'addition', ',', 'we', 'optionally', 'apply', 'intra-sentence', 'attention', 'to', 'endow', 'the', 'model', 'with', 'a', 'richer', 'encoding', 'of', 'substructures', 'prior', 'to', 'the', 'alignment', 'step', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'RB', 'VBP', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJR', 'NN', 'IN', 'NNS', 'RB', 'TO', 'DT', 'JJ', 'NN', '.']",24
natural_language_inference,90,112,The method was implemented in TensorFlow .,"['The', 'method', 'was', 'implemented', 'in', 'TensorFlow', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'VBD', 'VBN', 'IN', 'NNP', '.']",7
natural_language_inference,90,119,We use 300 dimensional GloVe embeddings to represent words .,"['We', 'use', '300', 'dimensional', 'GloVe', 'embeddings', 'to', 'represent', 'words', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'CD', 'JJ', 'NNP', 'NNS', 'TO', 'VB', 'NNS', '.']",10
natural_language_inference,90,120,"Each embedding vector was normalized to have 2 norm of 1 and projected down to 200 dimensions , a number determined via hyperparameter tuning .","['Each', 'embedding', 'vector', 'was', 'normalized', 'to', 'have', '2', 'norm', 'of', '1', 'and', 'projected', 'down', 'to', '200', 'dimensions', ',', 'a', 'number', 'determined', 'via', 'hyperparameter', 'tuning', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'VBG', 'NN', 'VBD', 'VBN', 'TO', 'VB', 'CD', 'NN', 'IN', 'CD', 'CC', 'VBD', 'RB', 'TO', 'CD', 'NNS', ',', 'DT', 'NN', 'VBN', 'IN', 'NN', 'NN', '.']",25
natural_language_inference,90,121,Out - of - vocabulary ( OOV ) words are hashed to one of 100 random embeddings each initialized to mean 0 and standard deviation 1 .,"['Out', '-', 'of', '-', 'vocabulary', '(', 'OOV', ')', 'words', 'are', 'hashed', 'to', 'one', 'of', '100', 'random', 'embeddings', 'each', 'initialized', 'to', 'mean', '0', 'and', 'standard', 'deviation', '1', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', ':', 'IN', ':', 'NN', '(', 'NNP', ')', 'NNS', 'VBP', 'VBN', 'TO', 'CD', 'IN', 'CD', 'JJ', 'NNS', 'DT', 'VBN', 'TO', 'VB', 'CD', 'CC', 'JJ', 'NN', 'CD', '.']",27
natural_language_inference,90,123,All other parameter weights ( hidden layers etc. ) were initialized from random Gaussians with mean 0 and standard deviation 0.01 .,"['All', 'other', 'parameter', 'weights', '(', 'hidden', 'layers', 'etc.', ')', 'were', 'initialized', 'from', 'random', 'Gaussians', 'with', 'mean', '0', 'and', 'standard', 'deviation', '0.01', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NNS', '(', 'JJ', 'NNS', 'VBP', ')', 'VBD', 'VBN', 'IN', 'JJ', 'NNPS', 'IN', 'JJ', 'CD', 'CC', 'JJ', 'NN', 'CD', '.']",22
natural_language_inference,90,124,"Each hyperparameter setting was run on a single machine with 10 asynchronous gradient - update threads , using Adagrad for optimization with the default initial accumulator value of 0.1 .","['Each', 'hyperparameter', 'setting', 'was', 'run', 'on', 'a', 'single', 'machine', 'with', '10', 'asynchronous', 'gradient', '-', 'update', 'threads', ',', 'using', 'Adagrad', 'for', 'optimization', 'with', 'the', 'default', 'initial', 'accumulator', 'value', 'of', '0.1', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'VBG', 'VBD', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', 'JJ', 'NN', ':', 'JJ', 'NNS', ',', 'VBG', 'NNP', 'IN', 'NN', 'IN', 'DT', 'NN', 'JJ', 'NN', 'NN', 'IN', 'CD', '.']",30
natural_language_inference,90,125,"Dropout regularization was used for all ReLU layers , but not for the final linear layer .","['Dropout', 'regularization', 'was', 'used', 'for', 'all', 'ReLU', 'layers', ',', 'but', 'not', 'for', 'the', 'final', 'linear', 'layer', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'VBD', 'VBN', 'IN', 'DT', 'NNP', 'NNS', ',', 'CC', 'RB', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",17
natural_language_inference,90,126,"We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( 0.2 ) and learning rate ( 0.05 - vanilla , 0.025 - intra-attention ) .","['We', 'additionally', 'tuned', 'the', 'following', 'hyperparameters', 'and', 'present', 'their', 'chosen', 'values', 'in', ',', '1', 'dropout', 'ratio', '(', '0.2', ')', 'and', 'learning', 'rate', '(', '0.05', '-', 'vanilla', ',', '0.025', '-', 'intra-attention', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'O']","['PRP', 'RB', 'VBD', 'DT', 'JJ', 'NNS', 'CC', 'JJ', 'PRP$', 'NN', 'NNS', 'IN', ',', 'CD', 'NN', 'NN', '(', 'CD', ')', 'CC', 'VBG', 'NN', '(', 'CD', ':', 'NN', ',', 'CD', ':', 'NN', ')', '.']",32
natural_language_inference,90,130,Our vanilla approach achieves state - of - theart results with almost an order of magnitude fewer parameters than the LSTMN of .,"['Our', 'vanilla', 'approach', 'achieves', 'state', '-', 'of', '-', 'theart', 'results', 'with', 'almost', 'an', 'order', 'of', 'magnitude', 'fewer', 'parameters', 'than', 'the', 'LSTMN', 'of', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O']","['PRP$', 'NN', 'NN', 'VBZ', 'NN', ':', 'IN', ':', 'NN', 'NNS', 'IN', 'RB', 'DT', 'NN', 'IN', 'NN', 'JJR', 'NNS', 'IN', 'DT', 'NNP', 'IN', '.']",23
natural_language_inference,90,131,Adding intra-sentence attention gives a considerable improvement of 0.5 percentage points over the existing state of the art .,"['Adding', 'intra-sentence', 'attention', 'gives', 'a', 'considerable', 'improvement', 'of', '0.5', 'percentage', 'points', 'over', 'the', 'existing', 'state', 'of', 'the', 'art', '.']","['B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NN', 'NNS', 'IN', 'DT', 'VBG', 'NN', 'IN', 'DT', 'NN', '.']",19
natural_language_inference,68,2,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,"['MACHINE', 'COMPREHENSION', 'USING', 'MATCH', '-', 'LSTM', 'AND', 'ANSWER', 'POINTER']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'NNP']",9
natural_language_inference,68,4,Machine comprehension of text is an important problem in natural language processing .,"['Machine', 'comprehension', 'of', 'text', 'is', 'an', 'important', 'problem', 'in', 'natural', 'language', 'processing', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'IN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",13
natural_language_inference,68,39,"Specifically , observing that in the SQuAD dataset many questions are paraphrases of sentences from the original text , we adopt a match - LSTM model that we developed earlier for textual entailment .","['Specifically', ',', 'observing', 'that', 'in', 'the', 'SQuAD', 'dataset', 'many', 'questions', 'are', 'paraphrases', 'of', 'sentences', 'from', 'the', 'original', 'text', ',', 'we', 'adopt', 'a', 'match', '-', 'LSTM', 'model', 'that', 'we', 'developed', 'earlier', 'for', 'textual', 'entailment', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'VBG', 'IN', 'IN', 'DT', 'NNP', 'VBD', 'JJ', 'NNS', 'VBP', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', ':', 'NNP', 'NN', 'IN', 'PRP', 'VBD', 'JJR', 'IN', 'JJ', 'NN', '.']",34
natural_language_inference,68,40,"We further adopt the Pointer Net ( Ptr - Net ) model developed by , which enables the predictions of tokens from the input sequence only rather than from a larger fixed vocabulary and thus allows us to generate answers that consist of multiple tokens from the original text .","['We', 'further', 'adopt', 'the', 'Pointer', 'Net', '(', 'Ptr', '-', 'Net', ')', 'model', 'developed', 'by', ',', 'which', 'enables', 'the', 'predictions', 'of', 'tokens', 'from', 'the', 'input', 'sequence', 'only', 'rather', 'than', 'from', 'a', 'larger', 'fixed', 'vocabulary', 'and', 'thus', 'allows', 'us', 'to', 'generate', 'answers', 'that', 'consist', 'of', 'multiple', 'tokens', 'from', 'the', 'original', 'text', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'VB', 'DT', 'NNP', 'NNP', '(', 'NNP', ':', 'NN', ')', 'NN', 'VBN', 'IN', ',', 'WDT', 'VBZ', 'DT', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'RB', 'RB', 'IN', 'IN', 'DT', 'JJR', 'JJ', 'NN', 'CC', 'RB', 'VBZ', 'PRP', 'TO', 'VB', 'NNS', 'IN', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",50
natural_language_inference,68,41,We propose two ways to apply the Ptr - Net model for our task : a sequence model and a boundary model .,"['We', 'propose', 'two', 'ways', 'to', 'apply', 'the', 'Ptr', '-', 'Net', 'model', 'for', 'our', 'task', ':', 'a', 'sequence', 'model', 'and', 'a', 'boundary', 'model', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'NNS', 'TO', 'VB', 'DT', 'NNP', ':', 'JJ', 'NN', 'IN', 'PRP$', 'NN', ':', 'DT', 'NN', 'NN', 'CC', 'DT', 'JJ', 'NN', '.']",23
natural_language_inference,68,42,We also further extend the boundary model with a search mechanism .,"['We', 'also', 'further', 'extend', 'the', 'boundary', 'model', 'with', 'a', 'search', 'mechanism', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'RBR', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",12
natural_language_inference,68,174,"We first tokenize all the passages , questions and answers .","['We', 'first', 'tokenize', 'all', 'the', 'passages', ',', 'questions', 'and', 'answers', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VB', 'PDT', 'DT', 'NNS', ',', 'NNS', 'CC', 'NNS', '.']",11
natural_language_inference,68,176,We use word embeddings from GloVe to initialize the model .,"['We', 'use', 'word', 'embeddings', 'from', 'GloVe', 'to', 'initialize', 'the', 'model', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'NN', 'NNS', 'IN', 'NNP', 'TO', 'VB', 'DT', 'NN', '.']",11
natural_language_inference,68,177,Words not found in Glo Ve are initialized as zero vectors .,"['Words', 'not', 'found', 'in', 'Glo', 'Ve', 'are', 'initialized', 'as', 'zero', 'vectors', '.']","['B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['NNS', 'RB', 'VBN', 'IN', 'NNP', 'NNP', 'VBP', 'VBN', 'IN', 'NN', 'NNS', '.']",12
natural_language_inference,68,179,The dimensionality l of the hidden layers is set to be 150 or 300 .,"['The', 'dimensionality', 'l', 'of', 'the', 'hidden', 'layers', 'is', 'set', 'to', 'be', '150', 'or', '300', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'VBZ', 'VBN', 'TO', 'VB', 'CD', 'CC', 'CD', '.']",15
natural_language_inference,68,180,We use ADAMAX with the coefficients ? 1 = 0.9 and ? 2 = 0.999 to optimize the model .,"['We', 'use', 'ADAMAX', 'with', 'the', 'coefficients', '?', '1', '=', '0.9', 'and', '?', '2', '=', '0.999', 'to', 'optimize', 'the', 'model', '.']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'IN', 'DT', 'NNS', '.', 'CD', 'JJ', 'CD', 'CC', '.', 'CD', '$', 'CD', 'TO', 'VB', 'DT', 'NN', '.']",20
natural_language_inference,68,192,"Furthermore , our boundary model has outperformed the sequence model , achieving an exact match score of 61.1 % and an F1 score of 71.2 % .","['Furthermore', ',', 'our', 'boundary', 'model', 'has', 'outperformed', 'the', 'sequence', 'model', ',', 'achieving', 'an', 'exact', 'match', 'score', 'of', '61.1', '%', 'and', 'an', 'F1', 'score', 'of', '71.2', '%', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'JJ', 'NN', 'VBZ', 'VBN', 'DT', 'NN', 'NN', ',', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'NN', 'CC', 'DT', 'NNP', 'NN', 'IN', 'CD', 'NN', '.']",27
natural_language_inference,68,193,"In particular , in terms of the exact match score , the boundary model has a clear advantage over the sequence model .","['In', 'particular', ',', 'in', 'terms', 'of', 'the', 'exact', 'match', 'score', ',', 'the', 'boundary', 'model', 'has', 'a', 'clear', 'advantage', 'over', 'the', 'sequence', 'model', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'JJ', ',', 'IN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",23
natural_language_inference,68,200,"While by adding Bi - Ans - Ptr with bi-directional pre-processing LSTM , we can get 1.2 % improvement in F1 .","['While', 'by', 'adding', 'Bi', '-', 'Ans', '-', 'Ptr', 'with', 'bi-directional', 'pre-processing', 'LSTM', ',', 'we', 'can', 'get', '1.2', '%', 'improvement', 'in', 'F1', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'IN', 'VBG', 'NNP', ':', 'NNP', ':', 'NN', 'IN', 'JJ', 'JJ', 'NNP', ',', 'PRP', 'MD', 'VB', 'CD', 'NN', 'NN', 'IN', 'NNP', '.']",22
natural_language_inference,26,4,"The latest work on language representations carefully integrates contextualized features into language model training , which enables a series of success especially in various machine reading comprehension and natural language inference tasks .","['The', 'latest', 'work', 'on', 'language', 'representations', 'carefully', 'integrates', 'contextualized', 'features', 'into', 'language', 'model', 'training', ',', 'which', 'enables', 'a', 'series', 'of', 'success', 'especially', 'in', 'various', 'machine', 'reading', 'comprehension', 'and', 'natural', 'language', 'inference', 'tasks', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJS', 'NN', 'IN', 'NN', 'NNS', 'RB', 'VBZ', 'JJ', 'NNS', 'IN', 'NN', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'NN', 'RB', 'IN', 'JJ', 'NN', 'NN', 'NN', 'CC', 'JJ', 'NN', 'NN', 'NNS', '.']",33
natural_language_inference,26,12,"Recently , deep contextual language model ( LM ) has been shown effective for learning universal language representations , achieving state - of - the - art results in a series of flagship natural language understanding ( NLU ) tasks .","['Recently', ',', 'deep', 'contextual', 'language', 'model', '(', 'LM', ')', 'has', 'been', 'shown', 'effective', 'for', 'learning', 'universal', 'language', 'representations', ',', 'achieving', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'in', 'a', 'series', 'of', 'flagship', 'natural', 'language', 'understanding', '(', 'NLU', ')', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'JJ', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'VBN', 'VBN', 'JJ', 'IN', 'VBG', 'JJ', 'NN', 'NNS', ',', 'VBG', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'NNS', '.']",41
natural_language_inference,26,26,"Thus we are motivated to enrich the sentence contextual semantics in multiple predicate - specific argument sequences by presenting SemBERT : Semantics - aware BERT , which is a fine - tuned BERT with explicit contextual semantic clues .","['Thus', 'we', 'are', 'motivated', 'to', 'enrich', 'the', 'sentence', 'contextual', 'semantics', 'in', 'multiple', 'predicate', '-', 'specific', 'argument', 'sequences', 'by', 'presenting', 'SemBERT', ':', 'Semantics', '-', 'aware', 'BERT', ',', 'which', 'is', 'a', 'fine', '-', 'tuned', 'BERT', 'with', 'explicit', 'contextual', 'semantic', 'clues', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', 'PRP', 'VBP', 'VBN', 'TO', 'VB', 'DT', 'NN', 'JJ', 'NNS', 'IN', 'JJ', 'NN', ':', 'JJ', 'NN', 'NNS', 'IN', 'VBG', 'NNP', ':', 'NNPS', ':', 'JJ', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'JJ', ':', 'VBD', 'NNP', 'IN', 'JJ', 'JJ', 'JJ', 'NNS', '.']",39
natural_language_inference,26,27,The proposed SemBERT learns the representation in a fine - grained manner and takes both strengths of BERT on plain context representation and explicit semantics for deeper meaning representation .,"['The', 'proposed', 'SemBERT', 'learns', 'the', 'representation', 'in', 'a', 'fine', '-', 'grained', 'manner', 'and', 'takes', 'both', 'strengths', 'of', 'BERT', 'on', 'plain', 'context', 'representation', 'and', 'explicit', 'semantics', 'for', 'deeper', 'meaning', 'representation', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'VBN', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'JJ', ':', 'JJ', 'NN', 'CC', 'VBZ', 'DT', 'NNS', 'IN', 'NNP', 'IN', 'NN', 'JJ', 'NN', 'CC', 'JJ', 'NNS', 'IN', 'JJR', 'NN', 'NN', '.']",30
natural_language_inference,26,28,Our model consists of three components :,"['Our', 'model', 'consists', 'of', 'three', 'components', ':']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'IN', 'CD', 'NNS', ':']",7
natural_language_inference,26,29,1 ) an out - ofshelf semantic role labeler to annotate the input sentences with a variety of semantic role labels ; 2 ) an sequence encoder where a pre-trained language model is used to build representation for input raw texts and the semantic role labels are mapped to embedding in parallel ; 3 ) a semantic integration component to integrate the text representation with the contextual explicit semantic embedding to obtain the joint representation for downstream tasks .,"['1', ')', 'an', 'out', '-', 'ofshelf', 'semantic', 'role', 'labeler', 'to', 'annotate', 'the', 'input', 'sentences', 'with', 'a', 'variety', 'of', 'semantic', 'role', 'labels', ';', '2', ')', 'an', 'sequence', 'encoder', 'where', 'a', 'pre-trained', 'language', 'model', 'is', 'used', 'to', 'build', 'representation', 'for', 'input', 'raw', 'texts', 'and', 'the', 'semantic', 'role', 'labels', 'are', 'mapped', 'to', 'embedding', 'in', 'parallel', ';', '3', ')', 'a', 'semantic', 'integration', 'component', 'to', 'integrate', 'the', 'text', 'representation', 'with', 'the', 'contextual', 'explicit', 'semantic', 'embedding', 'to', 'obtain', 'the', 'joint', 'representation', 'for', 'downstream', 'tasks', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['CD', ')', 'DT', 'RP', ':', 'PRP', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NNS', ':', 'CD', ')', 'DT', 'NN', 'NN', 'WRB', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'NN', 'IN', 'NN', 'JJ', 'NN', 'CC', 'DT', 'JJ', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'VBG', 'IN', 'NN', ':', 'CD', ')', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'JJ', 'VBG', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', '.']",79
natural_language_inference,26,124,Our implementation is based on the PyTorch implementation of BERT 6 .,"['Our', 'implementation', 'is', 'based', 'on', 'the', 'PyTorch', 'implementation', 'of', 'BERT', '6', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O']","['PRP$', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'NNP', 'CD', '.']",12
natural_language_inference,26,125,"We use the pre-trained weights of BERT and follow the same fine - tuning procedure as BERT without any modification , and all the layers are tuned with moderate model size increasing , as the extra SRL embedding volume is less than 15 % of the original encoder size .","['We', 'use', 'the', 'pre-trained', 'weights', 'of', 'BERT', 'and', 'follow', 'the', 'same', 'fine', '-', 'tuning', 'procedure', 'as', 'BERT', 'without', 'any', 'modification', ',', 'and', 'all', 'the', 'layers', 'are', 'tuned', 'with', 'moderate', 'model', 'size', 'increasing', ',', 'as', 'the', 'extra', 'SRL', 'embedding', 'volume', 'is', 'less', 'than', '15', '%', 'of', 'the', 'original', 'encoder', 'size', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NNS', 'IN', 'NNP', 'CC', 'VB', 'DT', 'JJ', 'JJ', ':', 'VBG', 'NN', 'IN', 'NNP', 'IN', 'DT', 'NN', ',', 'CC', 'PDT', 'DT', 'NNS', 'VBP', 'VBN', 'IN', 'JJ', 'NN', 'NN', 'VBG', ',', 'IN', 'DT', 'JJ', 'NNP', 'NN', 'NN', 'VBZ', 'JJR', 'IN', 'CD', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",50
natural_language_inference,26,126,"We set the initial learning rate in { 8e -6 , 1 e - 5 , 2 e - 5 , 3 e - 5 } with warm - up rate of 0.1 and L2 weight decay of 0.01 .","['We', 'set', 'the', 'initial', 'learning', 'rate', 'in', '{', '8e', '-6', ',', '1', 'e', '-', '5', ',', '2', 'e', '-', '5', ',', '3', 'e', '-', '5', '}', 'with', 'warm', '-', 'up', 'rate', 'of', '0.1', 'and', 'L2', 'weight', 'decay', 'of', '0.01', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', '(', 'CD', 'NN', ',', 'CD', 'SYM', ':', 'CD', ',', 'CD', 'SYM', ':', 'CD', ',', 'CD', 'SYM', ':', 'CD', ')', 'IN', 'JJ', ':', 'RB', 'NN', 'IN', 'CD', 'CC', 'NNP', 'VBD', 'NN', 'IN', 'CD', '.']",40
natural_language_inference,26,127,"The batch size is selected in { 16 , 24 , 32 } .","['The', 'batch', 'size', 'is', 'selected', 'in', '{', '16', ',', '24', ',', '32', '}', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', '(', 'CD', ',', 'CD', ',', 'CD', ')', '.']",14
natural_language_inference,26,128,"The maximum number of epochs is set in [ 2 , 5 ] depending on tasks .","['The', 'maximum', 'number', 'of', 'epochs', 'is', 'set', 'in', '[', '2', ',', '5', ']', 'depending', 'on', 'tasks', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', 'CD', ',', 'CD', 'NN', 'VBG', 'IN', 'NNS', '.']",17
natural_language_inference,26,129,"Texts are tokenized using wordpieces , with maximum length of 384 for SQuAD and 128 or 200 for other tasks .","['Texts', 'are', 'tokenized', 'using', 'wordpieces', ',', 'with', 'maximum', 'length', 'of', '384', 'for', 'SQuAD', 'and', '128', 'or', '200', 'for', 'other', 'tasks', '.']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNS', 'VBP', 'VBN', 'VBG', 'NNS', ',', 'IN', 'JJ', 'NN', 'IN', 'CD', 'IN', 'NNP', 'CC', 'CD', 'CC', 'CD', 'IN', 'JJ', 'NNS', '.']",21
natural_language_inference,26,130,The dimension of SRL embedding is set to 10 .,"['The', 'dimension', 'of', 'SRL', 'embedding', 'is', 'set', 'to', '10', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NNP', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",10
natural_language_inference,26,131,The default maximum number of predicateargument structures m is set to 3 .,"['The', 'default', 'maximum', 'number', 'of', 'predicateargument', 'structures', 'm', 'is', 'set', 'to', '3', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'FW', 'VBZ', 'VBN', 'TO', 'CD', '.']",13
natural_language_inference,26,171,"From the results , we observe that the concatenation would yield an improvement , verifying that integrating contextual semantics would be quite useful for language understanding .","['From', 'the', 'results', ',', 'we', 'observe', 'that', 'the', 'concatenation', 'would', 'yield', 'an', 'improvement', ',', 'verifying', 'that', 'integrating', 'contextual', 'semantics', 'would', 'be', 'quite', 'useful', 'for', 'language', 'understanding', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'MD', 'VB', 'DT', 'NN', ',', 'VBG', 'IN', 'VBG', 'JJ', 'NNS', 'MD', 'VB', 'RB', 'JJ', 'IN', 'NN', 'NN', '.']",27
natural_language_inference,26,172,"However , SemBERT still outperforms the simple BERT + SRL model just like the latter outperforms the original BERT by a large performance margin , which shows that SemBERT works more effectively for integrating both plain contextual representation and contextual semantics at the same time .","['However', ',', 'SemBERT', 'still', 'outperforms', 'the', 'simple', 'BERT', '+', 'SRL', 'model', 'just', 'like', 'the', 'latter', 'outperforms', 'the', 'original', 'BERT', 'by', 'a', 'large', 'performance', 'margin', ',', 'which', 'shows', 'that', 'SemBERT', 'works', 'more', 'effectively', 'for', 'integrating', 'both', 'plain', 'contextual', 'representation', 'and', 'contextual', 'semantics', 'at', 'the', 'same', 'time', '.']","['O', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'NNP', 'RB', 'VBZ', 'DT', 'JJ', 'NNP', 'NNP', 'NNP', 'NN', 'RB', 'IN', 'DT', 'JJ', 'VBZ', 'DT', 'JJ', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'WDT', 'VBZ', 'IN', 'NNP', 'VBZ', 'RBR', 'RB', 'IN', 'VBG', 'DT', 'VBP', 'JJ', 'NN', 'CC', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",46
natural_language_inference,1,2,Large - scale Simple Question Answering with Memory Networks,"['Large', '-', 'scale', 'Simple', 'Question', 'Answering', 'with', 'Memory', 'Networks']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['JJ', ':', 'NN', 'JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",9
natural_language_inference,1,4,Training large - scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions .,"['Training', 'large', '-', 'scale', 'question', 'answering', 'systems', 'is', 'complicated', 'because', 'training', 'sources', 'usually', 'cover', 'a', 'small', 'portion', 'of', 'the', 'range', 'of', 'possible', 'questions', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', ':', 'NN', 'NN', 'VBG', 'NNS', 'VBZ', 'VBN', 'IN', 'NN', 'NNS', 'RB', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', '.']",24
natural_language_inference,1,5,"This paper studies the impact of multitask and transfer learning for simple question answering ; a setting for which the reasoning required to answer is quite easy , as long as one can retrieve the correct evidence given a question , which can be difficult in large - scale conditions .","['This', 'paper', 'studies', 'the', 'impact', 'of', 'multitask', 'and', 'transfer', 'learning', 'for', 'simple', 'question', 'answering', ';', 'a', 'setting', 'for', 'which', 'the', 'reasoning', 'required', 'to', 'answer', 'is', 'quite', 'easy', ',', 'as', 'long', 'as', 'one', 'can', 'retrieve', 'the', 'correct', 'evidence', 'given', 'a', 'question', ',', 'which', 'can', 'be', 'difficult', 'in', 'large', '-', 'scale', 'conditions', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NNS', 'DT', 'NN', 'IN', 'NN', 'CC', 'VB', 'VBG', 'IN', 'JJ', 'NN', 'VBG', ':', 'DT', 'NN', 'IN', 'WDT', 'DT', 'NN', 'VBN', 'TO', 'VB', 'VBZ', 'RB', 'JJ', ',', 'RB', 'RB', 'IN', 'CD', 'MD', 'VB', 'DT', 'JJ', 'NN', 'VBN', 'DT', 'NN', ',', 'WDT', 'MD', 'VB', 'JJ', 'IN', 'JJ', ':', 'NN', 'NNS', '.']",51
natural_language_inference,1,12,"However , while most recent efforts have focused on designing systems with higher reasoning capabilities , that could jointly retrieve and use multiple facts to answer , the simpler problem of answering questions that refer to a single fact of the KB , which we call Simple Question Answering in this paper , is still far from solved .","['However', ',', 'while', 'most', 'recent', 'efforts', 'have', 'focused', 'on', 'designing', 'systems', 'with', 'higher', 'reasoning', 'capabilities', ',', 'that', 'could', 'jointly', 'retrieve', 'and', 'use', 'multiple', 'facts', 'to', 'answer', ',', 'the', 'simpler', 'problem', 'of', 'answering', 'questions', 'that', 'refer', 'to', 'a', 'single', 'fact', 'of', 'the', 'KB', ',', 'which', 'we', 'call', 'Simple', 'Question', 'Answering', 'in', 'this', 'paper', ',', 'is', 'still', 'far', 'from', 'solved', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'IN', 'JJS', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'VBG', 'NNS', 'IN', 'JJR', 'NN', 'NNS', ',', 'WDT', 'MD', 'RB', 'VB', 'CC', 'VB', 'JJ', 'NNS', 'TO', 'VB', ',', 'DT', 'NN', 'NN', 'IN', 'VBG', 'NNS', 'WDT', 'VBP', 'TO', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', ',', 'WDT', 'PRP', 'VBP', 'JJ', 'NNP', 'NNP', 'IN', 'DT', 'NN', ',', 'VBZ', 'RB', 'RB', 'IN', 'VBN', '.']",59
natural_language_inference,1,19,"First , as an effort to study the coverage of existing systems and the possibility to train jointly on different data sources via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .","['First', ',', 'as', 'an', 'effort', 'to', 'study', 'the', 'coverage', 'of', 'existing', 'systems', 'and', 'the', 'possibility', 'to', 'train', 'jointly', 'on', 'different', 'data', 'sources', 'via', 'multitasking', ',', 'we', 'collected', 'the', 'first', 'large', '-', 'scale', 'dataset', 'of', 'questions', 'and', 'answers', 'based', 'on', 'a', 'KB', ',', 'called', 'SimpleQuestions', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'O']","['RB', ',', 'IN', 'DT', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'VBG', 'NNS', 'CC', 'DT', 'NN', 'TO', 'VB', 'RB', 'IN', 'JJ', 'NNS', 'NNS', 'IN', 'NN', ',', 'PRP', 'VBD', 'DT', 'JJ', 'JJ', ':', 'NN', 'NN', 'IN', 'NNS', 'CC', 'NNS', 'VBN', 'IN', 'DT', 'NNP', ',', 'VBD', 'NNS', '.']",45
natural_language_inference,1,20,"This dataset , which is presented in Section 2 , contains more than 100 k questions written by human anno-What American cartoonist is the creator of Andy Lippincott ?","['This', 'dataset', ',', 'which', 'is', 'presented', 'in', 'Section', '2', ',', 'contains', 'more', 'than', '100', 'k', 'questions', 'written', 'by', 'human', 'anno-What', 'American', 'cartoonist', 'is', 'the', 'creator', 'of', 'Andy', 'Lippincott', '?']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', ',', 'WDT', 'VBZ', 'VBN', 'IN', 'NNP', 'CD', ',', 'VBZ', 'JJR', 'IN', 'CD', 'JJ', 'NNS', 'VBN', 'IN', 'JJ', 'JJ', 'JJ', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'NNP', 'NNP', '.']",29
natural_language_inference,1,24,"Second , in sections 3 and 4 , we present an embedding - based QA system developed under the framework of Memory Networks ( Mem NNs ) .","['Second', ',', 'in', 'sections', '3', 'and', '4', ',', 'we', 'present', 'an', 'embedding', '-', 'based', 'QA', 'system', 'developed', 'under', 'the', 'framework', 'of', 'Memory', 'Networks', '(', 'Mem', 'NNs', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', ',', 'IN', 'NNS', 'CD', 'CC', 'CD', ',', 'PRP', 'VBP', 'DT', 'VBG', ':', 'VBN', 'NNP', 'NN', 'VBD', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NNP', '(', 'NNP', 'NNP', ')', '.']",28
natural_language_inference,1,25,"Memory Networks are learning systems centered around a memory component that can be read and written to , with a particular focus on cases where the relationship between the input and response languages ( here natural language ) and the storage language ( here , the facts from KBs ) is performed by embedding all of them in the same vector space .","['Memory', 'Networks', 'are', 'learning', 'systems', 'centered', 'around', 'a', 'memory', 'component', 'that', 'can', 'be', 'read', 'and', 'written', 'to', ',', 'with', 'a', 'particular', 'focus', 'on', 'cases', 'where', 'the', 'relationship', 'between', 'the', 'input', 'and', 'response', 'languages', '(', 'here', 'natural', 'language', ')', 'and', 'the', 'storage', 'language', '(', 'here', ',', 'the', 'facts', 'from', 'KBs', ')', 'is', 'performed', 'by', 'embedding', 'all', 'of', 'them', 'in', 'the', 'same', 'vector', 'space', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NN', 'NNS', 'VBP', 'VBG', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'NN', 'WDT', 'MD', 'VB', 'VBN', 'CC', 'VBN', 'TO', ',', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'WRB', 'DT', 'NN', 'IN', 'DT', 'NN', 'CC', 'NN', 'NNS', '(', 'RB', 'JJ', 'NN', ')', 'CC', 'DT', 'NN', 'NN', '(', 'RB', ',', 'DT', 'NNS', 'IN', 'NNP', ')', 'VBZ', 'VBN', 'IN', 'VBG', 'DT', 'IN', 'PRP', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",63
natural_language_inference,1,26,The setting of the simple QA corresponds to the elementary operation of performing a single lookup in the memory .,"['The', 'setting', 'of', 'the', 'simple', 'QA', 'corresponds', 'to', 'the', 'elementary', 'operation', 'of', 'performing', 'a', 'single', 'lookup', 'in', 'the', 'memory', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['DT', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'NNS', 'TO', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",20
natural_language_inference,1,229,"The embedding dimension and the learning rate were chosen among { 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 } respectively , and the margin ?","['The', 'embedding', 'dimension', 'and', 'the', 'learning', 'rate', 'were', 'chosen', 'among', '{', '64', ',', '128', ',', '256', '}', 'and', '{', '1', ',', '0.1', ',', '...', ',', '1.0e', '?', '4', '}', 'respectively', ',', 'and', 'the', 'margin', '?']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O']","['DT', 'VBG', 'NN', 'CC', 'DT', 'NN', 'NN', 'VBD', 'VBN', 'IN', '(', 'CD', ',', 'CD', ',', 'CD', ')', 'CC', '(', 'CD', ',', 'CD', ',', ':', ',', 'CD', '.', 'CD', ')', 'RB', ',', 'CC', 'DT', 'NN', '.']",35
natural_language_inference,1,230,was set to 0.1 .,"['was', 'set', 'to', '0.1', '.']","['O', 'B-p', 'I-p', 'B-n', 'O']","['VBD', 'VBN', 'TO', 'CD', '.']",5
natural_language_inference,1,254,"On WebQuestions , not specifically designed as a simple QA dataset , 86 % of the questions can now be answered with a single supporting fact , and performance increases significantly ( from 36.2 % to 41.0 % F1-score ) .","['On', 'WebQuestions', ',', 'not', 'specifically', 'designed', 'as', 'a', 'simple', 'QA', 'dataset', ',', '86', '%', 'of', 'the', 'questions', 'can', 'now', 'be', 'answered', 'with', 'a', 'single', 'supporting', 'fact', ',', 'and', 'performance', 'increases', 'significantly', '(', 'from', '36.2', '%', 'to', '41.0', '%', 'F1-score', ')', '.']","['B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'NNS', ',', 'RB', 'RB', 'VBN', 'IN', 'DT', 'JJ', 'NNP', 'NN', ',', 'CD', 'NN', 'IN', 'DT', 'NNS', 'MD', 'RB', 'VB', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'CC', 'NN', 'NNS', 'RB', '(', 'IN', 'CD', 'NN', 'TO', 'CD', 'NN', 'NNP', ')', '.']",41
natural_language_inference,1,255,"Using the bigger FB5M as KB does not change performance on SimpleQuestions because it was based on FB2M , but the results show that our model is robust to the addition of more entities than necessary .","['Using', 'the', 'bigger', 'FB5M', 'as', 'KB', 'does', 'not', 'change', 'performance', 'on', 'SimpleQuestions', 'because', 'it', 'was', 'based', 'on', 'FB2M', ',', 'but', 'the', 'results', 'show', 'that', 'our', 'model', 'is', 'robust', 'to', 'the', 'addition', 'of', 'more', 'entities', 'than', 'necessary', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'DT', 'JJR', 'NNP', 'IN', 'NNP', 'VBZ', 'RB', 'VB', 'NN', 'IN', 'NNS', 'IN', 'PRP', 'VBD', 'VBN', 'IN', 'NNP', ',', 'CC', 'DT', 'NNS', 'VBP', 'IN', 'PRP$', 'NN', 'VBZ', 'JJ', 'TO', 'DT', 'NN', 'IN', 'JJR', 'NNS', 'IN', 'JJ', '.']",37
natural_language_inference,1,256,Transfer learning on Reverb,"['Transfer', 'learning', 'on', 'Reverb']","['B-n', 'I-n', 'I-n', 'I-n']","['NN', 'VBG', 'IN', 'NNP']",4
natural_language_inference,1,259,"Our best results are 67 % accuracy ( and 68 % for the ensemble of 5 models ) , which are better than the 54 % of the original paper and close to the stateof - the - art 73 % of .","['Our', 'best', 'results', 'are', '67', '%', 'accuracy', '(', 'and', '68', '%', 'for', 'the', 'ensemble', 'of', '5', 'models', ')', ',', 'which', 'are', 'better', 'than', 'the', '54', '%', 'of', 'the', 'original', 'paper', 'and', 'close', 'to', 'the', 'stateof', '-', 'the', '-', 'art', '73', '%', 'of', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'JJS', 'NNS', 'VBP', 'CD', 'NN', 'NN', '(', 'CC', 'CD', 'NN', 'IN', 'DT', 'NN', 'IN', 'CD', 'NNS', ')', ',', 'WDT', 'VBP', 'JJR', 'IN', 'DT', 'CD', 'NN', 'IN', 'DT', 'JJ', 'NN', 'CC', 'RB', 'TO', 'DT', 'NN', ':', 'DT', ':', 'NN', 'CD', 'NN', 'IN', '.']",43
natural_language_inference,1,262,"We first notice that models trained on a single QA dataset perform poorly on the other datasets ( e.g. 46.6 % accuracy on SimpleQuestions for the model trained on WebQuestions only ) , which shows that the performance on We-bQuestions does not necessarily guarantee high coverage for simple QA .","['We', 'first', 'notice', 'that', 'models', 'trained', 'on', 'a', 'single', 'QA', 'dataset', 'perform', 'poorly', 'on', 'the', 'other', 'datasets', '(', 'e.g.', '46.6', '%', 'accuracy', 'on', 'SimpleQuestions', 'for', 'the', 'model', 'trained', 'on', 'WebQuestions', 'only', ')', ',', 'which', 'shows', 'that', 'the', 'performance', 'on', 'We-bQuestions', 'does', 'not', 'necessarily', 'guarantee', 'high', 'coverage', 'for', 'simple', 'QA', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'JJ', 'NNP', 'NN', 'NN', 'RB', 'IN', 'DT', 'JJ', 'NNS', '(', 'VB', 'CD', 'NN', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'VBD', 'IN', 'NNP', 'RB', ')', ',', 'WDT', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'NNP', 'VBZ', 'RB', 'RB', 'VB', 'JJ', 'NN', 'IN', 'JJ', 'NNP', '.']",50
natural_language_inference,1,263,"On the other hand , training on both datasets only improves performance ; in particular , the model is able to capture all question patterns of the two datasets ; there is no "" negative interaction "" .","['On', 'the', 'other', 'hand', ',', 'training', 'on', 'both', 'datasets', 'only', 'improves', 'performance', ';', 'in', 'particular', ',', 'the', 'model', 'is', 'able', 'to', 'capture', 'all', 'question', 'patterns', 'of', 'the', 'two', 'datasets', ';', 'there', 'is', 'no', '""', 'negative', 'interaction', '""', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'NN', 'IN', 'DT', 'NNS', 'RB', 'VBZ', 'NN', ':', 'IN', 'JJ', ',', 'DT', 'NN', 'VBZ', 'JJ', 'TO', 'VB', 'DT', 'NN', 'NNS', 'IN', 'DT', 'CD', 'NNS', ':', 'EX', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'NN', '.']",38
natural_language_inference,1,264,Importance of data sources,"['Importance', 'of', 'data', 'sources']","['B-n', 'I-n', 'I-n', 'I-n']","['NN', 'IN', 'NNS', 'NNS']",4
natural_language_inference,1,266,"While paraphrases do not seem to help much on WebQuestions and SimpleQuestions , except when training only with synthetic questions , they have a dramatic impact on the performance on Reverb .","['While', 'paraphrases', 'do', 'not', 'seem', 'to', 'help', 'much', 'on', 'WebQuestions', 'and', 'SimpleQuestions', ',', 'except', 'when', 'training', 'only', 'with', 'synthetic', 'questions', ',', 'they', 'have', 'a', 'dramatic', 'impact', 'on', 'the', 'performance', 'on', 'Reverb', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['IN', 'NNS', 'VBP', 'RB', 'VB', 'TO', 'VB', 'RB', 'IN', 'NNS', 'CC', 'NNP', ',', 'IN', 'WRB', 'VBG', 'RB', 'IN', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', '.']",32
natural_language_inference,43,2,Evaluating Semantic Parsing against a Simple Web - based Question Answering Model,"['Evaluating', 'Semantic', 'Parsing', 'against', 'a', 'Simple', 'Web', '-', 'based', 'Question', 'Answering', 'Model']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', 'NNP', 'IN', 'DT', 'JJ', 'NNP', ':', 'VBN', 'NN', 'NNP', 'NNP']",12
natural_language_inference,43,18,"We develop a simple log - linear model , in the spirit of traditional web - based QA systems , that answers questions by querying the web and extracting the answer from returned web snippets .","['We', 'develop', 'a', 'simple', 'log', '-', 'linear', 'model', ',', 'in', 'the', 'spirit', 'of', 'traditional', 'web', '-', 'based', 'QA', 'systems', ',', 'that', 'answers', 'questions', 'by', 'querying', 'the', 'web', 'and', 'extracting', 'the', 'answer', 'from', 'returned', 'web', 'snippets', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', ':', 'JJ', 'NN', ',', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NN', ':', 'VBN', 'NNP', 'NNS', ',', 'IN', 'NNS', 'NNS', 'IN', 'VBG', 'DT', 'NN', 'CC', 'VBG', 'DT', 'NN', 'IN', 'VBN', 'JJ', 'NNS', '.']",36
natural_language_inference,43,19,"Thus , our evaluation scheme is suitable for semantic parsing benchmarks in which the knowledge required for answering questions is covered by the web ( in contrast with virtual assitants for which the knowledge is specific to an application ) .","['Thus', ',', 'our', 'evaluation', 'scheme', 'is', 'suitable', 'for', 'semantic', 'parsing', 'benchmarks', 'in', 'which', 'the', 'knowledge', 'required', 'for', 'answering', 'questions', 'is', 'covered', 'by', 'the', 'web', '(', 'in', 'contrast', 'with', 'virtual', 'assitants', 'for', 'which', 'the', 'knowledge', 'is', 'specific', 'to', 'an', 'application', ')', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP$', 'NN', 'NN', 'VBZ', 'JJ', 'IN', 'JJ', 'NN', 'NNS', 'IN', 'WDT', 'DT', 'NN', 'VBN', 'IN', 'VBG', 'NNS', 'VBZ', 'VBN', 'IN', 'DT', 'NN', '(', 'IN', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'WDT', 'DT', 'NN', 'VBZ', 'JJ', 'TO', 'DT', 'NN', ')', '.']",41
natural_language_inference,43,108,"We compare our model , WEBQA , to STAGG and COMPQ , which are to the best of our knowledge the highest performing semantic parsing models on both COMPLEXQUESTIONS and WEBQUES - TIONS .","['We', 'compare', 'our', 'model', ',', 'WEBQA', ',', 'to', 'STAGG', 'and', 'COMPQ', ',', 'which', 'are', 'to', 'the', 'best', 'of', 'our', 'knowledge', 'the', 'highest', 'performing', 'semantic', 'parsing', 'models', 'on', 'both', 'COMPLEXQUESTIONS', 'and', 'WEBQUES', '-', 'TIONS', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'NN', ',', 'NNP', ',', 'TO', 'NNP', 'CC', 'NNP', ',', 'WDT', 'VBP', 'TO', 'DT', 'JJS', 'IN', 'PRP$', 'NN', 'DT', 'JJS', 'NN', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'CC', 'NNP', ':', 'NN', '.']",34
natural_language_inference,43,117,"WEBQA obtained 32.6 F 1 ( 33.5 p@1 , 42.4 MRR ) compared to 40.9 F 1 of COMPQ .","['WEBQA', 'obtained', '32.6', 'F', '1', '(', '33.5', 'p@1', ',', '42.4', 'MRR', ')', 'compared', 'to', '40.9', 'F', '1', 'of', 'COMPQ', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', 'VBD', 'CD', 'NNP', 'CD', '(', 'CD', 'NN', ',', 'CD', 'NNP', ')', 'VBN', 'TO', 'CD', 'NNP', 'CD', 'IN', 'NNP', '.']",20
natural_language_inference,43,118,Our candidate extraction step finds the correct answer in the top - K candidates in 65.9 % of development examples and 62.7 % of test examples .,"['Our', 'candidate', 'extraction', 'step', 'finds', 'the', 'correct', 'answer', 'in', 'the', 'top', '-', 'K', 'candidates', 'in', '65.9', '%', 'of', 'development', 'examples', 'and', '62.7', '%', 'of', 'test', 'examples', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', ':', 'NNP', 'NNS', 'IN', 'CD', 'NN', 'IN', 'NN', 'NNS', 'CC', 'CD', 'NN', 'IN', 'NN', 'NNS', '.']",27
natural_language_inference,43,119,"Thus , our test F 1 on examples for which candidate extraction succeeded ( WEBQA - SUBSET ) is 51.9 ( 53.4 p@1 , 67.5 MRR ) .","['Thus', ',', 'our', 'test', 'F', '1', 'on', 'examples', 'for', 'which', 'candidate', 'extraction', 'succeeded', '(', 'WEBQA', '-', 'SUBSET', ')', 'is', '51.9', '(', '53.4', 'p@1', ',', '67.5', 'MRR', ')', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'NN', 'NNP', 'CD', 'IN', 'NNS', 'IN', 'WDT', 'NN', 'NN', 'VBN', '(', 'NNP', ':', 'NN', ')', 'VBZ', 'CD', '(', 'CD', 'NN', ',', 'CD', 'NNP', ')', '.']",28
natural_language_inference,43,121,"In this setup , COMPQ obtained 42.2 F 1 on the test set ( compared to 40.9 F 1 , when training on COM - PLEXQUESTIONS only , as we do ) .","['In', 'this', 'setup', ',', 'COMPQ', 'obtained', '42.2', 'F', '1', 'on', 'the', 'test', 'set', '(', 'compared', 'to', '40.9', 'F', '1', ',', 'when', 'training', 'on', 'COM', '-', 'PLEXQUESTIONS', 'only', ',', 'as', 'we', 'do', ')', '.']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'NNP', 'VBD', 'CD', 'NNP', 'CD', 'IN', 'DT', 'NN', 'NN', '(', 'VBN', 'TO', 'CD', 'NNP', 'CD', ',', 'WRB', 'VBG', 'IN', 'NNP', ':', 'NNP', 'RB', ',', 'IN', 'PRP', 'VBP', ')', '.']",33
natural_language_inference,43,122,"Restricting the predictions to the subset for which candidate extraction succeeded , the F 1 of COMPQ - SUBSET is 48.5 , which is 3.4 F 1 points lower than WEBQA - SUBSET , which was trained on less data .","['Restricting', 'the', 'predictions', 'to', 'the', 'subset', 'for', 'which', 'candidate', 'extraction', 'succeeded', ',', 'the', 'F', '1', 'of', 'COMPQ', '-', 'SUBSET', 'is', '48.5', ',', 'which', 'is', '3.4', 'F', '1', 'points', 'lower', 'than', 'WEBQA', '-', 'SUBSET', ',', 'which', 'was', 'trained', 'on', 'less', 'data', '.']","['B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'DT', 'NNS', 'TO', 'DT', 'NN', 'IN', 'WDT', 'NN', 'NN', 'VBD', ',', 'DT', 'NNP', 'CD', 'IN', 'NNP', ':', 'NNP', 'VBZ', 'CD', ',', 'WDT', 'VBZ', 'CD', 'NNP', 'CD', 'NNS', 'JJR', 'IN', 'NNP', ':', 'NNP', ',', 'WDT', 'VBD', 'VBN', 'IN', 'JJR', 'NNS', '.']",41
natural_language_inference,43,133,"Note that TF - IDF is by far the most impactful feature , leading to a large drop of 12 points in performance .","['Note', 'that', 'TF', '-', 'IDF', 'is', 'by', 'far', 'the', 'most', 'impactful', 'feature', ',', 'leading', 'to', 'a', 'large', 'drop', 'of', '12', 'points', 'in', 'performance', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O']","['NN', 'IN', 'NNP', ':', 'NN', 'VBZ', 'IN', 'RB', 'DT', 'RBS', 'JJ', 'NN', ',', 'VBG', 'TO', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NNS', 'IN', 'NN', '.']",24
natural_language_inference,43,155,"Code , data , annotations , and experiments for this paper are available on the CodaLab platform at https://worksheets. codalab.org/worksheets/ 0x91d77db37e0a4bbbaeb37b8972f4784f/.","['Code', ',', 'data', ',', 'annotations', ',', 'and', 'experiments', 'for', 'this', 'paper', 'are', 'available', 'on', 'the', 'CodaLab', 'platform', 'at', 'https://worksheets.', 'codalab.org/worksheets/', '0x91d77db37e0a4bbbaeb37b8972f4784f/.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', ',', 'NNS', ',', 'NNS', ',', 'CC', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'JJ', 'IN', 'DT', 'NNP', 'NN', 'IN', 'NN', 'NN', 'CD']",21
natural_language_inference,21,2,DiSAN : Directional Self - Attention Network for RNN / CNN - Free Language Understanding,"['DiSAN', ':', 'Directional', 'Self', '-', 'Attention', 'Network', 'for', 'RNN', '/', 'CNN', '-', 'Free', 'Language', 'Understanding']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NN', ':', 'NNP', 'NNP', ':', 'NN', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', ':', 'JJ', 'NN', 'VBG']",15
natural_language_inference,21,31,"We propose a novel attention mechanism that differs from previous ones in that it is 1 ) multi-dimensional : the attention w.r.t. each pair of elements from the source ( s ) is a vector , where each entry is the attention computed on each feature ; and 2 ) directional : it uses one or multiple positional masks to model the asymmetric attention between two elements .","['We', 'propose', 'a', 'novel', 'attention', 'mechanism', 'that', 'differs', 'from', 'previous', 'ones', 'in', 'that', 'it', 'is', '1', ')', 'multi-dimensional', ':', 'the', 'attention', 'w.r.t.', 'each', 'pair', 'of', 'elements', 'from', 'the', 'source', '(', 's', ')', 'is', 'a', 'vector', ',', 'where', 'each', 'entry', 'is', 'the', 'attention', 'computed', 'on', 'each', 'feature', ';', 'and', '2', ')', 'directional', ':', 'it', 'uses', 'one', 'or', 'multiple', 'positional', 'masks', 'to', 'model', 'the', 'asymmetric', 'attention', 'between', 'two', 'elements', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'PRP', 'VBZ', 'CD', ')', 'NN', ':', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', '(', 'NN', ')', 'VBZ', 'DT', 'NN', ',', 'WRB', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', ':', 'CC', 'CD', ')', 'NN', ':', 'PRP', 'VBZ', 'CD', 'CC', 'JJ', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NNS', '.']",68
natural_language_inference,21,32,"We compute feature - wise attention since each element in a sequence is usually represented by a vector , e.g. , word / character embedding , and attention on different features can contain different information about dependency , thus to handle the variation of contexts around the same word .","['We', 'compute', 'feature', '-', 'wise', 'attention', 'since', 'each', 'element', 'in', 'a', 'sequence', 'is', 'usually', 'represented', 'by', 'a', 'vector', ',', 'e.g.', ',', 'word', '/', 'character', 'embedding', ',', 'and', 'attention', 'on', 'different', 'features', 'can', 'contain', 'different', 'information', 'about', 'dependency', ',', 'thus', 'to', 'handle', 'the', 'variation', 'of', 'contexts', 'around', 'the', 'same', 'word', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', ':', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'DT', 'NN', ',', 'NN', ',', 'NN', 'NNP', 'NN', 'NN', ',', 'CC', 'NN', 'IN', 'JJ', 'NNS', 'MD', 'VB', 'JJ', 'NN', 'IN', 'NN', ',', 'RB', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",50
natural_language_inference,21,33,We apply positional masks to attention distribution since they can easily encode prior structure knowledge such as temporal order and dependency parsing .,"['We', 'apply', 'positional', 'masks', 'to', 'attention', 'distribution', 'since', 'they', 'can', 'easily', 'encode', 'prior', 'structure', 'knowledge', 'such', 'as', 'temporal', 'order', 'and', 'dependency', 'parsing', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NNS', 'TO', 'NN', 'NN', 'IN', 'PRP', 'MD', 'RB', 'VB', 'JJ', 'NN', 'NN', 'JJ', 'IN', 'JJ', 'NN', 'CC', 'NN', 'NN', '.']",23
natural_language_inference,21,35,"We then build a light - weight and RNN / CNN - free neural network , "" Directional Self - Attention Network ( DiSAN ) "" , for sentence encoding .","['We', 'then', 'build', 'a', 'light', '-', 'weight', 'and', 'RNN', '/', 'CNN', '-', 'free', 'neural', 'network', ',', '""', 'Directional', 'Self', '-', 'Attention', 'Network', '(', 'DiSAN', ')', '""', ',', 'for', 'sentence', 'encoding', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VB', 'DT', 'JJ', ':', 'NN', 'CC', 'NNP', 'NNP', 'NNP', ':', 'JJ', 'JJ', 'NN', ',', 'NNP', 'NNP', 'NNP', ':', 'NN', 'NNP', '(', 'NNP', ')', 'NN', ',', 'IN', 'NN', 'NN', '.']",31
natural_language_inference,21,37,"In DiSAN , the input sequence is processed by directional ( forward and backward ) self - attentions to model context dependency and produce context - aware representations for all tokens .","['In', 'DiSAN', ',', 'the', 'input', 'sequence', 'is', 'processed', 'by', 'directional', '(', 'forward', 'and', 'backward', ')', 'self', '-', 'attentions', 'to', 'model', 'context', 'dependency', 'and', 'produce', 'context', '-', 'aware', 'representations', 'for', 'all', 'tokens', '.']","['B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'NNP', ',', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', '(', 'RB', 'CC', 'RB', ')', 'PRP', ':', 'NNS', 'TO', 'VB', 'JJ', 'NN', 'CC', 'VB', 'JJ', ':', 'JJ', 'NNS', 'IN', 'DT', 'NNS', '.']",32
natural_language_inference,21,38,"Then , a multi-dimensional attention computes a vector representation of the entire sequence , which can be passed into a classification / regression module to compute the final prediction for a particular task .","['Then', ',', 'a', 'multi-dimensional', 'attention', 'computes', 'a', 'vector', 'representation', 'of', 'the', 'entire', 'sequence', ',', 'which', 'can', 'be', 'passed', 'into', 'a', 'classification', '/', 'regression', 'module', 'to', 'compute', 'the', 'final', 'prediction', 'for', 'a', 'particular', 'task', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'JJ', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'WDT', 'MD', 'VB', 'VBN', 'IN', 'DT', 'NN', 'POS', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",34
natural_language_inference,21,180,We use cross-entropy loss plus L2 regularization penalty as optimization objective .,"['We', 'use', 'cross-entropy', 'loss', 'plus', 'L2', 'regularization', 'penalty', 'as', 'optimization', 'objective', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'CC', 'NNP', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",12
natural_language_inference,21,181,We minimize it by Adadelta ) ( an optimizer of mini - batch SGD ) with batch size of 64 .,"['We', 'minimize', 'it', 'by', 'Adadelta', ')', '(', 'an', 'optimizer', 'of', 'mini', '-', 'batch', 'SGD', ')', 'with', 'batch', 'size', 'of', '64', '.']","['O', 'B-n', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'O']","['PRP', 'VBP', 'PRP', 'IN', 'NNP', ')', '(', 'DT', 'NN', 'IN', 'NN', ':', 'NN', 'NNP', ')', 'IN', 'NN', 'NN', 'IN', 'CD', '.']",21
natural_language_inference,21,183,Initial learning rate is set to 0.5 .,"['Initial', 'learning', 'rate', 'is', 'set', 'to', '0.5', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['JJ', 'VBG', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",8
natural_language_inference,21,184,"All weight matrices are initialized by Glorot Initialization , and the biases are initialized with 0 .","['All', 'weight', 'matrices', 'are', 'initialized', 'by', 'Glorot', 'Initialization', ',', 'and', 'the', 'biases', 'are', 'initialized', 'with', '0', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'NNP', ',', 'CC', 'DT', 'NNS', 'VBP', 'VBN', 'IN', 'CD', '.']",17
natural_language_inference,21,185,We initialize the word embedding in x by 300D Glo Ve 6B pre-trained vectors .,"['We', 'initialize', 'the', 'word', 'embedding', 'in', 'x', 'by', '300D', 'Glo', 'Ve', '6B', 'pre-trained', 'vectors', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'NN', 'IN', 'CD', 'NNP', 'NNP', 'CD', 'JJ', 'NNS', '.']",15
natural_language_inference,21,186,"The Out - of - Vocabulary words in training set are randomly initialized by uniform distribution between ( ? 0.05 , 0.05 ) .","['The', 'Out', '-', 'of', '-', 'Vocabulary', 'words', 'in', 'training', 'set', 'are', 'randomly', 'initialized', 'by', 'uniform', 'distribution', 'between', '(', '?', '0.05', ',', '0.05', ')', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', ':', 'IN', ':', 'JJ', 'NNS', 'IN', 'NN', 'VBN', 'VBP', 'JJ', 'VBN', 'IN', 'JJ', 'NN', 'IN', '(', '.', 'CD', ',', 'CD', ')', '.']",24
natural_language_inference,21,188,We use Dropout ) with keep probability 0.75 for language inference and 0.8 for sentiment analysis .,"['We', 'use', 'Dropout', ')', 'with', 'keep', 'probability', '0.75', 'for', 'language', 'inference', 'and', '0.8', 'for', 'sentiment', 'analysis', '.']","['O', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', ')', 'IN', 'JJ', 'NN', 'CD', 'IN', 'NN', 'NN', 'CC', 'CD', 'IN', 'NN', 'NN', '.']",17
natural_language_inference,21,189,"The L2 regularization decay factors ? are 5 10 ?5 and 10 ? 4 for language inference and sentiment analysis , respectively .","['The', 'L2', 'regularization', 'decay', 'factors', '?', 'are', '5', '10', '?5', 'and', '10', '?', '4', 'for', 'language', 'inference', 'and', 'sentiment', 'analysis', ',', 'respectively', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['DT', 'NNP', 'NN', 'NN', 'NNS', '.', 'VBP', 'CD', 'CD', 'NN', 'CC', 'CD', '.', 'CD', 'IN', 'NN', 'NN', 'CC', 'NN', 'NN', ',', 'RB', '.']",23
natural_language_inference,21,192,Hidden units number d h is set to 300 .,"['Hidden', 'units', 'number', 'd', 'h', 'is', 'set', 'to', '300', '.']","['B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['NNP', 'NNS', 'NN', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",10
natural_language_inference,21,193,"Activation functions ? ( ) are ELU ( exponential linear unit ) ( Clevert , Unterthiner , and Hochreiter 2016 ) if not specified .","['Activation', 'functions', '?', '(', ')', 'are', 'ELU', '(', 'exponential', 'linear', 'unit', ')', '(', 'Clevert', ',', 'Unterthiner', ',', 'and', 'Hochreiter', '2016', ')', 'if', 'not', 'specified', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NNS', '.', '(', ')', 'VBP', 'NNP', '(', 'JJ', 'JJ', 'NN', ')', '(', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', 'CD', ')', 'IN', 'RB', 'VBN', '.']",25
natural_language_inference,21,194,All models are implemented with TensorFlow 2 and run on sin - 3.0 m 83.9 80.6 1024D GRU encoders 15 m 98.8 81.4 300D Tree - based CNN encoders 3.5 m 83.3 82.1 300D SPINN - PI encoders 3.7 m 89.2 83.2 600D Bi- LSTM encoders 2.0 m 86.4 83.3 300D NTI - SLSTM - LSTM encoders 4.0 m 82.5 83.4 600D Bi-LSTM encoders+intra-attention 2.8 m 84.5 84.2 300D NSE encoders 3 gle Nvidia GTX 1080 Ti graphic card .,"['All', 'models', 'are', 'implemented', 'with', 'TensorFlow', '2', 'and', 'run', 'on', 'sin', '-', '3.0', 'm', '83.9', '80.6', '1024D', 'GRU', 'encoders', '15', 'm', '98.8', '81.4', '300D', 'Tree', '-', 'based', 'CNN', 'encoders', '3.5', 'm', '83.3', '82.1', '300D', 'SPINN', '-', 'PI', 'encoders', '3.7', 'm', '89.2', '83.2', '600D', 'Bi-', 'LSTM', 'encoders', '2.0', 'm', '86.4', '83.3', '300D', 'NTI', '-', 'SLSTM', '-', 'LSTM', 'encoders', '4.0', 'm', '82.5', '83.4', '600D', 'Bi-LSTM', 'encoders+intra-attention', '2.8', 'm', '84.5', '84.2', '300D', 'NSE', 'encoders', '3', 'gle', 'Nvidia', 'GTX', '1080', 'Ti', 'graphic', 'card', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'CD', 'CC', 'VB', 'IN', 'NN', ':', 'CD', 'NN', 'CD', 'CD', 'CD', 'NNP', 'NNS', 'CD', 'NNS', 'CD', 'CD', 'CD', 'NNP', ':', 'VBN', 'NNP', 'NNS', 'CD', 'NNS', 'CD', 'CD', 'CD', 'NNP', ':', 'NN', 'NNS', 'CD', 'NNS', 'CD', 'CD', 'CD', 'JJ', 'NNP', 'NNS', 'CD', 'NNS', 'CD', 'CD', 'CD', 'NNP', ':', 'NNP', ':', 'NNP', 'VBZ', 'CD', 'NN', 'CD', 'CD', 'CD', 'JJ', 'NN', 'CD', 'NN', 'CD', 'CD', 'CD', 'NNP', 'NNS', 'CD', 'JJ', 'NNP', 'NNP', 'CD', 'NNP', 'JJ', 'NN', '.']",80
natural_language_inference,21,195,Natural Language Inference,"['Natural', 'Language', 'Inference']","['B-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NN']",3
natural_language_inference,21,215,"Compared to the results from the official leaderboard of SNLI in , DiSAN outperforms previous works and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a remarkable margin of 1.02 % .","['Compared', 'to', 'the', 'results', 'from', 'the', 'official', 'leaderboard', 'of', 'SNLI', 'in', ',', 'DiSAN', 'outperforms', 'previous', 'works', 'and', 'improves', 'the', 'best', 'latest', 'test', 'accuracy', '(', 'achieved', 'by', 'a', 'memory', '-', 'based', 'NSE', 'encoder', 'network', ')', 'by', 'a', 'remarkable', 'margin', 'of', '1.02', '%', '.']","['B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['VBN', 'TO', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'IN', ',', 'NNP', 'VBZ', 'JJ', 'NNS', 'CC', 'VBZ', 'DT', 'JJS', 'JJS', 'NN', 'NN', '(', 'VBN', 'IN', 'DT', 'NN', ':', 'VBN', 'NNP', 'NN', 'NN', ')', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NN', '.']",42
natural_language_inference,21,216,"DiSAN surpasses the RNN / CNN based models with more complicated architecture and more parameters by large margins , e.g. , + 2.32 % to Bi - LSTM , + 1.42 % to Bi - LSTM with additive attention .","['DiSAN', 'surpasses', 'the', 'RNN', '/', 'CNN', 'based', 'models', 'with', 'more', 'complicated', 'architecture', 'and', 'more', 'parameters', 'by', 'large', 'margins', ',', 'e.g.', ',', '+', '2.32', '%', 'to', 'Bi', '-', 'LSTM', ',', '+', '1.42', '%', 'to', 'Bi', '-', 'LSTM', 'with', 'additive', 'attention', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'DT', 'NNP', 'NNP', 'NNP', 'VBN', 'NNS', 'IN', 'RBR', 'JJ', 'NN', 'CC', 'JJR', 'NNS', 'IN', 'JJ', 'NNS', ',', 'NN', ',', 'VB', 'CD', 'NN', 'TO', 'NNP', ':', 'NNP', ',', 'VBD', 'CD', 'NN', 'TO', 'NNP', ':', 'NN', 'IN', 'JJ', 'NN', '.']",40
natural_language_inference,21,217,"It even outperforms models with the assistance of a semantic parsing tree , e.g. , + 3.52 % to Tree - based CNN , + 2.42 % to SPINN - PI .","['It', 'even', 'outperforms', 'models', 'with', 'the', 'assistance', 'of', 'a', 'semantic', 'parsing', 'tree', ',', 'e.g.', ',', '+', '3.52', '%', 'to', 'Tree', '-', 'based', 'CNN', ',', '+', '2.42', '%', 'to', 'SPINN', '-', 'PI', '.']","['O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBZ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'NN', ',', 'VB', 'CD', 'NN', 'TO', 'NNP', ':', 'VBN', 'NNP', ',', 'VBD', 'CD', 'NN', 'TO', 'NNP', ':', 'NN', '.']",32
natural_language_inference,21,219,"First , a comparison between the first two models shows that changing token - wise attention to multi-dimensional / feature - wise attention leads to 3.31 % improvement on a word embedding based model .","['First', ',', 'a', 'comparison', 'between', 'the', 'first', 'two', 'models', 'shows', 'that', 'changing', 'token', '-', 'wise', 'attention', 'to', 'multi-dimensional', '/', 'feature', '-', 'wise', 'attention', 'leads', 'to', '3.31', '%', 'improvement', 'on', 'a', 'word', 'embedding', 'based', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'IN', 'DT', 'JJ', 'CD', 'NNS', 'VBZ', 'IN', 'VBG', 'SYM', ':', 'NN', 'NN', 'TO', 'JJ', 'JJ', 'NN', ':', 'NN', 'NN', 'VBZ', 'TO', 'CD', 'NN', 'NN', 'IN', 'DT', 'NN', 'VBG', 'VBN', 'NN', '.']",35
natural_language_inference,21,220,"Also , a comparison between the third baseline and DiSAN shows that DiSAN can substantially outperform multi-head attention by 1.45 % .","['Also', ',', 'a', 'comparison', 'between', 'the', 'third', 'baseline', 'and', 'DiSAN', 'shows', 'that', 'DiSAN', 'can', 'substantially', 'outperform', 'multi-head', 'attention', 'by', '1.45', '%', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'CC', 'NNP', 'VBZ', 'IN', 'NNP', 'MD', 'RB', 'VB', 'JJ', 'NN', 'IN', 'CD', 'NN', '.']",22
natural_language_inference,21,221,"Moreover , a comparison between the forth baseline and DiSAN shows that the DiSA block can even outperform Bi - LSTM layer in context encoding , improving test accuracy by 0.64 % .","['Moreover', ',', 'a', 'comparison', 'between', 'the', 'forth', 'baseline', 'and', 'DiSAN', 'shows', 'that', 'the', 'DiSA', 'block', 'can', 'even', 'outperform', 'Bi', '-', 'LSTM', 'layer', 'in', 'context', 'encoding', ',', 'improving', 'test', 'accuracy', 'by', '0.64', '%', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'NNP', 'VBZ', 'IN', 'DT', 'NNP', 'NN', 'MD', 'RB', 'VB', 'NNP', ':', 'NNP', 'NN', 'IN', 'NN', 'NN', ',', 'VBG', 'NN', 'NN', 'IN', 'CD', 'NN', '.']",33
natural_language_inference,21,222,A comparison between the fifth baseline and DiSAN shows that directional self - attention with forward and backward masks ( with temporal order encoded ) can bring 0.96 % improvement .,"['A', 'comparison', 'between', 'the', 'fifth', 'baseline', 'and', 'DiSAN', 'shows', 'that', 'directional', 'self', '-', 'attention', 'with', 'forward', 'and', 'backward', 'masks', '(', 'with', 'temporal', 'order', 'encoded', ')', 'can', 'bring', '0.96', '%', 'improvement', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'CC', 'NNP', 'VBZ', 'IN', 'JJ', 'NN', ':', 'NN', 'IN', 'NN', 'CC', 'NN', 'NNS', '(', 'IN', 'JJ', 'NN', 'VBD', ')', 'MD', 'VB', 'CD', 'NN', 'NN', '.']",31
natural_language_inference,21,226,Sentiment Analysis,"['Sentiment', 'Analysis']","['B-n', 'I-n']","['NN', 'NN']",2
natural_language_inference,21,235,"To the best of our knowledge , DiSAN improves the last best accuracy ( given by CNN - Tensor ) by 0.52 % .","['To', 'the', 'best', 'of', 'our', 'knowledge', ',', 'DiSAN', 'improves', 'the', 'last', 'best', 'accuracy', '(', 'given', 'by', 'CNN', '-', 'Tensor', ')', 'by', '0.52', '%', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['TO', 'DT', 'JJS', 'IN', 'PRP$', 'NN', ',', 'NNP', 'VBZ', 'DT', 'JJ', 'JJS', 'NN', '(', 'VBN', 'IN', 'NNP', ':', 'NN', ')', 'IN', 'CD', 'NN', '.']",24
natural_language_inference,21,236,"Compared to tree - based models with heavy use of the prior structure , e.g. , MV - RNN , RNTN and Tree - LSTM , DiSAN outperforms them by 7.32 % , 6.02 % and 0.72 % , respectively .","['Compared', 'to', 'tree', '-', 'based', 'models', 'with', 'heavy', 'use', 'of', 'the', 'prior', 'structure', ',', 'e.g.', ',', 'MV', '-', 'RNN', ',', 'RNTN', 'and', 'Tree', '-', 'LSTM', ',', 'DiSAN', 'outperforms', 'them', 'by', '7.32', '%', ',', '6.02', '%', 'and', '0.72', '%', ',', 'respectively', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['VBN', 'TO', 'VB', ':', 'VBN', 'NNS', 'IN', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'NN', ',', 'NNP', ':', 'NNP', ',', 'NNP', 'CC', 'NNP', ':', 'NNP', ',', 'NNP', 'VBZ', 'PRP', 'IN', 'CD', 'NN', ',', 'CD', 'NN', 'CC', 'CD', 'NN', ',', 'RB', '.']",41
natural_language_inference,21,237,"Additionally , DiSAN achieves better performance than CNN - based models .","['Additionally', ',', 'DiSAN', 'achieves', 'better', 'performance', 'than', 'CNN', '-', 'based', 'models', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'NNP', 'VBZ', 'JJR', 'NN', 'IN', 'NNP', ':', 'VBN', 'NNS', '.']",12
natural_language_inference,21,240,"Nonetheless , DiSAN still outperforms these fancy models , such as NCSL ( + 0.62 % ) and LR- Bi- LSTM ( + 1.12 % ) . :","['Nonetheless', ',', 'DiSAN', 'still', 'outperforms', 'these', 'fancy', 'models', ',', 'such', 'as', 'NCSL', '(', '+', '0.62', '%', ')', 'and', 'LR-', 'Bi-', 'LSTM', '(', '+', '1.12', '%', ')', '.', ':']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['RB', ',', 'NNP', 'RB', 'VBZ', 'DT', 'JJ', 'NNS', ',', 'JJ', 'IN', 'NNP', '(', 'VB', 'CD', 'NN', ')', 'CC', 'JJ', 'NNP', 'NNP', '(', 'VB', 'CD', 'NN', ')', '.', ':']",28
natural_language_inference,5,2,A Compare - Aggregate Model with Latent Clustering for Answer Selection,"['A', 'Compare', '-', 'Aggregate', 'Model', 'with', 'Latent', 'Clustering', 'for', 'Answer', 'Selection']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['DT', 'NNP', ':', 'NNP', 'NNP', 'IN', 'NNP', 'VBG', 'IN', 'NNP', 'NNP']",11
natural_language_inference,5,4,"In this paper , we propose a novel method for a sentence - level answer- selection task that is a fundamental problem in natural language processing .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', 'method', 'for', 'a', 'sentence', '-', 'level', 'answer-', 'selection', 'task', 'that', 'is', 'a', 'fundamental', 'problem', 'in', 'natural', 'language', 'processing', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', ':', 'NN', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",27
natural_language_inference,5,10,Automatic question answering ( QA ) is a primary objective of artificial intelligence .,"['Automatic', 'question', 'answering', '(', 'QA', ')', 'is', 'a', 'primary', 'objective', 'of', 'artificial', 'intelligence', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', '.']",14
natural_language_inference,5,21,"First , we explore the effect of additional information by adopting a pretrained language model ( LM ) to compute the vector representation of the input text .","['First', ',', 'we', 'explore', 'the', 'effect', 'of', 'additional', 'information', 'by', 'adopting', 'a', 'pretrained', 'language', 'model', '(', 'LM', ')', 'to', 'compute', 'the', 'vector', 'representation', 'of', 'the', 'input', 'text', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",28
natural_language_inference,5,23,"Following this study , we select an ELMo language model for this study .","['Following', 'this', 'study', ',', 'we', 'select', 'an', 'ELMo', 'language', 'model', 'for', 'this', 'study', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['VBG', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', 'NN', 'NN', 'IN', 'DT', 'NN', '.']",14
natural_language_inference,5,24,"We investigate the applicability of transfer learning ( TL ) using a large - scale corpus that is created for a relevant - sentence - selection task ( i.e. , question - answering NLI ( QNLI ) dataset ) .","['We', 'investigate', 'the', 'applicability', 'of', 'transfer', 'learning', '(', 'TL', ')', 'using', 'a', 'large', '-', 'scale', 'corpus', 'that', 'is', 'created', 'for', 'a', 'relevant', '-', 'sentence', '-', 'selection', 'task', '(', 'i.e.', ',', 'question', '-', 'answering', 'NLI', '(', 'QNLI', ')', 'dataset', ')', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'NN', '(', 'NNP', ')', 'VBG', 'DT', 'JJ', ':', 'NN', 'NN', 'WDT', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', ':', 'NN', ':', 'NN', 'NN', '(', 'FW', ',', 'NN', ':', 'NN', 'NNP', '(', 'NNP', ')', 'NN', ')', '.']",40
natural_language_inference,5,25,"Second , we further enhance one of the baseline models , Comp - Clip ( refer to the discussion in 3.1 ) , for the target QA task by proposing a novel latent clustering ( LC ) method .","['Second', ',', 'we', 'further', 'enhance', 'one', 'of', 'the', 'baseline', 'models', ',', 'Comp', '-', 'Clip', '(', 'refer', 'to', 'the', 'discussion', 'in', '3.1', ')', ',', 'for', 'the', 'target', 'QA', 'task', 'by', 'proposing', 'a', 'novel', 'latent', 'clustering', '(', 'LC', ')', 'method', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', ',', 'PRP', 'VBP', 'VB', 'CD', 'IN', 'DT', 'NN', 'NNS', ',', 'NNP', ':', 'NNP', '(', 'VBP', 'TO', 'DT', 'NN', 'IN', 'CD', ')', ',', 'IN', 'DT', 'NN', 'NNP', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'NN', '.']",39
natural_language_inference,5,26,The LC method computes latent cluster information for target samples by creating a latent memory space and calculating the similarity between the sample and the memory .,"['The', 'LC', 'method', 'computes', 'latent', 'cluster', 'information', 'for', 'target', 'samples', 'by', 'creating', 'a', 'latent', 'memory', 'space', 'and', 'calculating', 'the', 'similarity', 'between', 'the', 'sample', 'and', 'the', 'memory', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'O']","['DT', 'NNP', 'NN', 'VBZ', 'JJ', 'NN', 'NN', 'IN', 'NN', 'NNS', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'CC', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'CC', 'DT', 'NN', '.']",27
natural_language_inference,5,27,"By an endto - end learning process with the answer-selection task , the LC method assigns true - label question - answer pairs to similar clusters .","['By', 'an', 'endto', '-', 'end', 'learning', 'process', 'with', 'the', 'answer-selection', 'task', ',', 'the', 'LC', 'method', 'assigns', 'true', '-', 'label', 'question', '-', 'answer', 'pairs', 'to', 'similar', 'clusters', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ':', 'NN', 'VBG', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'DT', 'NNP', 'NN', 'VBZ', 'JJ', ':', 'NN', 'NN', ':', 'NN', 'NNS', 'TO', 'JJ', 'NNS', '.']",27
natural_language_inference,5,29,"Last , we explore the effect of different objective functions ( listwise and pointwise learning ) .","['Last', ',', 'we', 'explore', 'the', 'effect', 'of', 'different', 'objective', 'functions', '(', 'listwise', 'and', 'pointwise', 'learning', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['JJ', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'JJ', 'NNS', '(', 'NN', 'CC', 'VB', 'NN', ')', '.']",17
natural_language_inference,5,112,"To implement the Comp - Clip model , we apply a context projection weight matrix with 100 dimensions that are shared between the question part and the answer part ( eq. 1 ) .","['To', 'implement', 'the', 'Comp', '-', 'Clip', 'model', ',', 'we', 'apply', 'a', 'context', 'projection', 'weight', 'matrix', 'with', '100', 'dimensions', 'that', 'are', 'shared', 'between', 'the', 'question', 'part', 'and', 'the', 'answer', 'part', '(', 'eq.', '1', ')', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'NNP', ':', 'NNP', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'VBD', 'NN', 'IN', 'CD', 'NNS', 'WDT', 'VBP', 'VBN', 'IN', 'DT', 'NN', 'NN', 'CC', 'DT', 'JJR', 'NN', '(', 'JJ', 'CD', ')', '.']",34
natural_language_inference,5,113,"In the aggregation part , we use 1 - D CNN with a total of 500 filters , which involves five types of filters K ? R {1,2,3,4,5}100 , 100 per type .","['In', 'the', 'aggregation', 'part', ',', 'we', 'use', '1', '-', 'D', 'CNN', 'with', 'a', 'total', 'of', '500', 'filters', ',', 'which', 'involves', 'five', 'types', 'of', 'filters', 'K', '?', 'R', '{1,2,3,4,5}100', ',', '100', 'per', 'type', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'NN', ',', 'PRP', 'VBP', 'CD', ':', 'NNP', 'NNP', 'IN', 'DT', 'NN', 'IN', 'CD', 'NNS', ',', 'WDT', 'VBZ', 'CD', 'NNS', 'IN', 'NNS', 'NNP', '.', 'NNP', 'NNP', ',', 'CD', 'IN', 'NN', '.']",33
natural_language_inference,5,117,"We select k ( for the kmax - pool in equation 5 ) as 6 and 4 for the WikiQA and TREC - QA case , respectively .","['We', 'select', 'k', '(', 'for', 'the', 'kmax', '-', 'pool', 'in', 'equation', '5', ')', 'as', '6', 'and', '4', 'for', 'the', 'WikiQA', 'and', 'TREC', '-', 'QA', 'case', ',', 'respectively', '.']","['O', 'B-p', 'B-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['PRP', 'VBP', 'FW', '(', 'IN', 'DT', 'NN', ':', 'NN', 'IN', 'NN', 'CD', ')', 'IN', 'CD', 'CC', 'CD', 'IN', 'DT', 'NNP', 'CC', 'NNP', ':', 'NNP', 'NN', ',', 'RB', '.']",28
natural_language_inference,5,118,"In both datasets , we apply 8 latent clusters .","['In', 'both', 'datasets', ',', 'we', 'apply', '8', 'latent', 'clusters', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'PRP', 'VBP', 'CD', 'JJ', 'NNS', '.']",10
natural_language_inference,5,119,"The vocabulary size in the WiKiQA , TREC - QA and QNLI dataset are 30,104 , 56,908 and 154,442 , respectively .","['The', 'vocabulary', 'size', 'in', 'the', 'WiKiQA', ',', 'TREC', '-', 'QA', 'and', 'QNLI', 'dataset', 'are', '30,104', ',', '56,908', 'and', '154,442', ',', 'respectively', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', ',', 'NNP', ':', 'NN', 'CC', 'NNP', 'NN', 'VBP', 'CD', ',', 'CD', 'CC', 'CD', ',', 'RB', '.']",22
natural_language_inference,5,120,"When applying the TL , the vocabulary size is set to 154,442 , and the dimension of the context projection weight matrix is set to 300 .","['When', 'applying', 'the', 'TL', ',', 'the', 'vocabulary', 'size', 'is', 'set', 'to', '154,442', ',', 'and', 'the', 'dimension', 'of', 'the', 'context', 'projection', 'weight', 'matrix', 'is', 'set', 'to', '300', '.']","['O', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['WRB', 'VBG', 'DT', 'NNP', ',', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'CD', ',', 'CC', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",27
natural_language_inference,5,121,"We use the Adam optimizer , including gradient clipping , by the norm at a threshold of 5 .","['We', 'use', 'the', 'Adam', 'optimizer', ',', 'including', 'gradient', 'clipping', ',', 'by', 'the', 'norm', 'at', 'a', 'threshold', 'of', '5', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', ',', 'VBG', 'NN', 'NN', ',', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'CD', '.']",19
natural_language_inference,5,122,"For the purpose of regularization , we applied a dropout with a ratio of 0.5 ..","['For', 'the', 'purpose', 'of', 'regularization', ',', 'we', 'applied', 'a', 'dropout', 'with', 'a', 'ratio', 'of', '0.5', '..']","['B-p', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'O']","['IN', 'DT', 'NN', 'IN', 'NN', ',', 'PRP', 'VBD', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'CD', 'NNS']",16
natural_language_inference,5,127,"Wiki QA : For the WikiQA dataset , the pointwise learning approach shows a better performance than the listwise learning approach .","['Wiki', 'QA', ':', 'For', 'the', 'WikiQA', 'dataset', ',', 'the', 'pointwise', 'learning', 'approach', 'shows', 'a', 'better', 'performance', 'than', 'the', 'listwise', 'learning', 'approach', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', ':', 'IN', 'DT', 'NNP', 'NN', ',', 'DT', 'NN', 'NN', 'NN', 'VBZ', 'DT', 'JJR', 'NN', 'IN', 'DT', 'NN', 'VBG', 'NN', '.']",22
natural_language_inference,5,128,We combine LM with the base model ( Comp - Clip + LM ) and observe a significant improvement in performance in terms of MAP ( 0.714 to 0.746 absolute ) .,"['We', 'combine', 'LM', 'with', 'the', 'base', 'model', '(', 'Comp', '-', 'Clip', '+', 'LM', ')', 'and', 'observe', 'a', 'significant', 'improvement', 'in', 'performance', 'in', 'terms', 'of', 'MAP', '(', '0.714', 'to', '0.746', 'absolute', ')', '.']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'VBN', 'IN', 'DT', 'NN', 'NN', '(', 'NNP', ':', 'NNP', 'NNP', 'NNP', ')', 'CC', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'NNS', 'IN', 'NNP', '(', 'CD', 'TO', 'CD', 'NN', ')', '.']",32
natural_language_inference,5,129,"When we add the LC method ( Comp - Clip + LM + LC ) , the best previous results are surpassed in terms of MAP ( 0.718 to 0.764 absolute ) .","['When', 'we', 'add', 'the', 'LC', 'method', '(', 'Comp', '-', 'Clip', '+', 'LM', '+', 'LC', ')', ',', 'the', 'best', 'previous', 'results', 'are', 'surpassed', 'in', 'terms', 'of', 'MAP', '(', '0.718', 'to', '0.764', 'absolute', ')', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['WRB', 'PRP', 'VBP', 'DT', 'NNP', 'NN', '(', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', ')', ',', 'DT', 'JJS', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'NNS', 'IN', 'NNP', '(', 'CD', 'TO', 'CD', 'NN', ')', '.']",33
natural_language_inference,5,132,The pointwise learning approach also shows excellent performance with the TREC - QA dataset .,"['The', 'pointwise', 'learning', 'approach', 'also', 'shows', 'excellent', 'performance', 'with', 'the', 'TREC', '-', 'QA', 'dataset', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'NN', 'RB', 'VBZ', 'JJ', 'NN', 'IN', 'DT', 'NNP', ':', 'NN', 'NN', '.']",15
natural_language_inference,5,135,"As in the WikiQA case , we achieve additional performance gains in terms of the MAP as we apply LM , LC , and TL ( 0.850 , 0.868 and 0.875 , respectively ) .","['As', 'in', 'the', 'WikiQA', 'case', ',', 'we', 'achieve', 'additional', 'performance', 'gains', 'in', 'terms', 'of', 'the', 'MAP', 'as', 'we', 'apply', 'LM', ',', 'LC', ',', 'and', 'TL', '(', '0.850', ',', '0.868', 'and', '0.875', ',', 'respectively', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['IN', 'IN', 'DT', 'NNP', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'NNP', 'IN', 'PRP', 'VBP', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', '(', 'CD', ',', 'CD', 'CC', 'CD', ',', 'RB', ')', '.']",35
natural_language_inference,5,136,"In particular , our model outperforms the best previous result when we add LC method , ( Comp - Clip + LM + LC ) in terms of MAP ( 0.865 to 0.868 ) .","['In', 'particular', ',', 'our', 'model', 'outperforms', 'the', 'best', 'previous', 'result', 'when', 'we', 'add', 'LC', 'method', ',', '(', 'Comp', '-', 'Clip', '+', 'LM', '+', 'LC', ')', 'in', 'terms', 'of', 'MAP', '(', '0.865', 'to', '0.868', ')', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O']","['IN', 'JJ', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'JJS', 'JJ', 'NN', 'WRB', 'PRP', 'VBP', 'NNP', 'NN', ',', '(', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', ')', 'IN', 'NNS', 'IN', 'NNP', '(', 'CD', 'TO', 'CD', ')', '.']",35
natural_language_inference,11,2,Dynamic Integration of Background Knowledge in Neural NLU Systems,"['Dynamic', 'Integration', 'of', 'Background', 'Knowledge', 'in', 'Neural', 'NLU', 'Systems']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['NNP', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNPS']",9
natural_language_inference,11,4,"Common- sense and background knowledge is required to understand natural language , but in most neural natural language understanding ( NLU ) systems , this knowledge must be acquired from training corpora during learning , and then it is static at test time .","['Common-', 'sense', 'and', 'background', 'knowledge', 'is', 'required', 'to', 'understand', 'natural', 'language', ',', 'but', 'in', 'most', 'neural', 'natural', 'language', 'understanding', '(', 'NLU', ')', 'systems', ',', 'this', 'knowledge', 'must', 'be', 'acquired', 'from', 'training', 'corpora', 'during', 'learning', ',', 'and', 'then', 'it', 'is', 'static', 'at', 'test', 'time', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'CC', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'JJ', 'NN', ',', 'CC', 'IN', 'JJS', 'JJ', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'NNS', ',', 'DT', 'NN', 'MD', 'VB', 'VBN', 'IN', 'VBG', 'NN', 'IN', 'NN', ',', 'CC', 'RB', 'PRP', 'VBZ', 'JJ', 'IN', 'NN', 'NN', '.']",44
natural_language_inference,11,21,"In this paper , we develop a new architecture for dynamically incorporating external background knowledge in NLU models .","['In', 'this', 'paper', ',', 'we', 'develop', 'a', 'new', 'architecture', 'for', 'dynamically', 'incorporating', 'external', 'background', 'knowledge', 'in', 'NLU', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'RB', 'VBG', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'NNS', '.']",19
natural_language_inference,11,22,"Rather than relying only on static knowledge implicitly present in the training data , supplementary knowledge is retrieved from external knowledge sources ( in this paper , ConceptNet and Wikipedia ) to assist with understanding text inputs .","['Rather', 'than', 'relying', 'only', 'on', 'static', 'knowledge', 'implicitly', 'present', 'in', 'the', 'training', 'data', ',', 'supplementary', 'knowledge', 'is', 'retrieved', 'from', 'external', 'knowledge', 'sources', '(', 'in', 'this', 'paper', ',', 'ConceptNet', 'and', 'Wikipedia', ')', 'to', 'assist', 'with', 'understanding', 'text', 'inputs', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['RB', 'IN', 'VBG', 'RB', 'IN', 'JJ', 'NN', 'RB', 'JJ', 'IN', 'DT', 'NN', 'NNS', ',', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', 'NN', 'NNS', '(', 'IN', 'DT', 'NN', ',', 'NNP', 'CC', 'NNP', ')', 'TO', 'VB', 'IN', 'JJ', 'NN', 'NNS', '.']",38
natural_language_inference,11,24,The retrieved supplementary texts are read together with the task inputs by an initial reading module whose outputs are contextually refined word embeddings ( 3 ) .,"['The', 'retrieved', 'supplementary', 'texts', 'are', 'read', 'together', 'with', 'the', 'task', 'inputs', 'by', 'an', 'initial', 'reading', 'module', 'whose', 'outputs', 'are', 'contextually', 'refined', 'word', 'embeddings', '(', '3', ')', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'JJ', 'NNS', 'VBP', 'VBN', 'RB', 'IN', 'DT', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', 'WP$', 'NNS', 'VBP', 'RB', 'VBN', 'NN', 'NNS', '(', 'CD', ')', '.']",27
natural_language_inference,11,25,These refined embeddings are then used as input to a task - specific NLU architecture ( any architecture that reads text as a sequence of word embeddings can be used here ) .,"['These', 'refined', 'embeddings', 'are', 'then', 'used', 'as', 'input', 'to', 'a', 'task', '-', 'specific', 'NLU', 'architecture', '(', 'any', 'architecture', 'that', 'reads', 'text', 'as', 'a', 'sequence', 'of', 'word', 'embeddings', 'can', 'be', 'used', 'here', ')', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'VBN', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'NN', 'TO', 'DT', 'NN', ':', 'JJ', 'NNP', 'NN', '(', 'DT', 'NN', 'WDT', 'VBZ', 'RB', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', 'MD', 'VB', 'VBN', 'RB', ')', '.']",33
natural_language_inference,11,26,"The initial reading module and the task module are learnt jointly , end - to - end .","['The', 'initial', 'reading', 'module', 'and', 'the', 'task', 'module', 'are', 'learnt', 'jointly', ',', 'end', '-', 'to', '-', 'end', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'CC', 'DT', 'NN', 'NN', 'VBP', 'VBN', 'RB', ',', 'VB', ':', 'TO', ':', 'NN', '.']",18
natural_language_inference,11,101,All models are trained end - to - end jointly with the refinement module using a dimensionality of n = 300 for all but the TriviaQA experiments for which we had to reduce n to 150 due to memory constraints .,"['All', 'models', 'are', 'trained', 'end', '-', 'to', '-', 'end', 'jointly', 'with', 'the', 'refinement', 'module', 'using', 'a', 'dimensionality', 'of', 'n', '=', '300', 'for', 'all', 'but', 'the', 'TriviaQA', 'experiments', 'for', 'which', 'we', 'had', 'to', 'reduce', 'n', 'to', '150', 'due', 'to', 'memory', 'constraints', '.']","['B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'JJ', 'NN', ':', 'TO', ':', 'VB', 'RB', 'IN', 'DT', 'NN', 'NN', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'FW', 'CD', 'IN', 'DT', 'CC', 'DT', 'NNP', 'NNS', 'IN', 'WDT', 'PRP', 'VBD', 'TO', 'VB', 'NNS', 'TO', 'CD', 'JJ', 'TO', 'NN', 'NNS', '.']",41
natural_language_inference,11,102,All baselines operate on the unrefined word embeddings E 0 described in 3.1 .,"['All', 'baselines', 'operate', 'on', 'the', 'unrefined', 'word', 'embeddings', 'E', '0', 'described', 'in', '3.1', '.']","['B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'VBP', 'IN', 'DT', 'JJ', 'NN', 'NNS', 'NNP', 'CD', 'VBD', 'IN', 'CD', '.']",14
natural_language_inference,11,103,For the DQA baseline system we add the lemma in - question feature ( liq ) suggested in .,"['For', 'the', 'DQA', 'baseline', 'system', 'we', 'add', 'the', 'lemma', 'in', '-', 'question', 'feature', '(', 'liq', ')', 'suggested', 'in', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['IN', 'DT', 'NNP', 'NN', 'NN', 'PRP', 'VBP', 'DT', 'NN', 'IN', ':', 'NN', 'NN', '(', 'NN', ')', 'VBD', 'IN', '.']",19
natural_language_inference,11,138,"Wikipedia ( W ) yields further , significant improvements on TriviaQA , slightly outperforming the current state of the art model .","['Wikipedia', '(', 'W', ')', 'yields', 'further', ',', 'significant', 'improvements', 'on', 'TriviaQA', ',', 'slightly', 'outperforming', 'the', 'current', 'state', 'of', 'the', 'art', 'model', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', '(', 'NNP', ')', 'VBZ', 'JJ', ',', 'JJ', 'NNS', 'IN', 'NNP', ',', 'RB', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",22
natural_language_inference,11,147,shows the results of our RTE experiments .,"['shows', 'the', 'results', 'of', 'our', 'RTE', 'experiments', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['VBZ', 'DT', 'NNS', 'IN', 'PRP$', 'NNP', 'NNS', '.']",8
natural_language_inference,11,148,"In general , the introduction of our refinement strategy almost always helps , both with and without external knowledge .","['In', 'general', ',', 'the', 'introduction', 'of', 'our', 'refinement', 'strategy', 'almost', 'always', 'helps', ',', 'both', 'with', 'and', 'without', 'external', 'knowledge', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'JJ', ',', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'NN', 'RB', 'RB', 'VBZ', ',', 'DT', 'IN', 'CC', 'IN', 'JJ', 'NN', '.']",20
natural_language_inference,11,149,"When providing additional background knowledge from ConceptNet , our BiLSTM based models improve substantially , while the ESIM - based models improve only on the more difficult MultiNLI dataset .","['When', 'providing', 'additional', 'background', 'knowledge', 'from', 'ConceptNet', ',', 'our', 'BiLSTM', 'based', 'models', 'improve', 'substantially', ',', 'while', 'the', 'ESIM', '-', 'based', 'models', 'improve', 'only', 'on', 'the', 'more', 'difficult', 'MultiNLI', 'dataset', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['WRB', 'VBG', 'JJ', 'NN', 'NN', 'IN', 'NNP', ',', 'PRP$', 'NNP', 'VBN', 'NNS', 'VB', 'RB', ',', 'IN', 'DT', 'NNP', ':', 'VBN', 'NNS', 'VBP', 'RB', 'IN', 'DT', 'RBR', 'JJ', 'NNP', 'NN', '.']",30
natural_language_inference,11,150,"Compared to previously published state of the art systems , our models acquit themselves quite well on the MultiNLI benchmark , and competitively on the SNLI benchmark .","['Compared', 'to', 'previously', 'published', 'state', 'of', 'the', 'art', 'systems', ',', 'our', 'models', 'acquit', 'themselves', 'quite', 'well', 'on', 'the', 'MultiNLI', 'benchmark', ',', 'and', 'competitively', 'on', 'the', 'SNLI', 'benchmark', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['VBN', 'TO', 'RB', 'VBN', 'NN', 'IN', 'DT', 'NN', 'NNS', ',', 'PRP$', 'NNS', 'VBP', 'PRP', 'RB', 'RB', 'IN', 'DT', 'NNP', 'NN', ',', 'CC', 'RB', 'IN', 'DT', 'NNP', 'NN', '.']",28
natural_language_inference,11,154,"We do find that there is little impact of using external knowledge on the RTE task with ESIM , although the refinement strategy helps using just p + q.","['We', 'do', 'find', 'that', 'there', 'is', 'little', 'impact', 'of', 'using', 'external', 'knowledge', 'on', 'the', 'RTE', 'task', 'with', 'ESIM', ',', 'although', 'the', 'refinement', 'strategy', 'helps', 'using', 'just', 'p', '+', 'q.']","['O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'VB', 'IN', 'EX', 'VBZ', 'JJ', 'NN', 'IN', 'VBG', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'NNP', ',', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'VBG', 'RB', 'VBN', 'CD', 'NNS']",29
natural_language_inference,11,157,"Nevertheless , both ESIM and our BiL - STM models when trained with knowledge from ConceptNet are sensitive to the semantics of the provided assertions as demonstrated in our analysis in 5.3 .","['Nevertheless', ',', 'both', 'ESIM', 'and', 'our', 'BiL', '-', 'STM', 'models', 'when', 'trained', 'with', 'knowledge', 'from', 'ConceptNet', 'are', 'sensitive', 'to', 'the', 'semantics', 'of', 'the', 'provided', 'assertions', 'as', 'demonstrated', 'in', 'our', 'analysis', 'in', '5.3', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'DT', 'NNP', 'CC', 'PRP$', 'NNP', ':', 'NNP', 'NNS', 'WRB', 'VBN', 'IN', 'NN', 'IN', 'NNP', 'VBP', 'JJ', 'TO', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'VBN', 'IN', 'PRP$', 'NN', 'IN', 'CD', '.']",33
natural_language_inference,11,159,"Furthermore , increasing the coverage of assertions in ConceptNet would most likely yield improved performance even without retraining our models .","['Furthermore', ',', 'increasing', 'the', 'coverage', 'of', 'assertions', 'in', 'ConceptNet', 'would', 'most', 'likely', 'yield', 'improved', 'performance', 'even', 'without', 'retraining', 'our', 'models', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['RB', ',', 'VBG', 'DT', 'NN', 'IN', 'NNS', 'IN', 'NNP', 'MD', 'VB', 'JJ', 'NN', 'VBD', 'NN', 'RB', 'IN', 'VBG', 'PRP$', 'NNS', '.']",21
natural_language_inference,23,2,FUSIONNET : FUSING VIA FULLY - AWARE ATTENTION WITH APPLICATION TO MACHINE COMPREHENSION,"['FUSIONNET', ':', 'FUSING', 'VIA', 'FULLY', '-', 'AWARE', 'ATTENTION', 'WITH', 'APPLICATION', 'TO', 'MACHINE', 'COMPREHENSION']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NN', ':', 'NN', 'NNP', 'NNP', ':', 'JJ', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",13
natural_language_inference,23,24,"Teaching machines to read , process and comprehend text and then answer questions is one of key problems in artificial intelligence .","['Teaching', 'machines', 'to', 'read', ',', 'process', 'and', 'comprehend', 'text', 'and', 'then', 'answer', 'questions', 'is', 'one', 'of', 'key', 'problems', 'in', 'artificial', 'intelligence', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NNS', 'TO', 'VB', ',', 'NN', 'CC', 'VB', 'NN', 'CC', 'RB', 'VB', 'NNS', 'VBZ', 'CD', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NN', '.']",22
natural_language_inference,23,29,Many neural network models have been proposed for this challenge and they generally frame this problem as a machine reading comprehension ( MRC ) task .,"['Many', 'neural', 'network', 'models', 'have', 'been', 'proposed', 'for', 'this', 'challenge', 'and', 'they', 'generally', 'frame', 'this', 'problem', 'as', 'a', 'machine', 'reading', 'comprehension', '(', 'MRC', ')', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['JJ', 'JJ', 'NN', 'NNS', 'VBP', 'VBN', 'VBN', 'IN', 'DT', 'NN', 'CC', 'PRP', 'RB', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'NN', '(', 'NNP', ')', 'NN', '.']",26
natural_language_inference,23,35,We argue that this hypothesis also holds in language understanding and MRC .,"['We', 'argue', 'that', 'this', 'hypothesis', 'also', 'holds', 'in', 'language', 'understanding', 'and', 'MRC', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NN', 'RB', 'VBZ', 'IN', 'NN', 'NN', 'CC', 'NNP', '.']",13
natural_language_inference,23,39,"To alleviate this challenge , we identify an attention scoring function utilizing all layers of representation with less training burden .","['To', 'alleviate', 'this', 'challenge', ',', 'we', 'identify', 'an', 'attention', 'scoring', 'function', 'utilizing', 'all', 'layers', 'of', 'representation', 'with', 'less', 'training', 'burden', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBG', 'NN', 'VBG', 'DT', 'NNS', 'IN', 'NN', 'IN', 'JJR', 'VBG', 'NN', '.']",21
natural_language_inference,23,40,This leads to an attention that thoroughly captures the complete information between the question and the context .,"['This', 'leads', 'to', 'an', 'attention', 'that', 'thoroughly', 'captures', 'the', 'complete', 'information', 'between', 'the', 'question', 'and', 'the', 'context', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'TO', 'DT', 'NN', 'WDT', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'CC', 'DT', 'NN', '.']",18
natural_language_inference,23,41,"With this fully - aware attention , we put forward a multi -level attention mechanism to understand the information in the question , and exploit it layer by layer on the context side .","['With', 'this', 'fully', '-', 'aware', 'attention', ',', 'we', 'put', 'forward', 'a', 'multi', '-level', 'attention', 'mechanism', 'to', 'understand', 'the', 'information', 'in', 'the', 'question', ',', 'and', 'exploit', 'it', 'layer', 'by', 'layer', 'on', 'the', 'context', 'side', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'RB', ':', 'JJ', 'NN', ',', 'PRP', 'VBP', 'RB', 'DT', 'NN', 'NN', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'CC', 'VB', 'PRP', 'JJ', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",34
natural_language_inference,23,42,"All of these innovations are integrated into a new end - to - end structure called FusionNet in , with details described in Section 3 .","['All', 'of', 'these', 'innovations', 'are', 'integrated', 'into', 'a', 'new', 'end', '-', 'to', '-', 'end', 'structure', 'called', 'FusionNet', 'in', ',', 'with', 'details', 'described', 'in', 'Section', '3', '.']","['O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'IN', 'DT', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NN', ':', 'TO', ':', 'NN', 'NN', 'VBN', 'NNP', 'IN', ',', 'IN', 'NNS', 'VBN', 'IN', 'NN', 'CD', '.']",26
natural_language_inference,23,238,"From the results , we can see that our models not only perform well on the original SQuAD dataset , but also outperform all previous models by more than 5 % in EM score on the adversarial datasets .","['From', 'the', 'results', ',', 'we', 'can', 'see', 'that', 'our', 'models', 'not', 'only', 'perform', 'well', 'on', 'the', 'original', 'SQuAD', 'dataset', ',', 'but', 'also', 'outperform', 'all', 'previous', 'models', 'by', 'more', 'than', '5', '%', 'in', 'EM', 'score', 'on', 'the', 'adversarial', 'datasets', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'PRP', 'MD', 'VB', 'IN', 'PRP$', 'NNS', 'RB', 'RB', 'RB', 'RB', 'IN', 'DT', 'JJ', 'NNP', 'NN', ',', 'CC', 'RB', 'VBP', 'DT', 'JJ', 'NNS', 'IN', 'JJR', 'IN', 'CD', 'NN', 'IN', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NNS', '.']",39
natural_language_inference,23,239,This shows that FusionNet is better at language understanding of both the context and question .,"['This', 'shows', 'that', 'FusionNet', 'is', 'better', 'at', 'language', 'understanding', 'of', 'both', 'the', 'context', 'and', 'question', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'IN', 'NNP', 'VBZ', 'RBR', 'IN', 'NN', 'NN', 'IN', 'CC', 'DT', 'NN', 'CC', 'NN', '.']",16
natural_language_inference,24,2,Stochastic Answer Networks for Machine Reading Comprehension,"['Stochastic', 'Answer', 'Networks', 'for', 'Machine', 'Reading', 'Comprehension']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",7
natural_language_inference,24,8,Machine reading comprehension ( MRC ) is a challenging task : the goal is to have machines read a text passage and then answer any question about the passage .,"['Machine', 'reading', 'comprehension', '(', 'MRC', ')', 'is', 'a', 'challenging', 'task', ':', 'the', 'goal', 'is', 'to', 'have', 'machines', 'read', 'a', 'text', 'passage', 'and', 'then', 'answer', 'any', 'question', 'about', 'the', 'passage', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'VBG', 'NN', '(', 'NNP', ')', 'VBZ', 'DT', 'JJ', 'NN', ':', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'NNS', 'VB', 'DT', 'JJ', 'NN', 'CC', 'RB', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",30
natural_language_inference,24,10,It has been hypothesized that difficult MRC problems require some form of multi-step synthesis and reasoning .,"['It', 'has', 'been', 'hypothesized', 'that', 'difficult', 'MRC', 'problems', 'require', 'some', 'form', 'of', 'multi-step', 'synthesis', 'and', 'reasoning', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBZ', 'VBN', 'VBN', 'IN', 'JJ', 'NNP', 'NNS', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'NN', 'CC', 'NN', '.']",17
natural_language_inference,24,18,"In this work , we derive an alternative multi-step reasoning neural network for MRC .","['In', 'this', 'work', ',', 'we', 'derive', 'an', 'alternative', 'multi-step', 'reasoning', 'neural', 'network', 'for', 'MRC', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBG', 'JJ', 'NN', 'IN', 'NNP', '.']",15
natural_language_inference,24,19,"During training , we fix the number of reasoning steps , but perform stochastic dropout on the answer module ( final layer predictions ) .","['During', 'training', ',', 'we', 'fix', 'the', 'number', 'of', 'reasoning', 'steps', ',', 'but', 'perform', 'stochastic', 'dropout', 'on', 'the', 'answer', 'module', '(', 'final', 'layer', 'predictions', ')', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'NNS', ',', 'CC', 'VB', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '(', 'JJ', 'NN', 'NNS', ')', '.']",25
natural_language_inference,24,20,"During decoding , we generate answers based on the average of predictions in all steps , rather than the final step .","['During', 'decoding', ',', 'we', 'generate', 'answers', 'based', 'on', 'the', 'average', 'of', 'predictions', 'in', 'all', 'steps', ',', 'rather', 'than', 'the', 'final', 'step', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'VBG', ',', 'PRP', 'VBP', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NNS', ',', 'RB', 'IN', 'DT', 'JJ', 'NN', '.']",22
natural_language_inference,24,21,"We call this a stochastic answer network ( SAN ) because the stochastic dropout is applied to the answer module ; albeit simple , this technique significantly improves the robustness and over all accuracy of the model .","['We', 'call', 'this', 'a', 'stochastic', 'answer', 'network', '(', 'SAN', ')', 'because', 'the', 'stochastic', 'dropout', 'is', 'applied', 'to', 'the', 'answer', 'module', ';', 'albeit', 'simple', ',', 'this', 'technique', 'significantly', 'improves', 'the', 'robustness', 'and', 'over', 'all', 'accuracy', 'of', 'the', 'model', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'DT', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'DT', 'NN', 'NN', ':', 'CC', 'NN', ',', 'DT', 'NN', 'RB', 'VBZ', 'DT', 'NN', 'CC', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",38
natural_language_inference,24,127,"The spaCy tool 2 is used to tokenize the both passages and questions , and generate lemma , part - of - speech and named entity tags .","['The', 'spaCy', 'tool', '2', 'is', 'used', 'to', 'tokenize', 'the', 'both', 'passages', 'and', 'questions', ',', 'and', 'generate', 'lemma', ',', 'part', '-', 'of', '-', 'speech', 'and', 'named', 'entity', 'tags', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'CD', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'DT', 'NNS', 'CC', 'NNS', ',', 'CC', 'VB', 'NN', ',', 'NN', ':', 'IN', ':', 'NN', 'CC', 'VBN', 'NN', 'NNS', '.']",28
natural_language_inference,24,128,We use 2 - layer BiLSTM with d = 128 hidden units for both passage and question encoding .,"['We', 'use', '2', '-', 'layer', 'BiLSTM', 'with', 'd', '=', '128', 'hidden', 'units', 'for', 'both', 'passage', 'and', 'question', 'encoding', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', ':', 'NN', 'NNP', 'IN', 'JJ', '$', 'CD', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NN', 'NN', '.']",19
natural_language_inference,24,129,The mini-batch size is set to 32 and Adamax is used as our optimizer .,"['The', 'mini-batch', 'size', 'is', 'set', 'to', '32', 'and', 'Adamax', 'is', 'used', 'as', 'our', 'optimizer', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'CC', 'NNP', 'VBZ', 'VBN', 'IN', 'PRP$', 'NN', '.']",15
natural_language_inference,24,130,The learning rate is set to 0.002 at first and decreased by half after every 10 epochs .,"['The', 'learning', 'rate', 'is', 'set', 'to', '0.002', 'at', 'first', 'and', 'decreased', 'by', 'half', 'after', 'every', '10', 'epochs', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'IN', 'JJ', 'CC', 'VBN', 'IN', 'NN', 'IN', 'DT', 'CD', 'NN', '.']",18
natural_language_inference,24,131,"We set the dropout rate for all the hidden units of LSTM , and the answer module output layer to 0.4 .","['We', 'set', 'the', 'dropout', 'rate', 'for', 'all', 'the', 'hidden', 'units', 'of', 'LSTM', ',', 'and', 'the', 'answer', 'module', 'output', 'layer', 'to', '0.4', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'PDT', 'DT', 'JJ', 'NNS', 'IN', 'NNP', ',', 'CC', 'DT', 'NN', 'NN', 'NN', 'NN', 'TO', 'CD', '.']",22
natural_language_inference,24,132,"To prevent degenerate output , we ensure that at least one step in the answer module is active during training .","['To', 'prevent', 'degenerate', 'output', ',', 'we', 'ensure', 'that', 'at', 'least', 'one', 'step', 'in', 'the', 'answer', 'module', 'is', 'active', 'during', 'training', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['TO', 'VB', 'NN', 'NN', ',', 'PRP', 'VB', 'DT', 'IN', 'JJS', 'CD', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'JJ', 'IN', 'NN', '.']",21
natural_language_inference,24,146,"We observe that SAN achieves 76.235 EM and 84.056 F1 , outperforming all other models .","['We', 'observe', 'that', 'SAN', 'achieves', '76.235', 'EM', 'and', '84.056', 'F1', ',', 'outperforming', 'all', 'other', 'models', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'VBZ', 'CD', 'NNP', 'CC', 'CD', 'NNP', ',', 'VBG', 'DT', 'JJ', 'NNS', '.']",16
natural_language_inference,24,147,Standard 1 - step model only achieves 75.139 EM and dynamic steps ( via ReasoNet ) achieves only 75.355 EM .,"['Standard', '1', '-', 'step', 'model', 'only', 'achieves', '75.139', 'EM', 'and', 'dynamic', 'steps', '(', 'via', 'ReasoNet', ')', 'achieves', 'only', '75.355', 'EM', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'CD', ':', 'NN', 'VBZ', 'RB', 'VBZ', 'CD', 'NNP', 'CC', 'JJ', 'NNS', '(', 'IN', 'NNP', ')', 'VBZ', 'RB', 'CD', 'NNP', '.']",21
natural_language_inference,24,148,"SAN also outperforms a 5 - step memory net with averaging , which implies averaging predictions is not the only thing that led to SAN 's superior results ; indeed , stochastic prediction dropout is an effective technique .","['SAN', 'also', 'outperforms', 'a', '5', '-', 'step', 'memory', 'net', 'with', 'averaging', ',', 'which', 'implies', 'averaging', 'predictions', 'is', 'not', 'the', 'only', 'thing', 'that', 'led', 'to', 'SAN', ""'s"", 'superior', 'results', ';', 'indeed', ',', 'stochastic', 'prediction', 'dropout', 'is', 'an', 'effective', 'technique', '.']","['B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'RB', 'VBZ', 'DT', 'CD', ':', 'NN', 'NN', 'NN', 'IN', 'NN', ',', 'WDT', 'VBZ', 'VBG', 'NNS', 'VBZ', 'RB', 'DT', 'JJ', 'NN', 'WDT', 'VBD', 'TO', 'NNP', 'POS', 'JJ', 'NNS', ':', 'RB', ',', 'JJ', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', '.']",39
natural_language_inference,24,152,SAN also outperforms the other models in terms of K- best oracle scores .,"['SAN', 'also', 'outperforms', 'the', 'other', 'models', 'in', 'terms', 'of', 'K-', 'best', 'oracle', 'scores', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'RB', 'VBZ', 'DT', 'JJ', 'NNS', 'IN', 'NNS', 'IN', 'NNP', 'JJS', 'NN', 'NNS', '.']",14
natural_language_inference,24,156,We see that SAN is very competitive in both single and ensemble settings ( ranked in second ) despite its simplicity .,"['We', 'see', 'that', 'SAN', 'is', 'very', 'competitive', 'in', 'both', 'single', 'and', 'ensemble', 'settings', '(', 'ranked', 'in', 'second', ')', 'despite', 'its', 'simplicity', '.']","['O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'VBZ', 'RB', 'JJ', 'IN', 'DT', 'JJ', 'CC', 'JJ', 'NNS', '(', 'VBN', 'IN', 'NN', ')', 'IN', 'PRP$', 'NN', '.']",22
natural_language_inference,14,2,CODAH : An Adversarially - Authored Question Answering Dataset for Common Sense,"['CODAH', ':', 'An', 'Adversarially', '-', 'Authored', 'Question', 'Answering', 'Dataset', 'for', 'Common', 'Sense']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['NN', ':', 'DT', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",12
natural_language_inference,14,4,"Commonsense reasoning is a critical AI capability , but it is difficult to construct challenging datasets that test commonsense .","['Commonsense', 'reasoning', 'is', 'a', 'critical', 'AI', 'capability', ',', 'but', 'it', 'is', 'difficult', 'to', 'construct', 'challenging', 'datasets', 'that', 'test', 'commonsense', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'VBZ', 'DT', 'JJ', 'NNP', 'NN', ',', 'CC', 'PRP', 'VBZ', 'JJ', 'TO', 'VB', 'VBG', 'NNS', 'WDT', 'VBP', 'NN', '.']",20
natural_language_inference,14,16,The rise of datadriven methods has led to interest in developing large datasets for commonsense reasoning over text .,"['The', 'rise', 'of', 'datadriven', 'methods', 'has', 'led', 'to', 'interest', 'in', 'developing', 'large', 'datasets', 'for', 'commonsense', 'reasoning', 'over', 'text', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'JJ', 'NNS', 'VBZ', 'VBN', 'TO', 'NN', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'NN', 'NN', 'IN', 'NN', '.']",19
natural_language_inference,14,24,"In this work , we introduce the COmmonsense Dataset Adversarially - authored by Humans ( CODAH ) for commonsense question answering in the style of SWAG multiple choice sentence completion .","['In', 'this', 'work', ',', 'we', 'introduce', 'the', 'COmmonsense', 'Dataset', 'Adversarially', '-', 'authored', 'by', 'Humans', '(', 'CODAH', ')', 'for', 'commonsense', 'question', 'answering', 'in', 'the', 'style', 'of', 'SWAG', 'multiple', 'choice', 'sentence', 'completion', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', 'NNP', 'NNP', ':', 'VBN', 'IN', 'NNP', '(', 'NNP', ')', 'IN', 'NN', 'NN', 'VBG', 'IN', 'DT', 'NN', 'IN', 'NNP', 'JJ', 'NN', 'NN', 'NN', '.']",31
natural_language_inference,14,25,"We propose a novel method for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .","['We', 'propose', 'a', 'novel', 'method', 'for', 'question', 'generation', ',', 'in', 'which', 'human', 'annotators', 'are', 'educated', 'on', 'the', 'workings', 'of', 'a', 'state', '-', 'of', '-', 'the', '-', 'art', 'question', 'answering', 'model', ',', 'and', 'are', 'asked', 'to', 'submit', 'questions', 'that', 'adversarially', 'target', 'the', 'weaknesses', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NN', ',', 'IN', 'WDT', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'VBG', 'NN', ',', 'CC', 'VBP', 'VBN', 'TO', 'VB', 'NNS', 'WDT', 'RB', 'VBP', 'DT', 'NNS', '.']",43
natural_language_inference,14,26,"Annotators are rewarded for submissions in which the model fails to identify the correct sentence completion both before and after fine - tuning on a sample of the submitted questions , encouraging the creation of questions that are not easily learnable .","['Annotators', 'are', 'rewarded', 'for', 'submissions', 'in', 'which', 'the', 'model', 'fails', 'to', 'identify', 'the', 'correct', 'sentence', 'completion', 'both', 'before', 'and', 'after', 'fine', '-', 'tuning', 'on', 'a', 'sample', 'of', 'the', 'submitted', 'questions', ',', 'encouraging', 'the', 'creation', 'of', 'questions', 'that', 'are', 'not', 'easily', 'learnable', '.']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNS', 'VBP', 'VBN', 'IN', 'NNS', 'IN', 'WDT', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', 'DT', 'IN', 'CC', 'IN', 'JJ', ':', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNS', ',', 'VBG', 'DT', 'NN', 'IN', 'NNS', 'WDT', 'VBP', 'RB', 'RB', 'JJ', '.']",42
natural_language_inference,14,51,The full dataset is available at https://github.com/Websail-NU /CODAH .,"['The', 'full', 'dataset', 'is', 'available', 'at', 'https://github.com/Websail-NU', '/CODAH', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'JJ', 'IN', 'JJ', 'NN', '.']",9
natural_language_inference,14,114,"Also , when training the initial SWAG model we use the hyperparameters recommended in the BERT paper , namely a batch size of 16 , learning rate of 2 e - 5 , and 3 epochs .","['Also', ',', 'when', 'training', 'the', 'initial', 'SWAG', 'model', 'we', 'use', 'the', 'hyperparameters', 'recommended', 'in', 'the', 'BERT', 'paper', ',', 'namely', 'a', 'batch', 'size', 'of', '16', ',', 'learning', 'rate', 'of', '2', 'e', '-', '5', ',', 'and', '3', 'epochs', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'O']","['RB', ',', 'WRB', 'VBG', 'DT', 'JJ', 'NNP', 'NN', 'PRP', 'VBP', 'DT', 'NNS', 'VBD', 'IN', 'DT', 'NNP', 'NN', ',', 'RB', 'DT', 'NN', 'NN', 'IN', 'CD', ',', 'VBG', 'NN', 'IN', 'CD', 'SYM', ':', 'CD', ',', 'CC', 'CD', 'NNS', '.']",37
natural_language_inference,14,115,"In our initial experiments , we found that a lower learning rate and more training epochs produced higher accuracy on CODAH , so we replaced the 5e - 5 learning rate in the original grid search with 1 e - 5 , and we added a 6 - epoch setting .","['In', 'our', 'initial', 'experiments', ',', 'we', 'found', 'that', 'a', 'lower', 'learning', 'rate', 'and', 'more', 'training', 'epochs', 'produced', 'higher', 'accuracy', 'on', 'CODAH', ',', 'so', 'we', 'replaced', 'the', '5e', '-', '5', 'learning', 'rate', 'in', 'the', 'original', 'grid', 'search', 'with', '1', 'e', '-', '5', ',', 'and', 'we', 'added', 'a', '6', '-', 'epoch', 'setting', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'PRP$', 'JJ', 'NNS', ',', 'PRP', 'VBD', 'IN', 'DT', 'JJR', 'NN', 'NN', 'CC', 'JJR', 'NN', 'NN', 'VBD', 'JJR', 'NN', 'IN', 'NNP', ',', 'IN', 'PRP', 'VBD', 'DT', 'CD', ':', 'CD', 'VBG', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'CD', 'SYM', ':', 'CD', ',', 'CC', 'PRP', 'VBD', 'DT', 'CD', ':', 'NN', 'NN', '.']",51
natural_language_inference,14,116,The final hyperparameter grid is as follows :,"['The', 'final', 'hyperparameter', 'grid', 'is', 'as', 'follows', ':']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'IN', 'VBZ', ':']",8
natural_language_inference,14,117,"Batch size : 16 , 32 Learning rate : 1 e - 5 , 2 e - 5 , 3 e - 5 Number of epochs : 3 , 4 , 6 In addition , we observed that in rare cases BERT fails to train ; that is , after several training epochs it has accuracy approximately equal to that of random guessing .","['Batch', 'size', ':', '16', ',', '32', 'Learning', 'rate', ':', '1', 'e', '-', '5', ',', '2', 'e', '-', '5', ',', '3', 'e', '-', '5', 'Number', 'of', 'epochs', ':', '3', ',', '4', ',', '6', 'In', 'addition', ',', 'we', 'observed', 'that', 'in', 'rare', 'cases', 'BERT', 'fails', 'to', 'train', ';', 'that', 'is', ',', 'after', 'several', 'training', 'epochs', 'it', 'has', 'accuracy', 'approximately', 'equal', 'to', 'that', 'of', 'random', 'guessing', '.']","['B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', ':', 'CD', ',', 'CD', 'VBG', 'NN', ':', 'CD', 'SYM', ':', 'CD', ',', 'CD', 'SYM', ':', 'CD', ',', 'CD', 'SYM', ':', 'CD', 'NNP', 'IN', 'NN', ':', 'CD', ',', 'CD', ',', 'CD', 'IN', 'NN', ',', 'PRP', 'VBD', 'IN', 'IN', 'JJ', 'NNS', 'NNP', 'VBZ', 'TO', 'VB', ':', 'DT', 'VBZ', ',', 'IN', 'JJ', 'VBG', 'NN', 'PRP', 'VBZ', 'VBN', 'RB', 'JJ', 'TO', 'DT', 'IN', 'NN', 'NN', '.']",64
natural_language_inference,14,122,"As a baseline , we evaluate both models on the full SWAG training and validation sets , providing an accuracy of 84.2 % on BERT and 80.2 % on GPT .","['As', 'a', 'baseline', ',', 'we', 'evaluate', 'both', 'models', 'on', 'the', 'full', 'SWAG', 'training', 'and', 'validation', 'sets', ',', 'providing', 'an', 'accuracy', 'of', '84.2', '%', 'on', 'BERT', 'and', '80.2', '%', 'on', 'GPT', '.']","['B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NNP', 'NN', 'CC', 'NN', 'NNS', ',', 'VBG', 'DT', 'NN', 'IN', 'CD', 'NN', 'IN', 'NNP', 'CC', 'CD', 'NN', 'IN', 'NNP', '.']",31
natural_language_inference,14,123,"To adjust for the difference in size between our dataset and SWAG , we also train the models on a sample of 2,241 SWAG questions ( the size of the training set in each of CODAH 's crossvalidation folds ) and evaluate them on the full SWAG validation set .","['To', 'adjust', 'for', 'the', 'difference', 'in', 'size', 'between', 'our', 'dataset', 'and', 'SWAG', ',', 'we', 'also', 'train', 'the', 'models', 'on', 'a', 'sample', 'of', '2,241', 'SWAG', 'questions', '(', 'the', 'size', 'of', 'the', 'training', 'set', 'in', 'each', 'of', 'CODAH', ""'s"", 'crossvalidation', 'folds', ')', 'and', 'evaluate', 'them', 'on', 'the', 'full', 'SWAG', 'validation', 'set', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'IN', 'DT', 'NN', 'IN', 'NN', 'IN', 'PRP$', 'NN', 'CC', 'NNP', ',', 'PRP', 'RB', 'VBP', 'DT', 'NNS', 'IN', 'DT', 'NN', 'IN', 'CD', 'NNP', 'NNS', '(', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'IN', 'NNP', 'POS', 'NN', 'NNS', ')', 'CC', 'VB', 'PRP', 'IN', 'DT', 'JJ', 'NNP', 'NN', 'NN', '.']",50
natural_language_inference,14,124,This produces an accuracy of 75.2 % for BERT ( using the cross-validation grid search ) and 63.6 % for GPT . :,"['This', 'produces', 'an', 'accuracy', 'of', '75.2', '%', 'for', 'BERT', '(', 'using', 'the', 'cross-validation', 'grid', 'search', ')', 'and', '63.6', '%', 'for', 'GPT', '.', ':']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O']","['DT', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', 'IN', 'NNP', '(', 'VBG', 'DT', 'NN', 'JJ', 'NN', ')', 'CC', 'CD', 'NN', 'IN', 'NNP', '.', ':']",23
natural_language_inference,7,2,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,"['LEARNING', 'RECURRENT', 'SPAN', 'REPRESENTATIONS', 'FOR', 'EXTRACTIVE', 'QUESTION', 'ANSWERING']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",8
natural_language_inference,7,7,"In this paper , we focus on this answer extraction task , presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network .","['In', 'this', 'paper', ',', 'we', 'focus', 'on', 'this', 'answer', 'extraction', 'task', ',', 'presenting', 'a', 'novel', 'model', 'architecture', 'that', 'efficiently', 'builds', 'fixed', 'length', 'representations', 'of', 'all', 'spans', 'in', 'the', 'evidence', 'document', 'with', 'a', 'recurrent', 'network', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'NN', 'NN', ',', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'RB', 'VBZ', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",35
natural_language_inference,7,24,"To overcome this , we present a novel neural architecture called RASOR that builds fixed - length span representations , reusing recurrent computations for shared substructures .","['To', 'overcome', 'this', ',', 'we', 'present', 'a', 'novel', 'neural', 'architecture', 'called', 'RASOR', 'that', 'builds', 'fixed', '-', 'length', 'span', 'representations', ',', 'reusing', 'recurrent', 'computations', 'for', 'shared', 'substructures', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['TO', 'VB', 'DT', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'VBN', 'NNP', 'IN', 'VBZ', 'VBN', ':', 'NN', 'NN', 'NNS', ',', 'VBG', 'JJ', 'NNS', 'IN', 'VBN', 'NNS', '.']",27
natural_language_inference,7,25,"We demonstrate that directly classifying each of the competing spans , and training with global normalization over all possible spans , leads to a significant increase in performance .","['We', 'demonstrate', 'that', 'directly', 'classifying', 'each', 'of', 'the', 'competing', 'spans', ',', 'and', 'training', 'with', 'global', 'normalization', 'over', 'all', 'possible', 'spans', ',', 'leads', 'to', 'a', 'significant', 'increase', 'in', 'performance', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'RB', 'VBG', 'DT', 'IN', 'DT', 'VBG', 'NNS', ',', 'CC', 'VBG', 'IN', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NNS', ',', 'VBZ', 'TO', 'DT', 'JJ', 'NN', 'IN', 'NN', '.']",29
natural_language_inference,7,106,We represent each of the words in the question and document using 300 dimensional GloVe embeddings trained on a corpus of 840 bn words .,"['We', 'represent', 'each', 'of', 'the', 'words', 'in', 'the', 'question', 'and', 'document', 'using', '300', 'dimensional', 'GloVe', 'embeddings', 'trained', 'on', 'a', 'corpus', 'of', '840', 'bn', 'words', '.']","['O', 'B-p', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NN', 'VBG', 'CD', 'JJ', 'NNP', 'NNS', 'VBD', 'IN', 'DT', 'NN', 'IN', 'CD', 'NN', 'NNS', '.']",25
natural_language_inference,7,107,These embeddings cover 200 k words and all out of vocabulary ( OOV ) words are projected onto one of 1 m randomly initialized 300d embeddings .,"['These', 'embeddings', 'cover', '200', 'k', 'words', 'and', 'all', 'out', 'of', 'vocabulary', '(', 'OOV', ')', 'words', 'are', 'projected', 'onto', 'one', 'of', '1', 'm', 'randomly', 'initialized', '300d', 'embeddings', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'CD', 'JJ', 'NNS', 'CC', 'DT', 'IN', 'IN', 'JJ', '(', 'NNP', ')', 'NNS', 'VBP', 'VBN', 'IN', 'CD', 'IN', 'CD', 'NNS', 'RB', 'VBD', 'CD', 'NNS', '.']",27
natural_language_inference,7,108,"We couple the input and forget gates in our LSTMs , as described in , and we use a single dropout mask to apply dropout across all LSTM time - steps as proposed by .","['We', 'couple', 'the', 'input', 'and', 'forget', 'gates', 'in', 'our', 'LSTMs', ',', 'as', 'described', 'in', ',', 'and', 'we', 'use', 'a', 'single', 'dropout', 'mask', 'to', 'apply', 'dropout', 'across', 'all', 'LSTM', 'time', '-', 'steps', 'as', 'proposed', 'by', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'CC', 'VB', 'NNS', 'IN', 'PRP$', 'NNP', ',', 'IN', 'VBN', 'IN', ',', 'CC', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'RP', 'IN', 'DT', 'NNP', 'NN', ':', 'NNS', 'IN', 'VBN', 'IN', '.']",35
natural_language_inference,7,109,Hidden layers in the feed forward neural networks use rectified linear units .,"['Hidden', 'layers', 'in', 'the', 'feed', 'forward', 'neural', 'networks', 'use', 'rectified', 'linear', 'units', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNS', 'IN', 'DT', 'NN', 'RB', 'JJ', 'NNS', 'VBP', 'JJ', 'JJ', 'NNS', '.']",13
natural_language_inference,7,110,Answer candidates are limited to spans with at most 30 words .,"['Answer', 'candidates', 'are', 'limited', 'to', 'spans', 'with', 'at', 'most', '30', 'words', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNS', 'VBP', 'VBN', 'TO', 'NNS', 'IN', 'IN', 'JJS', 'CD', 'NNS', '.']",12
natural_language_inference,7,111,"To choose the final model configuration , we ran grid searches over : the dimensionality of the LSTM hidden states ; the width and depth of the feed forward neural networks ; dropout for the LSTMs ; the number of stacked LSTM layers ; and the decay multiplier [ 0.9 , 0.95 , 1.0 ] with which we multiply the learning rate every 10 k steps .","['To', 'choose', 'the', 'final', 'model', 'configuration', ',', 'we', 'ran', 'grid', 'searches', 'over', ':', 'the', 'dimensionality', 'of', 'the', 'LSTM', 'hidden', 'states', ';', 'the', 'width', 'and', 'depth', 'of', 'the', 'feed', 'forward', 'neural', 'networks', ';', 'dropout', 'for', 'the', 'LSTMs', ';', 'the', 'number', 'of', 'stacked', 'LSTM', 'layers', ';', 'and', 'the', 'decay', 'multiplier', '[', '0.9', ',', '0.95', ',', '1.0', ']', 'with', 'which', 'we', 'multiply', 'the', 'learning', 'rate', 'every', '10', 'k', 'steps', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'JJ', 'NN', 'NN', ',', 'PRP', 'VBD', 'JJ', 'NNS', 'IN', ':', 'DT', 'NN', 'IN', 'DT', 'NNP', 'JJ', 'NNS', ':', 'DT', 'NN', 'CC', 'NN', 'IN', 'DT', 'NN', 'RB', 'JJ', 'NNS', ':', 'NN', 'IN', 'DT', 'NNP', ':', 'DT', 'NN', 'IN', 'JJ', 'NNP', 'NNS', ':', 'CC', 'DT', 'NN', 'NN', 'VBZ', 'CD', ',', 'CD', ',', 'CD', 'NN', 'IN', 'WDT', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'DT', 'CD', 'NN', 'NNS', '.']",67
natural_language_inference,7,112,The best model uses 50d LSTM states ; two - layer BiLSTMs for the span encoder and the passage - independent question representation ; dropout of 0.1 throughout ; and a learning rate decay of 5 % every 10 k steps .,"['The', 'best', 'model', 'uses', '50d', 'LSTM', 'states', ';', 'two', '-', 'layer', 'BiLSTMs', 'for', 'the', 'span', 'encoder', 'and', 'the', 'passage', '-', 'independent', 'question', 'representation', ';', 'dropout', 'of', '0.1', 'throughout', ';', 'and', 'a', 'learning', 'rate', 'decay', 'of', '5', '%', 'every', '10', 'k', 'steps', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJS', 'NN', 'VBZ', 'CD', 'NNP', 'NNS', ':', 'CD', ':', 'NN', 'NNP', 'IN', 'DT', 'NN', 'NN', 'CC', 'DT', 'NN', ':', 'JJ', 'NN', 'NN', ':', 'NN', 'IN', 'CD', 'IN', ':', 'CC', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'NN', 'DT', 'CD', 'NN', 'NNS', '.']",42
natural_language_inference,7,113,All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the ADAM optimizer with a mini-batch size of 4 and trained using 10 asynchronous training threads on a single machine .,"['All', 'models', 'are', 'implemented', 'using', 'TensorFlow', '3', 'and', 'trained', 'on', 'the', 'SQUAD', 'training', 'set', 'using', 'the', 'ADAM', 'optimizer', 'with', 'a', 'mini-batch', 'size', 'of', '4', 'and', 'trained', 'using', '10', 'asynchronous', 'training', 'threads', 'on', 'a', 'single', 'machine', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'VBG', 'NNP', 'CD', 'CC', 'VBD', 'IN', 'DT', 'NNP', 'NN', 'NN', 'VBG', 'DT', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', 'CC', 'VBD', 'VBG', 'CD', 'JJ', 'VBG', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",36
natural_language_inference,7,121,"Despite not having access to any external representation of linguistic structure , RASOR achieves an error reduction of more than 50 % over this baseline , both in terms of exact match and F1 , relative to the human performance upper bound .","['Despite', 'not', 'having', 'access', 'to', 'any', 'external', 'representation', 'of', 'linguistic', 'structure', ',', 'RASOR', 'achieves', 'an', 'error', 'reduction', 'of', 'more', 'than', '50', '%', 'over', 'this', 'baseline', ',', 'both', 'in', 'terms', 'of', 'exact', 'match', 'and', 'F1', ',', 'relative', 'to', 'the', 'human', 'performance', 'upper', 'bound', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'RB', 'VBG', 'NN', 'TO', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', ',', 'NNP', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'JJR', 'IN', 'CD', 'NN', 'IN', 'DT', 'NN', ',', 'DT', 'IN', 'NNS', 'IN', 'JJ', 'NN', 'CC', 'NNP', ',', 'JJ', 'TO', 'DT', 'JJ', 'NN', 'JJ', 'NN', '.']",43
natural_language_inference,7,125,"In contrast , RASOR can efficiently and explicitly model the quadratic number of possible answers , which leads to a 14 % error reduction over the best performing Match - LSTM model .","['In', 'contrast', ',', 'RASOR', 'can', 'efficiently', 'and', 'explicitly', 'model', 'the', 'quadratic', 'number', 'of', 'possible', 'answers', ',', 'which', 'leads', 'to', 'a', '14', '%', 'error', 'reduction', 'over', 'the', 'best', 'performing', 'Match', '-', 'LSTM', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'NNP', 'MD', 'RB', 'CC', 'RB', 'VB', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', ',', 'WDT', 'VBZ', 'TO', 'DT', 'CD', 'NN', 'NN', 'NN', 'IN', 'DT', 'JJS', 'NN', 'NNP', ':', 'NNP', 'NN', '.']",33
natural_language_inference,7,131,"The passage - aligned question representation is crucial , since lexically similar regions of the passage provide strong signal for relevant answer spans .","['The', 'passage', '-', 'aligned', 'question', 'representation', 'is', 'crucial', ',', 'since', 'lexically', 'similar', 'regions', 'of', 'the', 'passage', 'provide', 'strong', 'signal', 'for', 'relevant', 'answer', 'spans', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', ':', 'VBN', 'NN', 'NN', 'VBZ', 'JJ', ',', 'IN', 'RB', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NNS', '.']",24
natural_language_inference,7,157,"First , we observe general improvements when using labels that closely align with the task .","['First', ',', 'we', 'observe', 'general', 'improvements', 'when', 'using', 'labels', 'that', 'closely', 'align', 'with', 'the', 'task', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['RB', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'WRB', 'VBG', 'NNS', 'WDT', 'RB', 'VBZ', 'IN', 'DT', 'NN', '.']",16
natural_language_inference,7,162,"Second , we observe the importance of allowing interactions between the endpoints using the spanlevel FFNN .","['Second', ',', 'we', 'observe', 'the', 'importance', 'of', 'allowing', 'interactions', 'between', 'the', 'endpoints', 'using', 'the', 'spanlevel', 'FFNN', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['JJ', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'NNS', 'IN', 'DT', 'NNS', 'VBG', 'DT', 'NN', 'NNP', '.']",17
natural_language_inference,7,163,"RASOR outperforms the endpoint prediction model by 1.1 in exact match , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .","['RASOR', 'outperforms', 'the', 'endpoint', 'prediction', 'model', 'by', '1.1', 'in', 'exact', 'match', ',', 'The', 'interaction', 'between', 'endpoints', 'enables', 'RASOR', 'to', 'enforce', 'consistency', 'across', 'its', 'two', 'substructures', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'VBZ', 'DT', 'NN', 'NN', 'NN', 'IN', 'CD', 'IN', 'JJ', 'NN', ',', 'DT', 'NN', 'IN', 'NNS', 'NNS', 'NNP', 'TO', 'VB', 'NN', 'IN', 'PRP$', 'CD', 'NNS', '.']",26
natural_language_inference,28,4,The concepts of unitary evolution matrices and associative memory have boosted the field of Recurrent Neural Networks ( RNN ) to state - of - the - art performance in a variety of sequential tasks .,"['The', 'concepts', 'of', 'unitary', 'evolution', 'matrices', 'and', 'associative', 'memory', 'have', 'boosted', 'the', 'field', 'of', 'Recurrent', 'Neural', 'Networks', '(', 'RNN', ')', 'to', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'in', 'a', 'variety', 'of', 'sequential', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'JJ', 'NN', 'NNS', 'CC', 'JJ', 'NN', 'VBP', 'VBN', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'TO', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', '.']",36
natural_language_inference,28,5,"However , RNN still have a limited capacity to manipulate long - term memory .","['However', ',', 'RNN', 'still', 'have', 'a', 'limited', 'capacity', 'to', 'manipulate', 'long', '-', 'term', 'memory', '.']","['O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'NNP', 'RB', 'VBP', 'DT', 'JJ', 'NN', 'TO', 'VB', 'JJ', ':', 'NN', 'NN', '.']",15
natural_language_inference,28,27,"Here , we propose a novel RNN cell that resolves simultaneously those weaknesses of basic RNN .","['Here', ',', 'we', 'propose', 'a', 'novel', 'RNN', 'cell', 'that', 'resolves', 'simultaneously', 'those', 'weaknesses', 'of', 'basic', 'RNN', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNP', 'NN', 'WDT', 'VBZ', 'RB', 'DT', 'NNS', 'IN', 'JJ', 'NNP', '.']",17
natural_language_inference,28,28,The Rotational Unit of Memory is a modified gated model whose rotational operation acts as associative memory and is strictly an orthogonal matrix .,"['The', 'Rotational', 'Unit', 'of', 'Memory', 'is', 'a', 'modified', 'gated', 'model', 'whose', 'rotational', 'operation', 'acts', 'as', 'associative', 'memory', 'and', 'is', 'strictly', 'an', 'orthogonal', 'matrix', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNP', 'NNP', 'IN', 'NNP', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'WP$', 'JJ', 'NN', 'NNS', 'IN', 'JJ', 'NN', 'CC', 'VBZ', 'RB', 'DT', 'JJ', 'NN', '.']",24
natural_language_inference,28,146,COPYING MEMORY TASK,"['COPYING', 'MEMORY', 'TASK']","['B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP']",3
natural_language_inference,28,151,"1 . RUM utilizes a different representation of memory that outperforms those of LSTM and GRU ; 2 . RUM solves the task completely , despite its update gate , which does not allow all of the information encoded in the hidden stay to pass through .","['1', '.', 'RUM', 'utilizes', 'a', 'different', 'representation', 'of', 'memory', 'that', 'outperforms', 'those', 'of', 'LSTM', 'and', 'GRU', ';', '2', '.', 'RUM', 'solves', 'the', 'task', 'completely', ',', 'despite', 'its', 'update', 'gate', ',', 'which', 'does', 'not', 'allow', 'all', 'of', 'the', 'information', 'encoded', 'in', 'the', 'hidden', 'stay', 'to', 'pass', 'through', '.']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['CD', '.', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'VBZ', 'DT', 'IN', 'NNP', 'CC', 'NNP', ':', 'CD', '.', 'NNP', 'VBZ', 'DT', 'NN', 'RB', ',', 'IN', 'PRP$', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'RB', 'VB', 'DT', 'IN', 'DT', 'NN', 'VBD', 'IN', 'DT', 'JJ', 'NN', 'TO', 'VB', 'IN', '.']",47
natural_language_inference,28,165,ASSOCIATIVE RECALL TASK,"['ASSOCIATIVE', 'RECALL', 'TASK']","['B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP']",3
natural_language_inference,28,175,All the models have the same hidden state N h = 50 for different lengths T .,"['All', 'the', 'models', 'have', 'the', 'same', 'hidden', 'state', 'N', 'h', '=', '50', 'for', 'different', 'lengths', 'T', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PDT', 'DT', 'NNS', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NNP', 'NN', 'VBZ', 'CD', 'IN', 'JJ', 'NNS', 'NNP', '.']",17
natural_language_inference,28,176,We use a batch size 128 .,"['We', 'use', 'a', 'batch', 'size', '128', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'CD', '.']",7
natural_language_inference,28,177,The optimizer is RMSProp with a learning rate 0.001 .,"['The', 'optimizer', 'is', 'RMSProp', 'with', 'a', 'learning', 'rate', '0.001', '.']","['O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O']","['DT', 'NN', 'VBZ', 'NNP', 'IN', 'DT', 'VBG', 'NN', 'CD', '.']",10
natural_language_inference,28,178,"We find that LSTM fails to learn the task , because of its lack of sufficient memory capacity .","['We', 'find', 'that', 'LSTM', 'fails', 'to', 'learn', 'the', 'task', ',', 'because', 'of', 'its', 'lack', 'of', 'sufficient', 'memory', 'capacity', '.']","['O', 'B-p', 'I-p', 'B-n', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'NNP', 'VBZ', 'TO', 'VB', 'DT', 'NN', ',', 'IN', 'IN', 'PRP$', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",19
natural_language_inference,28,179,"NTM and Fast - weight RNN fail longer tasks , which means they can not learn to manipulate their memory efficiently .","['NTM', 'and', 'Fast', '-', 'weight', 'RNN', 'fail', 'longer', 'tasks', ',', 'which', 'means', 'they', 'can', 'not', 'learn', 'to', 'manipulate', 'their', 'memory', 'efficiently', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'CC', 'NNP', ':', 'NN', 'NNP', 'NN', 'NN', 'NNS', ',', 'WDT', 'VBZ', 'PRP', 'MD', 'RB', 'VB', 'TO', 'VB', 'PRP$', 'NN', 'RB', '.']",22
natural_language_inference,28,180,QUESTION ANSWERING,"['QUESTION', 'ANSWERING']","['B-n', 'I-n']","['NN', 'NN']",2
natural_language_inference,28,187,"We compare our model with several baselines : a simple LSTM , an End - to - end Memory Network ) and a GORU .","['We', 'compare', 'our', 'model', 'with', 'several', 'baselines', ':', 'a', 'simple', 'LSTM', ',', 'an', 'End', '-', 'to', '-', 'end', 'Memory', 'Network', ')', 'and', 'a', 'GORU', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'IN', 'JJ', 'NNS', ':', 'DT', 'JJ', 'NNP', ',', 'DT', 'NN', ':', 'TO', ':', 'NN', 'NNP', 'NNP', ')', 'CC', 'DT', 'NNP', '.']",25
natural_language_inference,28,188,"We find that RUM outperforms significantly LSTM and GORU and achieves competitive result with those of MemN2N , which has an attention mechanism .","['We', 'find', 'that', 'RUM', 'outperforms', 'significantly', 'LSTM', 'and', 'GORU', 'and', 'achieves', 'competitive', 'result', 'with', 'those', 'of', 'MemN2N', ',', 'which', 'has', 'an', 'attention', 'mechanism', '.']","['O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'NNP', 'VBZ', 'RB', 'NNP', 'CC', 'NNP', 'CC', 'NNS', 'JJ', 'NN', 'IN', 'DT', 'IN', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NN', 'NN', '.']",24
natural_language_inference,28,193,CHARACTER LEVEL LANGUAGE MODELING,"['CHARACTER', 'LEVEL', 'LANGUAGE', 'MODELING']","['B-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'NNP']",4
natural_language_inference,28,196,PENN TREEBANK CORPUS DATA SET,"['PENN', 'TREEBANK', 'CORPUS', 'DATA', 'SET']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'NNP', 'NNP']",5
natural_language_inference,28,215,"FS - RUM - 2 generalizes better than other gated models , such as GRU and LSTM , because it learns efficient patterns for activation in its kernels .","['FS', '-', 'RUM', '-', '2', 'generalizes', 'better', 'than', 'other', 'gated', 'models', ',', 'such', 'as', 'GRU', 'and', 'LSTM', ',', 'because', 'it', 'learns', 'efficient', 'patterns', 'for', 'activation', 'in', 'its', 'kernels', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NNP', ':', 'CD', 'NNS', 'JJR', 'IN', 'JJ', 'JJ', 'NNS', ',', 'JJ', 'IN', 'NNP', 'CC', 'NNP', ',', 'IN', 'PRP', 'VBZ', 'JJ', 'NNS', 'IN', 'NN', 'IN', 'PRP$', 'NNS', '.']",29
natural_language_inference,48,2,Iterative Alternating Neural Attention for Machine Reading,"['Iterative', 'Alternating', 'Neural', 'Attention', 'for', 'Machine', 'Reading']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['JJ', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",7
natural_language_inference,48,4,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document .","['We', 'propose', 'a', 'novel', 'neural', 'attention', 'architecture', 'to', 'tackle', 'machine', 'comprehension', 'tasks', ',', 'such', 'as', 'answering', 'Cloze', '-', 'style', 'queries', 'with', 'respect', 'to', 'a', 'document', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'TO', 'VB', 'NN', 'NN', 'NNS', ',', 'JJ', 'IN', 'VBG', 'NNP', ':', 'NN', 'NNS', 'IN', 'NN', 'TO', 'DT', 'NN', '.']",26
natural_language_inference,48,19,"Encouraged by the recent success of deep learning attention architectures , we propose a novel neural attention - based inference model designed to perform machine reading comprehension tasks .","['Encouraged', 'by', 'the', 'recent', 'success', 'of', 'deep', 'learning', 'attention', 'architectures', ',', 'we', 'propose', 'a', 'novel', 'neural', 'attention', '-', 'based', 'inference', 'model', 'designed', 'to', 'perform', 'machine', 'reading', 'comprehension', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'VBG', 'NN', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', ':', 'VBN', 'NN', 'NN', 'VBN', 'TO', 'VB', 'NN', 'VBG', 'NN', 'NNS', '.']",29
natural_language_inference,48,20,The model first reads the document and the query using a recurrent neural network .,"['The', 'model', 'first', 'reads', 'the', 'document', 'and', 'the', 'query', 'using', 'a', 'recurrent', 'neural', 'network', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'RB', 'VBZ', 'DT', 'NN', 'CC', 'DT', 'NN', 'VBG', 'DT', 'JJ', 'JJ', 'NN', '.']",15
natural_language_inference,48,21,"Then , it deploys an iterative inference process to uncover the inferential links that exist between the missing query word , the query , and the document .","['Then', ',', 'it', 'deploys', 'an', 'iterative', 'inference', 'process', 'to', 'uncover', 'the', 'inferential', 'links', 'that', 'exist', 'between', 'the', 'missing', 'query', 'word', ',', 'the', 'query', ',', 'and', 'the', 'document', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'O']","['RB', ',', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'WDT', 'VBP', 'IN', 'DT', 'VBG', 'NN', 'NN', ',', 'DT', 'NN', ',', 'CC', 'DT', 'NN', '.']",28
natural_language_inference,48,22,"This phase involves a novel alternating attention mechanism ; it first attends to some parts of the query , then finds their corresponding matches by attending to the document .","['This', 'phase', 'involves', 'a', 'novel', 'alternating', 'attention', 'mechanism', ';', 'it', 'first', 'attends', 'to', 'some', 'parts', 'of', 'the', 'query', ',', 'then', 'finds', 'their', 'corresponding', 'matches', 'by', 'attending', 'to', 'the', 'document', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'NN', 'VBG', 'NN', 'NN', ':', 'PRP', 'RB', 'VBZ', 'TO', 'DT', 'NNS', 'IN', 'DT', 'NN', ',', 'RB', 'VBZ', 'PRP$', 'JJ', 'NNS', 'IN', 'VBG', 'TO', 'DT', 'NN', '.']",30
natural_language_inference,48,23,The result of this alternating search is fed back into the iterative inference process to seed the next search step .,"['The', 'result', 'of', 'this', 'alternating', 'search', 'is', 'fed', 'back', 'into', 'the', 'iterative', 'inference', 'process', 'to', 'seed', 'the', 'next', 'search', 'step', '.']","['O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'DT', 'VBG', 'NN', 'VBZ', 'VBN', 'RB', 'IN', 'DT', 'JJ', 'NN', 'NN', 'TO', 'NN', 'DT', 'JJ', 'NN', 'NN', '.']",21
natural_language_inference,48,25,"After a fixed number of iterations , the model uses a summary of its inference process to predict the answer .","['After', 'a', 'fixed', 'number', 'of', 'iterations', ',', 'the', 'model', 'uses', 'a', 'summary', 'of', 'its', 'inference', 'process', 'to', 'predict', 'the', 'answer', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', ',', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', '.']",21
natural_language_inference,48,118,"To train our model , we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .","['To', 'train', 'our', 'model', ',', 'we', 'used', 'stochastic', 'gradient', 'descent', 'with', 'the', 'ADAM', 'optimizer', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', ',', 'with', 'an', 'initial', 'learning', 'rate', 'of', '0.001', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['TO', 'VB', 'PRP$', 'NN', ',', 'PRP', 'VBD', 'JJ', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', ',', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', '.']",30
natural_language_inference,48,119,"We set the batch size to 32 and we decay the learning rate by 0.8 if the accuracy on the validation set does not increase after a half - epoch , i.e. 2000 batches ( for CBT ) and 5000 batches for ( CNN ) .","['We', 'set', 'the', 'batch', 'size', 'to', '32', 'and', 'we', 'decay', 'the', 'learning', 'rate', 'by', '0.8', 'if', 'the', 'accuracy', 'on', 'the', 'validation', 'set', 'does', 'not', 'increase', 'after', 'a', 'half', '-', 'epoch', ',', 'i.e.', '2000', 'batches', '(', 'for', 'CBT', ')', 'and', '5000', 'batches', 'for', '(', 'CNN', ')', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'CD', 'CC', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'RB', 'VB', 'IN', 'DT', 'JJ', ':', 'NN', ',', 'JJ', 'CD', 'NNS', '(', 'IN', 'NNP', ')', 'CC', 'CD', 'NNS', 'IN', '(', 'NNP', ')', '.']",46
natural_language_inference,48,120,"We initialize all weights of our model by sampling from the normal distribution N ( 0 , 0.05 ) .","['We', 'initialize', 'all', 'weights', 'of', 'our', 'model', 'by', 'sampling', 'from', 'the', 'normal', 'distribution', 'N', '(', '0', ',', '0.05', ')', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNS', 'IN', 'PRP$', 'NN', 'IN', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NNP', '(', 'CD', ',', 'CD', ')', '.']",20
natural_language_inference,48,121,"Following , the GRU recurrent weights are initialized to be orthogonal and biases are initialized to zero .","['Following', ',', 'the', 'GRU', 'recurrent', 'weights', 'are', 'initialized', 'to', 'be', 'orthogonal', 'and', 'biases', 'are', 'initialized', 'to', 'zero', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['VBG', ',', 'DT', 'NNP', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'VB', 'JJ', 'CC', 'NNS', 'VBP', 'VBN', 'TO', 'CD', '.']",18
natural_language_inference,48,122,"In order to stabilize the learning , we clip the gradients if their norm is greater than 5 and those marked with 2 are from .","['In', 'order', 'to', 'stabilize', 'the', 'learning', ',', 'we', 'clip', 'the', 'gradients', 'if', 'their', 'norm', 'is', 'greater', 'than', '5', 'and', 'those', 'marked', 'with', '2', 'are', 'from', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', 'TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNS', 'IN', 'PRP$', 'NN', 'VBZ', 'JJR', 'IN', 'CD', 'CC', 'DT', 'VBN', 'IN', 'CD', 'VBP', 'IN', '.']",26
natural_language_inference,48,124,"We found that setting embedding regularization to 0.0001 , T = 8 , d = 384 , h = 128 , s = 512 worked robustly across the datasets .","['We', 'found', 'that', 'setting', 'embedding', 'regularization', 'to', '0.0001', ',', 'T', '=', '8', ',', 'd', '=', '384', ',', 'h', '=', '128', ',', 's', '=', '512', 'worked', 'robustly', 'across', 'the', 'datasets', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'O']","['PRP', 'VBD', 'IN', 'VBG', 'VBG', 'NN', 'TO', 'CD', ',', 'NNP', 'NNP', 'CD', ',', 'NN', 'VBD', 'CD', ',', 'NN', 'NN', 'CD', ',', 'VBD', 'JJ', 'CD', 'VBD', 'RB', 'IN', 'DT', 'NNS', '.']",30
natural_language_inference,48,125,"Our model is implemented in Theano , using the Keras library .","['Our', 'model', 'is', 'implemented', 'in', 'Theano', ',', 'using', 'the', 'Keras', 'library', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'VBN', 'IN', 'NNP', ',', 'VBG', 'DT', 'NNP', 'NN', '.']",12
natural_language_inference,19,2,Distance - based Self - Attention Network for Natural Language Inference,"['Distance', '-', 'based', 'Self', '-', 'Attention', 'Network', 'for', 'Natural', 'Language', 'Inference']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', ':', 'VBN', 'NNP', ':', 'NN', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",11
natural_language_inference,19,10,"Our model shows good performance with NLI data , and it records the new state - of - the - art result with SNLI data .","['Our', 'model', 'shows', 'good', 'performance', 'with', 'NLI', 'data', ',', 'and', 'it', 'records', 'the', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'result', 'with', 'SNLI', 'data', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'VBZ', 'JJ', 'NN', 'IN', 'NNP', 'NNS', ',', 'CC', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'NNP', 'NNS', '.']",26
natural_language_inference,19,18,"More recently , models incorporating attention mechanisms have shown good performance in machine translation , Natural Language Inference ( NLI ) , and Question Answering ( QA ) etc .","['More', 'recently', ',', 'models', 'incorporating', 'attention', 'mechanisms', 'have', 'shown', 'good', 'performance', 'in', 'machine', 'translation', ',', 'Natural', 'Language', 'Inference', '(', 'NLI', ')', ',', 'and', 'Question', 'Answering', '(', 'QA', ')', 'etc', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RBR', 'RB', ',', 'NNS', 'VBG', 'NN', 'NNS', 'VBP', 'VBN', 'JJ', 'NN', 'IN', 'NN', 'NN', ',', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'CC', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', '.']",30
natural_language_inference,19,32,"To tackle this limitation , we propose Distancebased Self - Attention Network which introduces a distance mask which models the relative distance between words .","['To', 'tackle', 'this', 'limitation', ',', 'we', 'propose', 'Distancebased', 'Self', '-', 'Attention', 'Network', 'which', 'introduces', 'a', 'distance', 'mask', 'which', 'models', 'the', 'relative', 'distance', 'between', 'words', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'VBN', 'NNP', ':', 'NN', 'NNP', 'WDT', 'VBZ', 'DT', 'NN', 'NN', 'WDT', 'NNS', 'DT', 'JJ', 'NN', 'IN', 'NNS', '.']",25
natural_language_inference,19,33,"In conjunction with a directional mask , the distance mask allows us to incorporate complete positional information of words in our model .","['In', 'conjunction', 'with', 'a', 'directional', 'mask', ',', 'the', 'distance', 'mask', 'allows', 'us', 'to', 'incorporate', 'complete', 'positional', 'information', 'of', 'words', 'in', 'our', 'model', '.']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'DT', 'NN', 'NN', 'VBZ', 'PRP', 'TO', 'VB', 'JJ', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'PRP$', 'NN', '.']",23
natural_language_inference,19,165,We used the Glove 840B 300D 1 ( d e = 300 ) for the pre-trained word embedding without any finetuning .,"['We', 'used', 'the', 'Glove', '840B', '300D', '1', '(', 'd', 'e', '=', '300', ')', 'for', 'the', 'pre-trained', 'word', 'embedding', 'without', 'any', 'finetuning', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['PRP', 'VBD', 'DT', 'NNP', 'CD', 'CD', 'CD', '(', 'JJ', 'NN', 'NNP', 'CD', ')', 'IN', 'DT', 'JJ', 'NN', 'VBG', 'IN', 'DT', 'NN', '.']",22
natural_language_inference,19,166,This is to train the more universally usable sentence encoder .,"['This', 'is', 'to', 'train', 'the', 'more', 'universally', 'usable', 'sentence', 'encoder', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'TO', 'VB', 'DT', 'RBR', 'RB', 'JJ', 'NN', 'NN', '.']",11
natural_language_inference,19,167,"Layer normalization was applied to all linear projections of masked multihead attention , fusion gate , and multi-dimensional attention .","['Layer', 'normalization', 'was', 'applied', 'to', 'all', 'linear', 'projections', 'of', 'masked', 'multihead', 'attention', ',', 'fusion', 'gate', ',', 'and', 'multi-dimensional', 'attention', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']","['NNP', 'NN', 'VBD', 'VBN', 'TO', 'DT', 'JJ', 'NNS', 'IN', 'VBN', 'JJ', 'NN', ',', 'NN', 'NN', ',', 'CC', 'JJ', 'NN', '.']",20
natural_language_inference,19,168,"We applied residual dropout as used in , with dropout to the output of masked multi-head attention and SF +H F +b F of fusion gate .","['We', 'applied', 'residual', 'dropout', 'as', 'used', 'in', ',', 'with', 'dropout', 'to', 'the', 'output', 'of', 'masked', 'multi-head', 'attention', 'and', 'SF', '+H', 'F', '+b', 'F', 'of', 'fusion', 'gate', '.']","['O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'JJ', 'NN', 'IN', 'VBN', 'IN', ',', 'IN', 'NN', 'TO', 'DT', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'CC', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NN', 'NN', '.']",27
natural_language_inference,19,170,"We set h = 5 , ? = 1.5 in the masked multi-head attention , and the dropout probability was set to 0.1 .","['We', 'set', 'h', '=', '5', ',', '?', '=', '1.5', 'in', 'the', 'masked', 'multi-head', 'attention', ',', 'and', 'the', 'dropout', 'probability', 'was', 'set', 'to', '0.1', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'JJ', 'CD', ',', '.', '$', 'CD', 'IN', 'DT', 'JJ', 'JJ', 'NN', ',', 'CC', 'DT', 'NN', 'NN', 'VBD', 'VBN', 'TO', 'CD', '.']",24
natural_language_inference,19,171,"Batch size was 64 , and the model was trained with Adam optimizer , with a learning rate of 0.001 .","['Batch', 'size', 'was', '64', ',', 'and', 'the', 'model', 'was', 'trained', 'with', 'Adam', 'optimizer', ',', 'with', 'a', 'learning', 'rate', 'of', '0.001', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', 'NN', 'VBD', 'CD', ',', 'CC', 'DT', 'NN', 'VBD', 'VBN', 'IN', 'NNP', 'NN', ',', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', '.']",21
natural_language_inference,19,172,All models were implemented via Tensorflow on single Nvidia Geforce GTX 1080 Ti GPU .,"['All', 'models', 'were', 'implemented', 'via', 'Tensorflow', 'on', 'single', 'Nvidia', 'Geforce', 'GTX', '1080', 'Ti', 'GPU', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBD', 'VBN', 'IN', 'NNP', 'IN', 'JJ', 'NNP', 'NNP', 'NNP', 'CD', 'NNP', 'NNP', '.']",15
natural_language_inference,19,174,Experimental results of SNLI data compared with the existing models on the SNLI leader - board 2 are shown in .,"['Experimental', 'results', 'of', 'SNLI', 'data', 'compared', 'with', 'the', 'existing', 'models', 'on', 'the', 'SNLI', 'leader', '-', 'board', '2', 'are', 'shown', 'in', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNS', 'IN', 'NNP', 'NNS', 'VBN', 'IN', 'DT', 'VBG', 'NNS', 'IN', 'DT', 'NNP', 'NN', ':', 'NN', 'CD', 'VBP', 'VBN', 'IN', '.']",21
natural_language_inference,19,175,"Compared with the existing state - of - the - art model , the number of parameters and the training time increased , but our results show the new state - of - theart record .","['Compared', 'with', 'the', 'existing', 'state', '-', 'of', '-', 'the', '-', 'art', 'model', ',', 'the', 'number', 'of', 'parameters', 'and', 'the', 'training', 'time', 'increased', ',', 'but', 'our', 'results', 'show', 'the', 'new', 'state', '-', 'of', '-', 'theart', 'record', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'VBG', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', ',', 'DT', 'NN', 'IN', 'NNS', 'CC', 'DT', 'NN', 'NN', 'VBN', ',', 'CC', 'PRP$', 'NNS', 'VBP', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'NN', 'NN', '.']",36
natural_language_inference,19,177,Results show that the addition of the distance mask improved the performance without significantly affecting the training time or increasing the number of parameters . 49.4 50.4 + Unigram and bigram features 99.7 78.2 Sentence encoding - based models 100D LSTM encoders 220 k 84.8 77.6 300D LSTM encoders 3.0 m 83.9 80.6 1024D GRU encoders 15 m 98.8 81.4 300D Tree - based CNN encoders 3.5 m 83.3 82.1 300D SPINN - PI encoders 3.7 m 89.2 83.2 600D Bi- LSTM encoders 2.0 m 86.4 83.3 300D NTI - SLSTM - LSTM encoders 4.0 m 82.5 83.4 600D Bi-LSTM encoders+intra-attention 2.8 m 84.5 84.2 300D NSE encoders 3.0 m 86.2 84.6 600D,"['Results', 'show', 'that', 'the', 'addition', 'of', 'the', 'distance', 'mask', 'improved', 'the', 'performance', 'without', 'significantly', 'affecting', 'the', 'training', 'time', 'or', 'increasing', 'the', 'number', 'of', 'parameters', '.', '49.4', '50.4', '+', 'Unigram', 'and', 'bigram', 'features', '99.7', '78.2', 'Sentence', 'encoding', '-', 'based', 'models', '100D', 'LSTM', 'encoders', '220', 'k', '84.8', '77.6', '300D', 'LSTM', 'encoders', '3.0', 'm', '83.9', '80.6', '1024D', 'GRU', 'encoders', '15', 'm', '98.8', '81.4', '300D', 'Tree', '-', 'based', 'CNN', 'encoders', '3.5', 'm', '83.3', '82.1', '300D', 'SPINN', '-', 'PI', 'encoders', '3.7', 'm', '89.2', '83.2', '600D', 'Bi-', 'LSTM', 'encoders', '2.0', 'm', '86.4', '83.3', '300D', 'NTI', '-', 'SLSTM', '-', 'LSTM', 'encoders', '4.0', 'm', '82.5', '83.4', '600D', 'Bi-LSTM', 'encoders+intra-attention', '2.8', 'm', '84.5', '84.2', '300D', 'NSE', 'encoders', '3.0', 'm', '86.2', '84.6', '600D']","['O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNS', 'VBP', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBD', 'DT', 'NN', 'IN', 'RB', 'VBG', 'DT', 'NN', 'NN', 'CC', 'VBG', 'DT', 'NN', 'IN', 'NNS', '.', 'CD', 'CD', 'JJ', 'NNP', 'CC', 'NN', 'NNS', 'CD', 'CD', 'NN', 'VBG', ':', 'VBN', 'NNS', 'CD', 'NNP', 'NNS', 'CD', 'VBD', 'CD', 'CD', 'CD', 'NNP', 'NNS', 'CD', 'NNS', 'CD', 'CD', 'CD', 'NNP', 'NNS', 'CD', 'NNS', 'CD', 'CD', 'CD', 'NNP', ':', 'VBN', 'NNP', 'NNS', 'CD', 'NNS', 'CD', 'CD', 'CD', 'NNP', ':', 'NN', 'NNS', 'CD', 'NNS', 'CD', 'CD', 'CD', 'JJ', 'NNP', 'NNS', 'CD', 'NNS', 'CD', 'CD', 'CD', 'NNP', ':', 'NNP', ':', 'NNP', 'VBZ', 'CD', 'NN', 'CD', 'CD', 'CD', 'JJ', 'NN', 'CD', 'NN', 'CD', 'CD', 'CD', 'NNP', 'NNS', 'CD', 'NNS', 'CD', 'CD', 'CD']",113
natural_language_inference,19,180,"2 The improvement of the test accuracy by introducing the distance mask is only by 0.3 % point , potentially because SNLI data mostly consist of short sentences .","['2', 'The', 'improvement', 'of', 'the', 'test', 'accuracy', 'by', 'introducing', 'the', 'distance', 'mask', 'is', 'only', 'by', '0.3', '%', 'point', ',', 'potentially', 'because', 'SNLI', 'data', 'mostly', 'consist', 'of', 'short', 'sentences', '.']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['CD', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'NN', 'VBZ', 'RB', 'IN', 'CD', 'NN', 'NN', ',', 'RB', 'IN', 'NNP', 'NNS', 'RB', 'VBP', 'IN', 'JJ', 'NNS', '.']",29
natural_language_inference,19,186,The results of applying SNLI best model to MultiNLI dataset without additional parameter tuning are presented in .,"['The', 'results', 'of', 'applying', 'SNLI', 'best', 'model', 'to', 'MultiNLI', 'dataset', 'without', 'additional', 'parameter', 'tuning', 'are', 'presented', 'in', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'VBG', 'NNP', 'JJS', 'NN', 'TO', 'NNP', 'NN', 'IN', 'JJ', 'NN', 'VBG', 'VBP', 'VBN', 'IN', '.']",18
natural_language_inference,19,188,"Compared with the result of RepEVAL 2017 , we can see that the Distance - based Self - Attention Network performs well .","['Compared', 'with', 'the', 'result', 'of', 'RepEVAL', '2017', ',', 'we', 'can', 'see', 'that', 'the', 'Distance', '-', 'based', 'Self', '-', 'Attention', 'Network', 'performs', 'well', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['VBN', 'IN', 'DT', 'NN', 'IN', 'NNP', 'CD', ',', 'PRP', 'MD', 'VB', 'IN', 'DT', 'NNP', ':', 'VBN', 'NNP', ':', 'NN', 'NN', 'NNS', 'RB', '.']",23
natural_language_inference,19,189,"When compared with the model of , our model showed similar average test accuracy with much lower number of parameters .","['When', 'compared', 'with', 'the', 'model', 'of', ',', 'our', 'model', 'showed', 'similar', 'average', 'test', 'accuracy', 'with', 'much', 'lower', 'number', 'of', 'parameters', '.']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['WRB', 'VBN', 'IN', 'DT', 'NN', 'IN', ',', 'PRP$', 'NN', 'VBD', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'JJR', 'NN', 'IN', 'NNS', '.']",21
natural_language_inference,76,16,"Recognizing textual entailment ( RTE ) is the task of determining whether two natural language sentences are ( i ) contradicting each other , ( ii ) not related , or whether ( iii ) the first sentence ( called premise ) entails the second sentence ( called hypothesis ) .","['Recognizing', 'textual', 'entailment', '(', 'RTE', ')', 'is', 'the', 'task', 'of', 'determining', 'whether', 'two', 'natural', 'language', 'sentences', 'are', '(', 'i', ')', 'contradicting', 'each', 'other', ',', '(', 'ii', ')', 'not', 'related', ',', 'or', 'whether', '(', 'iii', ')', 'the', 'first', 'sentence', '(', 'called', 'premise', ')', 'entails', 'the', 'second', 'sentence', '(', 'called', 'hypothesis', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', 'NN', '(', 'NNP', ')', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'IN', 'CD', 'JJ', 'NN', 'NNS', 'VBP', '(', 'JJ', ')', 'VBG', 'DT', 'JJ', ',', '(', 'NN', ')', 'RB', 'VBN', ',', 'CC', 'IN', '(', 'NN', ')', 'DT', 'JJ', 'NN', '(', 'VBN', 'NN', ')', 'VBZ', 'DT', 'JJ', 'NN', '(', 'VBN', 'NN', ')', '.']",51
natural_language_inference,76,17,"This task is important since many natural language processing ( NLP ) problems , such as information extraction , relation extraction , text summarization or machine translation , rely on it explicitly or implicitly and could benefit from more accurate RTE systems .","['This', 'task', 'is', 'important', 'since', 'many', 'natural', 'language', 'processing', '(', 'NLP', ')', 'problems', ',', 'such', 'as', 'information', 'extraction', ',', 'relation', 'extraction', ',', 'text', 'summarization', 'or', 'machine', 'translation', ',', 'rely', 'on', 'it', 'explicitly', 'or', 'implicitly', 'and', 'could', 'benefit', 'from', 'more', 'accurate', 'RTE', 'systems', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O']","['DT', 'NN', 'VBZ', 'JJ', 'IN', 'JJ', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'NNS', ',', 'JJ', 'IN', 'NN', 'NN', ',', 'NN', 'NN', ',', 'JJ', 'NN', 'CC', 'NN', 'NN', ',', 'RB', 'IN', 'PRP', 'RB', 'CC', 'RB', 'CC', 'MD', 'VB', 'IN', 'JJR', 'JJ', 'NNP', 'NNS', '.']",43
natural_language_inference,76,25,"In contrast , we are proposing an attentive neural network that is capable of reasoning over entailments of pairs of words and phrases by processing the hypothesis conditioned on the premise .","['In', 'contrast', ',', 'we', 'are', 'proposing', 'an', 'attentive', 'neural', 'network', 'that', 'is', 'capable', 'of', 'reasoning', 'over', 'entailments', 'of', 'pairs', 'of', 'words', 'and', 'phrases', 'by', 'processing', 'the', 'hypothesis', 'conditioned', 'on', 'the', 'premise', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'VBG', 'DT', 'JJ', 'JJ', 'NN', 'WDT', 'VBZ', 'JJ', 'IN', 'VBG', 'IN', 'NNS', 'IN', 'NNS', 'IN', 'NNS', 'CC', 'NNS', 'IN', 'VBG', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', '.']",32
natural_language_inference,76,26,"Our contributions are threefold : ( i ) We present a neural model based on LSTMs that reads two sentences in one go to determine entailment , as opposed to mapping each sentence independently into a semantic space ( 2.2 ) , ( ii ) We extend this model with a neural word - by - word attention mechanism to encourage reasoning over entailments of pairs of words and phrases ( 2.4 ) , and ( iii ) We provide a detailed qualitative analysis of neural attention for RTE ( 4.1 ) .","['Our', 'contributions', 'are', 'threefold', ':', '(', 'i', ')', 'We', 'present', 'a', 'neural', 'model', 'based', 'on', 'LSTMs', 'that', 'reads', 'two', 'sentences', 'in', 'one', 'go', 'to', 'determine', 'entailment', ',', 'as', 'opposed', 'to', 'mapping', 'each', 'sentence', 'independently', 'into', 'a', 'semantic', 'space', '(', '2.2', ')', ',', '(', 'ii', ')', 'We', 'extend', 'this', 'model', 'with', 'a', 'neural', 'word', '-', 'by', '-', 'word', 'attention', 'mechanism', 'to', 'encourage', 'reasoning', 'over', 'entailments', 'of', 'pairs', 'of', 'words', 'and', 'phrases', '(', '2.4', ')', ',', 'and', '(', 'iii', ')', 'We', 'provide', 'a', 'detailed', 'qualitative', 'analysis', 'of', 'neural', 'attention', 'for', 'RTE', '(', '4.1', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NNS', 'VBP', 'VBN', ':', '(', 'NN', ')', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBN', 'IN', 'NNP', 'WDT', 'VBZ', 'CD', 'NNS', 'IN', 'CD', 'NN', 'TO', 'VB', 'NN', ',', 'IN', 'VBN', 'TO', 'VBG', 'DT', 'NN', 'RB', 'IN', 'DT', 'JJ', 'NN', '(', 'CD', ')', ',', '(', 'NN', ')', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'NN', 'NN', 'NN', 'TO', 'VB', 'VBG', 'IN', 'NNS', 'IN', 'NNS', 'IN', 'NNS', 'CC', 'NNS', '(', 'CD', ')', ',', 'CC', '(', 'NN', ')', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'IN', 'NNP', '(', 'CD', ')', '.']",93
natural_language_inference,76,99,We found that processing the hypothesis conditioned on the premise instead of encoding each sentence independently gives an improvement of 3.3 percentage points in accuracy over Bowman et al. 's LSTM .,"['We', 'found', 'that', 'processing', 'the', 'hypothesis', 'conditioned', 'on', 'the', 'premise', 'instead', 'of', 'encoding', 'each', 'sentence', 'independently', 'gives', 'an', 'improvement', 'of', '3.3', 'percentage', 'points', 'in', 'accuracy', 'over', 'Bowman', 'et', 'al.', ""'s"", 'LSTM', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'IN', 'VBG', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', 'RB', 'IN', 'VBG', 'DT', 'NN', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', 'NNS', 'IN', 'NN', 'IN', 'NNP', 'CC', 'NN', 'POS', 'NNP', '.']",32
natural_language_inference,76,105,Our LSTM outperforms a simple lexicalized classifier by 2.7 percentage points .,"['Our', 'LSTM', 'outperforms', 'a', 'simple', 'lexicalized', 'classifier', 'by', '2.7', 'percentage', 'points', '.']","['B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'NNP', 'VBZ', 'DT', 'JJ', 'VBN', 'NN', 'IN', 'CD', 'NN', 'NNS', '.']",12
natural_language_inference,76,108,"By incorporating an attention mechanism we found a 0.9 percentage point improvement over a single LSTM with a hidden size of 159 , and a 1.4 percentage point increase over a benchmark model that uses two LSTMs for conditional encoding ( one for the premise and one for the hypothesis conditioned on the representation of the premise ) .","['By', 'incorporating', 'an', 'attention', 'mechanism', 'we', 'found', 'a', '0.9', 'percentage', 'point', 'improvement', 'over', 'a', 'single', 'LSTM', 'with', 'a', 'hidden', 'size', 'of', '159', ',', 'and', 'a', '1.4', 'percentage', 'point', 'increase', 'over', 'a', 'benchmark', 'model', 'that', 'uses', 'two', 'LSTMs', 'for', 'conditional', 'encoding', '(', 'one', 'for', 'the', 'premise', 'and', 'one', 'for', 'the', 'hypothesis', 'conditioned', 'on', 'the', 'representation', 'of', 'the', 'premise', ')', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBG', 'DT', 'NN', 'NN', 'PRP', 'VBD', 'DT', 'CD', 'NN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', ',', 'CC', 'DT', 'CD', 'NN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'CD', 'NNP', 'IN', 'JJ', 'NN', '(', 'CD', 'IN', 'DT', 'NN', 'CC', 'CD', 'IN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', ')', '.']",59
natural_language_inference,76,114,Enabling the model to attend over output vectors of the premise for every word in the hypothesis yields another 1.2 percentage point improvement compared to attending based only on the last output vector of the premise .,"['Enabling', 'the', 'model', 'to', 'attend', 'over', 'output', 'vectors', 'of', 'the', 'premise', 'for', 'every', 'word', 'in', 'the', 'hypothesis', 'yields', 'another', '1.2', 'percentage', 'point', 'improvement', 'compared', 'to', 'attending', 'based', 'only', 'on', 'the', 'last', 'output', 'vector', 'of', 'the', 'premise', '.']","['B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['VBG', 'DT', 'NN', 'TO', 'VB', 'RP', 'NN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'DT', 'CD', 'NN', 'NN', 'NN', 'VBN', 'TO', 'VBG', 'VBN', 'RB', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', '.']",37
natural_language_inference,76,117,Allowing the model to also attend over the hypothesis based on the premise does not seem to improve performance for RTE .,"['Allowing', 'the', 'model', 'to', 'also', 'attend', 'over', 'the', 'hypothesis', 'based', 'on', 'the', 'premise', 'does', 'not', 'seem', 'to', 'improve', 'performance', 'for', 'RTE', '.']","['B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'O', 'B-n', 'B-n', 'B-p', 'B-n', 'O']","['VBG', 'DT', 'NN', 'TO', 'RB', 'VB', 'IN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'VB', 'TO', 'VB', 'NN', 'IN', 'NNP', '.']",22
natural_language_inference,65,10,"Neural networks ( NN ) with attention mechanisms have recently proven to be successful at different computer vision ( CV ) and natural language processing ( NLP ) tasks such as image captioning , machine translation and factoid question answering .","['Neural', 'networks', '(', 'NN', ')', 'with', 'attention', 'mechanisms', 'have', 'recently', 'proven', 'to', 'be', 'successful', 'at', 'different', 'computer', 'vision', '(', 'CV', ')', 'and', 'natural', 'language', 'processing', '(', 'NLP', ')', 'tasks', 'such', 'as', 'image', 'captioning', ',', 'machine', 'translation', 'and', 'factoid', 'question', 'answering', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNS', '(', 'NNP', ')', 'IN', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'TO', 'VB', 'JJ', 'IN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'NNS', 'JJ', 'IN', 'NN', 'NN', ',', 'NN', 'NN', 'CC', 'JJ', 'NN', 'NN', '.']",41
natural_language_inference,65,11,"However , most recent work on neural attention models have focused on one - way attention mechanisms based on recurrent neural networks designed . for generation tasks .","['However', ',', 'most', 'recent', 'work', 'on', 'neural', 'attention', 'models', 'have', 'focused', 'on', 'one', '-', 'way', 'attention', 'mechanisms', 'based', 'on', 'recurrent', 'neural', 'networks', 'designed', '.', 'for', 'generation', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'JJS', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'CD', ':', 'NN', 'NN', 'NNS', 'VBN', 'IN', 'JJ', 'JJ', 'NNS', 'VBN', '.', 'IN', 'NN', 'NNS', '.']",28
natural_language_inference,65,15,"The key contribution of this work is that we propose Attentive Pooling ( AP ) , a two - way attention mechanism , that significantly improves such discriminative models ' performance on pair - wise ranking or classification , by enabling a joint learning of the representations of both inputs as well as their similarity measurement .","['The', 'key', 'contribution', 'of', 'this', 'work', 'is', 'that', 'we', 'propose', 'Attentive', 'Pooling', '(', 'AP', ')', ',', 'a', 'two', '-', 'way', 'attention', 'mechanism', ',', 'that', 'significantly', 'improves', 'such', 'discriminative', 'models', ""'"", 'performance', 'on', 'pair', '-', 'wise', 'ranking', 'or', 'classification', ',', 'by', 'enabling', 'a', 'joint', 'learning', 'of', 'the', 'representations', 'of', 'both', 'inputs', 'as', 'well', 'as', 'their', 'similarity', 'measurement', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'IN', 'PRP', 'VBP', 'JJ', 'NNP', '(', 'NNP', ')', ',', 'DT', 'CD', ':', 'NN', 'NN', 'NN', ',', 'WDT', 'RB', 'VBZ', 'JJ', 'JJ', 'NNS', 'POS', 'NN', 'IN', 'NN', ':', 'NN', 'NN', 'CC', 'NN', ',', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNS', 'RB', 'RB', 'IN', 'PRP$', 'NN', 'NN', '.']",57
natural_language_inference,65,16,"Specifically , AP enables the pooling layer to be aware of the current input pair , in a way that information from the two input items can directly influence the computation of each other 's representations .","['Specifically', ',', 'AP', 'enables', 'the', 'pooling', 'layer', 'to', 'be', 'aware', 'of', 'the', 'current', 'input', 'pair', ',', 'in', 'a', 'way', 'that', 'information', 'from', 'the', 'two', 'input', 'items', 'can', 'directly', 'influence', 'the', 'computation', 'of', 'each', 'other', ""'s"", 'representations', '.']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'NNP', 'VBZ', 'DT', 'VBG', 'NN', 'TO', 'VB', 'JJ', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'IN', 'DT', 'NN', 'WDT', 'NN', 'IN', 'DT', 'CD', 'NN', 'NNS', 'MD', 'RB', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'POS', 'NNS', '.']",37
natural_language_inference,65,17,"The main idea in AP consists of learning a similarity measure over projected segments ( e.g. trigrams ) of the two items in the input pair , and using the similarity scores between the segments to compute attention vectors in both directions .","['The', 'main', 'idea', 'in', 'AP', 'consists', 'of', 'learning', 'a', 'similarity', 'measure', 'over', 'projected', 'segments', '(', 'e.g.', 'trigrams', ')', 'of', 'the', 'two', 'items', 'in', 'the', 'input', 'pair', ',', 'and', 'using', 'the', 'similarity', 'scores', 'between', 'the', 'segments', 'to', 'compute', 'attention', 'vectors', 'in', 'both', 'directions', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'NNP', 'NNS', 'IN', 'VBG', 'DT', 'NN', 'NN', 'IN', 'VBN', 'NNS', '(', 'JJ', 'NNS', ')', 'IN', 'DT', 'CD', 'NNS', 'IN', 'DT', 'NN', 'NN', ',', 'CC', 'VBG', 'DT', 'NN', 'VBZ', 'IN', 'DT', 'NNS', 'TO', 'VB', 'NN', 'NNS', 'IN', 'DT', 'NNS', '.']",43
natural_language_inference,65,18,"Next , the attention vectors are used to perform pooling .","['Next', ',', 'the', 'attention', 'vectors', 'are', 'used', 'to', 'perform', 'pooling', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['RB', ',', 'DT', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'VB', 'NN', '.']",11
natural_language_inference,65,171,"We use a context window of size 3 for Insurance QA , while we set this parameter to 4 for TREC - QA and Wiki QA .","['We', 'use', 'a', 'context', 'window', 'of', 'size', '3', 'for', 'Insurance', 'QA', ',', 'while', 'we', 'set', 'this', 'parameter', 'to', '4', 'for', 'TREC', '-', 'QA', 'and', 'Wiki', 'QA', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'NN', 'CD', 'IN', 'NNP', 'NNP', ',', 'IN', 'PRP', 'VBP', 'DT', 'NN', 'TO', 'CD', 'IN', 'NNP', ':', 'NN', 'CC', 'NNP', 'NNP', '.']",27
natural_language_inference,65,172,"Using the selected hyperparameters , the best results are normally achieved using between 15 and 25 training epochs .","['Using', 'the', 'selected', 'hyperparameters', ',', 'the', 'best', 'results', 'are', 'normally', 'achieved', 'using', 'between', '15', 'and', '25', 'training', 'epochs', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'DT', 'VBN', 'NNS', ',', 'DT', 'JJS', 'NNS', 'VBP', 'RB', 'VBN', 'VBG', 'IN', 'CD', 'CC', 'CD', 'NN', 'NN', '.']",19
natural_language_inference,65,173,"For AP - CNN , AP - biLSTM and QA - LSTM , we also use a learning rate schedule that decreases the learning rate ?","['For', 'AP', '-', 'CNN', ',', 'AP', '-', 'biLSTM', 'and', 'QA', '-', 'LSTM', ',', 'we', 'also', 'use', 'a', 'learning', 'rate', 'schedule', 'that', 'decreases', 'the', 'learning', 'rate', '?']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'O']","['IN', 'NNP', ':', 'NNP', ',', 'NNP', ':', 'NN', 'CC', 'NNP', ':', 'NNP', ',', 'PRP', 'RB', 'VBP', 'DT', 'NN', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'VBG', 'NN', '.']",26
natural_language_inference,65,178,"In our experiments , the four NN architectures QA - CNN , AP - CNN , QA - biLSTM and AP - biLSTM are implemented using Theano .","['In', 'our', 'experiments', ',', 'the', 'four', 'NN', 'architectures', 'QA', '-', 'CNN', ',', 'AP', '-', 'CNN', ',', 'QA', '-', 'biLSTM', 'and', 'AP', '-', 'biLSTM', 'are', 'implemented', 'using', 'Theano', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'PRP$', 'NNS', ',', 'DT', 'CD', 'NNP', 'NNS', 'NNP', ':', 'NNP', ',', 'NNP', ':', 'NNP', ',', 'NNP', ':', 'NN', 'CC', 'NNP', ':', 'NN', 'VBP', 'VBN', 'VBG', 'NNP', '.']",28
natural_language_inference,65,181,"In , we present the experimental results of the four NNs for the Insurance QA dataset .","['In', ',', 'we', 'present', 'the', 'experimental', 'results', 'of', 'the', 'four', 'NNs', 'for', 'the', 'Insurance', 'QA', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'CD', 'NNP', 'IN', 'DT', 'NNP', 'NNP', 'NN', '.']",17
natural_language_inference,65,183,"On the bottom part of this table , we can see that AP - CNN outperforms QA - CNN by a large margin in both test sets , as well as in the dev set .","['On', 'the', 'bottom', 'part', 'of', 'this', 'table', ',', 'we', 'can', 'see', 'that', 'AP', '-', 'CNN', 'outperforms', 'QA', '-', 'CNN', 'by', 'a', 'large', 'margin', 'in', 'both', 'test', 'sets', ',', 'as', 'well', 'as', 'in', 'the', 'dev', 'set', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', ',', 'PRP', 'MD', 'VB', 'DT', 'NNP', ':', 'NN', 'NNS', 'NNP', ':', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NNS', ',', 'RB', 'RB', 'IN', 'IN', 'DT', 'NN', 'NN', '.']",36
natural_language_inference,65,185,AP - CNN and AP - biLSTM have similar performance .,"['AP', '-', 'CNN', 'and', 'AP', '-', 'biLSTM', 'have', 'similar', 'performance', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', ':', 'NN', 'CC', 'NNP', ':', 'NN', 'VBP', 'JJ', 'NN', '.']",11
natural_language_inference,65,189,Both AP - CNN and AP - biLSTM outperform the state - of - the - art systems .,"['Both', 'AP', '-', 'CNN', 'and', 'AP', '-', 'biLSTM', 'outperform', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'systems', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', ':', 'NN', 'CC', 'NNP', ':', 'NN', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', '.']",19
natural_language_inference,65,201,"In , we present the experimental results of the four NNs for the TREC - QA dataset .","['In', ',', 'we', 'present', 'the', 'experimental', 'results', 'of', 'the', 'four', 'NNs', 'for', 'the', 'TREC', '-', 'QA', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'CD', 'NNP', 'IN', 'DT', 'NNP', ':', 'NN', 'NN', '.']",18
natural_language_inference,65,203,We use the official trec eval that AP - CNN outperforms QA - CNN by a large margin in both metrics .,"['We', 'use', 'the', 'official', 'trec', 'eval', 'that', 'AP', '-', 'CNN', 'outperforms', 'QA', '-', 'CNN', 'by', 'a', 'large', 'margin', 'in', 'both', 'metrics', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'NNP', ':', 'NN', 'NNS', 'NNP', ':', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNS', '.']",22
natural_language_inference,65,204,"AP - biLSTM outperforms the QA - biLSTM , but its performance is not as good as the of AP - CNN .","['AP', '-', 'biLSTM', 'outperforms', 'the', 'QA', '-', 'biLSTM', ',', 'but', 'its', 'performance', 'is', 'not', 'as', 'good', 'as', 'the', 'of', 'AP', '-', 'CNN', '.']","['B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', 'VBZ', 'DT', 'NNP', ':', 'NN', ',', 'CC', 'PRP$', 'NN', 'VBZ', 'RB', 'RB', 'JJ', 'IN', 'DT', 'IN', 'NNP', ':', 'NN', '.']",23
natural_language_inference,65,210,"AP - CNN outperforms the state - of - the - art systems in both metrics , MAP and MRR .","['AP', '-', 'CNN', 'outperforms', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'systems', 'in', 'both', 'metrics', ',', 'MAP', 'and', 'MRR', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'NNS', ',', 'NNP', 'CC', 'NNP', '.']",21
natural_language_inference,65,211,shows the experimental results of the four NNs for the WikiQA dataset .,"['shows', 'the', 'experimental', 'results', 'of', 'the', 'four', 'NNs', 'for', 'the', 'WikiQA', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['VBZ', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'CD', 'NNP', 'IN', 'DT', 'NNP', 'NN', '.']",13
natural_language_inference,65,212,"Like in the other two datasets , AP - CNN outperforms QA - CNN , and AP - biLSTM outperforms the QA - biLSTM .","['Like', 'in', 'the', 'other', 'two', 'datasets', ',', 'AP', '-', 'CNN', 'outperforms', 'QA', '-', 'CNN', ',', 'and', 'AP', '-', 'biLSTM', 'outperforms', 'the', 'QA', '-', 'biLSTM', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'IN', 'DT', 'JJ', 'CD', 'NNS', ',', 'NNP', ':', 'NN', 'NNS', 'NNP', ':', 'NNP', ',', 'CC', 'NNP', ':', 'NN', 'VBZ', 'DT', 'NNP', ':', 'NN', '.']",25
natural_language_inference,65,213,The difference of performance between AP - CNN and QA - CNN is smaller than the one for the Insurance QA dataset .,"['The', 'difference', 'of', 'performance', 'between', 'AP', '-', 'CNN', 'and', 'QA', '-', 'CNN', 'is', 'smaller', 'than', 'the', 'one', 'for', 'the', 'Insurance', 'QA', 'dataset', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'NN', 'IN', 'NNP', ':', 'NN', 'CC', 'NNP', ':', 'NN', 'VBZ', 'JJR', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'NN', '.']",23
natural_language_inference,20,2,Sentence Embeddings in NLI with Iterative Refinement Encoders,"['Sentence', 'Embeddings', 'in', 'NLI', 'with', 'Iterative', 'Refinement', 'Encoders']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NNS', 'IN', 'NNP', 'IN', 'NNP', 'NNP', 'NNS']",8
natural_language_inference,20,4,Sentence - level representations are necessary for various NLP tasks .,"['Sentence', '-', 'level', 'representations', 'are', 'necessary', 'for', 'various', 'NLP', 'tasks', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', 'NNS', 'VBP', 'JJ', 'IN', 'JJ', 'NNP', 'NNS', '.']",11
natural_language_inference,20,20,"With the goal of obtaining general - purpose sentence representations in mind , we opt for the sentence encoding approach .","['With', 'the', 'goal', 'of', 'obtaining', 'general', '-', 'purpose', 'sentence', 'representations', 'in', 'mind', ',', 'we', 'opt', 'for', 'the', 'sentence', 'encoding', 'approach', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'IN', 'VBG', 'JJ', ':', 'JJ', 'NN', 'NNS', 'IN', 'NN', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'VBG', 'NN', '.']",21
natural_language_inference,20,21,Motivated by the success of the InferSent architecture we extend their architecture with a hierarchylike structure of bidirectional LSTM ( BiLSTM ) layers with max pooling .,"['Motivated', 'by', 'the', 'success', 'of', 'the', 'InferSent', 'architecture', 'we', 'extend', 'their', 'architecture', 'with', 'a', 'hierarchylike', 'structure', 'of', 'bidirectional', 'LSTM', '(', 'BiLSTM', ')', 'layers', 'with', 'max', 'pooling', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', 'PRP', 'VBP', 'PRP$', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNP', '(', 'NNP', ')', 'NNS', 'IN', 'JJ', 'NN', '.']",27
natural_language_inference,20,71,The architecture was implemented using PyTorch .,"['The', 'architecture', 'was', 'implemented', 'using', 'PyTorch', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'VBD', 'VBN', 'VBG', 'NNP', '.']",7
natural_language_inference,20,72,We have published our code in GitHub : https://github.com/Helsinki-NLP/HBMP.,"['We', 'have', 'published', 'our', 'code', 'in', 'GitHub', ':', 'https://github.com/Helsinki-NLP/HBMP.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']","['PRP', 'VBP', 'VBN', 'PRP$', 'NN', 'IN', 'NNP', ':', 'NN']",9
natural_language_inference,20,73,"For all of our models we used a gradient descent optimization algorithm based on the Adam update rule , which is pre-implemented in PyTorch .","['For', 'all', 'of', 'our', 'models', 'we', 'used', 'a', 'gradient', 'descent', 'optimization', 'algorithm', 'based', 'on', 'the', 'Adam', 'update', 'rule', ',', 'which', 'is', 'pre-implemented', 'in', 'PyTorch', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'DT', 'IN', 'PRP$', 'NNS', 'PRP', 'VBD', 'DT', 'JJ', 'NN', 'NN', 'NN', 'VBN', 'IN', 'DT', 'NNP', 'NN', 'NN', ',', 'WDT', 'VBZ', 'JJ', 'IN', 'NNP', '.']",25
natural_language_inference,20,74,We used a learning rate of 5e - 4 for all our models .,"['We', 'used', 'a', 'learning', 'rate', 'of', '5e', '-', '4', 'for', 'all', 'our', 'models', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'CD', ':', 'CD', 'IN', 'DT', 'PRP$', 'NNS', '.']",14
natural_language_inference,20,75,The learning rate was decreased by the factor of 0.2 after each epoch if the model did not improve .,"['The', 'learning', 'rate', 'was', 'decreased', 'by', 'the', 'factor', 'of', '0.2', 'after', 'each', 'epoch', 'if', 'the', 'model', 'did', 'not', 'improve', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBD', 'VBN', 'IN', 'DT', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBD', 'RB', 'VB', '.']",20
natural_language_inference,20,76,We used a batch size of 64 .,"['We', 'used', 'a', 'batch', 'size', 'of', '64', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBD', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",8
natural_language_inference,20,79,"We use pre-trained Glo Ve word embeddings of size 300 dimensions ( Glo Ve 840B 300D ; , which were fine - tuned during training .","['We', 'use', 'pre-trained', 'Glo', 'Ve', 'word', 'embeddings', 'of', 'size', '300', 'dimensions', '(', 'Glo', 'Ve', '840B', '300D', ';', ',', 'which', 'were', 'fine', '-', 'tuned', 'during', 'training', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', 'NNP', 'NNP', 'NN', 'NNS', 'IN', 'NN', 'CD', 'NNS', '(', 'NNP', 'NNP', 'CD', 'CD', ':', ',', 'WDT', 'VBD', 'JJ', ':', 'VBN', 'IN', 'NN', '.']",26
natural_language_inference,20,80,"The sentence embeddings have hidden size of 600 for both direction ( except for SentEval test , where we test models with 600D and 1200D per direction ) and the 3 - layer multilayer perceptron ( MLP ) have the size of 600 dimensions .","['The', 'sentence', 'embeddings', 'have', 'hidden', 'size', 'of', '600', 'for', 'both', 'direction', '(', 'except', 'for', 'SentEval', 'test', ',', 'where', 'we', 'test', 'models', 'with', '600D', 'and', '1200D', 'per', 'direction', ')', 'and', 'the', '3', '-', 'layer', 'multilayer', 'perceptron', '(', 'MLP', ')', 'have', 'the', 'size', 'of', '600', 'dimensions', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'VBN', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', '(', 'IN', 'IN', 'NNP', 'NN', ',', 'WRB', 'PRP', 'VBP', 'NNS', 'IN', 'CD', 'CC', 'CD', 'IN', 'NN', ')', 'CC', 'DT', 'CD', ':', 'NN', 'NN', 'NN', '(', 'NNP', ')', 'VBP', 'DT', 'NN', 'IN', 'CD', 'NNS', '.']",45
natural_language_inference,20,81,We use a dropout of 0.1 between the MLP layers ( except just before the final layer ) .,"['We', 'use', 'a', 'dropout', 'of', '0.1', 'between', 'the', 'MLP', 'layers', '(', 'except', 'just', 'before', 'the', 'final', 'layer', ')', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'CD', 'IN', 'DT', 'NNP', 'NNS', '(', 'IN', 'RB', 'IN', 'DT', 'JJ', 'NN', ')', '.']",19
natural_language_inference,20,82,Our models were trained using one NVIDIA Tesla P100 GPU .,"['Our', 'models', 'were', 'trained', 'using', 'one', 'NVIDIA', 'Tesla', 'P100', 'GPU', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NNS', 'VBD', 'VBN', 'VBG', 'CD', 'NNP', 'NNP', 'NNP', 'NNP', '.']",11
natural_language_inference,20,124,It clearly outperforms the similar but non-hierarchical BiLSTM models reported in the literature and fares well in comparison to other state of the art architectures in the sentence encoding category .,"['It', 'clearly', 'outperforms', 'the', 'similar', 'but', 'non-hierarchical', 'BiLSTM', 'models', 'reported', 'in', 'the', 'literature', 'and', 'fares', 'well', 'in', 'comparison', 'to', 'other', 'state', 'of', 'the', 'art', 'architectures', 'in', 'the', 'sentence', 'encoding', 'category', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBZ', 'DT', 'JJ', 'CC', 'JJ', 'NNP', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'CC', 'NNS', 'RB', 'IN', 'NN', 'TO', 'JJ', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'VBG', 'NN', '.']",31
natural_language_inference,20,125,"In particular , our results are close to the current state of the art on SNLI in this category and strong on both , the matched and mismatched test sets of MultiNLI .","['In', 'particular', ',', 'our', 'results', 'are', 'close', 'to', 'the', 'current', 'state', 'of', 'the', 'art', 'on', 'SNLI', 'in', 'this', 'category', 'and', 'strong', 'on', 'both', ',', 'the', 'matched', 'and', 'mismatched', 'test', 'sets', 'of', 'MultiNLI', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'JJ', ',', 'PRP$', 'NNS', 'VBP', 'JJ', 'TO', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', 'IN', 'DT', 'NN', 'CC', 'JJ', 'IN', 'DT', ',', 'DT', 'NNS', 'CC', 'JJ', 'NN', 'NNS', 'IN', 'NNP', '.']",33
natural_language_inference,20,126,"Finally , on SciTail , we achieve the new state of the art with an accuracy of 86.0 % .","['Finally', ',', 'on', 'SciTail', ',', 'we', 'achieve', 'the', 'new', 'state', 'of', 'the', 'art', 'with', 'an', 'accuracy', 'of', '86.0', '%', '.']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-p', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'IN', 'NNP', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'CD', 'NN', '.']",20
natural_language_inference,20,130,"For the SNLI dataset , our model provides the test accuracy of 86.6 % after 4 epochs of training .","['For', 'the', 'SNLI', 'dataset', ',', 'our', 'model', 'provides', 'the', 'test', 'accuracy', 'of', '86.6', '%', 'after', '4', 'epochs', 'of', 'training', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NNP', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'CD', 'NN', 'IN', 'CD', 'NN', 'IN', 'NN', '.']",20
natural_language_inference,20,133,"For the MultiNLI matched test set ( MultiNLI - m ) our model achieves a test accuracy of 73.7 % after 3 epochs of training , which is 0.8 % points lower than the state of the art 74.5 % by .","['For', 'the', 'MultiNLI', 'matched', 'test', 'set', '(', 'MultiNLI', '-', 'm', ')', 'our', 'model', 'achieves', 'a', 'test', 'accuracy', 'of', '73.7', '%', 'after', '3', 'epochs', 'of', 'training', ',', 'which', 'is', '0.8', '%', 'points', 'lower', 'than', 'the', 'state', 'of', 'the', 'art', '74.5', '%', 'by', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'DT', 'NNP', 'VBD', 'NN', 'NN', '(', 'NNP', ':', 'NN', ')', 'PRP$', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'CD', 'NN', 'IN', 'CD', 'NN', 'IN', 'NN', ',', 'WDT', 'VBZ', 'CD', 'NN', 'NNS', 'JJR', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'CD', 'NN', 'IN', '.']",42
natural_language_inference,20,134,"For the mismatched test set ( MultiNLI - mm ) our model achieves a test accuracy of 73.0 % after 3 epochs of training , which is 0.6 % points lower than the state of the art 73.6 % by Chen , Zhu , Ling , Wei , Jiang , and Inkpen ( 2017 b ) .","['For', 'the', 'mismatched', 'test', 'set', '(', 'MultiNLI', '-', 'mm', ')', 'our', 'model', 'achieves', 'a', 'test', 'accuracy', 'of', '73.0', '%', 'after', '3', 'epochs', 'of', 'training', ',', 'which', 'is', '0.6', '%', 'points', 'lower', 'than', 'the', 'state', 'of', 'the', 'art', '73.6', '%', 'by', 'Chen', ',', 'Zhu', ',', 'Ling', ',', 'Wei', ',', 'Jiang', ',', 'and', 'Inkpen', '(', '2017', 'b', ')', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', 'NN', '(', 'NNP', ':', 'NN', ')', 'PRP$', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'CD', 'NN', 'IN', 'CD', 'NN', 'IN', 'NN', ',', 'WDT', 'VBZ', 'CD', 'NN', 'NNS', 'JJR', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'CD', 'NN', 'IN', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', '(', 'CD', 'NN', ')', '.']",57
natural_language_inference,20,138,"On the SciTail dataset we compared our model also against non-sentence embedding - based models , as no results have been previously published which are based on independent sentence embeddings .","['On', 'the', 'SciTail', 'dataset', 'we', 'compared', 'our', 'model', 'also', 'against', 'non-sentence', 'embedding', '-', 'based', 'models', ',', 'as', 'no', 'results', 'have', 'been', 'previously', 'published', 'which', 'are', 'based', 'on', 'independent', 'sentence', 'embeddings', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNP', 'NN', 'PRP', 'VBN', 'PRP$', 'NN', 'RB', 'IN', 'NN', 'VBG', ':', 'VBN', 'NNS', ',', 'IN', 'DT', 'NNS', 'VBP', 'VBN', 'RB', 'VBN', 'WDT', 'VBP', 'VBN', 'IN', 'JJ', 'NN', 'NNS', '.']",31
natural_language_inference,20,139,"We obtain a score of 86.0 % after 4 epochs of training , which is + 2.7 % points absolute improvement on the previous published state of the art by .","['We', 'obtain', 'a', 'score', 'of', '86.0', '%', 'after', '4', 'epochs', 'of', 'training', ',', 'which', 'is', '+', '2.7', '%', 'points', 'absolute', 'improvement', 'on', 'the', 'previous', 'published', 'state', 'of', 'the', 'art', 'by', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VB', 'DT', 'NN', 'IN', 'CD', 'NN', 'IN', 'CD', 'NN', 'IN', 'NN', ',', 'WDT', 'VBZ', 'JJ', 'CD', 'NN', 'NNS', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'VBN', 'NN', 'IN', 'DT', 'NN', 'IN', '.']",31
natural_language_inference,20,140,Our model also outperforms In - fer Sent which achieves an accuracy of 85.1 % in our experiments .,"['Our', 'model', 'also', 'outperforms', 'In', '-', 'fer', 'Sent', 'which', 'achieves', 'an', 'accuracy', 'of', '85.1', '%', 'in', 'our', 'experiments', '.']","['B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-p', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'RB', 'VBZ', 'IN', ':', 'NN', 'NNP', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', 'IN', 'PRP$', 'NNS', '.']",19
natural_language_inference,20,142,The results achieved by our proposed model are significantly higher than the previously published results .,"['The', 'results', 'achieved', 'by', 'our', 'proposed', 'model', 'are', 'significantly', 'higher', 'than', 'the', 'previously', 'published', 'results', '.']","['O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBN', 'IN', 'PRP$', 'VBN', 'NN', 'VBP', 'RB', 'JJR', 'IN', 'DT', 'RB', 'VBN', 'NNS', '.']",16
natural_language_inference,18,2,Neural Variational Inference for Text Processing Phil Blunsom 12,"['Neural', 'Variational', 'Inference', 'for', 'Text', 'Processing', 'Phil', 'Blunsom', '12']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', 'CD']",9
natural_language_inference,18,22,"This paper introduces a neural variational framework for generative models of text , inspired by the variational autoencoder .","['This', 'paper', 'introduces', 'a', 'neural', 'variational', 'framework', 'for', 'generative', 'models', 'of', 'text', ',', 'inspired', 'by', 'the', 'variational', 'autoencoder', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NN', ',', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",19
natural_language_inference,18,23,"The principle idea is to build an inference network , implemented by a deep neural network conditioned on text , to approximate the intractable distributions over the latent variables .","['The', 'principle', 'idea', 'is', 'to', 'build', 'an', 'inference', 'network', ',', 'implemented', 'by', 'a', 'deep', 'neural', 'network', 'conditioned', 'on', 'text', ',', 'to', 'approximate', 'the', 'intractable', 'distributions', 'over', 'the', 'latent', 'variables', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'NN', ',', 'VBN', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'VBN', 'IN', 'NN', ',', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NNS', '.']",30
natural_language_inference,18,24,"Instead of providing an analytic approximation , as in traditional variational Bayes , neural variational inference learns to model the posterior probability , thus endowing the model with strong generalis ation abilities .","['Instead', 'of', 'providing', 'an', 'analytic', 'approximation', ',', 'as', 'in', 'traditional', 'variational', 'Bayes', ',', 'neural', 'variational', 'inference', 'learns', 'to', 'model', 'the', 'posterior', 'probability', ',', 'thus', 'endowing', 'the', 'model', 'with', 'strong', 'generalis', 'ation', 'abilities', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'IN', 'VBG', 'DT', 'JJ', 'NN', ',', 'IN', 'IN', 'JJ', 'JJ', 'NNP', ',', 'JJ', 'JJ', 'NN', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NN', ',', 'RB', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'NN', 'NNS', '.']",33
natural_language_inference,18,27,"By using the reparameteris ation method , the inference network is trained through back - propagating unbiased and low variance gradients w.r.t. the latent variables .","['By', 'using', 'the', 'reparameteris', 'ation', 'method', ',', 'the', 'inference', 'network', 'is', 'trained', 'through', 'back', '-', 'propagating', 'unbiased', 'and', 'low', 'variance', 'gradients', 'w.r.t.', 'the', 'latent', 'variables', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'VBG', 'DT', 'NN', 'NN', 'NN', ',', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'RB', ':', 'NN', 'JJ', 'CC', 'JJ', 'NN', 'NNS', 'VBP', 'DT', 'NN', 'NNS', '.']",26
natural_language_inference,18,28,"Within this framework , we propose a Neural Variational Document Model ( NVDM ) for document modelling and a Neural Answer Selection Model ( NASM ) for question answering , a task that selects the sentences that correctly answer a factoid question from a set of candidate sentences .","['Within', 'this', 'framework', ',', 'we', 'propose', 'a', 'Neural', 'Variational', 'Document', 'Model', '(', 'NVDM', ')', 'for', 'document', 'modelling', 'and', 'a', 'Neural', 'Answer', 'Selection', 'Model', '(', 'NASM', ')', 'for', 'question', 'answering', ',', 'a', 'task', 'that', 'selects', 'the', 'sentences', 'that', 'correctly', 'answer', 'a', 'factoid', 'question', 'from', 'a', 'set', 'of', 'candidate', 'sentences', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'IN', 'NN', 'NN', 'CC', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'IN', 'NN', 'NN', ',', 'DT', 'NN', 'WDT', 'VBZ', 'DT', 'NNS', 'WDT', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', '.']",49
natural_language_inference,18,31,A primary feature of NVDM is that each word is generated directly from a dense continuous document representation instead of the more common binary semantic vector .,"['A', 'primary', 'feature', 'of', 'NVDM', 'is', 'that', 'each', 'word', 'is', 'generated', 'directly', 'from', 'a', 'dense', 'continuous', 'document', 'representation', 'instead', 'of', 'the', 'more', 'common', 'binary', 'semantic', 'vector', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'NNP', 'VBZ', 'IN', 'DT', 'NN', 'VBZ', 'VBN', 'RB', 'IN', 'DT', 'NN', 'JJ', 'NN', 'NN', 'RB', 'IN', 'DT', 'RBR', 'JJ', 'JJ', 'JJ', 'NN', '.']",27
natural_language_inference,18,33,The NASM ( is a supervised conditional model which imbues LSTMs with a latent stochastic attention mechanism to model the semantics of question - answer pairs and predict their relatedness .,"['The', 'NASM', '(', 'is', 'a', 'supervised', 'conditional', 'model', 'which', 'imbues', 'LSTMs', 'with', 'a', 'latent', 'stochastic', 'attention', 'mechanism', 'to', 'model', 'the', 'semantics', 'of', 'question', '-', 'answer', 'pairs', 'and', 'predict', 'their', 'relatedness', '.']","['O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', '(', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'NNP', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'NNS', 'IN', 'NN', ':', 'NN', 'NNS', 'CC', 'VBP', 'PRP$', 'NN', '.']",31
natural_language_inference,18,34,The attention model is designed to focus on the phrases of an answer that are strongly connected to the question semantics and is modelled by a latent distribution .,"['The', 'attention', 'model', 'is', 'designed', 'to', 'focus', 'on', 'the', 'phrases', 'of', 'an', 'answer', 'that', 'are', 'strongly', 'connected', 'to', 'the', 'question', 'semantics', 'and', 'is', 'modelled', 'by', 'a', 'latent', 'distribution', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'WDT', 'VBP', 'RB', 'VBN', 'TO', 'DT', 'NN', 'NNS', 'CC', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",29
natural_language_inference,18,36,"Bayesian inference provides a natural safeguard against overfitting , especially as the training sets available for this task are small .","['Bayesian', 'inference', 'provides', 'a', 'natural', 'safeguard', 'against', 'overfitting', ',', 'especially', 'as', 'the', 'training', 'sets', 'available', 'for', 'this', 'task', 'are', 'small', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'VBG', ',', 'RB', 'IN', 'DT', 'NN', 'VBZ', 'JJ', 'IN', 'DT', 'NN', 'VBP', 'JJ', '.']",21
natural_language_inference,18,141,Experiments on Document Modelling,"['Experiments', 'on', 'Document', 'Modelling']","['O', 'B-p', 'B-n', 'I-n']","['NNS', 'IN', 'NNP', 'VBG']",4
natural_language_inference,18,145,The experimental results indicate that NVDM achieves the best performance on both datasets .,"['The', 'experimental', 'results', 'indicate', 'that', 'NVDM', 'achieves', 'the', 'best', 'performance', 'on', 'both', 'datasets', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NNS', 'VBP', 'IN', 'NNP', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NNS', '.']",14
natural_language_inference,18,146,"For the experiments on RCV1 - v2 dataset , the NVDM with latent variable of 50 dimension performs even better than the fDARN with 200 dimension .","['For', 'the', 'experiments', 'on', 'RCV1', '-', 'v2', 'dataset', ',', 'the', 'NVDM', 'with', 'latent', 'variable', 'of', '50', 'dimension', 'performs', 'even', 'better', 'than', 'the', 'fDARN', 'with', '200', 'dimension', '.']","['B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNS', 'IN', 'NNP', ':', 'NN', 'NN', ',', 'DT', 'NNP', 'IN', 'JJ', 'NN', 'IN', 'CD', 'NN', 'NNS', 'RB', 'RBR', 'IN', 'DT', 'NN', 'IN', 'CD', 'NN', '.']",27
natural_language_inference,18,155,Dataset & Setup for Answer Sentence Selection,"['Dataset', '&', 'Setup', 'for', 'Answer', 'Sentence', 'Selection']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n']","['NNP', 'CC', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",7
natural_language_inference,18,173,The word embeddings ( K = 50 ) are obtained by running the word2vec tool on the English Wikipedia dump and the AQUAINT 5 corpus .,"['The', 'word', 'embeddings', '(', 'K', '=', '50', ')', 'are', 'obtained', 'by', 'running', 'the', 'word2vec', 'tool', 'on', 'the', 'English', 'Wikipedia', 'dump', 'and', 'the', 'AQUAINT', '5', 'corpus', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NNS', '(', 'NNP', 'NNP', 'CD', ')', 'VBP', 'VBN', 'IN', 'VBG', 'DT', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'NN', 'CC', 'DT', 'NNP', 'CD', 'NN', '.']",26
natural_language_inference,18,174,"We use LSTMs with 3 layers and 50 hidden units , and apply 40 % dropout after the embedding layer .","['We', 'use', 'LSTMs', 'with', '3', 'layers', 'and', '50', 'hidden', 'units', ',', 'and', 'apply', '40', '%', 'dropout', 'after', 'the', 'embedding', 'layer', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'IN', 'CD', 'NNS', 'CC', 'CD', 'JJ', 'NNS', ',', 'CC', 'VB', 'CD', 'NN', 'NN', 'IN', 'DT', 'VBG', 'NN', '.']",21
natural_language_inference,18,175,"For the construction of the inference network , we use an MLP ( Eq. 10 ) with 2 layers and tanh units of 50 dimension , and an MLP ( Eq. 17 ) with 2 layers and tanh units of 150 dimension for modelling the joint representation .","['For', 'the', 'construction', 'of', 'the', 'inference', 'network', ',', 'we', 'use', 'an', 'MLP', '(', 'Eq.', '10', ')', 'with', '2', 'layers', 'and', 'tanh', 'units', 'of', '50', 'dimension', ',', 'and', 'an', 'MLP', '(', 'Eq.', '17', ')', 'with', '2', 'layers', 'and', 'tanh', 'units', 'of', '150', 'dimension', 'for', 'modelling', 'the', 'joint', 'representation', '.']","['B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', '(', 'NNP', 'CD', ')', 'IN', 'CD', 'NNS', 'CC', 'JJ', 'NNS', 'IN', 'CD', 'NN', ',', 'CC', 'DT', 'NNP', '(', 'NNP', 'CD', ')', 'IN', 'CD', 'NNS', 'CC', 'JJ', 'NNS', 'IN', 'CD', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', '.']",48
natural_language_inference,18,176,"During training we carry out stochastic estimation by taking one sample for computing the gradients , while in prediction we use 20 samples to calculate the expectation of the lower bound .","['During', 'training', 'we', 'carry', 'out', 'stochastic', 'estimation', 'by', 'taking', 'one', 'sample', 'for', 'computing', 'the', 'gradients', ',', 'while', 'in', 'prediction', 'we', 'use', '20', 'samples', 'to', 'calculate', 'the', 'expectation', 'of', 'the', 'lower', 'bound', '.']","['B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'VBG', 'PRP', 'VBP', 'RP', 'JJ', 'NN', 'IN', 'VBG', 'CD', 'NN', 'IN', 'VBG', 'DT', 'NNS', ',', 'IN', 'IN', 'NN', 'PRP', 'VBP', 'CD', 'NNS', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJR', 'NN', '.']",32
natural_language_inference,18,187,"The LSTM + Att performs slightly better than the vanilla LSTM model , and our NASM improves the results further .","['The', 'LSTM', '+', 'Att', 'performs', 'slightly', 'better', 'than', 'the', 'vanilla', 'LSTM', 'model', ',', 'and', 'our', 'NASM', 'improves', 'the', 'results', 'further', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O']","['DT', 'NNP', 'NNP', 'NNP', 'VBZ', 'RB', 'JJR', 'IN', 'DT', 'NN', 'NNP', 'NN', ',', 'CC', 'PRP$', 'NNP', 'VBZ', 'DT', 'NNS', 'RBR', '.']",21
natural_language_inference,18,188,"Since the QASent dataset is biased towards lexical overlapping features , after combining with a co-occurrence word count feature , our best model NASM outperforms all the previous models , including both neural network based models and classifiers with a set of hand - crafted features ( e.g. LCLR ) .","['Since', 'the', 'QASent', 'dataset', 'is', 'biased', 'towards', 'lexical', 'overlapping', 'features', ',', 'after', 'combining', 'with', 'a', 'co-occurrence', 'word', 'count', 'feature', ',', 'our', 'best', 'model', 'NASM', 'outperforms', 'all', 'the', 'previous', 'models', ',', 'including', 'both', 'neural', 'network', 'based', 'models', 'and', 'classifiers', 'with', 'a', 'set', 'of', 'hand', '-', 'crafted', 'features', '(', 'e.g.', 'LCLR', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNP', 'NN', 'VBZ', 'VBN', 'NNS', 'JJ', 'VBG', 'NNS', ',', 'IN', 'VBG', 'IN', 'DT', 'NN', 'NN', 'NN', 'NN', ',', 'PRP$', 'JJS', 'NN', 'NNP', 'VBZ', 'PDT', 'DT', 'JJ', 'NNS', ',', 'VBG', 'DT', 'JJ', 'NN', 'VBN', 'NNS', 'CC', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NN', ':', 'VBN', 'NNS', '(', 'JJ', 'NNP', ')', '.']",51
natural_language_inference,18,189,"Similarly , on the Wik - iQA dataset , all of our models outperform the previous distributional models by a large margin .","['Similarly', ',', 'on', 'the', 'Wik', '-', 'iQA', 'dataset', ',', 'all', 'of', 'our', 'models', 'outperform', 'the', 'previous', 'distributional', 'models', 'by', 'a', 'large', 'margin', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'IN', 'DT', 'NNP', ':', 'NN', 'NN', ',', 'DT', 'IN', 'PRP$', 'NNS', 'IN', 'DT', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",23
natural_language_inference,18,190,"By including a word count feature , our models improve further and achieve the state - of - the - art .","['By', 'including', 'a', 'word', 'count', 'feature', ',', 'our', 'models', 'improve', 'further', 'and', 'achieve', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBG', 'DT', 'NN', 'NN', 'NN', ',', 'PRP$', 'NNS', 'VB', 'RBR', 'CC', 'VB', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', '.']",22
natural_language_inference,42,2,A Fully Attention - Based Information Retriever,"['A', 'Fully', 'Attention', '-', 'Based', 'Information', 'Retriever']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['DT', 'NNP', 'NNP', ':', 'VBN', 'NN', 'NNP']",7
natural_language_inference,42,9,Question - answering ( QA ) systems that can answer queries expressed in natural language have been a perennial goal of the artificial intelligence community .,"['Question', '-', 'answering', '(', 'QA', ')', 'systems', 'that', 'can', 'answer', 'queries', 'expressed', 'in', 'natural', 'language', 'have', 'been', 'a', 'perennial', 'goal', 'of', 'the', 'artificial', 'intelligence', 'community', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', '(', 'NNP', ')', 'NNS', 'WDT', 'MD', 'VB', 'NNS', 'VBN', 'IN', 'JJ', 'NN', 'VBP', 'VBN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",26
natural_language_inference,42,12,"That is , in fact , the proposed focus of recent open - domain QA datasets , such as SQuAD .","['That', 'is', ',', 'in', 'fact', ',', 'the', 'proposed', 'focus', 'of', 'recent', 'open', '-', 'domain', 'QA', 'datasets', ',', 'such', 'as', 'SQuAD', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'VBZ', ',', 'IN', 'NN', ',', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'JJ', ':', 'NN', 'NNP', 'NNS', ',', 'JJ', 'IN', 'NNP', '.']",21
natural_language_inference,42,13,"In SQuAD , each problem instance consists of a passage P and a question Q. A QA system must then provide an answer A by selecting a snippet from P .","['In', 'SQuAD', ',', 'each', 'problem', 'instance', 'consists', 'of', 'a', 'passage', 'P', 'and', 'a', 'question', 'Q.', 'A', 'QA', 'system', 'must', 'then', 'provide', 'an', 'answer', 'A', 'by', 'selecting', 'a', 'snippet', 'from', 'P', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ',', 'DT', 'NN', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'NNP', 'CC', 'DT', 'NN', 'NNP', 'NNP', 'NNP', 'NN', 'MD', 'RB', 'VB', 'DT', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'NNP', '.']",31
natural_language_inference,42,20,"Inspired by the positive results of Vaswani et al. in machine translation , we have applied a similar architecture to the domain of question - answering , a model that we have named Fully Attention - Based Information Retriever ( FABIR ) .","['Inspired', 'by', 'the', 'positive', 'results', 'of', 'Vaswani', 'et', 'al.', 'in', 'machine', 'translation', ',', 'we', 'have', 'applied', 'a', 'similar', 'architecture', 'to', 'the', 'domain', 'of', 'question', '-', 'answering', ',', 'a', 'model', 'that', 'we', 'have', 'named', 'Fully', 'Attention', '-', 'Based', 'Information', 'Retriever', '(', 'FABIR', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NNP', 'FW', 'NN', 'IN', 'NN', 'NN', ',', 'PRP', 'VBP', 'VBN', 'DT', 'JJ', 'NN', 'TO', 'DT', 'NN', 'IN', 'NN', ':', 'NN', ',', 'DT', 'NN', 'IN', 'PRP', 'VBP', 'VBN', 'NNP', 'NNP', ':', 'VBN', 'NN', 'NNP', '(', 'NNP', ')', '.']",43
natural_language_inference,42,21,"Our goal then was to verify how much performance we can get exclusively from the attention mechanism , without combining it with several other techniques .","['Our', 'goal', 'then', 'was', 'to', 'verify', 'how', 'much', 'performance', 'we', 'can', 'get', 'exclusively', 'from', 'the', 'attention', 'mechanism', ',', 'without', 'combining', 'it', 'with', 'several', 'other', 'techniques', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'RB', 'VBD', 'TO', 'VB', 'WRB', 'JJ', 'NN', 'PRP', 'MD', 'VB', 'RB', 'IN', 'DT', 'NN', 'NN', ',', 'IN', 'VBG', 'PRP', 'IN', 'JJ', 'JJ', 'NNS', '.']",26
natural_language_inference,42,24,"Convolutional attention : a novel attention mechanism that encodes many - to - many relationships between words , enabling richer contextual representations .","['Convolutional', 'attention', ':', 'a', 'novel', 'attention', 'mechanism', 'that', 'encodes', 'many', '-', 'to', '-', 'many', 'relationships', 'between', 'words', ',', 'enabling', 'richer', 'contextual', 'representations', '.']","['B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['JJ', 'NN', ':', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'JJ', ':', 'TO', ':', 'JJ', 'NNS', 'IN', 'NNS', ',', 'VBG', 'JJR', 'JJ', 'NNS', '.']",23
natural_language_inference,42,25,Reduction layer : a new layer design that fits the pipeline proposed by Vaswani et al .,"['Reduction', 'layer', ':', 'a', 'new', 'layer', 'design', 'that', 'fits', 'the', 'pipeline', 'proposed', 'by', 'Vaswani', 'et', 'al', '.']","['B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NN', ':', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'VBN', 'IN', 'NNP', 'CC', 'NN', '.']",17
natural_language_inference,42,27,Column - wise cross - attention : we modify the crossattention operation by and propose a new technique that is better suited to question - answering .,"['Column', '-', 'wise', 'cross', '-', 'attention', ':', 'we', 'modify', 'the', 'crossattention', 'operation', 'by', 'and', 'propose', 'a', 'new', 'technique', 'that', 'is', 'better', 'suited', 'to', 'question', '-', 'answering', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', 'NN', ':', 'NN', ':', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CC', 'VB', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'RB', 'VBN', 'TO', 'VB', ':', 'NN', '.']",27
natural_language_inference,42,217,We have trained our FABIR model during 54 epochs with a batch size of 75 in a GPU NVidia Titan X with 12 GB of RAM .,"['We', 'have', 'trained', 'our', 'FABIR', 'model', 'during', '54', 'epochs', 'with', 'a', 'batch', 'size', 'of', '75', 'in', 'a', 'GPU', 'NVidia', 'Titan', 'X', 'with', '12', 'GB', 'of', 'RAM', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'VBN', 'PRP$', 'NNP', 'NN', 'IN', 'CD', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'CD', 'NNP', 'IN', 'NNP', '.']",27
natural_language_inference,42,218,We developed our model in Tensorflow and made it available at https://worksheets.codalab.org/worksheets/ 0xee647ea284674396831ecb5aae9ca297 / for replicability .,"['We', 'developed', 'our', 'model', 'in', 'Tensorflow', 'and', 'made', 'it', 'available', 'at', 'https://worksheets.codalab.org/worksheets/', '0xee647ea284674396831ecb5aae9ca297', '/', 'for', 'replicability', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'PRP$', 'NN', 'IN', 'NNP', 'CC', 'VBD', 'PRP', 'JJ', 'IN', 'JJ', 'CD', 'NN', 'IN', 'NN', '.']",17
natural_language_inference,42,219,We pre-processed the texts with the NLTK Tokenizer .,"['We', 'pre-processed', 'the', 'texts', 'with', 'the', 'NLTK', 'Tokenizer', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBD', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NNP', '.']",9
natural_language_inference,42,221,"For regularization , we applied residual and attention dropout of 0.9 in processing layers and of 0.8 in the reduction layer .","['For', 'regularization', ',', 'we', 'applied', 'residual', 'and', 'attention', 'dropout', 'of', '0.9', 'in', 'processing', 'layers', 'and', 'of', '0.8', 'in', 'the', 'reduction', 'layer', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBD', 'JJ', 'CC', 'NN', 'NN', 'IN', 'CD', 'IN', 'VBG', 'NNS', 'CC', 'IN', 'CD', 'IN', 'DT', 'NN', 'NN', '.']",22
natural_language_inference,42,222,"In the character - level embedding process , a dropout of 0.75 was added before the convolution .","['In', 'the', 'character', '-', 'level', 'embedding', 'process', ',', 'a', 'dropout', 'of', '0.75', 'was', 'added', 'before', 'the', 'convolution', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['IN', 'DT', 'NN', ':', 'NN', 'VBG', 'NN', ',', 'DT', 'NN', 'IN', 'CD', 'VBD', 'VBN', 'IN', 'DT', 'NN', '.']",18
natural_language_inference,42,223,"Additionally , a dropout of 0.8 was added before each convolutional layer in the answer selector .","['Additionally', ',', 'a', 'dropout', 'of', '0.8', 'was', 'added', 'before', 'each', 'convolutional', 'layer', 'in', 'the', 'answer', 'selector', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'IN', 'CD', 'VBD', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",17
natural_language_inference,42,224,"We set processing layers dimension d model to 100 , the number of heads n heads in each attention sublayer to 4 , the feed - forward hidden size to 200 in processing layers and 400 in the reduction layer .","['We', 'set', 'processing', 'layers', 'dimension', 'd', 'model', 'to', '100', ',', 'the', 'number', 'of', 'heads', 'n', 'heads', 'in', 'each', 'attention', 'sublayer', 'to', '4', ',', 'the', 'feed', '-', 'forward', 'hidden', 'size', 'to', '200', 'in', 'processing', 'layers', 'and', '400', 'in', 'the', 'reduction', 'layer', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'VBG', 'NNS', 'NN', 'VBZ', 'NN', 'TO', 'CD', ',', 'DT', 'NN', 'IN', 'NNS', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', 'TO', 'CD', ',', 'DT', 'NN', ':', 'RB', 'VBZ', 'NN', 'TO', 'CD', 'IN', 'VBG', 'NNS', 'CC', 'CD', 'IN', 'DT', 'NN', 'NN', '.']",41
natural_language_inference,42,229,"This analysis confirms the effectiveness of char- embeddings , as its addition increased the F1 and EM scores , by 2.7 % and 3.1 % , respectively .","['This', 'analysis', 'confirms', 'the', 'effectiveness', 'of', 'char-', 'embeddings', ',', 'as', 'its', 'addition', 'increased', 'the', 'F1', 'and', 'EM', 'scores', ',', 'by', '2.7', '%', 'and', '3.1', '%', ',', 'respectively', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NNS', ',', 'IN', 'PRP$', 'NN', 'VBD', 'DT', 'NNP', 'CC', 'NNP', 'NNS', ',', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', ',', 'RB', '.']",28
natural_language_inference,42,230,"Most importantly , when the convolutional attention was replaced by the standard attention mechanism proposed in , the performance dropped by 2.4 % in F1 and 2.5 % in EM .","['Most', 'importantly', ',', 'when', 'the', 'convolutional', 'attention', 'was', 'replaced', 'by', 'the', 'standard', 'attention', 'mechanism', 'proposed', 'in', ',', 'the', 'performance', 'dropped', 'by', '2.4', '%', 'in', 'F1', 'and', '2.5', '%', 'in', 'EM', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['JJS', 'RB', ',', 'WRB', 'DT', 'JJ', 'NN', 'VBD', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'VBN', 'IN', ',', 'DT', 'NN', 'VBD', 'IN', 'CD', 'NN', 'IN', 'NNP', 'CC', 'CD', 'NN', 'IN', 'NNP', '.']",31
natural_language_inference,42,232,"Moreover , the tests also indicate that the reduction layer is capable of producing useful word representations when compressing the embeddings .","['Moreover', ',', 'the', 'tests', 'also', 'indicate', 'that', 'the', 'reduction', 'layer', 'is', 'capable', 'of', 'producing', 'useful', 'word', 'representations', 'when', 'compressing', 'the', 'embeddings', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'O']","['RB', ',', 'DT', 'NNS', 'RB', 'VBP', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'JJ', 'IN', 'VBG', 'JJ', 'NN', 'NNS', 'WRB', 'VBG', 'DT', 'NNS', '.']",22
natural_language_inference,42,233,"Indeed , when we replaced that layer by a standard feedforward layer with the same reduction ratio , there was a drop of 2.1 % and 2.5 % in the F1 and EM scores , respectively .","['Indeed', ',', 'when', 'we', 'replaced', 'that', 'layer', 'by', 'a', 'standard', 'feedforward', 'layer', 'with', 'the', 'same', 'reduction', 'ratio', ',', 'there', 'was', 'a', 'drop', 'of', '2.1', '%', 'and', '2.5', '%', 'in', 'the', 'F1', 'and', 'EM', 'scores', ',', 'respectively', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['RB', ',', 'WRB', 'PRP', 'VBD', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'EX', 'VBD', 'DT', 'NN', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NNS', ',', 'RB', '.']",37
natural_language_inference,42,245,"Regarding EM and F 1 scores , FABIR and BiDAF showed similar performances .","['Regarding', 'EM', 'and', 'F', '1', 'scores', ',', 'FABIR', 'and', 'BiDAF', 'showed', 'similar', 'performances', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['VBG', 'NNP', 'CC', 'NNP', 'CD', 'NNS', ',', 'NNP', 'CC', 'NNP', 'VBD', 'JJ', 'NNS', '.']",14
natural_language_inference,42,255,In this section we analyze the performance of FABIR and BiDAF in the different types of question in SQuAD .,"['In', 'this', 'section', 'we', 'analyze', 'the', 'performance', 'of', 'FABIR', 'and', 'BiDAF', 'in', 'the', 'different', 'types', 'of', 'question', 'in', 'SQuAD', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NN', 'IN', 'NNP', '.']",20
natural_language_inference,42,256,"shows that shorter answers are easier for both models : while they reach more than 75 % F1 for answers that are shorter than four words , for answers longer than ten words these scores drop to 60.4 % and 67.3 % for FABIR and BiDAF , respectively .","['shows', 'that', 'shorter', 'answers', 'are', 'easier', 'for', 'both', 'models', ':', 'while', 'they', 'reach', 'more', 'than', '75', '%', 'F1', 'for', 'answers', 'that', 'are', 'shorter', 'than', 'four', 'words', ',', 'for', 'answers', 'longer', 'than', 'ten', 'words', 'these', 'scores', 'drop', 'to', '60.4', '%', 'and', '67.3', '%', 'for', 'FABIR', 'and', 'BiDAF', ',', 'respectively', '.']","['B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNS', 'WDT', 'JJR', 'NNS', 'VBP', 'JJR', 'IN', 'DT', 'NNS', ':', 'IN', 'PRP', 'VBP', 'JJR', 'IN', 'CD', 'NN', 'NNP', 'IN', 'NNS', 'WDT', 'VBP', 'JJR', 'IN', 'CD', 'NNS', ',', 'IN', 'NNS', 'JJR', 'IN', 'VB', 'NNS', 'DT', 'NNS', 'NN', 'TO', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'NNP', 'CC', 'NNP', ',', 'RB', '.']",49
natural_language_inference,42,262,"shows that both models had their best performance with "" when "" questions .","['shows', 'that', 'both', 'models', 'had', 'their', 'best', 'performance', 'with', '""', 'when', '""', 'questions', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'IN', 'DT', 'NNS', 'VBD', 'PRP$', 'JJS', 'NN', 'IN', 'NN', 'WRB', 'JJ', 'NNS', '.']",14
natural_language_inference,42,264,"Together with "" when "" questions , "" how long "" and "" how many "" also proved easier to respond , as they possess the same property of having a smaller universe of possible answers .","['Together', 'with', '""', 'when', '""', 'questions', ',', '""', 'how', 'long', '""', 'and', '""', 'how', 'many', '""', 'also', 'proved', 'easier', 'to', 'respond', ',', 'as', 'they', 'possess', 'the', 'same', 'property', 'of', 'having', 'a', 'smaller', 'universe', 'of', 'possible', 'answers', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', 'IN', 'NN', 'WRB', 'NN', 'NNS', ',', 'VB', 'WRB', 'JJ', 'NN', 'CC', 'VB', 'WRB', 'JJ', 'NNP', 'RB', 'VBD', 'JJR', 'TO', 'VB', ',', 'IN', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'JJR', 'NN', 'IN', 'JJ', 'NNS', '.']",37
natural_language_inference,42,265,"In contrast to these , "" how "" and "" why "" questions resulted in considerably lower F1 and EM scores , as they can be answered by any sentence , and hence require a deeper understanding of the text .","['In', 'contrast', 'to', 'these', ',', '""', 'how', '""', 'and', '""', 'why', '""', 'questions', 'resulted', 'in', 'considerably', 'lower', 'F1', 'and', 'EM', 'scores', ',', 'as', 'they', 'can', 'be', 'answered', 'by', 'any', 'sentence', ',', 'and', 'hence', 'require', 'a', 'deeper', 'understanding', 'of', 'the', 'text', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', 'TO', 'DT', ',', 'VB', 'WRB', 'JJ', 'CC', 'JJ', 'WRB', 'JJ', 'NNS', 'VBD', 'IN', 'RB', 'JJR', 'NNP', 'CC', 'NNP', 'NNS', ',', 'IN', 'PRP', 'MD', 'VB', 'VBN', 'IN', 'DT', 'NN', ',', 'CC', 'RB', 'VB', 'DT', 'JJR', 'NN', 'IN', 'DT', 'NN', '.']",41
natural_language_inference,42,270,"Questions which expect a "" yes "" or a "" no "" as an answer are also difficult because it is not always possible to find those words in a snippet from the passage .","['Questions', 'which', 'expect', 'a', '""', 'yes', '""', 'or', 'a', '""', 'no', '""', 'as', 'an', 'answer', 'are', 'also', 'difficult', 'because', 'it', 'is', 'not', 'always', 'possible', 'to', 'find', 'those', 'words', 'in', 'a', 'snippet', 'from', 'the', 'passage', '.']","['B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNS', 'WDT', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'CC', 'DT', 'JJ', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBP', 'RB', 'JJ', 'IN', 'PRP', 'VBZ', 'RB', 'RB', 'JJ', 'TO', 'VB', 'DT', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",35
natural_language_inference,42,272,It is curious that shorter passages showed the worst performance for both models .,"['It', 'is', 'curious', 'that', 'shorter', 'passages', 'showed', 'the', 'worst', 'performance', 'for', 'both', 'models', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBZ', 'JJ', 'IN', 'JJR', 'NNS', 'VBD', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NNS', '.']",14
natural_language_inference,4,2,DCN + : MIXED OBJECTIVE AND DEEP RESIDUAL COATTENTION FOR QUESTION ANSWERING,"['DCN', '+', ':', 'MIXED', 'OBJECTIVE', 'AND', 'DEEP', 'RESIDUAL', 'COATTENTION', 'FOR', 'QUESTION', 'ANSWERING']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', 'NN', ':', 'JJ', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",12
natural_language_inference,4,18,"To address this problem , we propose a mixed objective that combines traditional cross entropy loss over positions with a measure of word overlap trained with reinforcement learning .","['To', 'address', 'this', 'problem', ',', 'we', 'propose', 'a', 'mixed', 'objective', 'that', 'combines', 'traditional', 'cross', 'entropy', 'loss', 'over', 'positions', 'with', 'a', 'measure', 'of', 'word', 'overlap', 'trained', 'with', 'reinforcement', 'learning', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'VBZ', 'JJ', 'NN', 'NN', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', 'VBN', 'IN', 'NN', 'NN', '.']",29
natural_language_inference,4,19,We obtain the latter objective using self - critical policy learning in which the reward is based on word overlap between the proposed answer and the ground truth answer .,"['We', 'obtain', 'the', 'latter', 'objective', 'using', 'self', '-', 'critical', 'policy', 'learning', 'in', 'which', 'the', 'reward', 'is', 'based', 'on', 'word', 'overlap', 'between', 'the', 'proposed', 'answer', 'and', 'the', 'ground', 'truth', 'answer', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VB', 'DT', 'JJ', 'JJ', 'VBG', 'PRP', ':', 'JJ', 'NN', 'NN', 'IN', 'WDT', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'NN', 'NN', 'IN', 'DT', 'VBN', 'NN', 'CC', 'DT', 'NN', 'NN', 'NN', '.']",30
natural_language_inference,4,22,"In addition to our mixed training objective , we extend the Dynamic Coattention Network ( DCN ) by with a deep residual coattention encoder .","['In', 'addition', 'to', 'our', 'mixed', 'training', 'objective', ',', 'we', 'extend', 'the', 'Dynamic', 'Coattention', 'Network', '(', 'DCN', ')', 'by', 'with', 'a', 'deep', 'residual', 'coattention', 'encoder', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'PRP$', 'JJ', 'NN', 'JJ', ',', 'PRP', 'VBP', 'DT', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'IN', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'NN', '.']",25
natural_language_inference,4,126,"To preprocess the corpus , we use the reversible tokenizer from Stanford CoreNLP .","['To', 'preprocess', 'the', 'corpus', ',', 'we', 'use', 'the', 'reversible', 'tokenizer', 'from', 'Stanford', 'CoreNLP', '.']","['O', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNP', '.']",14
natural_language_inference,4,127,"For word embeddings , we use GloVe embeddings pretrained on the 840B Common Crawl corpus as well as character ngram embeddings by .","['For', 'word', 'embeddings', ',', 'we', 'use', 'GloVe', 'embeddings', 'pretrained', 'on', 'the', '840B', 'Common', 'Crawl', 'corpus', 'as', 'well', 'as', 'character', 'ngram', 'embeddings', 'by', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', 'NNS', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'VBN', 'IN', 'DT', 'CD', 'NNP', 'NNP', 'NN', 'RB', 'RB', 'IN', 'NN', 'IN', 'NNS', 'IN', '.']",23
natural_language_inference,4,128,"In addition , we concatenate these embeddings with context vectors ( CoVe ) trained on .","['In', 'addition', ',', 'we', 'concatenate', 'these', 'embeddings', 'with', 'context', 'vectors', '(', 'CoVe', ')', 'trained', 'on', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNS', 'IN', 'JJ', 'NNS', '(', 'NNP', ')', 'VBD', 'IN', '.']",16
natural_language_inference,4,129,"For out of vocabulary words , we set the embeddings and context vectors to zero .","['For', 'out', 'of', 'vocabulary', 'words', ',', 'we', 'set', 'the', 'embeddings', 'and', 'context', 'vectors', 'to', 'zero', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'IN', 'IN', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NNS', 'CC', 'JJ', 'NNS', 'TO', 'CD', '.']",16
natural_language_inference,4,130,We perform word dropout on the document which zeros a word embedding with probability 0.075 .,"['We', 'perform', 'word', 'dropout', 'on', 'the', 'document', 'which', 'zeros', 'a', 'word', 'embedding', 'with', 'probability', '0.075', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'NN', 'NN', 'IN', 'DT', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'VBG', 'IN', 'NN', 'CD', '.']",16
natural_language_inference,4,131,"In addition , we swap the first maxout layer of the highway maxout network in the DCN decoder with a sparse mixture of experts layer .","['In', 'addition', ',', 'we', 'swap', 'the', 'first', 'maxout', 'layer', 'of', 'the', 'highway', 'maxout', 'network', 'in', 'the', 'DCN', 'decoder', 'with', 'a', 'sparse', 'mixture', 'of', 'experts', 'layer', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'NN', '.']",26
natural_language_inference,4,135,Comparison to baseline DCN with CoVe. DCN + outperforms the baseline by 3.2 % exact match accuracy and 3.2 % F1 on the SQuAD development set .,"['Comparison', 'to', 'baseline', 'DCN', 'with', 'CoVe.', 'DCN', '+', 'outperforms', 'the', 'baseline', 'by', '3.2', '%', 'exact', 'match', 'accuracy', 'and', '3.2', '%', 'F1', 'on', 'the', 'SQuAD', 'development', 'set', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'TO', 'VB', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', 'JJ', 'NN', 'NN', 'CC', 'CD', 'NN', 'NNP', 'IN', 'DT', 'NNP', 'NN', 'NN', '.']",27
natural_language_inference,4,136,"shows the consistent performance gain of DCN + over the baseline across question types , question lengths , and answer lengths .","['shows', 'the', 'consistent', 'performance', 'gain', 'of', 'DCN', '+', 'over', 'the', 'baseline', 'across', 'question', 'types', ',', 'question', 'lengths', ',', 'and', 'answer', 'lengths', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']","['VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'NNP', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', ',', 'NN', 'NNS', ',', 'CC', 'JJR', 'NNS', '.']",22
natural_language_inference,4,137,"In particular , DCN + provides a significant advantage for long questions .","['In', 'particular', ',', 'DCN', '+', 'provides', 'a', 'significant', 'advantage', 'for', 'long', 'questions', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'JJ', ',', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', '.']",13
natural_language_inference,4,140,"We note that the deep residual coattention yielded the highest contribution to model performance , followed by the mixed objective .","['We', 'note', 'that', 'the', 'deep', 'residual', 'coattention', 'yielded', 'the', 'highest', 'contribution', 'to', 'model', 'performance', ',', 'followed', 'by', 'the', 'mixed', 'objective', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'VBD', 'DT', 'JJS', 'NN', 'TO', 'VB', 'NN', ',', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",21
natural_language_inference,4,141,The sparse mixture of experts layer in the decoder added minor improvements to the model performance . :,"['The', 'sparse', 'mixture', 'of', 'experts', 'layer', 'in', 'the', 'decoder', 'added', 'minor', 'improvements', 'to', 'the', 'model', 'performance', '.', ':']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O']","['DT', 'JJ', 'NN', 'IN', 'NNS', 'NN', 'IN', 'DT', 'NN', 'VBD', 'JJ', 'NNS', 'TO', 'DT', 'NN', 'NN', '.', ':']",18
natural_language_inference,6,2,Massively Multilingual Sentence Embeddings for Zero - Shot Cross - Lingual Transfer and Beyond,"['Massively', 'Multilingual', 'Sentence', 'Embeddings', 'for', 'Zero', '-', 'Shot', 'Cross', '-', 'Lingual', 'Transfer', 'and', 'Beyond']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'JJ', 'NNP', 'NNP', 'IN', 'NNP', ':', 'NNP', 'NNP', ':', 'JJ', 'NN', 'CC', 'NN']",14
natural_language_inference,6,4,"We introduce an architecture to learn joint multilingual sentence representations for 93 languages , belonging to more than 30 different families and written in 28 different scripts .","['We', 'introduce', 'an', 'architecture', 'to', 'learn', 'joint', 'multilingual', 'sentence', 'representations', 'for', '93', 'languages', ',', 'belonging', 'to', 'more', 'than', '30', 'different', 'families', 'and', 'written', 'in', '28', 'different', 'scripts', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'TO', 'VB', 'JJ', 'JJ', 'NN', 'NNS', 'IN', 'CD', 'NNS', ',', 'VBG', 'TO', 'JJR', 'IN', 'CD', 'JJ', 'NNS', 'CC', 'VBN', 'IN', 'CD', 'JJ', 'NNS', '.']",28
natural_language_inference,6,9,"Our implementation , the pretrained encoder and the multilingual test set are available at https://github.com / facebookresearch/LASER . . 2018 .","['Our', 'implementation', ',', 'the', 'pretrained', 'encoder', 'and', 'the', 'multilingual', 'test', 'set', 'are', 'available', 'at', 'https://github.com', '/', 'facebookresearch/LASER', '.', '.', '2018', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP$', 'NN', ',', 'DT', 'JJ', 'NN', 'CC', 'DT', 'JJ', 'NN', 'NN', 'VBP', 'JJ', 'IN', 'NN', 'NNP', 'NN', '.', '.', 'CD', '.']",21
natural_language_inference,6,21,"In this work , we are interested in universal language agnostic sentence embeddings , that is , vector representations of sentences that are general with respect to two dimensions : the input language and the NLP task .","['In', 'this', 'work', ',', 'we', 'are', 'interested', 'in', 'universal', 'language', 'agnostic', 'sentence', 'embeddings', ',', 'that', 'is', ',', 'vector', 'representations', 'of', 'sentences', 'that', 'are', 'general', 'with', 'respect', 'to', 'two', 'dimensions', ':', 'the', 'input', 'language', 'and', 'the', 'NLP', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'IN', 'JJ', 'NN', 'JJ', 'NN', 'NNS', ',', 'DT', 'VBZ', ',', 'JJ', 'NNS', 'IN', 'NNS', 'WDT', 'VBP', 'JJ', 'IN', 'NN', 'TO', 'CD', 'NNS', ':', 'DT', 'NN', 'NN', 'CC', 'DT', 'NNP', 'NN', '.']",38
natural_language_inference,6,23,"To that end , we train a single encoder to handle multiple languages , so that semantically similar sentences in different languages are close in the embedding space .","['To', 'that', 'end', ',', 'we', 'train', 'a', 'single', 'encoder', 'to', 'handle', 'multiple', 'languages', ',', 'so', 'that', 'semantically', 'similar', 'sentences', 'in', 'different', 'languages', 'are', 'close', 'in', 'the', 'embedding', 'space', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'NNS', ',', 'IN', 'DT', 'RB', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', 'VBP', 'RB', 'IN', 'DT', 'JJ', 'NN', '.']",29
natural_language_inference,6,95,XNLI : cross - lingual NLI,"['XNLI', ':', 'cross', '-', 'lingual', 'NLI']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NN', ':', 'NN', ':', 'JJ', 'NNP']",6
natural_language_inference,6,106,9 Our proposed method obtains the best results in zero - shot cross - lingual transfer for all languages but Spanish .,"['9', 'Our', 'proposed', 'method', 'obtains', 'the', 'best', 'results', 'in', 'zero', '-', 'shot', 'cross', '-', 'lingual', 'transfer', 'for', 'all', 'languages', 'but', 'Spanish', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', 'PRP$', 'VBN', 'NN', 'VBZ', 'DT', 'JJS', 'NNS', 'IN', 'CD', ':', 'NN', 'NN', ':', 'JJ', 'NN', 'IN', 'DT', 'NNS', 'CC', 'NNP', '.']",22
natural_language_inference,6,107,"Moreover , our transfer results are strong and homogeneous across all languages :","['Moreover', ',', 'our', 'transfer', 'results', 'are', 'strong', 'and', 'homogeneous', 'across', 'all', 'languages', ':']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'NN', 'NNS', 'VBP', 'JJ', 'CC', 'JJ', 'IN', 'DT', 'NNS', ':']",13
natural_language_inference,6,110,"for 11 of them , the zero - short performance is ( at most ) 5 % lower than the one on English , including distant languages like Arabic , Chinese and Vietnamese , and we also achieve remarkable good results on low - resource languages like Swahili .","['for', '11', 'of', 'them', ',', 'the', 'zero', '-', 'short', 'performance', 'is', '(', 'at', 'most', ')', '5', '%', 'lower', 'than', 'the', 'one', 'on', 'English', ',', 'including', 'distant', 'languages', 'like', 'Arabic', ',', 'Chinese', 'and', 'Vietnamese', ',', 'and', 'we', 'also', 'achieve', 'remarkable', 'good', 'results', 'on', 'low', '-', 'resource', 'languages', 'like', 'Swahili', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'CD', 'IN', 'PRP', ',', 'DT', 'CD', ':', 'JJ', 'NN', 'VBZ', '(', 'IN', 'JJS', ')', 'CD', 'NN', 'JJR', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'VBG', 'JJ', 'NNS', 'IN', 'NNP', ',', 'NNP', 'CC', 'NNP', ',', 'CC', 'PRP', 'RB', 'VBP', 'JJ', 'JJ', 'NNS', 'IN', 'JJ', ':', 'NN', 'NNS', 'IN', 'NNP', '.']",49
natural_language_inference,6,113,"10 Finally , we also outperform all baselines of Conneau et al. by a substantial margin , with the additional advantage that we use a single pre-trained encoder , whereas X - BiLSTM learns a separate encoder for each language .","['10', 'Finally', ',', 'we', 'also', 'outperform', 'all', 'baselines', 'of', 'Conneau', 'et', 'al.', 'by', 'a', 'substantial', 'margin', ',', 'with', 'the', 'additional', 'advantage', 'that', 'we', 'use', 'a', 'single', 'pre-trained', 'encoder', ',', 'whereas', 'X', '-', 'BiLSTM', 'learns', 'a', 'separate', 'encoder', 'for', 'each', 'language', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['CD', 'RB', ',', 'PRP', 'RB', 'VBP', 'DT', 'NNS', 'IN', 'NNP', 'CC', 'VBN', 'IN', 'DT', 'JJ', 'NN', ',', 'IN', 'DT', 'JJ', 'NN', 'IN', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', ',', 'WP', 'SYM', ':', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",41
natural_language_inference,6,125,MLDoc : cross - lingual classification,"['MLDoc', ':', 'cross', '-', 'lingual', 'classification']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NN', ':', 'NN', ':', 'JJ', 'NN']",6
natural_language_inference,6,131,"As shown in , our system obtains the best published results for 5 of the 7 transfer languages .","['As', 'shown', 'in', ',', 'our', 'system', 'obtains', 'the', 'best', 'published', 'results', 'for', '5', 'of', 'the', '7', 'transfer', 'languages', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'IN', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'JJS', 'VBN', 'NNS', 'IN', 'CD', 'IN', 'DT', 'CD', 'NN', 'NNS', '.']",19
natural_language_inference,6,133,BUCC : bitext mining,"['BUCC', ':', 'bitext', 'mining']","['B-n', 'I-n', 'I-n', 'I-n']","['NN', ':', 'NN', 'NN']",4
natural_language_inference,6,145,"As shown in , our system establishes a new state - of - the - art for all language pairs with the exception of English - Chinese test .","['As', 'shown', 'in', ',', 'our', 'system', 'establishes', 'a', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'for', 'all', 'language', 'pairs', 'with', 'the', 'exception', 'of', 'English', '-', 'Chinese', 'test', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'IN', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'NNP', ':', 'JJ', 'NN', '.']",29
natural_language_inference,6,146,"We also outperform Artetxe and Schwenk ( 2018 ) themselves , who use two separate models covering 4 languages each .","['We', 'also', 'outperform', 'Artetxe', 'and', 'Schwenk', '(', '2018', ')', 'themselves', ',', 'who', 'use', 'two', 'separate', 'models', 'covering', '4', 'languages', 'each', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'NNP', 'CC', 'NNP', '(', 'CD', ')', 'PRP', ',', 'WP', 'VBP', 'CD', 'JJ', 'NNS', 'VBG', 'CD', 'NNS', 'DT', '.']",21
natural_language_inference,6,148,Tatoeba : similarity search,"['Tatoeba', ':', 'similarity', 'search']","['B-n', 'I-n', 'I-n', 'I-n']","['NN', ':', 'NN', 'NN']",4
natural_language_inference,6,156,"Contrasting these results with those of XNLI , one would assume that similarity error rates below 5 % are indicative of strong downstream performance .","['Contrasting', 'these', 'results', 'with', 'those', 'of', 'XNLI', ',', 'one', 'would', 'assume', 'that', 'similarity', 'error', 'rates', 'below', '5', '%', 'are', 'indicative', 'of', 'strong', 'downstream', 'performance', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'DT', 'NNS', 'IN', 'DT', 'IN', 'NNP', ',', 'CD', 'MD', 'VB', 'IN', 'NN', 'NN', 'NNS', 'IN', 'CD', 'NN', 'VBP', 'JJ', 'IN', 'JJ', 'NN', 'NN', '.']",25
natural_language_inference,6,157,"11 This is the case for 37 languages , while there are 48 languages with an error rate below 10 % and 55 with less than 20 % .","['11', 'This', 'is', 'the', 'case', 'for', '37', 'languages', ',', 'while', 'there', 'are', '48', 'languages', 'with', 'an', 'error', 'rate', 'below', '10', '%', 'and', '55', 'with', 'less', 'than', '20', '%', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', 'DT', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NNS', ',', 'IN', 'EX', 'VBP', 'CD', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', 'NN', 'CC', 'CD', 'IN', 'JJR', 'IN', 'CD', 'NN', '.']",29
natural_language_inference,6,158,There are only 15 languages with error rates above 50 % .,"['There', 'are', 'only', '15', 'languages', 'with', 'error', 'rates', 'above', '50', '%', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['EX', 'VBP', 'RB', 'CD', 'NNS', 'IN', 'NN', 'NNS', 'IN', 'CD', 'NN', '.']",12
natural_language_inference,6,167,We were notable to achieve good convergence with deeper models .,"['We', 'were', 'notable', 'to', 'achieve', 'good', 'convergence', 'with', 'deeper', 'models', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBD', 'JJ', 'TO', 'VB', 'JJ', 'NN', 'IN', 'JJR', 'NNS', '.']",11
natural_language_inference,6,168,"It can be seen that all tasks benefit from deeper models , in particular XNLI and Tatoeba , suggesting that a single layer BiLSTM has not enough capacity to encode so many languages .","['It', 'can', 'be', 'seen', 'that', 'all', 'tasks', 'benefit', 'from', 'deeper', 'models', ',', 'in', 'particular', 'XNLI', 'and', 'Tatoeba', ',', 'suggesting', 'that', 'a', 'single', 'layer', 'BiLSTM', 'has', 'not', 'enough', 'capacity', 'to', 'encode', 'so', 'many', 'languages', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'MD', 'VB', 'VBN', 'IN', 'DT', 'NNS', 'VBP', 'IN', 'JJR', 'NNS', ',', 'IN', 'JJ', 'NN', 'CC', 'NNP', ',', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'VBZ', 'RB', 'JJ', 'NN', 'TO', 'VB', 'RB', 'JJ', 'NNS', '.']",34
natural_language_inference,6,171,Multitask learning has been shown to be helpful to learn English sentence embeddings .,"['Multitask', 'learning', 'has', 'been', 'shown', 'to', 'be', 'helpful', 'to', 'learn', 'English', 'sentence', 'embeddings', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'VBZ', 'VBN', 'VBN', 'TO', 'VB', 'JJ', 'TO', 'VB', 'JJ', 'NN', 'NNS', '.']",14
natural_language_inference,6,173,"As shown in , the NLI objective leads to a better performance on the English NLI test set , but this comes at the cost of a worse cross - lingual transfer performance in XNLI and Tatoeba .","['As', 'shown', 'in', ',', 'the', 'NLI', 'objective', 'leads', 'to', 'a', 'better', 'performance', 'on', 'the', 'English', 'NLI', 'test', 'set', ',', 'but', 'this', 'comes', 'at', 'the', 'cost', 'of', 'a', 'worse', 'cross', '-', 'lingual', 'transfer', 'performance', 'in', 'XNLI', 'and', 'Tatoeba', '.']","['O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'IN', ',', 'DT', 'NNP', 'JJ', 'NNS', 'TO', 'DT', 'JJR', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'NN', 'NN', ',', 'CC', 'DT', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJR', 'NN', ':', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'CC', 'NNP', '.']",38
natural_language_inference,10,2,Learning to Compose Task - Specific Tree Structures,"['Learning', 'to', 'Compose', 'Task', '-', 'Specific', 'Tree', 'Structures']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['VBG', 'TO', 'NNP', 'NNP', ':', 'JJ', 'NNP', 'NNP']",8
natural_language_inference,10,6,"In this paper , we propose Gumbel Tree - LSTM , a novel tree - structured long short - term memory architecture that learns how to compose task - specific tree structures only from plain text data efficiently .","['In', 'this', 'paper', ',', 'we', 'propose', 'Gumbel', 'Tree', '-', 'LSTM', ',', 'a', 'novel', 'tree', '-', 'structured', 'long', 'short', '-', 'term', 'memory', 'architecture', 'that', 'learns', 'how', 'to', 'compose', 'task', '-', 'specific', 'tree', 'structures', 'only', 'from', 'plain', 'text', 'data', 'efficiently', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'NNP', 'NNP', ':', 'NNP', ',', 'DT', 'JJ', 'NN', ':', 'VBN', 'RB', 'JJ', ':', 'NN', 'NN', 'NN', 'WDT', 'VBZ', 'WRB', 'TO', 'VB', 'NN', ':', 'JJ', 'NN', 'VBZ', 'RB', 'IN', 'JJ', 'JJ', 'NNS', 'RB', '.']",39
natural_language_inference,10,23,"In this paper , we propose Gumbel Tree - LSTM , which is a novel RvNN architecture that does not require structured data and learns to compose task - specific tree structures without explicit guidance .","['In', 'this', 'paper', ',', 'we', 'propose', 'Gumbel', 'Tree', '-', 'LSTM', ',', 'which', 'is', 'a', 'novel', 'RvNN', 'architecture', 'that', 'does', 'not', 'require', 'structured', 'data', 'and', 'learns', 'to', 'compose', 'task', '-', 'specific', 'tree', 'structures', 'without', 'explicit', 'guidance', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'NNP', 'NNP', ':', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NNP', 'NN', 'WDT', 'VBZ', 'RB', 'VB', 'VBN', 'NNS', 'CC', 'NNS', 'TO', 'VB', 'NN', ':', 'JJ', 'JJ', 'NNS', 'IN', 'JJ', 'NN', '.']",36
natural_language_inference,10,24,"Our Gumbel Tree - LSTM model is based on tree - structured long short - term memory ( Tree - LSTM ) architecture , which is one of the most renowned variants of RvNN .","['Our', 'Gumbel', 'Tree', '-', 'LSTM', 'model', 'is', 'based', 'on', 'tree', '-', 'structured', 'long', 'short', '-', 'term', 'memory', '(', 'Tree', '-', 'LSTM', ')', 'architecture', ',', 'which', 'is', 'one', 'of', 'the', 'most', 'renowned', 'variants', 'of', 'RvNN', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NNP', 'NNP', ':', 'NNP', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', ':', 'VBN', 'RB', 'JJ', ':', 'NN', 'NN', '(', 'NNP', ':', 'NNP', ')', 'NN', ',', 'WDT', 'VBZ', 'CD', 'IN', 'DT', 'RBS', 'JJ', 'NNS', 'IN', 'NNP', '.']",35
natural_language_inference,10,25,"To learn how to compose task - specific tree structures without depending on structured input , our model introduces composition query vector that measures validity of a composition .","['To', 'learn', 'how', 'to', 'compose', 'task', '-', 'specific', 'tree', 'structures', 'without', 'depending', 'on', 'structured', 'input', ',', 'our', 'model', 'introduces', 'composition', 'query', 'vector', 'that', 'measures', 'validity', 'of', 'a', 'composition', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'O']","['TO', 'VB', 'WRB', 'TO', 'VB', 'NN', ':', 'JJ', 'JJ', 'NNS', 'IN', 'VBG', 'IN', 'VBN', 'NN', ',', 'PRP$', 'NN', 'NNS', 'NN', 'NN', 'NN', 'WDT', 'VBZ', 'NN', 'IN', 'DT', 'NN', '.']",29
natural_language_inference,10,26,"Using validity scores computed by the composition query vector , our model recursively selects compositions until only a single representation remains .","['Using', 'validity', 'scores', 'computed', 'by', 'the', 'composition', 'query', 'vector', ',', 'our', 'model', 'recursively', 'selects', 'compositions', 'until', 'only', 'a', 'single', 'representation', 'remains', '.']","['B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'NN', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'NN', 'NN', ',', 'PRP$', 'NN', 'RB', 'VBZ', 'NNS', 'IN', 'RB', 'DT', 'JJ', 'NN', 'VBZ', '.']",22
natural_language_inference,10,27,We use Straight - Through ( ST ) Gumbel - Softmax estimator to sample compositions in the training phase .,"['We', 'use', 'Straight', '-', 'Through', '(', 'ST', ')', 'Gumbel', '-', 'Softmax', 'estimator', 'to', 'sample', 'compositions', 'in', 'the', 'training', 'phase', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', ':', 'IN', '(', 'NNP', ')', 'NNP', ':', 'NNP', 'NN', 'TO', 'VB', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",20
natural_language_inference,10,28,"ST Gumbel - Softmax estimator relaxes the discrete sampling operation to be continuous in the backward pass , thus our model can be trained via the standard backpropagation .","['ST', 'Gumbel', '-', 'Softmax', 'estimator', 'relaxes', 'the', 'discrete', 'sampling', 'operation', 'to', 'be', 'continuous', 'in', 'the', 'backward', 'pass', ',', 'thus', 'our', 'model', 'can', 'be', 'trained', 'via', 'the', 'standard', 'backpropagation', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', ':', 'NNP', 'NN', 'VBD', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'JJ', 'IN', 'DT', 'NN', 'NN', ',', 'RB', 'PRP$', 'NN', 'MD', 'VB', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",29
natural_language_inference,10,139,Natural Language Inference,"['Natural', 'Language', 'Inference']","['B-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NN']",3
natural_language_inference,10,150,"Similar to 100D experiments , we initialize the word embedding matrix with GloVe 300D pretrained vectors 4 , however we do not update the word representations during training .","['Similar', 'to', '100D', 'experiments', ',', 'we', 'initialize', 'the', 'word', 'embedding', 'matrix', 'with', 'GloVe', '300D', 'pretrained', 'vectors', '4', ',', 'however', 'we', 'do', 'not', 'update', 'the', 'word', 'representations', 'during', 'training', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'TO', 'CD', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBG', 'NN', 'IN', 'NNP', 'CD', 'VBD', 'NNS', 'CD', ',', 'RB', 'PRP', 'VBP', 'RB', 'VB', 'DT', 'NN', 'NNS', 'IN', 'NN', '.']",29
natural_language_inference,10,153,The dropout probability is set to 0.2 and word embeddings are not updated during training .,"['The', 'dropout', 'probability', 'is', 'set', 'to', '0.2', 'and', 'word', 'embeddings', 'are', 'not', 'updated', 'during', 'training', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'CC', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'NN', '.']",16
natural_language_inference,10,154,"The size of mini-batches is set to 128 in all experiments , and hyperparameters are tuned using the validation split .","['The', 'size', 'of', 'mini-batches', 'is', 'set', 'to', '128', 'in', 'all', 'experiments', ',', 'and', 'hyperparameters', 'are', 'tuned', 'using', 'the', 'validation', 'split', '.']","['O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'NNS', 'VBZ', 'VBN', 'TO', 'CD', 'IN', 'DT', 'NNS', ',', 'CC', 'NNS', 'VBP', 'VBN', 'VBG', 'DT', 'NN', 'NN', '.']",21
natural_language_inference,10,155,"The temperature parameter ? of Gumbel - Softmax is set to 1.0 , and we did not find that temperature annealing improves performance .","['The', 'temperature', 'parameter', '?', 'of', 'Gumbel', '-', 'Softmax', 'is', 'set', 'to', '1.0', ',', 'and', 'we', 'did', 'not', 'find', 'that', 'temperature', 'annealing', 'improves', 'performance', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', '.', 'IN', 'NNP', ':', 'NN', 'VBZ', 'VBN', 'TO', 'CD', ',', 'CC', 'PRP', 'VBD', 'RB', 'VB', 'IN', 'NN', 'VBG', 'NNS', 'NN', '.']",24
natural_language_inference,10,156,"For training models , Adam optimizer is used .","['For', 'training', 'models', ',', 'Adam', 'optimizer', 'is', 'used', '.']","['B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O']","['IN', 'NN', 'NNS', ',', 'NNP', 'NN', 'VBZ', 'VBN', '.']",9
natural_language_inference,10,158,"First , we can see that LSTM - based leaf transformation has a clear advantage over the affine - transformation - based one .","['First', ',', 'we', 'can', 'see', 'that', 'LSTM', '-', 'based', 'leaf', 'transformation', 'has', 'a', 'clear', 'advantage', 'over', 'the', 'affine', '-', 'transformation', '-', 'based', 'one', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'MD', 'VB', 'DT', 'NNP', ':', 'VBN', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', ':', 'NN', ':', 'VBN', 'CD', '.']",24
natural_language_inference,10,160,"Secondly , comparing ours with other models , we find that our 100D and 300D model outperform all other models of similar numbers of parameters .","['Secondly', ',', 'comparing', 'ours', 'with', 'other', 'models', ',', 'we', 'find', 'that', 'our', '100D', 'and', '300D', 'model', 'outperform', 'all', 'other', 'models', 'of', 'similar', 'numbers', 'of', 'parameters', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'VBG', 'NNS', 'IN', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'IN', 'PRP$', 'CD', 'CC', 'CD', 'NN', 'NN', 'DT', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'NNS', '.']",26
natural_language_inference,10,161,"Our 600D model achieves the accuracy of 86.0 % , which is comparable to that of the state - of - the - art model , while using far less parameters .","['Our', '600D', 'model', 'achieves', 'the', 'accuracy', 'of', '86.0', '%', ',', 'which', 'is', 'comparable', 'to', 'that', 'of', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'model', ',', 'while', 'using', 'far', 'less', 'parameters', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'CD', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', ',', 'WDT', 'VBZ', 'JJ', 'TO', 'DT', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', ',', 'IN', 'VBG', 'RB', 'JJR', 'NNS', '.']",32
natural_language_inference,10,163,All of our models converged within a few hours on a machine with NVIDIA Titan Xp GPU .,"['All', 'of', 'our', 'models', 'converged', 'within', 'a', 'few', 'hours', 'on', 'a', 'machine', 'with', 'NVIDIA', 'Titan', 'Xp', 'GPU', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'IN', 'PRP$', 'NNS', 'VBN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', '.']",18
natural_language_inference,10,166,Sentiment Analysis,"['Sentiment', 'Analysis']","['B-n', 'I-n']","['NN', 'NN']",2
natural_language_inference,10,175,is a single - hidden layer MLP with the ReLU activation function .,"['is', 'a', 'single', '-', 'hidden', 'layer', 'MLP', 'with', 'the', 'ReLU', 'activation', 'function', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['VBZ', 'DT', 'JJ', ':', 'JJ', 'NN', 'NNP', 'IN', 'DT', 'NNP', 'NN', 'NN', '.']",13
natural_language_inference,10,177,"We trained our SST - 2 model with hyperparameters D x = 300 , D h = 300 , D c = 300 .","['We', 'trained', 'our', 'SST', '-', '2', 'model', 'with', 'hyperparameters', 'D', 'x', '=', '300', ',', 'D', 'h', '=', '300', ',', 'D', 'c', '=', '300', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'PRP$', 'NNP', ':', 'CD', 'NN', 'IN', 'NNS', 'NNP', 'NNP', 'NNP', 'CD', ',', 'NNP', 'VBD', 'JJ', 'CD', ',', 'NNP', 'VBD', 'JJ', 'CD', '.']",24
natural_language_inference,10,178,The word vectors are initialized with GloVe 300D pretrained vectors and fine - tuned during training .,"['The', 'word', 'vectors', 'are', 'initialized', 'with', 'GloVe', '300D', 'pretrained', 'vectors', 'and', 'fine', '-', 'tuned', 'during', 'training', '.']","['O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'CD', 'VBD', 'NNS', 'CC', 'JJ', ':', 'VBN', 'IN', 'NN', '.']",17
natural_language_inference,10,179,We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the MLP layer .,"['We', 'apply', 'dropout', '(', 'p', '=', '0.5', ')', 'on', 'the', 'output', 'of', 'the', 'word', 'embedding', 'layer', 'and', 'the', 'input', 'and', 'the', 'output', 'of', 'the', 'MLP', 'layer', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'RB', '(', 'JJ', 'NNP', 'CD', ')', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBG', 'NN', 'CC', 'DT', 'NN', 'CC', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",27
natural_language_inference,10,180,The size of mini-batches is set to 32 and Adadelta optimizer is used for optimization .,"['The', 'size', 'of', 'mini-batches', 'is', 'set', 'to', '32', 'and', 'Adadelta', 'optimizer', 'is', 'used', 'for', 'optimization', '.']","['O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NNS', 'VBZ', 'VBN', 'TO', 'CD', 'CC', 'NNP', 'NN', 'VBZ', 'VBN', 'IN', 'NN', '.']",16
natural_language_inference,10,181,"For our SST - 5 model , hyperparameters are set to D x = 300 , D h = 300 , D c = 1024 . Similar to the SST - 2 model , we optimize the model using Adadelta optimizer with batch size 64 and apply dropout with p = 0.5 .","['For', 'our', 'SST', '-', '5', 'model', ',', 'hyperparameters', 'are', 'set', 'to', 'D', 'x', '=', '300', ',', 'D', 'h', '=', '300', ',', 'D', 'c', '=', '1024', '.', 'Similar', 'to', 'the', 'SST', '-', '2', 'model', ',', 'we', 'optimize', 'the', 'model', 'using', 'Adadelta', 'optimizer', 'with', 'batch', 'size', '64', 'and', 'apply', 'dropout', 'with', 'p', '=', '0.5', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'PRP$', 'NNP', ':', 'CD', 'NN', ',', 'NNS', 'VBP', 'VBN', 'TO', 'NNP', 'NNP', 'NNP', 'CD', ',', 'NNP', 'VBD', 'JJ', 'CD', ',', 'NNP', 'VBZ', 'JJ', 'CD', '.', 'JJ', 'TO', 'DT', 'NNP', ':', 'CD', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBG', 'NNP', 'NN', 'IN', 'NN', 'NN', 'CD', 'CC', 'VB', 'NN', 'IN', 'JJ', '$', 'CD', '.']",53
natural_language_inference,10,183,"Our SST - 2 model outperforms all other models substantially except byte - m LSTM , where a byte - level language model trained on the large product review dataset is used to obtain sentence representations .","['Our', 'SST', '-', '2', 'model', 'outperforms', 'all', 'other', 'models', 'substantially', 'except', 'byte', '-', 'm', 'LSTM', ',', 'where', 'a', 'byte', '-', 'level', 'language', 'model', 'trained', 'on', 'the', 'large', 'product', 'review', 'dataset', 'is', 'used', 'to', 'obtain', 'sentence', 'representations', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NNP', ':', 'CD', 'NN', 'NNS', 'DT', 'JJ', 'NNS', 'RB', 'IN', 'SYM', ':', 'NN', 'NNP', ',', 'WRB', 'DT', 'NN', ':', 'NN', 'NN', 'NN', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'NN', 'NNS', '.']",37
natural_language_inference,10,184,"We also see that the performance of our SST - 5 model is on par with that of the current state - of - the - art model , which is pretrained on large parallel datasets and uses character n-gram embeddings alongside word embeddings , even though our model does not utilize external resources other than GloVe vectors and only uses wordlevel representations .","['We', 'also', 'see', 'that', 'the', 'performance', 'of', 'our', 'SST', '-', '5', 'model', 'is', 'on', 'par', 'with', 'that', 'of', 'the', 'current', 'state', '-', 'of', '-', 'the', '-', 'art', 'model', ',', 'which', 'is', 'pretrained', 'on', 'large', 'parallel', 'datasets', 'and', 'uses', 'character', 'n-gram', 'embeddings', 'alongside', 'word', 'embeddings', ',', 'even', 'though', 'our', 'model', 'does', 'not', 'utilize', 'external', 'resources', 'other', 'than', 'GloVe', 'vectors', 'and', 'only', 'uses', 'wordlevel', 'representations', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'IN', 'DT', 'NN', 'IN', 'PRP$', 'NNP', ':', 'CD', 'NN', 'VBZ', 'IN', 'NN', 'IN', 'DT', 'IN', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', ',', 'WDT', 'VBZ', 'VBN', 'IN', 'JJ', 'JJ', 'NNS', 'CC', 'VBZ', 'JJR', 'JJ', 'NNS', 'IN', 'NN', 'NNS', ',', 'RB', 'IN', 'PRP$', 'NN', 'VBZ', 'RB', 'VB', 'JJ', 'NNS', 'JJ', 'IN', 'NNP', 'NNS', 'CC', 'JJ', 'NNS', 'VBP', 'NNS', '.']",64
natural_language_inference,10,185,The authors of stated that utilizing pretraining and character n-gram embeddings improves validation accuracy by 2.8 % ( SST - 2 ) or 1.7 % ( SST - 5 ) .,"['The', 'authors', 'of', 'stated', 'that', 'utilizing', 'pretraining', 'and', 'character', 'n-gram', 'embeddings', 'improves', 'validation', 'accuracy', 'by', '2.8', '%', '(', 'SST', '-', '2', ')', 'or', '1.7', '%', '(', 'SST', '-', '5', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'VBN', 'IN', 'VBG', 'NN', 'CC', 'NN', 'JJ', 'NNS', 'VBZ', 'NN', 'NN', 'IN', 'CD', 'NN', '(', 'NNP', ':', 'CD', ')', 'CC', 'CD', 'NN', '(', 'NNP', ':', 'CD', ')', '.']",31
natural_language_inference,33,2,Published as a conference paper at ICLR 2018 LEARNING GENERAL PURPOSE DISTRIBUTED SEN - TENCE REPRESENTATIONS VIA LARGE SCALE MULTI - TASK LEARNING,"['Published', 'as', 'a', 'conference', 'paper', 'at', 'ICLR', '2018', 'LEARNING', 'GENERAL', 'PURPOSE', 'DISTRIBUTED', 'SEN', '-', 'TENCE', 'REPRESENTATIONS', 'VIA', 'LARGE', 'SCALE', 'MULTI', '-', 'TASK', 'LEARNING']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'IN', 'DT', 'NN', 'NN', 'IN', 'NNP', 'CD', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', ':', 'NN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', ':', 'NN', 'NN']",23
natural_language_inference,33,4,A lot of the recent success in natural language processing ( NLP ) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner .,"['A', 'lot', 'of', 'the', 'recent', 'success', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', 'has', 'been', 'driven', 'by', 'distributed', 'vector', 'representations', 'of', 'words', 'trained', 'on', 'large', 'amounts', 'of', 'text', 'in', 'an', 'unsupervised', 'manner', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'VBN', 'VBN', 'IN', 'JJ', 'NN', 'NNS', 'IN', 'NNS', 'VBN', 'IN', 'JJ', 'NNS', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",33
natural_language_inference,33,6,"However , extending this success to learning representations of sequences of words , such as sentences , remains an open problem .","['However', ',', 'extending', 'this', 'success', 'to', 'learning', 'representations', 'of', 'sequences', 'of', 'words', ',', 'such', 'as', 'sentences', ',', 'remains', 'an', 'open', 'problem', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'VBG', 'DT', 'NN', 'TO', 'VBG', 'NNS', 'IN', 'NNS', 'IN', 'NNS', ',', 'JJ', 'IN', 'NNS', ',', 'VBZ', 'DT', 'JJ', 'NN', '.']",22
natural_language_inference,33,18,Some recent work has addressed this by learning general - purpose sentence representations .,"['Some', 'recent', 'work', 'has', 'addressed', 'this', 'by', 'learning', 'general', '-', 'purpose', 'sentence', 'representations', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'VBN', 'DT', 'IN', 'VBG', 'JJ', ':', 'JJ', 'NN', 'NNS', '.']",14
natural_language_inference,33,26,"Our work exploits this in the context of a simple one - to - many multi -task learning ( MTL ) framework , wherein a single recurrent sentence encoder is shared across multiple tasks .","['Our', 'work', 'exploits', 'this', 'in', 'the', 'context', 'of', 'a', 'simple', 'one', '-', 'to', '-', 'many', 'multi', '-task', 'learning', '(', 'MTL', ')', 'framework', ',', 'wherein', 'a', 'single', 'recurrent', 'sentence', 'encoder', 'is', 'shared', 'across', 'multiple', 'tasks', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'DT', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'CD', ':', 'TO', ':', 'JJ', 'VBP', 'JJ', 'NN', '(', 'NNP', ')', 'NN', ',', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', 'NNS', '.']",35
natural_language_inference,33,29,"While our work aims at learning fixed - length distributed sentence representations , it is not always practical to assume that the entire "" meaning "" of a sentence can be encoded into a fixed - length vector .","['While', 'our', 'work', 'aims', 'at', 'learning', 'fixed', '-', 'length', 'distributed', 'sentence', 'representations', ',', 'it', 'is', 'not', 'always', 'practical', 'to', 'assume', 'that', 'the', 'entire', '""', 'meaning', '""', 'of', 'a', 'sentence', 'can', 'be', 'encoded', 'into', 'a', 'fixed', '-', 'length', 'vector', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PRP$', 'NN', 'VBZ', 'IN', 'VBG', 'VBN', ':', 'NN', 'VBD', 'NN', 'NNS', ',', 'PRP', 'VBZ', 'RB', 'RB', 'JJ', 'TO', 'VB', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', 'IN', 'DT', 'NN', 'MD', 'VB', 'VBN', 'IN', 'DT', 'JJ', ':', 'NN', 'NN', '.']",39
natural_language_inference,33,31,The primary contribution of our work is to combine the benefits of diverse sentence - representation learning objectives into a single multi-task framework .,"['The', 'primary', 'contribution', 'of', 'our', 'work', 'is', 'to', 'combine', 'the', 'benefits', 'of', 'diverse', 'sentence', '-', 'representation', 'learning', 'objectives', 'into', 'a', 'single', 'multi-task', 'framework', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'IN', 'PRP$', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NNS', 'IN', 'JJ', 'NN', ':', 'NN', 'VBG', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",24
natural_language_inference,33,136,It is evident from that adding more tasks improves the transfer performance of our model .,"['It', 'is', 'evident', 'from', 'that', 'adding', 'more', 'tasks', 'improves', 'the', 'transfer', 'performance', 'of', 'our', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBZ', 'JJ', 'IN', 'IN', 'VBG', 'JJR', 'NNS', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'PRP$', 'NN', '.']",16
natural_language_inference,33,137,Increasing the capacity our sentence encoder with more hidden units ( + L ) as well as an additional layer ( + 2L ) also lead to improved transfer performance .,"['Increasing', 'the', 'capacity', 'our', 'sentence', 'encoder', 'with', 'more', 'hidden', 'units', '(', '+', 'L', ')', 'as', 'well', 'as', 'an', 'additional', 'layer', '(', '+', '2L', ')', 'also', 'lead', 'to', 'improved', 'transfer', 'performance', '.']","['B-p', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['VBG', 'DT', 'NN', 'PRP$', 'NN', 'NN', 'IN', 'RBR', 'JJ', 'NNS', '(', 'NNP', 'NNP', ')', 'RB', 'RB', 'IN', 'DT', 'JJ', 'NN', '(', 'JJ', 'CD', ')', 'RB', 'VBP', 'TO', 'VBN', 'NN', 'NN', '.']",31
natural_language_inference,33,138,"We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over Infersent .","['We', 'observe', 'gains', 'of', '1.1', '-', '2.0', '%', 'on', 'the', 'sentiment', 'classification', 'tasks', '(', 'MR', ',', 'CR', ',', 'SUBJ', '&', 'MPQA', ')', 'over', 'Infersent', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'NNS', 'IN', 'CD', ':', 'CD', 'NN', 'IN', 'DT', 'NN', 'NN', 'NNS', '(', 'NNP', ',', 'NNP', ',', 'NNP', 'CC', 'NNP', ')', 'IN', 'NNP', '.']",25
natural_language_inference,33,139,"We demonstrate substantial gains on TREC ( 6 % over Infersent and roughly 2 % over the CNN - LSTM ) , outperforming even a competitive supervised baseline .","['We', 'demonstrate', 'substantial', 'gains', 'on', 'TREC', '(', '6', '%', 'over', 'Infersent', 'and', 'roughly', '2', '%', 'over', 'the', 'CNN', '-', 'LSTM', ')', ',', 'outperforming', 'even', 'a', 'competitive', 'supervised', 'baseline', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NNS', 'IN', 'NNP', '(', 'CD', 'NN', 'IN', 'NNP', 'CC', 'RB', 'CD', 'NN', 'IN', 'DT', 'NNP', ':', 'NNP', ')', ',', 'VBG', 'RB', 'DT', 'JJ', 'JJ', 'NN', '.']",29
natural_language_inference,33,140,"We see similar gains ( 2.3 % ) on paraphrase identification ( MPRC ) , closing the gap on supervised approaches trained from scratch .","['We', 'see', 'similar', 'gains', '(', '2.3', '%', ')', 'on', 'paraphrase', 'identification', '(', 'MPRC', ')', ',', 'closing', 'the', 'gap', 'on', 'supervised', 'approaches', 'trained', 'from', 'scratch', '.']","['O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'NNS', '(', 'CD', 'NN', ')', 'IN', 'NN', 'NN', '(', 'NNP', ')', ',', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'VBN', 'IN', 'NN', '.']",25
natural_language_inference,33,141,The addition of constituency parsing improves performance on sentence relatedness ( SICK - R ) and entailment ( SICK - E ) consistent with observations made by .,"['The', 'addition', 'of', 'constituency', 'parsing', 'improves', 'performance', 'on', 'sentence', 'relatedness', '(', 'SICK', '-', 'R', ')', 'and', 'entailment', '(', 'SICK', '-', 'E', ')', 'consistent', 'with', 'observations', 'made', 'by', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'NN', 'VBG', 'NNS', 'NN', 'IN', 'NN', 'NN', '(', 'NNP', ':', 'NN', ')', 'CC', 'NN', '(', 'NNP', ':', 'NN', ')', 'NN', 'IN', 'NNS', 'VBN', 'IN', '.']",28
natural_language_inference,33,142,"In , we show that simply training an MLP on top of our fixed sentence representations outperforms several strong & complex supervised approaches that use attention mechanisms , even on this fairly large dataset .","['In', ',', 'we', 'show', 'that', 'simply', 'training', 'an', 'MLP', 'on', 'top', 'of', 'our', 'fixed', 'sentence', 'representations', 'outperforms', 'several', 'strong', '&', 'complex', 'supervised', 'approaches', 'that', 'use', 'attention', 'mechanisms', ',', 'even', 'on', 'this', 'fairly', 'large', 'dataset', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', ',', 'PRP', 'VBP', 'IN', 'RB', 'VBG', 'DT', 'NNP', 'IN', 'NN', 'IN', 'PRP$', 'JJ', 'NN', 'NNS', 'VBZ', 'JJ', 'JJ', 'CC', 'JJ', 'VBD', 'NNS', 'WDT', 'VBP', 'NN', 'NNS', ',', 'RB', 'IN', 'DT', 'RB', 'JJ', 'NN', '.']",35
natural_language_inference,33,143,"For example , we observe a 0.2-0.5 % improvement over the decomposable attention model .","['For', 'example', ',', 'we', 'observe', 'a', '0.2-0.5', '%', 'improvement', 'over', 'the', 'decomposable', 'attention', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",15
natural_language_inference,33,144,"When using only a small fraction of the training data , indicated by the columns 1 k - 25 k , we are able to outperform the Siamese and Multi - Perspective CNN using roughly 6 % of the available training set .","['When', 'using', 'only', 'a', 'small', 'fraction', 'of', 'the', 'training', 'data', ',', 'indicated', 'by', 'the', 'columns', '1', 'k', '-', '25', 'k', ',', 'we', 'are', 'able', 'to', 'outperform', 'the', 'Siamese', 'and', 'Multi', '-', 'Perspective', 'CNN', 'using', 'roughly', '6', '%', 'of', 'the', 'available', 'training', 'set', '.']","['B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['WRB', 'VBG', 'RB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NNS', ',', 'VBN', 'IN', 'DT', 'NN', 'CD', 'NN', ':', 'CD', 'NN', ',', 'PRP', 'VBP', 'JJ', 'TO', 'VB', 'DT', 'NNP', 'CC', 'NNP', ':', 'NN', 'NNP', 'VBG', 'RB', 'CD', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",43
natural_language_inference,33,145,We also outperform the Deconv LVM model proposed by in this low - resource setting .,"['We', 'also', 'outperform', 'the', 'Deconv', 'LVM', 'model', 'proposed', 'by', 'in', 'this', 'low', '-', 'resource', 'setting', '.']","['O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NNP', 'NNP', 'NN', 'VBN', 'IN', 'IN', 'DT', 'JJ', ':', 'NN', 'NN', '.']",16
natural_language_inference,33,147,"Somewhat surprisingly , in we observe that the learned word embeddings are competitive with popular methods such as GloVe , word2vec , and fasttext on the benchmarks presented by and .","['Somewhat', 'surprisingly', ',', 'in', 'we', 'observe', 'that', 'the', 'learned', 'word', 'embeddings', 'are', 'competitive', 'with', 'popular', 'methods', 'such', 'as', 'GloVe', ',', 'word2vec', ',', 'and', 'fasttext', 'on', 'the', 'benchmarks', 'presented', 'by', 'and', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'RB', ',', 'IN', 'PRP', 'VBP', 'IN', 'DT', 'JJ', 'NN', 'NNS', 'VBP', 'JJ', 'IN', 'JJ', 'NNS', 'JJ', 'IN', 'NNP', ',', 'NN', ',', 'CC', 'NN', 'IN', 'DT', 'NNS', 'VBN', 'IN', 'CC', '.']",31
natural_language_inference,33,150,Representations learned solely from NLI do appear to encode syntax but incorporation into our multi-task framework does not amplify this signal .,"['Representations', 'learned', 'solely', 'from', 'NLI', 'do', 'appear', 'to', 'encode', 'syntax', 'but', 'incorporation', 'into', 'our', 'multi-task', 'framework', 'does', 'not', 'amplify', 'this', 'signal', '.']","['B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O']","['NNS', 'VBD', 'RB', 'IN', 'NNP', 'VBP', 'VB', 'TO', 'VB', 'NN', 'CC', 'NN', 'IN', 'PRP$', 'JJ', 'NN', 'VBZ', 'RB', 'VB', 'DT', 'NN', '.']",22
natural_language_inference,33,151,"Similarly , we observe that sentence characteristics such as length and word order are better encoded with the addition of parsing .","['Similarly', ',', 'we', 'observe', 'that', 'sentence', 'characteristics', 'such', 'as', 'length', 'and', 'word', 'order', 'are', 'better', 'encoded', 'with', 'the', 'addition', 'of', 'parsing', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'NNS', 'JJ', 'IN', 'NN', 'CC', 'NN', 'NN', 'VBP', 'RBR', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NN', '.']",22
natural_language_inference,61,5,"is paper proposes an a ention boosted natural language inference model named a ESIM by adding word a ention and adaptive direction - oriented a ention mechanisms to the traditional Bi - LSTM layer of natural language inference models , e.g. ESIM . is makes the inference model a ESIM has the ability toe ectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .","['is', 'paper', 'proposes', 'an', 'a', 'ention', 'boosted', 'natural', 'language', 'inference', 'model', 'named', 'a', 'ESIM', 'by', 'adding', 'word', 'a', 'ention', 'and', 'adaptive', 'direction', '-', 'oriented', 'a', 'ention', 'mechanisms', 'to', 'the', 'traditional', 'Bi', '-', 'LSTM', 'layer', 'of', 'natural', 'language', 'inference', 'models', ',', 'e.g.', 'ESIM', '.', 'is', 'makes', 'the', 'inference', 'model', 'a', 'ESIM', 'has', 'the', 'ability', 'toe', 'ectively', 'learn', 'the', 'representation', 'of', 'words', 'and', 'model', 'the', 'local', 'subsentential', 'inference', 'between', 'pairs', 'of', 'premise', 'and', 'hypothesis', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBZ', 'NN', 'VBZ', 'DT', 'DT', 'NN', 'VBD', 'JJ', 'NN', 'NN', 'NN', 'VBD', 'DT', 'NNP', 'IN', 'VBG', 'NN', 'DT', 'NN', 'CC', 'JJ', 'NN', ':', 'VBD', 'DT', 'NN', 'NN', 'TO', 'DT', 'JJ', 'NNP', ':', 'NNP', 'NN', 'IN', 'JJ', 'NN', 'NN', 'NNS', ',', 'JJ', 'NNP', '.', 'VBZ', 'VBZ', 'DT', 'NN', 'NN', 'DT', 'NNP', 'VBZ', 'DT', 'NN', 'NN', 'RB', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'CC', 'VB', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'NN', 'CC', 'NN', '.']",73
natural_language_inference,61,9,Natural language inference ( NLI ) is an important and signicant task in natural language processing ( NLP ) .,"['Natural', 'language', 'inference', '(', 'NLI', ')', 'is', 'an', 'important', 'and', 'signicant', 'task', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'DT', 'JJ', 'CC', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', '.']",20
natural_language_inference,61,12,"In the literature , the task of NLI is usually viewed as a relation classi cation .","['In', 'the', 'literature', ',', 'the', 'task', 'of', 'NLI', 'is', 'usually', 'viewed', 'as', 'a', 'relation', 'classi', 'cation', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'DT', 'NN', 'IN', 'NNP', 'VBZ', 'RB', 'VBN', 'IN', 'DT', 'NN', 'NN', 'NN', '.']",17
natural_language_inference,61,51,"erefore , in this study , using ESIM model as the baseline , we add an a ention layer behind each Bi - LSTM layer , then use an adaptive orientation embedding layer to jointly represent the forward and backward vectors .","['erefore', ',', 'in', 'this', 'study', ',', 'using', 'ESIM', 'model', 'as', 'the', 'baseline', ',', 'we', 'add', 'an', 'a', 'ention', 'layer', 'behind', 'each', 'Bi', '-', 'LSTM', 'layer', ',', 'then', 'use', 'an', 'adaptive', 'orientation', 'embedding', 'layer', 'to', 'jointly', 'represent', 'the', 'forward', 'and', 'backward', 'vectors', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'IN', 'DT', 'NN', ',', 'VBG', 'NNP', 'NN', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'DT', 'NN', 'NN', 'IN', 'DT', 'NNP', ':', 'NNP', 'NN', ',', 'RB', 'VB', 'DT', 'JJ', 'NN', 'VBG', 'NN', 'TO', 'RB', 'VB', 'DT', 'NN', 'CC', 'NN', 'NNS', '.']",42
natural_language_inference,61,52,"We name this a ention boosted Bi - LSTM as Bi - a LSTM , and denote the modi ed ESIM as aESIM .","['We', 'name', 'this', 'a', 'ention', 'boosted', 'Bi', '-', 'LSTM', 'as', 'Bi', '-', 'a', 'LSTM', ',', 'and', 'denote', 'the', 'modi', 'ed', 'ESIM', 'as', 'aESIM', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'DT', 'NN', 'VBD', 'NNP', ':', 'NNP', 'IN', 'NNP', ':', 'DT', 'NNP', ',', 'CC', 'VB', 'DT', 'NN', 'NN', 'NNP', 'IN', 'NN', '.']",24
natural_language_inference,61,121,We use the Adam method for optimization .,"['We', 'use', 'the', 'Adam', 'method', 'for', 'optimization', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', 'IN', 'NN', '.']",8
natural_language_inference,61,123,"e initial learning rate is set to 0.0005 , and the batch size is 128 .","['e', 'initial', 'learning', 'rate', 'is', 'set', 'to', '0.0005', ',', 'and', 'the', 'batch', 'size', 'is', '128', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['RB', 'JJ', 'VBG', 'NN', 'VBZ', 'VBN', 'TO', 'CD', ',', 'CC', 'DT', 'NN', 'NN', 'VBZ', 'CD', '.']",16
natural_language_inference,61,124,e dimensions of all hidden states of Bi - aLSTM and word embedding are 300 .,"['e', 'dimensions', 'of', 'all', 'hidden', 'states', 'of', 'Bi', '-', 'aLSTM', 'and', 'word', 'embedding', 'are', '300', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['JJ', 'NNS', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NNP', ':', 'NN', 'CC', 'NN', 'NN', 'VBP', 'CD', '.']",16
natural_language_inference,61,125,We employ non-linearity function f = selu replacing recti ed linear unit ReLU on account of its faster convergence rate .,"['We', 'employ', 'non-linearity', 'function', 'f', '=', 'selu', 'replacing', 'recti', 'ed', 'linear', 'unit', 'ReLU', 'on', 'account', 'of', 'its', 'faster', 'convergence', 'rate', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'NN', 'NNP', 'NN', 'VBG', 'NN', 'FW', 'JJ', 'NN', 'NNP', 'IN', 'NN', 'IN', 'PRP$', 'JJ', 'NN', 'NN', '.']",21
natural_language_inference,61,126,Dropout rate is set to 0.2 during training .,"['Dropout', 'rate', 'is', 'set', 'to', '0.2', 'during', 'training', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O']","['NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'IN', 'NN', '.']",9
natural_language_inference,61,127,We use pre-trained 300 - D Glove 840B vectors to initialize word embeddings .,"['We', 'use', 'pre-trained', '300', '-', 'D', 'Glove', '840B', 'vectors', 'to', 'initialize', 'word', 'embeddings', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'CD', ':', 'NNP', 'NNP', 'CD', 'NNS', 'TO', 'VB', 'NN', 'NNS', '.']",14
natural_language_inference,61,128,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,"['Out', '-', 'of', '-', 'vocabulary', '(', 'OOV', ')', 'words', 'are', 'initialized', 'randomly', 'with', 'Gaussian', 'samples', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', ':', 'IN', ':', 'NN', '(', 'NNP', ')', 'NNS', 'VBP', 'VBN', 'RB', 'IN', 'JJ', 'NNS', '.']",16
natural_language_inference,61,138,"According to the results in , a ESIM model achieved 88.1 % on SNLI corpus , elevating 0.8 percent higher than ESIM model .","['According', 'to', 'the', 'results', 'in', ',', 'a', 'ESIM', 'model', 'achieved', '88.1', '%', 'on', 'SNLI', 'corpus', ',', 'elevating', '0.8', 'percent', 'higher', 'than', 'ESIM', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['VBG', 'TO', 'DT', 'NNS', 'IN', ',', 'DT', 'NNP', 'NN', 'VBD', 'CD', 'NN', 'IN', 'NNP', 'NN', ',', 'VBG', 'CD', 'NN', 'JJR', 'IN', 'NNP', 'NN', '.']",24
natural_language_inference,61,139,It promoted almost 0.5 percent accuracy and outperformed the baselines on MultiNLI .,"['It', 'promoted', 'almost', '0.5', 'percent', 'accuracy', 'and', 'outperformed', 'the', 'baselines', 'on', 'MultiNLI', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'VBD', 'RB', 'CD', 'NN', 'NN', 'CC', 'VBD', 'DT', 'NNS', 'IN', 'NNP', '.']",13
natural_language_inference,70,4,This paper describes the KeLP system participating in the SemEval - 2016 Community Question Answering ( c QA ) task .,"['This', 'paper', 'describes', 'the', 'KeLP', 'system', 'participating', 'in', 'the', 'SemEval', '-', '2016', 'Community', 'Question', 'Answering', '(', 'c', 'QA', ')', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['DT', 'NN', 'VBZ', 'DT', 'NNP', 'NN', 'VBG', 'IN', 'DT', 'NNP', ':', 'CD', 'NNP', 'NNP', 'NNP', '(', 'VB', 'NNP', ')', 'NN', '.']",21
natural_language_inference,70,14,"In this task , participants are asked to automatically provide good answers in a c QA setting .","['In', 'this', 'task', ',', 'participants', 'are', 'asked', 'to', 'automatically', 'provide', 'good', 'answers', 'in', 'a', 'c', 'QA', 'setting', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O']","['IN', 'DT', 'NN', ',', 'NNS', 'VBP', 'VBN', 'TO', 'RB', 'VB', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NNP', 'NN', '.']",18
natural_language_inference,70,20,All the above subtasks have been modeled as binary classification problems : kernel - based classifiers are trained and the classification score is used to sort the instances and produce the final ranking .,"['All', 'the', 'above', 'subtasks', 'have', 'been', 'modeled', 'as', 'binary', 'classification', 'problems', ':', 'kernel', '-', 'based', 'classifiers', 'are', 'trained', 'and', 'the', 'classification', 'score', 'is', 'used', 'to', 'sort', 'the', 'instances', 'and', 'produce', 'the', 'final', 'ranking', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PDT', 'DT', 'JJ', 'NNS', 'VBP', 'VBN', 'VBN', 'IN', 'JJ', 'NN', 'NNS', ':', 'VB', ':', 'VBN', 'NNS', 'VBP', 'VBN', 'CC', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NNS', 'CC', 'VB', 'DT', 'JJ', 'NN', '.']",34
natural_language_inference,70,21,"All classifiers and kernels have been implemented within the Kernel - based Learning Platform 2 ( KeLP ) , thus determining the team 's name .","['All', 'classifiers', 'and', 'kernels', 'have', 'been', 'implemented', 'within', 'the', 'Kernel', '-', 'based', 'Learning', 'Platform', '2', '(', 'KeLP', ')', ',', 'thus', 'determining', 'the', 'team', ""'s"", 'name', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'CC', 'NNS', 'VBP', 'VBN', 'VBN', 'IN', 'DT', 'NNP', ':', 'VBN', 'NNP', 'NNP', 'CD', '(', 'NNP', ')', ',', 'RB', 'VBG', 'DT', 'NN', 'POS', 'NN', '.']",26
natural_language_inference,70,22,"The proposed solution provides three main contributions : ( i ) we employ the approach proposed in , which applies tree kernels directly to question and answer texts modeled as pairs of linked syntactic trees .","['The', 'proposed', 'solution', 'provides', 'three', 'main', 'contributions', ':', '(', 'i', ')', 'we', 'employ', 'the', 'approach', 'proposed', 'in', ',', 'which', 'applies', 'tree', 'kernels', 'directly', 'to', 'question', 'and', 'answer', 'texts', 'modeled', 'as', 'pairs', 'of', 'linked', 'syntactic', 'trees', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'VBN', 'NN', 'VBZ', 'CD', 'JJ', 'NNS', ':', '(', 'NN', ')', 'PRP', 'VBP', 'DT', 'NN', 'VBN', 'IN', ',', 'WDT', 'VBZ', 'NN', 'NNS', 'RB', 'TO', 'NN', 'CC', 'NN', 'NN', 'VBD', 'IN', 'NNS', 'IN', 'VBN', 'JJ', 'NNS', '.']",36
natural_language_inference,70,25,( iii ) we propose a stacking schema so that classifiers for Subtask B and C exploit the inferences obtained in the previous subtasks .,"['(', 'iii', ')', 'we', 'propose', 'a', 'stacking', 'schema', 'so', 'that', 'classifiers', 'for', 'Subtask', 'B', 'and', 'C', 'exploit', 'the', 'inferences', 'obtained', 'in', 'the', 'previous', 'subtasks', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['(', 'NN', ')', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'NNS', 'IN', 'NNP', 'NNP', 'CC', 'NNP', 'VBP', 'DT', 'NNS', 'VBN', 'IN', 'DT', 'JJ', 'NNS', '.']",25
natural_language_inference,70,133,Subtask A,"['Subtask', 'A']","['B-n', 'I-n']","['VB', 'DT']",2
natural_language_inference,70,139,Results : reports the outcome on Subtask A .,"['Results', ':', 'reports', 'the', 'outcome', 'on', 'Subtask', 'A', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNS', ':', 'VBZ', 'DT', 'NN', 'IN', 'NNP', 'NNP', '.']",9
natural_language_inference,70,140,"The good results on the 10 fold cross validations are confirmed on the official test set : the model is very accurate and achieved the first position among 12 systems , with the best MAP .","['The', 'good', 'results', 'on', 'the', '10', 'fold', 'cross', 'validations', 'are', 'confirmed', 'on', 'the', 'official', 'test', 'set', ':', 'the', 'model', 'is', 'very', 'accurate', 'and', 'achieved', 'the', 'first', 'position', 'among', '12', 'systems', ',', 'with', 'the', 'best', 'MAP', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NNS', 'IN', 'DT', 'CD', 'JJ', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', ':', 'DT', 'NN', 'VBZ', 'RB', 'JJ', 'CC', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NNS', ',', 'IN', 'DT', 'JJS', 'NN', '.']",36
natural_language_inference,70,143,Subtask B,"['Subtask', 'B']","['B-n', 'I-n']","['NN', 'NNP']",2
natural_language_inference,70,155,"On the official test set , our primary submission achieved the third position w.r.t. MAP among 11 systems .","['On', 'the', 'official', 'test', 'set', ',', 'our', 'primary', 'submission', 'achieved', 'the', 'third', 'position', 'w.r.t.', 'MAP', 'among', '11', 'systems', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'NN', ',', 'PRP$', 'JJ', 'NN', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'IN', 'CD', 'NNS', '.']",19
natural_language_inference,70,157,The primary system achieves the highest F 1 and accuracy on both tuning and test stages .,"['The', 'primary', 'system', 'achieves', 'the', 'highest', 'F', '1', 'and', 'accuracy', 'on', 'both', 'tuning', 'and', 'test', 'stages', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJS', 'JJ', 'CD', 'CC', 'NN', 'IN', 'DT', 'NN', 'CC', 'NN', 'NNS', '.']",17
natural_language_inference,70,159,Subtask C Model :,"['Subtask', 'C', 'Model', ':']","['B-n', 'I-n', 'O', 'O']","['NNP', 'NNP', 'NNP', ':']",4
natural_language_inference,70,171,"Our primary submission achieved the second highest MAP , while our Contrastive 2 is the best result .","['Our', 'primary', 'submission', 'achieved', 'the', 'second', 'highest', 'MAP', ',', 'while', 'our', 'Contrastive', '2', 'is', 'the', 'best', 'result', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'JJ', 'NN', 'VBD', 'DT', 'JJ', 'JJS', 'NNP', ',', 'IN', 'PRP$', 'JJ', 'CD', 'VBZ', 'DT', 'JJS', 'NN', '.']",18
natural_language_inference,70,172,It should be also noted that the F 1 our system is the best among 10 primary submissions .,"['It', 'should', 'be', 'also', 'noted', 'that', 'the', 'F', '1', 'our', 'system', 'is', 'the', 'best', 'among', '10', 'primary', 'submissions', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'RB', 'VBN', 'IN', 'DT', 'NNP', 'CD', 'PRP$', 'NN', 'VBZ', 'DT', 'JJS', 'IN', 'CD', 'JJ', 'NNS', '.']",19
natural_language_inference,72,2,CliCR : A Dataset of Clinical Case Reports for Machine Reading Comprehension *,"['CliCR', ':', 'A', 'Dataset', 'of', 'Clinical', 'Case', 'Reports', 'for', 'Machine', 'Reading', 'Comprehension', '*']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NN', ':', 'DT', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'NN']",13
natural_language_inference,72,4,We present a new dataset for machine comprehension in the medical domain .,"['We', 'present', 'a', 'new', 'dataset', 'for', 'machine', 'comprehension', 'in', 'the', 'medical', 'domain', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",13
natural_language_inference,72,33,"For our dataset , we construct queries , answers and supporting passages from BMJ Case Reports , the largest online repository of such documents .","['For', 'our', 'dataset', ',', 'we', 'construct', 'queries', ',', 'answers', 'and', 'supporting', 'passages', 'from', 'BMJ', 'Case', 'Reports', ',', 'the', 'largest', 'online', 'repository', 'of', 'such', 'documents', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PRP$', 'NN', ',', 'PRP', 'VBP', 'NNS', ',', 'NNS', 'CC', 'VBG', 'NNS', 'IN', 'NNP', 'NNP', 'NNP', ',', 'DT', 'JJS', 'JJ', 'NN', 'IN', 'JJ', 'NNS', '.']",25
natural_language_inference,72,34,"A case report is a detailed description of a clinical case that focuses on rare diseases , unusual presentation of common conditions and novel treatment methods .","['A', 'case', 'report', 'is', 'a', 'detailed', 'description', 'of', 'a', 'clinical', 'case', 'that', 'focuses', 'on', 'rare', 'diseases', ',', 'unusual', 'presentation', 'of', 'common', 'conditions', 'and', 'novel', 'treatment', 'methods', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'IN', 'JJ', 'NNS', ',', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'CC', 'JJ', 'NN', 'NNS', '.']",27
natural_language_inference,72,35,"Each report contains a Learning points section , summarizing the key pieces of information from that report .","['Each', 'report', 'contains', 'a', 'Learning', 'points', 'section', ',', 'summarizing', 'the', 'key', 'pieces', 'of', 'information', 'from', 'that', 'report', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'DT', 'NNP', 'NNS', 'NN', ',', 'VBG', 'DT', 'JJ', 'NNS', 'IN', 'NN', 'IN', 'DT', 'NN', '.']",18
natural_language_inference,72,37,We use these learning points to create queries by blanking out a medical entity .,"['We', 'use', 'these', 'learning', 'points', 'to', 'create', 'queries', 'by', 'blanking', 'out', 'a', 'medical', 'entity', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'VBG', 'NNS', 'TO', 'VB', 'NNS', 'IN', 'VBG', 'RP', 'DT', 'JJ', 'NN', '.']",15
natural_language_inference,72,39,"Our dataset contains around 100,000 queries on 12,000 case reports , has long support passages ( around 1,500 tokens on average ) and includes answers which are single - or multiword medical entities .","['Our', 'dataset', 'contains', 'around', '100,000', 'queries', 'on', '12,000', 'case', 'reports', ',', 'has', 'long', 'support', 'passages', '(', 'around', '1,500', 'tokens', 'on', 'average', ')', 'and', 'includes', 'answers', 'which', 'are', 'single', '-', 'or', 'multiword', 'medical', 'entities', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'IN', 'CD', 'NNS', 'IN', 'CD', 'NN', 'NNS', ',', 'VBZ', 'JJ', 'NN', 'NNS', '(', 'IN', 'CD', 'NNS', 'IN', 'NN', ')', 'CC', 'VBZ', 'NNS', 'WDT', 'VBP', 'JJ', ':', 'CC', 'VB', 'JJ', 'NNS', '.']",34
natural_language_inference,72,149,We also include a distance - based method that uses word embeddings ( sim-entity ) .,"['We', 'also', 'include', 'a', 'distance', '-', 'based', 'method', 'that', 'uses', 'word', 'embeddings', '(', 'sim-entity', ')', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', ':', 'VBN', 'NN', 'WDT', 'VBZ', 'NN', 'NNS', '(', 'NN', ')', '.']",16
natural_language_inference,72,157,We trained a 4 - gram Kneser - Ney model on CliCR training data ( with multi-word entities represented as a single token ) using SRILM .,"['We', 'trained', 'a', '4', '-', 'gram', 'Kneser', '-', 'Ney', 'model', 'on', 'CliCR', 'training', 'data', '(', 'with', 'multi-word', 'entities', 'represented', 'as', 'a', 'single', 'token', ')', 'using', 'SRILM', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O']","['PRP', 'VBD', 'DT', 'CD', ':', 'NN', 'NNP', ':', 'NNP', 'NN', 'IN', 'NNP', 'NN', 'NNS', '(', 'IN', 'NN', 'NNS', 'VBD', 'IN', 'DT', 'JJ', 'NN', ')', 'VBG', 'NNP', '.']",27
natural_language_inference,72,208,"We see that answer prediction based on contextual representation of queries and passages ( sim -entity ) achieves a strong base performance that is only outperformed by GA 7 In precision , the number of correct words is divided by the number of all predicted words .","['We', 'see', 'that', 'answer', 'prediction', 'based', 'on', 'contextual', 'representation', 'of', 'queries', 'and', 'passages', '(', 'sim', '-entity', ')', 'achieves', 'a', 'strong', 'base', 'performance', 'that', 'is', 'only', 'outperformed', 'by', 'GA', '7', 'In', 'precision', ',', 'the', 'number', 'of', 'correct', 'words', 'is', 'divided', 'by', 'the', 'number', 'of', 'all', 'predicted', 'words', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJR', 'NN', 'VBN', 'IN', 'JJ', 'NN', 'IN', 'NNS', 'CC', 'NNS', '(', 'JJ', 'NN', ')', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'RB', 'VBN', 'IN', 'NNP', 'CD', 'IN', 'NN', ',', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'VBN', 'NNS', '.']",47
natural_language_inference,72,211,"The language model performs poorly on EM and F1 , but the embedding - metric score is higher , likely reflecting the fact that the predicted answers - though mostly incorrect - are related to the ground - truth answers .","['The', 'language', 'model', 'performs', 'poorly', 'on', 'EM', 'and', 'F1', ',', 'but', 'the', 'embedding', '-', 'metric', 'score', 'is', 'higher', ',', 'likely', 'reflecting', 'the', 'fact', 'that', 'the', 'predicted', 'answers', '-', 'though', 'mostly', 'incorrect', '-', 'are', 'related', 'to', 'the', 'ground', '-', 'truth', 'answers', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', 'NNS', 'RB', 'IN', 'NNP', 'CC', 'NNP', ',', 'CC', 'DT', 'VBG', ':', 'JJ', 'NN', 'VBZ', 'JJR', ',', 'JJ', 'VBG', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNS', ':', 'IN', 'RB', 'JJ', ':', 'VBP', 'VBN', 'TO', 'DT', 'NN', ':', 'NN', 'NNS', '.']",41
natural_language_inference,72,213,"The GA reader performs well across all entity set - ups , even when the entities are not marked in the passage .","['The', 'GA', 'reader', 'performs', 'well', 'across', 'all', 'entity', 'set', '-', 'ups', ',', 'even', 'when', 'the', 'entities', 'are', 'not', 'marked', 'in', 'the', 'passage', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NN', 'NNS', 'RB', 'IN', 'DT', 'NN', 'VBN', ':', 'NNS', ',', 'RB', 'WRB', 'DT', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'DT', 'NN', '.']",23
natural_language_inference,72,215,"Upon inspecting the predicted answers more closely , we have observed that GA - NoEnt tends to predict longer answers than GA - Ent / Anonym .","['Upon', 'inspecting', 'the', 'predicted', 'answers', 'more', 'closely', ',', 'we', 'have', 'observed', 'that', 'GA', '-', 'NoEnt', 'tends', 'to', 'predict', 'longer', 'answers', 'than', 'GA', '-', 'Ent', '/', 'Anonym', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBG', 'DT', 'JJ', 'NNS', 'RBR', 'RB', ',', 'PRP', 'VBP', 'VBN', 'IN', 'NNP', ':', 'NNP', 'VBZ', 'TO', 'VB', 'JJR', 'NNS', 'IN', 'NNP', ':', 'NNP', 'NNP', 'NNP', '.']",27
natural_language_inference,72,223,The results for SA reader are far below the per-formance of GA reader .,"['The', 'results', 'for', 'SA', 'reader', 'are', 'far', 'below', 'the', 'per-formance', 'of', 'GA', 'reader', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'NNP', 'NN', 'VBP', 'RB', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NN', '.']",14
natural_language_inference,72,224,We also see that it performs much better on anonymized entities than on non-anonymized ones .,"['We', 'also', 'see', 'that', 'it', 'performs', 'much', 'better', 'on', 'anonymized', 'entities', 'than', 'on', 'non-anonymized', 'ones', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'IN', 'PRP', 'VBZ', 'RB', 'JJR', 'IN', 'JJ', 'NNS', 'IN', 'IN', 'JJ', 'NNS', '.']",16
natural_language_inference,29,2,Product - Aware Answer Generation in E - Commerce Question - Answering,"['Product', '-', 'Aware', 'Answer', 'Generation', 'in', 'E', '-', 'Commerce', 'Question', '-', 'Answering']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'JJ', 'NNP', 'NNP', 'IN', 'NNP', ':', 'NNP', 'NNP', ':', 'NN']",12
natural_language_inference,29,14,"In recent years , the explosive popularity of question - answering ( QA ) is revitalizing the task of reading comprehension with promising results .","['In', 'recent', 'years', ',', 'the', 'explosive', 'popularity', 'of', 'question', '-', 'answering', '(', 'QA', ')', 'is', 'revitalizing', 'the', 'task', 'of', 'reading', 'comprehension', 'with', 'promising', 'results', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['IN', 'JJ', 'NNS', ',', 'DT', 'JJ', 'NN', 'IN', 'NN', ':', 'NN', '(', 'NNP', ')', 'VBZ', 'VBG', 'DT', 'NN', 'IN', 'VBG', 'NN', 'IN', 'JJ', 'NNS', '.']",25
natural_language_inference,29,38,"In this paper , we propose the product - aware answer generator ( PAAG ) , a product related question answering model which incorporates customer reviews with product attributes .","['In', 'this', 'paper', ',', 'we', 'propose', 'the', 'product', '-', 'aware', 'answer', 'generator', '(', 'PAAG', ')', ',', 'a', 'product', 'related', 'question', 'answering', 'model', 'which', 'incorporates', 'customer', 'reviews', 'with', 'product', 'attributes', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', ':', 'JJ', 'NN', 'NN', '(', 'NNP', ')', ',', 'DT', 'NN', 'VBN', 'NN', 'VBG', 'NN', 'WDT', 'VBZ', 'NN', 'NNS', 'IN', 'NN', 'NNS', '.']",30
natural_language_inference,29,39,"Specifically , at the beginning we employ an attention mechanism to model interactions between a question and reviews .","['Specifically', ',', 'at', 'the', 'beginning', 'we', 'employ', 'an', 'attention', 'mechanism', 'to', 'model', 'interactions', 'between', 'a', 'question', 'and', 'reviews', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'IN', 'DT', 'NN', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'VB', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NN', '.']",19
natural_language_inference,29,40,"Simultaneously , we employ a key - value memory network to store the product attributes and extract the relevance values according to the question .","['Simultaneously', ',', 'we', 'employ', 'a', 'key', '-', 'value', 'memory', 'network', 'to', 'store', 'the', 'product', 'attributes', 'and', 'extract', 'the', 'relevance', 'values', 'according', 'to', 'the', 'question', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'JJ', ':', 'NN', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'VBZ', 'CC', 'VB', 'DT', 'NN', 'NNS', 'VBG', 'TO', 'DT', 'NN', '.']",25
natural_language_inference,29,41,"Eventually , we propose a recurrent neural network ( RNN ) based decoder , which combines product - aware review representation and attributes to generate the answer .","['Eventually', ',', 'we', 'propose', 'a', 'recurrent', 'neural', 'network', '(', 'RNN', ')', 'based', 'decoder', ',', 'which', 'combines', 'product', '-', 'aware', 'review', 'representation', 'and', 'attributes', 'to', 'generate', 'the', 'answer', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'JJ', 'NN', '(', 'NNP', ')', 'VBN', 'NN', ',', 'WDT', 'VBZ', 'NN', ':', 'JJ', 'NN', 'NN', 'CC', 'VBZ', 'TO', 'VB', 'DT', 'NN', '.']",28
natural_language_inference,29,42,"More importantly , to tackle the problem of meaningless answers , we propose an adversarial learning mechanism in the loss calculation for optimizing parameters .","['More', 'importantly', ',', 'to', 'tackle', 'the', 'problem', 'of', 'meaningless', 'answers', ',', 'we', 'propose', 'an', 'adversarial', 'learning', 'mechanism', 'in', 'the', 'loss', 'calculation', 'for', 'optimizing', 'parameters', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['RBR', 'RB', ',', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'VBG', 'NNS', '.']",25
natural_language_inference,29,269,( 1 ) S2SA : Sequence - to - sequence framework has been proposed for language generation task .,"['(', '1', ')', 'S2SA', ':', 'Sequence', '-', 'to', '-', 'sequence', 'framework', 'has', 'been', 'proposed', 'for', 'language', 'generation', 'task', '.']","['O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'NN', ':', 'NNP', ':', 'TO', ':', 'NN', 'NN', 'VBZ', 'VBN', 'VBN', 'IN', 'NN', 'NN', 'NN', '.']",19
natural_language_inference,29,272,( 2 ) S2SAR : We implement a simple method which can incorporate the review information when generating the answer .,"['(', '2', ')', 'S2SAR', ':', 'We', 'implement', 'a', 'simple', 'method', 'which', 'can', 'incorporate', 'the', 'review', 'information', 'when', 'generating', 'the', 'answer', '.']","['O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['(', 'CD', ')', 'NN', ':', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'WDT', 'MD', 'VB', 'DT', 'NN', 'NN', 'WRB', 'VBG', 'DT', 'NN', '.']",21
natural_language_inference,29,274,( 3 ) SNet : S- Net is a two - stage state - of - the - art model which extracts some text spans from multiple documents context and synthesis the answer from those spans .,"['(', '3', ')', 'SNet', ':', 'S-', 'Net', 'is', 'a', 'two', '-', 'stage', 'state', '-', 'of', '-', 'the', '-', 'art', 'model', 'which', 'extracts', 'some', 'text', 'spans', 'from', 'multiple', 'documents', 'context', 'and', 'synthesis', 'the', 'answer', 'from', 'those', 'spans', '.']","['O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']","['(', 'CD', ')', 'NN', ':', 'JJ', 'NN', 'VBZ', 'DT', 'CD', ':', 'NN', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'NNS', 'IN', 'JJ', 'NNS', 'NN', 'CC', 'NN', 'DT', 'NN', 'IN', 'DT', 'NNS', '.']",37
natural_language_inference,29,277,( 4 ) QS : We implement the query - based summarization model proposed by Hasselqvist et al ..,"['(', '4', ')', 'QS', ':', 'We', 'implement', 'the', 'query', '-', 'based', 'summarization', 'model', 'proposed', 'by', 'Hasselqvist', 'et', 'al', '..']","['O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'CD', ')', 'NN', ':', 'PRP', 'VBP', 'DT', 'NN', ':', 'VBN', 'NN', 'NN', 'VBN', 'IN', 'NNP', 'CC', 'NN', 'NN']",19
natural_language_inference,29,279,( 5 ) BM25 : BM25 is a bag - of - words retrieval function that ranks a set of reviews based on the question terms appearing in each review .,"['(', '5', ')', 'BM25', ':', 'BM25', 'is', 'a', 'bag', '-', 'of', '-', 'words', 'retrieval', 'function', 'that', 'ranks', 'a', 'set', 'of', 'reviews', 'based', 'on', 'the', 'question', 'terms', 'appearing', 'in', 'each', 'review', '.']","['O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['(', 'CD', ')', 'NN', ':', 'NNP', 'VBZ', 'DT', 'JJ', ':', 'IN', ':', 'NNS', 'JJ', 'NN', 'IN', 'VBZ', 'DT', 'NN', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'NNS', 'VBG', 'IN', 'DT', 'NN', '.']",31
natural_language_inference,29,281,( 6 ) TF - IDF : Term Frequency - Inverse Document Frequency is a numerical statistic that is intended to reflect how important a question word is to a review .,"['(', '6', ')', 'TF', '-', 'IDF', ':', 'Term', 'Frequency', '-', 'Inverse', 'Document', 'Frequency', 'is', 'a', 'numerical', 'statistic', 'that', 'is', 'intended', 'to', 'reflect', 'how', 'important', 'a', 'question', 'word', 'is', 'to', 'a', 'review', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O']","['(', 'CD', ')', 'NNP', ':', 'NN', ':', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'VBN', 'TO', 'VB', 'WRB', 'JJ', 'DT', 'NN', 'NN', 'VBZ', 'TO', 'DT', 'NN', '.']",32
natural_language_inference,29,284,"Without using pre-trained embeddings , we randomly initialize the network parameters at the beginning of our experiments .","['Without', 'using', 'pre-trained', 'embeddings', ',', 'we', 'randomly', 'initialize', 'the', 'network', 'parameters', 'at', 'the', 'beginning', 'of', 'our', 'experiments', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']","['IN', 'VBG', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'VB', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'PRP$', 'NNS', '.']",18
natural_language_inference,29,285,All the RNN networks have 512 hidden units and the dimension of word embedding is 256 .,"['All', 'the', 'RNN', 'networks', 'have', '512', 'hidden', 'units', 'and', 'the', 'dimension', 'of', 'word', 'embedding', 'is', '256', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PDT', 'DT', 'NNP', 'NNS', 'VBP', 'CD', 'VBN', 'NNS', 'CC', 'DT', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'CD', '.']",17
natural_language_inference,29,286,"To produce better answers , we use beam search with beam size","['To', 'produce', 'better', 'answers', ',', 'we', 'use', 'beam', 'search', 'with', 'beam', 'size']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n']","['TO', 'VB', 'JJR', 'NNS', ',', 'PRP', 'VBP', 'JJ', 'NN', 'IN', 'NN', 'NN']",12
natural_language_inference,29,287,4 .,"['4', '.']","['B-n', 'O']","['CD', '.']",2
natural_language_inference,29,288,Adagrad with learning rate 0.1 is used to optimize the parameters and batch size is 64 .,"['Adagrad', 'with', 'learning', 'rate', '0.1', 'is', 'used', 'to', 'optimize', 'the', 'parameters', 'and', 'batch', 'size', 'is', '64', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'B-n', 'B-p', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['NNP', 'IN', 'VBG', 'NN', 'CD', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NNS', 'CC', 'NN', 'NN', 'VBZ', 'CD', '.']",17
natural_language_inference,29,289,We implement our model using TensorFlow framework and train our model and all baseline models on NVIDIA Tesla P40 GPU .,"['We', 'implement', 'our', 'model', 'using', 'TensorFlow', 'framework', 'and', 'train', 'our', 'model', 'and', 'all', 'baseline', 'models', 'on', 'NVIDIA', 'Tesla', 'P40', 'GPU', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'VBG', 'NNP', 'NN', 'CC', 'VB', 'PRP$', 'NN', 'CC', 'DT', 'NN', 'NNS', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', '.']",21
natural_language_inference,29,294,"In these experimental results , we see that PAAG achieves a 111 % , 8 % and 62.73 % increment over the stateof - the - art baseline SNet in terms of BLEU , embedding greedy and consistency score , respectively .","['In', 'these', 'experimental', 'results', ',', 'we', 'see', 'that', 'PAAG', 'achieves', 'a', '111', '%', ',', '8', '%', 'and', '62.73', '%', 'increment', 'over', 'the', 'stateof', '-', 'the', '-', 'art', 'baseline', 'SNet', 'in', 'terms', 'of', 'BLEU', ',', 'embedding', 'greedy', 'and', 'consistency', 'score', ',', 'respectively', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'IN', 'NNP', 'VBZ', 'DT', 'CD', 'NN', ',', 'CD', 'NN', 'CC', 'CD', 'NN', 'NN', 'IN', 'DT', 'JJ', ':', 'DT', ':', 'NN', 'NN', 'NNP', 'IN', 'NNS', 'IN', 'NNP', ',', 'VBG', 'NN', 'CC', 'NN', 'NN', ',', 'RB', '.']",42
natural_language_inference,29,295,"In , we see that our PAAG outperforms all the baseline significantly in semantic distance with respect to the ground truth .","['In', ',', 'we', 'see', 'that', 'our', 'PAAG', 'outperforms', 'all', 'the', 'baseline', 'significantly', 'in', 'semantic', 'distance', 'with', 'respect', 'to', 'the', 'ground', 'truth', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', ',', 'PRP', 'VBP', 'IN', 'PRP$', 'NNP', 'VBZ', 'PDT', 'DT', 'NN', 'RB', 'IN', 'JJ', 'NN', 'IN', 'NN', 'TO', 'DT', 'NN', 'NN', '.']",22
natural_language_inference,29,299,"In , we can see that PAAG outperforms other baseline models in both sentence fluency and consistency with the facts .","['In', ',', 'we', 'can', 'see', 'that', 'PAAG', 'outperforms', 'other', 'baseline', 'models', 'in', 'both', 'sentence', 'fluency', 'and', 'consistency', 'with', 'the', 'facts', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['IN', ',', 'PRP', 'MD', 'VB', 'DT', 'NNP', 'VBZ', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'CC', 'NN', 'IN', 'DT', 'NNS', '.']",21
natural_language_inference,29,305,"Although there is a small increment of S2 SAR with respect to S2SA in all metrics , we still find a noticeable gap between S2SAR and PAAG .","['Although', 'there', 'is', 'a', 'small', 'increment', 'of', 'S2', 'SAR', 'with', 'respect', 'to', 'S2SA', 'in', 'all', 'metrics', ',', 'we', 'still', 'find', 'a', 'noticeable', 'gap', 'between', 'S2SAR', 'and', 'PAAG', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'EX', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'IN', 'NN', 'TO', 'NNP', 'IN', 'DT', 'NNS', ',', 'PRP', 'RB', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'CC', 'NNP', '.']",28
natural_language_inference,29,323,"There is a slight increment from RAGF to RAGFD , which demonstrates the effectiveness of discriminator .","['There', 'is', 'a', 'slight', 'increment', 'from', 'RAGF', 'to', 'RAGFD', ',', 'which', 'demonstrates', 'the', 'effectiveness', 'of', 'discriminator', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['EX', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'TO', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'NN', '.']",17
natural_language_inference,29,324,"From , we find that RAGFWD achieves a 4.3 % improvement over RAGFD in terms of BLEU , and PAAG outperforms RAGFWD 4.1 % in terms of BLEU .","['From', ',', 'we', 'find', 'that', 'RAGFWD', 'achieves', 'a', '4.3', '%', 'improvement', 'over', 'RAGFD', 'in', 'terms', 'of', 'BLEU', ',', 'and', 'PAAG', 'outperforms', 'RAGFWD', '4.1', '%', 'in', 'terms', 'of', 'BLEU', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'B-n', 'B-n', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['IN', ',', 'PRP', 'VBP', 'IN', 'NNP', 'VBZ', 'DT', 'CD', 'NN', 'NN', 'IN', 'NNP', 'IN', 'NNS', 'IN', 'NNP', ',', 'CC', 'NNP', 'VBZ', 'NNP', 'CD', 'NN', 'IN', 'NNS', 'IN', 'NNP', '.']",29
natural_language_inference,29,325,"Accordingly , we conclude that the performance of PAAG benefits from using Wasserstein distance based adversarial learning with gradient penalty .","['Accordingly', ',', 'we', 'conclude', 'that', 'the', 'performance', 'of', 'PAAG', 'benefits', 'from', 'using', 'Wasserstein', 'distance', 'based', 'adversarial', 'learning', 'with', 'gradient', 'penalty', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NNS', 'IN', 'VBG', 'NNP', 'NN', 'VBN', 'JJ', 'VBG', 'IN', 'NN', 'NN', '.']",21
natural_language_inference,29,326,This approach can help our model to achieve a better performance than the model using the vanilla GAN architecture .,"['This', 'approach', 'can', 'help', 'our', 'model', 'to', 'achieve', 'a', 'better', 'performance', 'than', 'the', 'model', 'using', 'the', 'vanilla', 'GAN', 'architecture', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'MD', 'VB', 'PRP$', 'NN', 'TO', 'VB', 'DT', 'JJR', 'NN', 'IN', 'DT', 'NN', 'VBG', 'DT', 'NN', 'NNP', 'NN', '.']",20
natural_language_inference,17,2,Reinforced Mnemonic Reader for Machine Reading Comprehension,"['Reinforced', 'Mnemonic', 'Reader', 'for', 'Machine', 'Reading', 'Comprehension']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",7
natural_language_inference,17,27,"To address the first problem , we present a reattention mechanism that temporally memorizes past attentions and uses them to refine current attentions in a multi-round alignment architecture .","['To', 'address', 'the', 'first', 'problem', ',', 'we', 'present', 'a', 'reattention', 'mechanism', 'that', 'temporally', 'memorizes', 'past', 'attentions', 'and', 'uses', 'them', 'to', 'refine', 'current', 'attentions', 'in', 'a', 'multi-round', 'alignment', 'architecture', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'WDT', 'RB', 'VBZ', 'JJ', 'NNS', 'CC', 'VBZ', 'PRP', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",29
natural_language_inference,17,28,"The computation is based on the fact that two words should share similar semantics if their attentions about same texts are highly overlapped , and be less similar vice versa .","['The', 'computation', 'is', 'based', 'on', 'the', 'fact', 'that', 'two', 'words', 'should', 'share', 'similar', 'semantics', 'if', 'their', 'attentions', 'about', 'same', 'texts', 'are', 'highly', 'overlapped', ',', 'and', 'be', 'less', 'similar', 'vice', 'versa', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'IN', 'CD', 'NNS', 'MD', 'NN', 'JJ', 'NNS', 'IN', 'PRP$', 'NNS', 'IN', 'JJ', 'NNS', 'VBP', 'RB', 'VBN', ',', 'CC', 'VB', 'JJR', 'JJ', 'NN', 'NN', '.']",31
natural_language_inference,17,29,"Therefore , the reattention can be more concentrated if past attentions focus on same parts of the input , or be relatively more distracted so as to focus on new regions if past attentions are not overlapped at all .","['Therefore', ',', 'the', 'reattention', 'can', 'be', 'more', 'concentrated', 'if', 'past', 'attentions', 'focus', 'on', 'same', 'parts', 'of', 'the', 'input', ',', 'or', 'be', 'relatively', 'more', 'distracted', 'so', 'as', 'to', 'focus', 'on', 'new', 'regions', 'if', 'past', 'attentions', 'are', 'not', 'overlapped', 'at', 'all', '.']","['O', 'O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'MD', 'VB', 'RBR', 'JJ', 'IN', 'JJ', 'NNS', 'VBP', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', ',', 'CC', 'VB', 'RB', 'RBR', 'JJ', 'RB', 'IN', 'TO', 'VB', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'DT', '.']",40
natural_language_inference,17,30,"As for the second problem , we extend the traditional training method with a novel approach called dynamic - critical reinforcement learning .","['As', 'for', 'the', 'second', 'problem', ',', 'we', 'extend', 'the', 'traditional', 'training', 'method', 'with', 'a', 'novel', 'approach', 'called', 'dynamic', '-', 'critical', 'reinforcement', 'learning', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'IN', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBN', 'JJ', ':', 'JJ', 'NN', 'NN', '.']",23
natural_language_inference,17,31,"Unlike the traditional reinforcement learning algorithm where the reward and baseline are statically sampled , our approach dynamically decides the reward and the baseline according to two sampling strategies , Context : The American Football Conference ( AFC ) champion Denver Broncos defeated the National Football Conference ( NFC ) champion Carolina Panthers 24 - 10 to earn their third Super Bowl title .","['Unlike', 'the', 'traditional', 'reinforcement', 'learning', 'algorithm', 'where', 'the', 'reward', 'and', 'baseline', 'are', 'statically', 'sampled', ',', 'our', 'approach', 'dynamically', 'decides', 'the', 'reward', 'and', 'the', 'baseline', 'according', 'to', 'two', 'sampling', 'strategies', ',', 'Context', ':', 'The', 'American', 'Football', 'Conference', '(', 'AFC', ')', 'champion', 'Denver', 'Broncos', 'defeated', 'the', 'National', 'Football', 'Conference', '(', 'NFC', ')', 'champion', 'Carolina', 'Panthers', '24', '-', '10', 'to', 'earn', 'their', 'third', 'Super', 'Bowl', 'title', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', 'NN', 'NN', 'WRB', 'DT', 'NN', 'CC', 'NN', 'VBP', 'RB', 'VBN', ',', 'PRP$', 'NN', 'RB', 'VBZ', 'DT', 'NN', 'CC', 'DT', 'NN', 'VBG', 'TO', 'CD', 'VBG', 'NNS', ',', 'NNP', ':', 'DT', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', 'NNP', 'NNP', 'VBD', 'DT', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', 'NNP', 'NNP', 'CD', ':', 'CD', 'TO', 'VB', 'PRP$', 'JJ', 'NNP', 'NNP', 'NN', '.']",64
natural_language_inference,17,190,"We use the Adam optimizer [ Kingma and Ba , 2014 ] for both ML and DCRL training .","['We', 'use', 'the', 'Adam', 'optimizer', '[', 'Kingma', 'and', 'Ba', ',', '2014', ']', 'for', 'both', 'ML', 'and', 'DCRL', 'training', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', 'NNP', 'NNP', 'CC', 'NNP', ',', 'CD', 'NN', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NN', '.']",19
natural_language_inference,17,191,"The initial learning rates are 0.0008 and 0.0001 respectively , and are halved whenever meeting a bad iteration .","['The', 'initial', 'learning', 'rates', 'are', '0.0008', 'and', '0.0001', 'respectively', ',', 'and', 'are', 'halved', 'whenever', 'meeting', 'a', 'bad', 'iteration', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NNS', 'VBP', 'CD', 'CC', 'CD', 'RB', ',', 'CC', 'VBP', 'VBN', 'WRB', 'VBG', 'DT', 'JJ', 'NN', '.']",19
natural_language_inference,17,192,The batch size is 48 and a dropout rate of 0.3 is used to prevent overfitting .,"['The', 'batch', 'size', 'is', '48', 'and', 'a', 'dropout', 'rate', 'of', '0.3', 'is', 'used', 'to', 'prevent', 'overfitting', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'CD', 'CC', 'DT', 'NN', 'NN', 'IN', 'CD', 'VBZ', 'VBN', 'TO', 'VB', 'NN', '.']",17
natural_language_inference,17,193,Word embeddings remain fixed during training .,"['Word', 'embeddings', 'remain', 'fixed', 'during', 'training', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['NNP', 'VBZ', 'VBP', 'JJ', 'IN', 'NN', '.']",7
natural_language_inference,17,194,"For out of vocabulary words , we set the embeddings from Gaussian distributions and keep them trainable .","['For', 'out', 'of', 'vocabulary', 'words', ',', 'we', 'set', 'the', 'embeddings', 'from', 'Gaussian', 'distributions', 'and', 'keep', 'them', 'trainable', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'IN', 'IN', 'JJ', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NNS', 'IN', 'JJ', 'NNS', 'CC', 'VB', 'PRP', 'JJ', '.']",18
natural_language_inference,17,195,"The size of character embedding and corresponding LSTMs is 50 , the main hidden size is 100 , and the hyperparameter ? is 3 .","['The', 'size', 'of', 'character', 'embedding', 'and', 'corresponding', 'LSTMs', 'is', '50', ',', 'the', 'main', 'hidden', 'size', 'is', '100', ',', 'and', 'the', 'hyperparameter', '?', 'is', '3', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NN', 'NN', 'CC', 'VBG', 'NNP', 'VBZ', 'CD', ',', 'DT', 'JJ', 'JJ', 'NN', 'VBZ', 'CD', ',', 'CC', 'DT', 'NN', '.', 'VBZ', 'CD', '.']",25
natural_language_inference,17,198,We submitted our model on the hidden test set of SQuAD for evaluation .,"['We', 'submitted', 'our', 'model', 'on', 'the', 'hidden', 'test', 'set', 'of', 'SQuAD', 'for', 'evaluation', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O']","['PRP', 'VBD', 'PRP$', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'IN', 'NN', '.']",14
natural_language_inference,17,200,"As shown in , R.M - Reader achieves an EM score of 79.5 % and F1 score of 86.6 % .","['As', 'shown', 'in', ',', 'R.M', '-', 'Reader', 'achieves', 'an', 'EM', 'score', 'of', '79.5', '%', 'and', 'F1', 'score', 'of', '86.6', '%', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'VBN', 'IN', ',', 'NNP', ':', 'NN', 'VBZ', 'DT', 'NNP', 'NN', 'IN', 'CD', 'NN', 'CC', 'NNP', 'NN', 'IN', 'CD', 'NN', '.']",21
natural_language_inference,17,202,"Our ensemble model improves the metrics to 82.3 % and 88.5 % respectively 2 . shows the performance comparison on two adversarial datasets , Add Sent and Add OneSent .","['Our', 'ensemble', 'model', 'improves', 'the', 'metrics', 'to', '82.3', '%', 'and', '88.5', '%', 'respectively', '2', '.', 'shows', 'the', 'performance', 'comparison', 'on', 'two', 'adversarial', 'datasets', ',', 'Add', 'Sent', 'and', 'Add', 'OneSent', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'JJ', 'NN', 'VBZ', 'DT', 'NNS', 'TO', 'CD', 'NN', 'CC', 'CD', 'NN', 'RB', 'CD', '.', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'CD', 'JJ', 'NNS', ',', 'NNP', 'NNP', 'CC', 'NNP', 'NNP', '.']",30
natural_language_inference,17,204,"As we can see , R.M - Reader comfortably outperforms all previous models by more than 6 % in both EM and F 1 scores , indicating that our model is more robust against adversarial attacks .","['As', 'we', 'can', 'see', ',', 'R.M', '-', 'Reader', 'comfortably', 'outperforms', 'all', 'previous', 'models', 'by', 'more', 'than', '6', '%', 'in', 'both', 'EM', 'and', 'F', '1', 'scores', ',', 'indicating', 'that', 'our', 'model', 'is', 'more', 'robust', 'against', 'adversarial', 'attacks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PRP', 'MD', 'VB', ',', 'NNP', ':', 'NN', 'RB', 'VBZ', 'DT', 'JJ', 'NNS', 'IN', 'JJR', 'IN', 'CD', 'NN', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'CD', 'NNS', ',', 'VBG', 'IN', 'PRP$', 'NN', 'VBZ', 'RBR', 'JJ', 'IN', 'JJ', 'NNS', '.']",37
natural_language_inference,17,208,"We notice that reattention has more influences on EM score while DCRL contributes more to F1 metric , and removing both of them results in huge drops on both metrics .","['We', 'notice', 'that', 'reattention', 'has', 'more', 'influences', 'on', 'EM', 'score', 'while', 'DCRL', 'contributes', 'more', 'to', 'F1', 'metric', ',', 'and', 'removing', 'both', 'of', 'them', 'results', 'in', 'huge', 'drops', 'on', 'both', 'metrics', '.']","['O', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'VBZ', 'JJR', 'NNS', 'IN', 'NNP', 'NN', 'IN', 'NNP', 'VBZ', 'JJR', 'TO', 'NNP', 'JJ', ',', 'CC', 'VBG', 'DT', 'IN', 'PRP', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NNS', '.']",31
natural_language_inference,17,209,Replacing DCRL with SCST also causes a marginal decline of performance on both metrics .,"['Replacing', 'DCRL', 'with', 'SCST', 'also', 'causes', 'a', 'marginal', 'decline', 'of', 'performance', 'on', 'both', 'metrics', '.']","['B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['VBG', 'NNP', 'IN', 'NNP', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'DT', 'NNS', '.']",15
natural_language_inference,17,210,"Next , we relace the default attention function with the dot product : f ( u , v ) = u v ( 5 ) , and both metrics suffer from degradations .","['Next', ',', 'we', 'relace', 'the', 'default', 'attention', 'function', 'with', 'the', 'dot', 'product', ':', 'f', '(', 'u', ',', 'v', ')', '=', 'u', 'v', '(', '5', ')', ',', 'and', 'both', 'metrics', 'suffer', 'from', 'degradations', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['JJ', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'NN', '(', 'JJ', ',', 'NN', ')', 'VBZ', 'JJ', 'NN', '(', 'CD', ')', ',', 'CC', 'DT', 'NNS', 'VBP', 'IN', 'NNS', '.']",33
natural_language_inference,17,212,"Removing any of the two heuristics leads to some performance declines , and heuristic subtraction is more effective than multiplication .","['Removing', 'any', 'of', 'the', 'two', 'heuristics', 'leads', 'to', 'some', 'performance', 'declines', ',', 'and', 'heuristic', 'subtraction', 'is', 'more', 'effective', 'than', 'multiplication', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['VBG', 'DT', 'IN', 'DT', 'CD', 'NNS', 'VBZ', 'TO', 'DT', 'NN', 'NNS', ',', 'CC', 'JJ', 'NN', 'VBZ', 'RBR', 'JJ', 'IN', 'NN', '.']",21
natural_language_inference,17,214,In both cases the highway - like function has outperformed its simpler variants .,"['In', 'both', 'cases', 'the', 'highway', '-', 'like', 'function', 'has', 'outperformed', 'its', 'simpler', 'variants', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNS', 'DT', 'NN', ':', 'IN', 'NN', 'VBZ', 'VBN', 'PRP$', 'NN', 'NNS', '.']",14
natural_language_inference,17,216,"We notice that using 2 blocks causes a slight performance drop , while increasing to 4 blocks barely affects the SoTA result .","['We', 'notice', 'that', 'using', '2', 'blocks', 'causes', 'a', 'slight', 'performance', 'drop', ',', 'while', 'increasing', 'to', '4', 'blocks', 'barely', 'affects', 'the', 'SoTA', 'result', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'VBG', 'CD', 'NNS', 'VBZ', 'DT', 'JJ', 'NN', 'NN', ',', 'IN', 'VBG', 'TO', 'CD', 'NNS', 'RB', 'VBZ', 'DT', 'NNP', 'NN', '.']",23
natural_language_inference,17,217,"Interestingly , a very deep alignment with 5 blocks results in a significant performance decline .","['Interestingly', ',', 'a', 'very', 'deep', 'alignment', 'with', '5', 'blocks', 'results', 'in', 'a', 'significant', 'performance', 'decline', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'RB', 'JJ', 'NN', 'IN', 'CD', 'NNS', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",16
natural_language_inference,2,2,A Question - Focused Multi- Factor Attention Network for Question Answering,"['A', 'Question', '-', 'Focused', 'Multi-', 'Factor', 'Attention', 'Network', 'for', 'Question', 'Answering']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['DT', 'NNP', ':', 'VBD', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",11
natural_language_inference,2,4,Neural network models recently proposed for question answering ( QA ) primarily focus on capturing the passagequestion relation .,"['Neural', 'network', 'models', 'recently', 'proposed', 'for', 'question', 'answering', '(', 'QA', ')', 'primarily', 'focus', 'on', 'capturing', 'the', 'passagequestion', 'relation', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'NNS', 'RB', 'VBD', 'IN', 'NN', 'NN', '(', 'NNP', ')', 'RB', 'VB', 'IN', 'VBG', 'DT', 'NN', 'NN', '.']",19
natural_language_inference,2,6,They also do not explicitly focus on the question and answer type which often plays a critical role in QA .,"['They', 'also', 'do', 'not', 'explicitly', 'focus', 'on', 'the', 'question', 'and', 'answer', 'type', 'which', 'often', 'plays', 'a', 'critical', 'role', 'in', 'QA', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']","['PRP', 'RB', 'VBP', 'RB', 'RB', 'VB', 'IN', 'DT', 'NN', 'CC', 'NN', 'NN', 'WDT', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', '.']",21
natural_language_inference,2,13,"In machine comprehension - based ( MC ) question answering ( QA ) , a machine is expected to provide an answer for a given question by understanding texts .","['In', 'machine', 'comprehension', '-', 'based', '(', 'MC', ')', 'question', 'answering', '(', 'QA', ')', ',', 'a', 'machine', 'is', 'expected', 'to', 'provide', 'an', 'answer', 'for', 'a', 'given', 'question', 'by', 'understanding', 'texts', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', 'NN', ':', 'VBN', '(', 'NNP', ')', 'NN', 'VBG', '(', 'NNP', ')', ',', 'DT', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'VBN', 'NN', 'IN', 'VBG', 'NN', '.']",30
natural_language_inference,2,33,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the answer .","['In', 'this', 'work', ',', 'we', 'propose', 'an', 'end', '-', 'to', '-', 'end', 'question', '-', 'focused', 'multi-factor', 'attention', 'network', 'for', 'document', '-', 'based', 'question', 'answering', '(', 'AMANDA', ')', ',', 'which', 'learns', 'to', 'aggregate', 'evidence', 'distributed', 'across', 'multiple', 'sentences', 'and', 'identifies', 'the', 'important', 'question', 'words', 'to', 'help', 'extract', 'the', 'answer', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', ':', 'TO', ':', 'VB', 'NN', ':', 'VBD', 'NN', 'NN', 'NN', 'IN', 'NN', ':', 'VBN', 'NN', 'VBG', '(', 'NNP', ')', ',', 'WDT', 'VBZ', 'TO', 'VB', 'NN', 'VBN', 'IN', 'JJ', 'NNS', 'CC', 'VBZ', 'DT', 'JJ', 'NN', 'NNS', 'TO', 'VB', 'VB', 'DT', 'NN', '.']",49
natural_language_inference,2,34,"Intuitively , AMANDA extracts the answer not only by synthesizing relevant facts from the passage but also by implicitly determining the suitable answer type during prediction .","['Intuitively', ',', 'AMANDA', 'extracts', 'the', 'answer', 'not', 'only', 'by', 'synthesizing', 'relevant', 'facts', 'from', 'the', 'passage', 'but', 'also', 'by', 'implicitly', 'determining', 'the', 'suitable', 'answer', 'type', 'during', 'prediction', '.']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['RB', ',', 'NNP', 'VBZ', 'DT', 'NN', 'RB', 'RB', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CC', 'RB', 'IN', 'RB', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NN', '.']",27
natural_language_inference,2,174,We tokenize the corpora with NLTK 2 .,"['We', 'tokenize', 'the', 'corpora', 'with', 'NLTK', '2', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'NNP', 'CD', '.']",8
natural_language_inference,2,175,"We use the 300 dimension pre-trained word vectors from GloVe ( Pennington , Socher , and Manning 2014 ) and we do not update them during training .","['We', 'use', 'the', '300', 'dimension', 'pre-trained', 'word', 'vectors', 'from', 'GloVe', '(', 'Pennington', ',', 'Socher', ',', 'and', 'Manning', '2014', ')', 'and', 'we', 'do', 'not', 'update', 'them', 'during', 'training', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'CD', 'NN', 'JJ', 'NN', 'NNS', 'IN', 'NNP', '(', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', 'CD', ')', 'CC', 'PRP', 'VBP', 'RB', 'VB', 'PRP', 'IN', 'NN', '.']",28
natural_language_inference,2,176,The out - of - vocabulary words are initialized with zero vectors .,"['The', 'out', '-', 'of', '-', 'vocabulary', 'words', 'are', 'initialized', 'with', 'zero', 'vectors', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'RP', ':', 'IN', ':', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'NN', 'NNS', '.']",13
natural_language_inference,2,177,We use 50 - dimension character - level embedding vectors .,"['We', 'use', '50', '-', 'dimension', 'character', '-', 'level', 'embedding', 'vectors', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', ':', 'NN', 'NN', ':', 'NN', 'VBG', 'NNS', '.']",11
natural_language_inference,2,178,The number of hidden units in all the LSTMs is 150 .,"['The', 'number', 'of', 'hidden', 'units', 'in', 'all', 'the', 'LSTMs', 'is', '150', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'PDT', 'DT', 'NNP', 'VBZ', 'CD', '.']",12
natural_language_inference,2,179,We use dropout ) with probability 0.3 for every learnable layer .,"['We', 'use', 'dropout', ')', 'with', 'probability', '0.3', 'for', 'every', 'learnable', 'layer', '.']","['O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NN', ')', 'IN', 'NN', 'CD', 'IN', 'DT', 'JJ', 'NN', '.']",12
natural_language_inference,2,180,"For multi-factor attentive encoding , we choose 4 factors ( m ) based on our experimental findings ( refer to ) .","['For', 'multi-factor', 'attentive', 'encoding', ',', 'we', 'choose', '4', 'factors', '(', 'm', ')', 'based', 'on', 'our', 'experimental', 'findings', '(', 'refer', 'to', ')', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', 'JJ', 'NN', ',', 'PRP', 'VBP', 'CD', 'NNS', '(', 'NN', ')', 'VBN', 'IN', 'PRP$', 'JJ', 'NNS', '(', 'VB', 'TO', ')', '.']",22
natural_language_inference,2,181,"During training , the minibatch size is fixed at 60 .","['During', 'training', ',', 'the', 'minibatch', 'size', 'is', 'fixed', 'at', '60', '.']","['B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'NN', ',', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', '.']",11
natural_language_inference,2,182,We use the Adam optimizer with learning rate of 0.001 and clipnorm of 5 .,"['We', 'use', 'the', 'Adam', 'optimizer', 'with', 'learning', 'rate', 'of', '0.001', 'and', 'clipnorm', 'of', '5', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', 'IN', 'VBG', 'NN', 'IN', 'CD', 'CC', 'NN', 'IN', 'CD', '.']",15
natural_language_inference,2,185,shows that AMANDA outperforms all the stateof - the - art models by a significant margin on the New s QA dataset .,"['shows', 'that', 'AMANDA', 'outperforms', 'all', 'the', 'stateof', '-', 'the', '-', 'art', 'models', 'by', 'a', 'significant', 'margin', 'on', 'the', 'New', 's', 'QA', 'dataset', '.']","['B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'WDT', 'VBP', 'VBZ', 'PDT', 'DT', 'NN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NNP', 'NN', '.']",23
natural_language_inference,2,186,shows the results on the TriviaQA dataset .,"['shows', 'the', 'results', 'on', 'the', 'TriviaQA', 'dataset', '.']","['B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['VBZ', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'NN', '.']",8
natural_language_inference,2,191,shows that AMANDA achieves state - of the - art results in both Wikipedia and Web domain on distantly supervised and verified data .,"['shows', 'that', 'AMANDA', 'achieves', 'state', '-', 'of', 'the', '-', 'art', 'results', 'in', 'both', 'Wikipedia', 'and', 'Web', 'domain', 'on', 'distantly', 'supervised', 'and', 'verified', 'data', '.']","['B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'WDT', 'VBP', 'NNS', 'NN', ':', 'IN', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'VBP', 'IN', 'RB', 'VBN', 'CC', 'VBN', 'NNS', '.']",24
natural_language_inference,2,193,Results on the Search QA dataset are shown in .,"['Results', 'on', 'the', 'Search', 'QA', 'dataset', 'are', 'shown', 'in', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NNS', 'IN', 'DT', 'NNP', 'NNP', 'NN', 'VBP', 'VBN', 'IN', '.']",10
natural_language_inference,2,200,"AMANDA outperforms both systems , especially for multi-word - answer questions by a huge margin .","['AMANDA', 'outperforms', 'both', 'systems', ',', 'especially', 'for', 'multi-word', '-', 'answer', 'questions', 'by', 'a', 'huge', 'margin', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NN', 'VBZ', 'DT', 'NNS', ',', 'RB', 'IN', 'JJ', ':', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",16
natural_language_inference,2,202,"shows that AMANDA performs better than any of the ablated models which include the ablation of multifactor attentive encoding , max - attentional question aggregation ( q ma ) , and question type representation ( q f ) .","['shows', 'that', 'AMANDA', 'performs', 'better', 'than', 'any', 'of', 'the', 'ablated', 'models', 'which', 'include', 'the', 'ablation', 'of', 'multifactor', 'attentive', 'encoding', ',', 'max', '-', 'attentional', 'question', 'aggregation', '(', 'q', 'ma', ')', ',', 'and', 'question', 'type', 'representation', '(', 'q', 'f', ')', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'WDT', 'VBP', 'NNS', 'RBR', 'IN', 'DT', 'IN', 'DT', 'JJ', 'NNS', 'WDT', 'VBP', 'DT', 'NN', 'IN', 'NN', 'JJ', 'NN', ',', 'SYM', ':', 'JJ', 'NN', 'NN', '(', 'JJ', 'NN', ')', ',', 'CC', 'NN', 'NN', 'NN', '(', 'JJ', 'NN', ')', '.']",39
natural_language_inference,32,2,FLOWQA : GRASPING FLOW IN HISTORY FOR CONVERSATIONAL MACHINE COMPREHENSION,"['FLOWQA', ':', 'GRASPING', 'FLOW', 'IN', 'HISTORY', 'FOR', 'CONVERSATIONAL', 'MACHINE', 'COMPREHENSION']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NN', ':', 'NN', 'NN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",10
natural_language_inference,32,13,"We present FLOWQA , a model designed for conversational machine comprehension .","['We', 'present', 'FLOWQA', ',', 'a', 'model', 'designed', 'for', 'conversational', 'machine', 'comprehension', '.']","['O', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'JJ', 'NNP', ',', 'DT', 'NN', 'VBN', 'IN', 'JJ', 'NN', 'NN', '.']",12
natural_language_inference,32,14,FLOWQA consists of two main components : a base neural model for single - turn MC and a FLOW mechanism that encodes the conversation history .,"['FLOWQA', 'consists', 'of', 'two', 'main', 'components', ':', 'a', 'base', 'neural', 'model', 'for', 'single', '-', 'turn', 'MC', 'and', 'a', 'FLOW', 'mechanism', 'that', 'encodes', 'the', 'conversation', 'history', '.']","['B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'IN', 'CD', 'JJ', 'NNS', ':', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'JJ', ':', 'NN', 'NNP', 'CC', 'DT', 'NNP', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'NN', '.']",26
natural_language_inference,32,15,"Instead of using the shallow history , i.e. , previous questions and answers , we feed the model with the entire hidden representations generated during the process of answering previous questions .","['Instead', 'of', 'using', 'the', 'shallow', 'history', ',', 'i.e.', ',', 'previous', 'questions', 'and', 'answers', ',', 'we', 'feed', 'the', 'model', 'with', 'the', 'entire', 'hidden', 'representations', 'generated', 'during', 'the', 'process', 'of', 'answering', 'previous', 'questions', '.']","['O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', 'IN', 'VBG', 'DT', 'JJ', 'NN', ',', 'FW', ',', 'JJ', 'NNS', 'CC', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNS', 'VBD', 'IN', 'DT', 'NN', 'IN', 'VBG', 'JJ', 'NNS', '.']",32
natural_language_inference,32,17,"This FLOW mechanism is also remarkably effective at tracking the world states for sequential instruction understanding ( Long et al. , 2016 ) : after mapping world states as context and instructions as questions , FLOWQA can interpret a sequence of inter-connected instructions and generate corresponding world state changes as answers .","['This', 'FLOW', 'mechanism', 'is', 'also', 'remarkably', 'effective', 'at', 'tracking', 'the', 'world', 'states', 'for', 'sequential', 'instruction', 'understanding', '(', 'Long', 'et', 'al.', ',', '2016', ')', ':', 'after', 'mapping', 'world', 'states', 'as', 'context', 'and', 'instructions', 'as', 'questions', ',', 'FLOWQA', 'can', 'interpret', 'a', 'sequence', 'of', 'inter-connected', 'instructions', 'and', 'generate', 'corresponding', 'world', 'state', 'changes', 'as', 'answers', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'NN', 'VBZ', 'RB', 'RB', 'JJ', 'IN', 'VBG', 'DT', 'NN', 'VBZ', 'IN', 'JJ', 'NN', 'NN', '(', 'JJ', 'NN', 'NN', ',', 'CD', ')', ':', 'IN', 'VBG', 'NN', 'NNS', 'IN', 'NN', 'CC', 'NNS', 'IN', 'NNS', ',', 'NNP', 'MD', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'CC', 'NN', 'NN', 'NN', 'NN', 'NNS', 'IN', 'NNS', '.']",52
natural_language_inference,32,18,"The FLOW mechanism can be viewed as stacking single - turn QA models along the dialog progression ( i.e. , the question turns ) and building information flow along the dialog .","['The', 'FLOW', 'mechanism', 'can', 'be', 'viewed', 'as', 'stacking', 'single', '-', 'turn', 'QA', 'models', 'along', 'the', 'dialog', 'progression', '(', 'i.e.', ',', 'the', 'question', 'turns', ')', 'and', 'building', 'information', 'flow', 'along', 'the', 'dialog', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['DT', 'NNP', 'NN', 'MD', 'VB', 'VBN', 'IN', 'VBG', 'JJ', ':', 'NN', 'NNP', 'NNS', 'IN', 'DT', 'NN', 'NN', '(', 'FW', ',', 'DT', 'NN', 'VBZ', ')', 'CC', 'NN', 'NN', 'NN', 'IN', 'DT', 'NN', '.']",32
natural_language_inference,32,19,"This information transfer happens for each context word , allowing rich information in the reasoning process to flow .","['This', 'information', 'transfer', 'happens', 'for', 'each', 'context', 'word', ',', 'allowing', 'rich', 'information', 'in', 'the', 'reasoning', 'process', 'to', 'flow', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'NN', ',', 'VBG', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', '.']",19
natural_language_inference,32,22,"To handle this issue , we propose an alternating parallel processing structure , which alternates between sequentially processing one dimension in parallel of the other dimension , and thus speeds up training significantly .","['To', 'handle', 'this', 'issue', ',', 'we', 'propose', 'an', 'alternating', 'parallel', 'processing', 'structure', ',', 'which', 'alternates', 'between', 'sequentially', 'processing', 'one', 'dimension', 'in', 'parallel', 'of', 'the', 'other', 'dimension', ',', 'and', 'thus', 'speeds', 'up', 'training', 'significantly', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'B-n', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'VBG', 'JJ', 'NN', 'NN', ',', 'WDT', 'VBZ', 'IN', 'RB', 'VBG', 'CD', 'NN', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'CC', 'RB', 'VB', 'RP', 'VBG', 'RB', '.']",34
natural_language_inference,32,29,Recently proposed conversational machine comprehension ( MC ) datasets aim to enable models to assist in such information seeking dialogs .,"['Recently', 'proposed', 'conversational', 'machine', 'comprehension', '(', 'MC', ')', 'datasets', 'aim', 'to', 'enable', 'models', 'to', 'assist', 'in', 'such', 'information', 'seeking', 'dialogs', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'VBN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'JJ', 'TO', 'JJ', 'NNS', 'TO', 'VB', 'IN', 'JJ', 'NN', 'VBG', 'NNS', '.']",21
natural_language_inference,32,37,Our code can be found in https://github.com/momohuang/FlowQA.,"['Our', 'code', 'can', 'be', 'found', 'in', 'https://github.com/momohuang/FlowQA.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n']","['PRP$', 'NN', 'MD', 'VB', 'VBN', 'IN', 'NN']",7
natural_language_inference,32,151,"applied BiDAF ++ , a strong extractive QA model to QuAC dataset .","['applied', 'BiDAF', '++', ',', 'a', 'strong', 'extractive', 'QA', 'model', 'to', 'QuAC', 'dataset', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['VBN', 'NNP', 'NNP', ',', 'DT', 'JJ', 'JJ', 'NNP', 'NN', 'TO', 'NNP', 'NN', '.']",13
natural_language_inference,32,156,"Here we briefly describe the ablated systems : "" - FLOW "" removes the flow component from IF layer ( Eq. 2 in Section 3.2 ) , "" - QHIER - RNN "" removes the hierarchical LSTM layers on final question vectors ( Eq. 7 in Section 3.3 ) .","['Here', 'we', 'briefly', 'describe', 'the', 'ablated', 'systems', ':', '""', '-', 'FLOW', '""', 'removes', 'the', 'flow', 'component', 'from', 'IF', 'layer', '(', 'Eq.', '2', 'in', 'Section', '3.2', ')', ',', '""', '-', 'QHIER', '-', 'RNN', '""', 'removes', 'the', 'hierarchical', 'LSTM', 'layers', 'on', 'final', 'question', 'vectors', '(', 'Eq.', '7', 'in', 'Section', '3.3', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'PRP', 'VBP', 'VB', 'DT', 'JJ', 'NNS', ':', 'JJ', ':', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NN', '(', 'NNP', 'CD', 'IN', 'NNP', 'CD', ')', ',', 'SYM', ':', 'SYM', ':', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NNP', 'NNS', 'IN', 'JJ', 'NN', 'NNS', '(', 'NNP', 'CD', 'IN', 'NNP', 'CD', ')', '.']",50
natural_language_inference,32,158,"FLOWQA yields substantial improvement over existing models on both datasets ( + 7.2 % F 1 on CoQA , + 4.0 % F 1 on QuAC ) .","['FLOWQA', 'yields', 'substantial', 'improvement', 'over', 'existing', 'models', 'on', 'both', 'datasets', '(', '+', '7.2', '%', 'F', '1', 'on', 'CoQA', ',', '+', '4.0', '%', 'F', '1', 'on', 'QuAC', ')', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O']","['NNP', 'NNS', 'JJ', 'NN', 'IN', 'VBG', 'NNS', 'IN', 'DT', 'NNS', '(', 'VB', 'CD', 'NN', 'NNP', 'CD', 'IN', 'NNP', ',', 'VBD', 'CD', 'NN', 'NNP', 'CD', 'IN', 'NNP', ')', '.']",28
natural_language_inference,32,161,We find that FLOW is a critical component .,"['We', 'find', 'that', 'FLOW', 'is', 'a', 'critical', 'component', '.']","['O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', '.']",9
natural_language_inference,32,162,"Removing QHier - RNN has a minor impact ( 0.1 % on both datasets ) , while removing FLOW results in a substantial performance drop , with or without using QHierRNN ( 2 - 3 % on QuAC , 4.1 % on CoQA ) .","['Removing', 'QHier', '-', 'RNN', 'has', 'a', 'minor', 'impact', '(', '0.1', '%', 'on', 'both', 'datasets', ')', ',', 'while', 'removing', 'FLOW', 'results', 'in', 'a', 'substantial', 'performance', 'drop', ',', 'with', 'or', 'without', 'using', 'QHierRNN', '(', '2', '-', '3', '%', 'on', 'QuAC', ',', '4.1', '%', 'on', 'CoQA', ')', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O']","['VBG', 'NNP', ':', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', '(', 'CD', 'NN', 'IN', 'DT', 'NNS', ')', ',', 'IN', 'VBG', 'NNP', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'IN', 'CC', 'IN', 'VBG', 'NNP', '(', 'CD', ':', 'CD', 'NN', 'IN', 'NNP', ',', 'CD', 'NN', 'IN', 'NNP', ')', '.']",45
natural_language_inference,32,166,"By comparing 0 - Ans and 1 - Ans on two datasets , we can see that providing gold answers is more crucial for QuAC .","['By', 'comparing', '0', '-', 'Ans', 'and', '1', '-', 'Ans', 'on', 'two', 'datasets', ',', 'we', 'can', 'see', 'that', 'providing', 'gold', 'answers', 'is', 'more', 'crucial', 'for', 'QuAC', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'VBG', 'CD', ':', 'NN', 'CC', 'CD', ':', 'NN', 'IN', 'CD', 'NNS', ',', 'PRP', 'MD', 'VB', 'IN', 'VBG', 'NN', 'NNS', 'VBZ', 'RBR', 'JJ', 'IN', 'NNP', '.']",26
natural_language_inference,32,171,"Based on the training time each epoch takes ( i.e. , time needed for passing through the data once ) , the speedup is 8.1x on CoQA and 4.2 x on QuAC .","['Based', 'on', 'the', 'training', 'time', 'each', 'epoch', 'takes', '(', 'i.e.', ',', 'time', 'needed', 'for', 'passing', 'through', 'the', 'data', 'once', ')', ',', 'the', 'speedup', 'is', '8.1x', 'on', 'CoQA', 'and', '4.2', 'x', 'on', 'QuAC', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['VBN', 'IN', 'DT', 'NN', 'NN', 'DT', 'NN', 'VBZ', '(', 'FW', ',', 'NN', 'VBN', 'IN', 'VBG', 'IN', 'DT', 'NNS', 'RB', ')', ',', 'DT', 'NN', 'VBZ', 'CD', 'IN', 'NNP', 'CC', 'CD', 'NN', 'IN', 'NNP', '.']",33
natural_language_inference,41,2,"BART : Denoising Sequence - to - Sequence Pre-training for Natural Language Generation , Translation , and Comprehension","['BART', ':', 'Denoising', 'Sequence', '-', 'to', '-', 'Sequence', 'Pre-training', 'for', 'Natural', 'Language', 'Generation', ',', 'Translation', ',', 'and', 'Comprehension']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNS', ':', 'VBG', 'NNP', ':', 'TO', ':', 'NN', 'NN', 'IN', 'NNP', 'NNP', 'NNP', ',', 'NNP', ',', 'CC', 'NNP']",18
natural_language_inference,41,4,"We present BART , a denoising autoencoder for pretraining sequence - to - sequence models .","['We', 'present', 'BART', ',', 'a', 'denoising', 'autoencoder', 'for', 'pretraining', 'sequence', '-', 'to', '-', 'sequence', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'JJ', 'NNP', ',', 'DT', 'NN', 'NN', 'IN', 'VBG', 'NN', ':', 'TO', ':', 'NN', 'NNS', '.']",16
natural_language_inference,41,17,"In this paper , we present BART , which pre-trains a model combining Bidirectional and Auto - Regressive Transformers .","['In', 'this', 'paper', ',', 'we', 'present', 'BART', ',', 'which', 'pre-trains', 'a', 'model', 'combining', 'Bidirectional', 'and', 'Auto', '-', 'Regressive', 'Transformers', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'JJ', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NN', 'VBG', 'NNP', 'CC', 'NNP', ':', 'JJ', 'NNS', '.']",20
natural_language_inference,41,18,BART is a denoising autoencoder built with a sequence - to - sequence model that is applicable to a very wide range of end tasks .,"['BART', 'is', 'a', 'denoising', 'autoencoder', 'built', 'with', 'a', 'sequence', '-', 'to', '-', 'sequence', 'model', 'that', 'is', 'applicable', 'to', 'a', 'very', 'wide', 'range', 'of', 'end', 'tasks', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'VBG', 'NN', 'VBN', 'IN', 'DT', 'NN', ':', 'TO', ':', 'NN', 'NN', 'WDT', 'VBZ', 'JJ', 'TO', 'DT', 'RB', 'JJ', 'NN', 'IN', 'NN', 'NNS', '.']",26
natural_language_inference,41,19,"Pretraining has two stages ( 1 ) text is corrupted with an arbitrary noising function , and ( 2 ) a sequence - to - sequence model is learned to reconstruct the original text .","['Pretraining', 'has', 'two', 'stages', '(', '1', ')', 'text', 'is', 'corrupted', 'with', 'an', 'arbitrary', 'noising', 'function', ',', 'and', '(', '2', ')', 'a', 'sequence', '-', 'to', '-', 'sequence', 'model', 'is', 'learned', 'to', 'reconstruct', 'the', 'original', 'text', '.']","['B-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['VBG', 'VBZ', 'CD', 'NNS', '(', 'CD', ')', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'CC', '(', 'CD', ')', 'DT', 'NN', ':', 'TO', ':', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NN', '.']",35
natural_language_inference,41,20,"BART uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes ( see .","['BART', 'uses', 'a', 'standard', 'Tranformer', '-', 'based', 'neural', 'machine', 'translation', 'architecture', 'which', ',', 'despite', 'its', 'simplicity', ',', 'can', 'be', 'seen', 'as', 'generalizing', 'BERT', '(', 'due', 'to', 'the', 'bidirectional', 'encoder', ')', ',', 'GPT', '(', 'with', 'the', 'left', '-', 'to', '-', 'right', 'decoder', ')', ',', 'and', 'many', 'other', 'more', 'recent', 'pretraining', 'schemes', '(', 'see', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NNP', ':', 'VBN', 'JJ', 'NN', 'NN', 'NN', 'WDT', ',', 'IN', 'PRP$', 'NN', ',', 'MD', 'VB', 'VBN', 'IN', 'VBG', 'NNP', '(', 'JJ', 'TO', 'DT', 'JJ', 'NN', ')', ',', 'NNP', '(', 'IN', 'DT', 'JJ', ':', 'TO', ':', 'JJ', 'NN', ')', ',', 'CC', 'JJ', 'JJ', 'JJR', 'JJ', 'NN', 'NNS', '(', 'VB', '.']",53
natural_language_inference,41,154,"We pre-train a large model with 12 layers in each of the encoder and decoder , and a hidden size of 1024 .","['We', 'pre-train', 'a', 'large', 'model', 'with', '12', 'layers', 'in', 'each', 'of', 'the', 'encoder', 'and', 'decoder', ',', 'and', 'a', 'hidden', 'size', 'of', '1024', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NNS', 'IN', 'DT', 'IN', 'DT', 'NN', 'CC', 'NN', ',', 'CC', 'DT', 'JJ', 'NN', 'IN', 'CD', '.']",23
natural_language_inference,41,155,"Following RoBERTa , we use a batch size of 8000 , and train the model for 500000 steps .","['Following', 'RoBERTa', ',', 'we', 'use', 'a', 'batch', 'size', 'of', '8000', ',', 'and', 'train', 'the', 'model', 'for', '500000', 'steps', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['VBG', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CD', ',', 'CC', 'VB', 'DT', 'NN', 'IN', 'CD', 'NNS', '.']",19
natural_language_inference,41,156,Documents are tokenized with the same byte - pair encoding as GPT - 2 .,"['Documents', 'are', 'tokenized', 'with', 'the', 'same', 'byte', '-', 'pair', 'encoding', 'as', 'GPT', '-', '2', '.']","['B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNS', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NN', ':', 'NN', 'VBG', 'IN', 'NNP', ':', 'CD', '.']",15
natural_language_inference,41,157,"Based on the results in Section 4 , we use a combination of text infilling and sentence permutation .","['Based', 'on', 'the', 'results', 'in', 'Section', '4', ',', 'we', 'use', 'a', 'combination', 'of', 'text', 'infilling', 'and', 'sentence', 'permutation', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'NNS', 'IN', 'NNP', 'CD', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'NN', 'CC', 'NN', 'NN', '.']",19
natural_language_inference,41,158,"We mask 30 % of tokens in each document , and permute all sentences .","['We', 'mask', '30', '%', 'of', 'tokens', 'in', 'each', 'document', ',', 'and', 'permute', 'all', 'sentences', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', ',', 'CC', 'VB', 'DT', 'NNS', '.']",15
natural_language_inference,41,160,"To help the model better fit the data , we dis abled dropout for the final 10 % of training steps .","['To', 'help', 'the', 'model', 'better', 'fit', 'the', 'data', ',', 'we', 'dis', 'abled', 'dropout', 'for', 'the', 'final', '10', '%', 'of', 'training', 'steps', '.']","['B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', 'RBR', 'VBP', 'DT', 'NNS', ',', 'PRP', 'VBP', 'VBN', 'NN', 'IN', 'DT', 'JJ', 'CD', 'NN', 'IN', 'NN', 'NNS', '.']",22
natural_language_inference,41,163,"The most directly comparable baseline is RoBERTa , which was pre-trained with the same resources , but a different objective .","['The', 'most', 'directly', 'comparable', 'baseline', 'is', 'RoBERTa', ',', 'which', 'was', 'pre-trained', 'with', 'the', 'same', 'resources', ',', 'but', 'a', 'different', 'objective', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['DT', 'RBS', 'RB', 'JJ', 'NN', 'VBZ', 'NNP', ',', 'WDT', 'VBD', 'JJ', 'IN', 'DT', 'JJ', 'NNS', ',', 'CC', 'DT', 'JJ', 'NN', '.']",21
natural_language_inference,41,169,We also experiment with several text generation tasks .,"['We', 'also', 'experiment', 'with', 'several', 'text', 'generation', 'tasks', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'IN', 'JJ', 'JJ', 'NN', 'NNS', '.']",9
natural_language_inference,41,175,"To provide a comparison with the state - of - the - art in summarization , we present results on two summarization datasets , CNN / DailyMail and XSum , which have distinct properties .","['To', 'provide', 'a', 'comparison', 'with', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'in', 'summarization', ',', 'we', 'present', 'results', 'on', 'two', 'summarization', 'datasets', ',', 'CNN', '/', 'DailyMail', 'and', 'XSum', ',', 'which', 'have', 'distinct', 'properties', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'NN', ',', 'PRP', 'VBP', 'NNS', 'IN', 'CD', 'NN', 'NNS', ',', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', ',', 'WDT', 'VBP', 'JJ', 'NNS', '.']",35
natural_language_inference,41,178,"Nevertheless , BART outperforms all existing work .","['Nevertheless', ',', 'BART', 'outperforms', 'all', 'existing', 'work', '.']","['O', 'O', 'B-n', 'B-n', 'O', 'O', 'O', 'O']","['RB', ',', 'NNP', 'VBZ', 'DT', 'VBG', 'NN', '.']",8
natural_language_inference,41,180,"BART outperforms the best previous work , which leverages BERT , by roughly 6.0 points on all ROUGE metrics - representing a significant advance in performance on this problem .","['BART', 'outperforms', 'the', 'best', 'previous', 'work', ',', 'which', 'leverages', 'BERT', ',', 'by', 'roughly', '6.0', 'points', 'on', 'all', 'ROUGE', 'metrics', '-', 'representing', 'a', 'significant', 'advance', 'in', 'performance', 'on', 'this', 'problem', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'DT', 'JJS', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'NNP', ',', 'IN', 'RB', 'CD', 'NNS', 'IN', 'DT', 'NNP', 'NNS', ':', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'DT', 'NN', '.']",30
natural_language_inference,41,183,"We evaluate dialogue response generation on CONVAI2 , in which agents must generate responses conditioned on both the previous context and a textually - specified persona .","['We', 'evaluate', 'dialogue', 'response', 'generation', 'on', 'CONVAI2', ',', 'in', 'which', 'agents', 'must', 'generate', 'responses', 'conditioned', 'on', 'both', 'the', 'previous', 'context', 'and', 'a', 'textually', '-', 'specified', 'persona', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'NN', 'IN', 'NNP', ',', 'IN', 'WDT', 'NNS', 'MD', 'VB', 'NNS', 'VBN', 'IN', 'DT', 'DT', 'JJ', 'NN', 'CC', 'DT', 'RB', ':', 'VBN', 'NN', '.']",27
natural_language_inference,41,184,BART outperforms previous work on two automated metrics .,"['BART', 'outperforms', 'previous', 'work', 'on', 'two', 'automated', 'metrics', '.']","['B-n', 'B-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'JJ', 'NN', 'IN', 'CD', 'JJ', 'NNS', '.']",9
natural_language_inference,41,186,We use the recently proposed ELI5 dataset to test the model 's ability to generate long freeform answers .,"['We', 'use', 'the', 'recently', 'proposed', 'ELI5', 'dataset', 'to', 'test', 'the', 'model', ""'s"", 'ability', 'to', 'generate', 'long', 'freeform', 'answers', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'RB', 'VBN', 'NNP', 'NN', 'TO', 'VB', 'DT', 'NN', 'POS', 'NN', 'TO', 'VB', 'JJ', 'NN', 'NNS', '.']",19
natural_language_inference,41,187,"We find BART outperforms the best previous work by 1.2 ROUGE - L , but the dataset remains a challenging , because answers are only weakly specified by the question .","['We', 'find', 'BART', 'outperforms', 'the', 'best', 'previous', 'work', 'by', '1.2', 'ROUGE', '-', 'L', ',', 'but', 'the', 'dataset', 'remains', 'a', 'challenging', ',', 'because', 'answers', 'are', 'only', 'weakly', 'specified', 'by', 'the', 'question', '.']","['O', 'B-p', 'B-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', 'VBZ', 'DT', 'JJS', 'JJ', 'NN', 'IN', 'CD', 'NNP', ':', 'NN', ',', 'CC', 'DT', 'NN', 'VBZ', 'DT', 'NN', ',', 'IN', 'NNS', 'VBP', 'RB', 'RB', 'VBN', 'IN', 'DT', 'NN', '.']",31
natural_language_inference,41,194,For each row we experiment on the original WMT16 Romanian - English augmented with back - translation data .,"['For', 'each', 'row', 'we', 'experiment', 'on', 'the', 'original', 'WMT16', 'Romanian', '-', 'English', 'augmented', 'with', 'back', '-', 'translation', 'data', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'PRP', 'VBP', 'IN', 'DT', 'JJ', 'NNP', 'NNP', ':', 'JJ', 'VBN', 'IN', 'JJ', ':', 'NN', 'NNS', '.']",19
natural_language_inference,41,196,"1 . Preliminary results suggested that our approach was less effective without back - translation data , and prone to overfitting - future work should explore additional regularization techniques .","['1', '.', 'Preliminary', 'results', 'suggested', 'that', 'our', 'approach', 'was', 'less', 'effective', 'without', 'back', '-', 'translation', 'data', ',', 'and', 'prone', 'to', 'overfitting', '-', 'future', 'work', 'should', 'explore', 'additional', 'regularization', 'techniques', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['CD', '.', 'JJ', 'NNS', 'VBD', 'IN', 'PRP$', 'NN', 'VBD', 'RBR', 'JJ', 'IN', 'RB', ':', 'NN', 'NNS', ',', 'CC', 'NN', 'TO', 'VBG', ':', 'JJ', 'NN', 'MD', 'VB', 'JJ', 'NN', 'NNS', '.']",30
natural_language_inference,85,2,Enhanced LSTM for Natural Language Inference,"['Enhanced', 'LSTM', 'for', 'Natural', 'Language', 'Inference']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",6
natural_language_inference,85,4,Reasoning and inference are central to human and artificial intelligence .,"['Reasoning', 'and', 'inference', 'are', 'central', 'to', 'human', 'and', 'artificial', 'intelligence', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'CC', 'NN', 'VBP', 'JJ', 'TO', 'JJ', 'CC', 'JJ', 'NN', '.']",11
natural_language_inference,85,15,"Specifically , natural language inference ( NLI ) is concerned with determining whether a naturallanguage hypothesis h can be inferred from a premise p , as depicted in the following example from MacCartney ( 2009 ) , where the hypothesis is regarded to be entailed from the premise .","['Specifically', ',', 'natural', 'language', 'inference', '(', 'NLI', ')', 'is', 'concerned', 'with', 'determining', 'whether', 'a', 'naturallanguage', 'hypothesis', 'h', 'can', 'be', 'inferred', 'from', 'a', 'premise', 'p', ',', 'as', 'depicted', 'in', 'the', 'following', 'example', 'from', 'MacCartney', '(', '2009', ')', ',', 'where', 'the', 'hypothesis', 'is', 'regarded', 'to', 'be', 'entailed', 'from', 'the', 'premise', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'VBN', 'IN', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NN', 'MD', 'VB', 'VBN', 'IN', 'DT', 'NN', 'NN', ',', 'IN', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNP', '(', 'CD', ')', ',', 'WRB', 'DT', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'VBN', 'IN', 'DT', 'NN', '.']",49
natural_language_inference,85,24,"While some previous top - performing models use rather complicated network architectures to achieve the state - of - the - art results , we demonstrate in this paper that enhancing sequential inference models based on chain models can outperform all previous results , suggesting that the potentials of such sequential inference approaches have not been fully exploited yet .","['While', 'some', 'previous', 'top', '-', 'performing', 'models', 'use', 'rather', 'complicated', 'network', 'architectures', 'to', 'achieve', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', ',', 'we', 'demonstrate', 'in', 'this', 'paper', 'that', 'enhancing', 'sequential', 'inference', 'models', 'based', 'on', 'chain', 'models', 'can', 'outperform', 'all', 'previous', 'results', ',', 'suggesting', 'that', 'the', 'potentials', 'of', 'such', 'sequential', 'inference', 'approaches', 'have', 'not', 'been', 'fully', 'exploited', 'yet', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'JJ', ':', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'NN', 'NNS', 'TO', 'VB', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'IN', 'VBG', 'JJ', 'NN', 'NNS', 'VBN', 'IN', 'NN', 'NNS', 'MD', 'VB', 'DT', 'JJ', 'NNS', ',', 'VBG', 'IN', 'DT', 'NNS', 'IN', 'JJ', 'JJ', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'RB', 'VBN', 'RB', '.']",60
natural_language_inference,85,26,Exploring syntax for NLI is very attractive to us .,"['Exploring', 'syntax', 'for', 'NLI', 'is', 'very', 'attractive', 'to', 'us', '.']","['O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NN', 'IN', 'NNP', 'VBZ', 'RB', 'JJ', 'TO', 'PRP', '.']",10
natural_language_inference,85,30,"We show that by explicitly encoding parsing information with recursive networks in both local inference modeling and inference composition and by incorporating it into our framework , we achieve additional improvement , increasing the performance to a new state of the art with an 88.6 % accuracy .","['We', 'show', 'that', 'by', 'explicitly', 'encoding', 'parsing', 'information', 'with', 'recursive', 'networks', 'in', 'both', 'local', 'inference', 'modeling', 'and', 'inference', 'composition', 'and', 'by', 'incorporating', 'it', 'into', 'our', 'framework', ',', 'we', 'achieve', 'additional', 'improvement', ',', 'increasing', 'the', 'performance', 'to', 'a', 'new', 'state', 'of', 'the', 'art', 'with', 'an', '88.6', '%', 'accuracy', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'IN', 'RB', 'VBG', 'VBG', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', 'CC', 'NN', 'NN', 'CC', 'IN', 'VBG', 'PRP', 'IN', 'PRP$', 'NN', ',', 'PRP', 'VBP', 'JJ', 'NN', ',', 'VBG', 'DT', 'NN', 'TO', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'CD', 'NN', 'NN', '.']",48
natural_language_inference,85,167,"We use the Adam method ( Kingma and Ba , 2014 ) for optimization .","['We', 'use', 'the', 'Adam', 'method', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', 'for', 'optimization', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'IN', 'NN', '.']",15
natural_language_inference,85,168,The first momentum is set to be 0.9 and the second 0.999 .,"['The', 'first', 'momentum', 'is', 'set', 'to', 'be', '0.9', 'and', 'the', 'second', '0.999', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'B-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'CD', 'CC', 'DT', 'JJ', 'CD', '.']",13
natural_language_inference,85,169,The initial learning rate is 0.0004 and the batch size is 32 .,"['The', 'initial', 'learning', 'rate', 'is', '0.0004', 'and', 'the', 'batch', 'size', 'is', '32', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'CD', 'CC', 'DT', 'NN', 'NN', 'VBZ', 'CD', '.']",13
natural_language_inference,85,170,"All hidden states of LSTMs , tree - LSTMs , and word embeddings have 300 dimensions .","['All', 'hidden', 'states', 'of', 'LSTMs', ',', 'tree', '-', 'LSTMs', ',', 'and', 'word', 'embeddings', 'have', '300', 'dimensions', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NNS', 'IN', 'NNP', ',', 'JJ', ':', 'NN', ',', 'CC', 'NN', 'NNS', 'VBP', 'CD', 'NNS', '.']",17
natural_language_inference,85,171,"We use dropout with a rate of 0.5 , which is applied to all feedforward connections .","['We', 'use', 'dropout', 'with', 'a', 'rate', 'of', '0.5', ',', 'which', 'is', 'applied', 'to', 'all', 'feedforward', 'connections', '.']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'RP', 'IN', 'DT', 'NN', 'IN', 'CD', ',', 'WDT', 'VBZ', 'VBN', 'TO', 'DT', 'JJ', 'NNS', '.']",17
natural_language_inference,85,172,We use pre-trained 300 - D Glove 840B vectors to initialize our word embeddings .,"['We', 'use', 'pre-trained', '300', '-', 'D', 'Glove', '840B', 'vectors', 'to', 'initialize', 'our', 'word', 'embeddings', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'CD', ':', 'NNP', 'NNP', 'CD', 'NNS', 'TO', 'VB', 'PRP$', 'NN', 'NNS', '.']",15
natural_language_inference,85,173,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,"['Out', '-', 'of', '-', 'vocabulary', '(', 'OOV', ')', 'words', 'are', 'initialized', 'randomly', 'with', 'Gaussian', 'samples', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', ':', 'IN', ':', 'NN', '(', 'NNP', ')', 'NNS', 'VBP', 'VBN', 'RB', 'IN', 'JJ', 'NNS', '.']",16
natural_language_inference,85,185,"Our final model achieves the accuracy of 88.6 % , the best result observed on SNLI , while our enhanced sequential encoding model attains an accuracy of 88.0 % , which also outperform the previous models .","['Our', 'final', 'model', 'achieves', 'the', 'accuracy', 'of', '88.6', '%', ',', 'the', 'best', 'result', 'observed', 'on', 'SNLI', ',', 'while', 'our', 'enhanced', 'sequential', 'encoding', 'model', 'attains', 'an', 'accuracy', 'of', '88.0', '%', ',', 'which', 'also', 'outperform', 'the', 'previous', 'models', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'JJ', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', ',', 'DT', 'JJS', 'NN', 'VBD', 'IN', 'NNP', ',', 'IN', 'PRP$', 'JJ', 'JJ', 'VBG', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', ',', 'WDT', 'RB', 'VBP', 'DT', 'JJ', 'NNS', '.']",37
natural_language_inference,85,191,"In general , adding intra-sentence attention yields further improvement , which is not very surprising as it could help align the relevant text spans between premise and hypothesis .","['In', 'general', ',', 'adding', 'intra-sentence', 'attention', 'yields', 'further', 'improvement', ',', 'which', 'is', 'not', 'very', 'surprising', 'as', 'it', 'could', 'help', 'align', 'the', 'relevant', 'text', 'spans', 'between', 'premise', 'and', 'hypothesis', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', ',', 'VBG', 'JJ', 'NN', 'NNS', 'RBR', 'NN', ',', 'WDT', 'VBZ', 'RB', 'RB', 'JJ', 'IN', 'PRP', 'MD', 'VB', 'VB', 'DT', 'JJ', 'NN', 'NNS', 'IN', 'NN', 'CC', 'NN', '.']",29
natural_language_inference,85,195,"The table shows that our ESIM model achieves an accuracy of 88.0 % , which has already outperformed all the previous models , including those using much more complicated network architectures .","['The', 'table', 'shows', 'that', 'our', 'ESIM', 'model', 'achieves', 'an', 'accuracy', 'of', '88.0', '%', ',', 'which', 'has', 'already', 'outperformed', 'all', 'the', 'previous', 'models', ',', 'including', 'those', 'using', 'much', 'more', 'complicated', 'network', 'architectures', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'IN', 'PRP$', 'NNP', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', ',', 'WDT', 'VBZ', 'RB', 'VBN', 'PDT', 'DT', 'JJ', 'NNS', ',', 'VBG', 'DT', 'VBG', 'RB', 'RBR', 'JJ', 'NN', 'NNS', '.']",32
natural_language_inference,85,196,"We ensemble our ESIM model with syntactic tree - LSTMs based on syntactic parse trees and achieve significant improvement over our best sequential encoding model ESIM , attaining an accuracy of 88.6 % .","['We', 'ensemble', 'our', 'ESIM', 'model', 'with', 'syntactic', 'tree', '-', 'LSTMs', 'based', 'on', 'syntactic', 'parse', 'trees', 'and', 'achieve', 'significant', 'improvement', 'over', 'our', 'best', 'sequential', 'encoding', 'model', 'ESIM', ',', 'attaining', 'an', 'accuracy', 'of', '88.6', '%', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'NNP', 'NN', 'IN', 'JJ', 'NN', ':', 'NNP', 'VBN', 'IN', 'JJ', 'NN', 'NNS', 'CC', 'VB', 'JJ', 'NN', 'IN', 'PRP$', 'JJS', 'JJ', 'VBG', 'NN', 'NNP', ',', 'VBG', 'DT', 'NN', 'IN', 'CD', 'NN', '.']",34
natural_language_inference,85,204,Each tree node is implemented with a tree - LSTM block same as in model .,"['Each', 'tree', 'node', 'is', 'implemented', 'with', 'a', 'tree', '-', 'LSTM', 'block', 'same', 'as', 'in', 'model', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', ':', 'NNP', 'NN', 'JJ', 'IN', 'IN', 'NN', '.']",16
natural_language_inference,85,205,"shows that with this replacement , the performance drops to 88.2 % .","['shows', 'that', 'with', 'this', 'replacement', ',', 'the', 'performance', 'drops', 'to', '88.2', '%', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['NNS', 'WDT', 'IN', 'DT', 'NN', ',', 'DT', 'NN', 'VBZ', 'TO', 'CD', 'NN', '.']",13
natural_language_inference,85,207,"If we remove the pooling layer in inference composition and replace it with summation as in , the accuracy drops to 87.1 % .","['If', 'we', 'remove', 'the', 'pooling', 'layer', 'in', 'inference', 'composition', 'and', 'replace', 'it', 'with', 'summation', 'as', 'in', ',', 'the', 'accuracy', 'drops', 'to', '87.1', '%', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'PRP', 'VBP', 'DT', 'VBG', 'NN', 'IN', 'NN', 'NN', 'CC', 'VB', 'PRP', 'IN', 'NN', 'IN', 'IN', ',', 'DT', 'NN', 'VBZ', 'TO', 'CD', 'NN', '.']",24
natural_language_inference,85,208,"If we remove the difference and elementwise product from the local inference enhancement layer , the accuracy drops to 87.0 % .","['If', 'we', 'remove', 'the', 'difference', 'and', 'elementwise', 'product', 'from', 'the', 'local', 'inference', 'enhancement', 'layer', ',', 'the', 'accuracy', 'drops', 'to', '87.0', '%', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'PRP', 'VBP', 'DT', 'NN', 'CC', 'VB', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', ',', 'DT', 'NN', 'VBZ', 'TO', 'CD', 'NN', '.']",22
natural_language_inference,85,209,"To provide some detailed comparison with , replacing bidirectional LSTMs in inference composition and also input encoding with feedforward neural network reduces the accuracy to 87.3 % and 86.3 % respectively .","['To', 'provide', 'some', 'detailed', 'comparison', 'with', ',', 'replacing', 'bidirectional', 'LSTMs', 'in', 'inference', 'composition', 'and', 'also', 'input', 'encoding', 'with', 'feedforward', 'neural', 'network', 'reduces', 'the', 'accuracy', 'to', '87.3', '%', 'and', '86.3', '%', 'respectively', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['TO', 'VB', 'DT', 'JJ', 'NN', 'IN', ',', 'VBG', 'JJ', 'NNP', 'IN', 'NN', 'NN', 'CC', 'RB', 'VB', 'VBG', 'IN', 'JJ', 'JJ', 'NN', 'VBZ', 'DT', 'NN', 'TO', 'CD', 'NN', 'CC', 'CD', 'NN', 'RB', '.']",32
natural_language_inference,85,213,"If we remove the premise - based attention from ESIM ( model 23 ) , the accuracy drops to 87.2 % on the test set .","['If', 'we', 'remove', 'the', 'premise', '-', 'based', 'attention', 'from', 'ESIM', '(', 'model', '23', ')', ',', 'the', 'accuracy', 'drops', 'to', '87.2', '%', 'on', 'the', 'test', 'set', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'PRP', 'VBP', 'DT', 'NN', ':', 'VBN', 'NN', 'IN', 'NNP', '(', 'FW', 'CD', ')', ',', 'DT', 'NN', 'VBZ', 'TO', 'CD', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",26
natural_language_inference,85,215,"Removing the hypothesis - based attention ( model 24 ) decrease the accuracy to 86.5 % , where hypothesis - based attention is the attention performed on the other direction for the sentence pairs .","['Removing', 'the', 'hypothesis', '-', 'based', 'attention', '(', 'model', '24', ')', 'decrease', 'the', 'accuracy', 'to', '86.5', '%', ',', 'where', 'hypothesis', '-', 'based', 'attention', 'is', 'the', 'attention', 'performed', 'on', 'the', 'other', 'direction', 'for', 'the', 'sentence', 'pairs', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'DT', 'NN', ':', 'VBN', 'NN', '(', 'FW', 'CD', ')', 'VB', 'DT', 'NN', 'TO', 'CD', 'NN', ',', 'WRB', 'NN', ':', 'VBN', 'NN', 'VBZ', 'DT', 'NN', 'VBD', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",35
natural_language_inference,87,2,SG - Net : Syntax - Guided Machine Reading Comprehension,"['SG', '-', 'Net', ':', 'Syntax', '-', 'Guided', 'Machine', 'Reading', 'Comprehension']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', ':', 'NN', ':', 'NNP', ':', 'VBD', 'NNP', 'NNP', 'NNP']",10
natural_language_inference,87,13,"Understanding the meaning of a sentence is a prerequisite to solve many natural language understanding ( NLU ) problems , such as machine reading comprehension ( MRC ) based question answering .","['Understanding', 'the', 'meaning', 'of', 'a', 'sentence', 'is', 'a', 'prerequisite', 'to', 'solve', 'many', 'natural', 'language', 'understanding', '(', 'NLU', ')', 'problems', ',', 'such', 'as', 'machine', 'reading', 'comprehension', '(', 'MRC', ')', 'based', 'question', 'answering', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'TO', 'VB', 'JJ', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'NNS', ',', 'JJ', 'IN', 'NN', 'NN', 'NN', '(', 'NNP', ')', 'VBN', 'NN', 'VBG', '.']",32
natural_language_inference,87,17,We observe that the accuracy of MRC models decreases when answering long questions ( shown in Section 5.1 ) .,"['We', 'observe', 'that', 'the', 'accuracy', 'of', 'MRC', 'models', 'decreases', 'when', 'answering', 'long', 'questions', '(', 'shown', 'in', 'Section', '5.1', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NNS', 'VBZ', 'WRB', 'VBG', 'JJ', 'NNS', '(', 'VBN', 'IN', 'NNP', 'CD', ')', '.']",20
natural_language_inference,87,27,"In this paper , we extend the self - attention mechanism with syntax - guided constraint , to capture syntax related parts with each concerned word .","['In', 'this', 'paper', ',', 'we', 'extend', 'the', 'self', '-', 'attention', 'mechanism', 'with', 'syntax', '-', 'guided', 'constraint', ',', 'to', 'capture', 'syntax', 'related', 'parts', 'with', 'each', 'concerned', 'word', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', ':', 'NN', 'NN', 'IN', 'JJ', ':', 'VBD', 'NN', ',', 'TO', 'VB', 'NN', 'VBN', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",27
natural_language_inference,87,28,"Specifically , we adopt pre-trained dependency syntactic parse tree structure to produce the related nodes for each word in a sentence , namely syntactic dependency of interest ( SDOI ) , by regarding each word as a child node and the SDOI consists all its ancestor nodes and itself in the dependency parsing tree .","['Specifically', ',', 'we', 'adopt', 'pre-trained', 'dependency', 'syntactic', 'parse', 'tree', 'structure', 'to', 'produce', 'the', 'related', 'nodes', 'for', 'each', 'word', 'in', 'a', 'sentence', ',', 'namely', 'syntactic', 'dependency', 'of', 'interest', '(', 'SDOI', ')', ',', 'by', 'regarding', 'each', 'word', 'as', 'a', 'child', 'node', 'and', 'the', 'SDOI', 'consists', 'all', 'its', 'ancestor', 'nodes', 'and', 'itself', 'in', 'the', 'dependency', 'parsing', 'tree', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'JJ', 'NN', 'JJ', 'NN', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'RB', 'JJ', 'NN', 'IN', 'NN', '(', 'NNP', ')', ',', 'IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'DT', 'NNP', 'VBZ', 'DT', 'PRP$', 'NN', 'NNS', 'CC', 'PRP', 'IN', 'DT', 'NN', 'VBG', 'NN', '.']",55
natural_language_inference,87,30,"To effectively accommodate such SDOI information , we propose a novel syntax - guided network ( SG - Net ) , which fuses the original SAN and SDOI - SAN , to provide more linguistically inspired representation for challenging reading comprehension tasks 1 .","['To', 'effectively', 'accommodate', 'such', 'SDOI', 'information', ',', 'we', 'propose', 'a', 'novel', 'syntax', '-', 'guided', 'network', '(', 'SG', '-', 'Net', ')', ',', 'which', 'fuses', 'the', 'original', 'SAN', 'and', 'SDOI', '-', 'SAN', ',', 'to', 'provide', 'more', 'linguistically', 'inspired', 'representation', 'for', 'challenging', 'reading', 'comprehension', 'tasks', '1', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['TO', 'RB', 'VB', 'JJ', 'NNP', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', ':', 'VBN', 'NN', '(', 'NNP', ':', 'NN', ')', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NNP', 'CC', 'NNP', ':', 'NN', ',', 'TO', 'VB', 'JJR', 'RB', 'JJ', 'NN', 'IN', 'VBG', 'VBG', 'NN', 'NNS', 'CD', '.']",44
natural_language_inference,87,129,We adopt the Whole Word Masking BERT as the baseline 6 .,"['We', 'adopt', 'the', 'Whole', 'Word', 'Masking', 'BERT', 'as', 'the', 'baseline', '6', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'DT', 'NN', 'CD', '.']",12
natural_language_inference,87,130,"The initial learning rate is set in { 8e -6 , 1 e - 5 , 2 e - 5 , 3 e - 5 } with warm - up rate of 0.1 and L2 weight decay of 0.01 .","['The', 'initial', 'learning', 'rate', 'is', 'set', 'in', '{', '8e', '-6', ',', '1', 'e', '-', '5', ',', '2', 'e', '-', '5', ',', '3', 'e', '-', '5', '}', 'with', 'warm', '-', 'up', 'rate', 'of', '0.1', 'and', 'L2', 'weight', 'decay', 'of', '0.01', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'IN', '(', 'CD', 'NN', ',', 'CD', 'SYM', ':', 'CD', ',', 'CD', 'SYM', ':', 'CD', ',', 'CD', 'SYM', ':', 'CD', ')', 'IN', 'JJ', ':', 'RB', 'NN', 'IN', 'CD', 'CC', 'NNP', 'VBD', 'NN', 'IN', 'CD', '.']",40
natural_language_inference,87,131,"The batch size is selected in { 16 , 20 , 32 } .","['The', 'batch', 'size', 'is', 'selected', 'in', '{', '16', ',', '20', ',', '32', '}', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', '(', 'CD', ',', 'CD', ',', 'CD', ')', '.']",14
natural_language_inference,87,132,The maximum number of epochs is set to 3 or 10 depending on tasks .,"['The', 'maximum', 'number', 'of', 'epochs', 'is', 'set', 'to', '3', 'or', '10', 'depending', 'on', 'tasks', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'IN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'CC', 'CD', 'VBG', 'IN', 'NNS', '.']",15
natural_language_inference,87,133,The weight ?,"['The', 'weight', '?']","['O', 'B-n', 'O']","['DT', 'NN', '.']",3
natural_language_inference,87,134,in the dual context aggregation is 0.5 .,"['in', 'the', 'dual', 'context', 'aggregation', 'is', '0.5', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'CD', '.']",8
natural_language_inference,87,135,"All the texts are tokenized using wordpieces , and the maximum input length is set to 384 for both of SQuAD and RACE .","['All', 'the', 'texts', 'are', 'tokenized', 'using', 'wordpieces', ',', 'and', 'the', 'maximum', 'input', 'length', 'is', 'set', 'to', '384', 'for', 'both', 'of', 'SQuAD', 'and', 'RACE', '.']","['O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PDT', 'DT', 'NN', 'VBP', 'VBN', 'VBG', 'NNS', ',', 'CC', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'IN', 'DT', 'IN', 'NNP', 'CC', 'NNP', '.']",24
natural_language_inference,87,143,It also outperforms all the published works and achieves the 2nd place on the leaderboard when submitting SG - NET .,"['It', 'also', 'outperforms', 'all', 'the', 'published', 'works', 'and', 'achieves', 'the', '2nd', 'place', 'on', 'the', 'leaderboard', 'when', 'submitting', 'SG', '-', 'NET', '.']","['O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBZ', 'PDT', 'DT', 'VBN', 'NNS', 'CC', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'WRB', 'VBG', 'NNP', ':', 'NN', '.']",21
natural_language_inference,87,144,"We also find that adding an extra answer verifier module could yield better result , which is pre-trained only to determine whether question is answerable or not with the same training data as SG - Net .","['We', 'also', 'find', 'that', 'adding', 'an', 'extra', 'answer', 'verifier', 'module', 'could', 'yield', 'better', 'result', ',', 'which', 'is', 'pre-trained', 'only', 'to', 'determine', 'whether', 'question', 'is', 'answerable', 'or', 'not', 'with', 'the', 'same', 'training', 'data', 'as', 'SG', '-', 'Net', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'NN', 'MD', 'VB', 'RB', 'NN', ',', 'WDT', 'VBZ', 'JJ', 'RB', 'TO', 'VB', 'IN', 'NN', 'VBZ', 'JJ', 'CC', 'RB', 'IN', 'DT', 'JJ', 'NN', 'NNS', 'IN', 'NNP', ':', 'NN', '.']",37
natural_language_inference,96,2,Swag : A Large - Scale Adversarial Dataset for Grounded Commonsense Inference,"['Swag', ':', 'A', 'Large', '-', 'Scale', 'Adversarial', 'Dataset', 'for', 'Grounded', 'Commonsense', 'Inference']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NN', ':', 'DT', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",12
natural_language_inference,96,36,We use this method to construct Swag : an adversarial dataset with 113 k multiple - choice questions .,"['We', 'use', 'this', 'method', 'to', 'construct', 'Swag', ':', 'an', 'adversarial', 'dataset', 'with', '113', 'k', 'multiple', '-', 'choice', 'questions', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'TO', 'VB', 'NNP', ':', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NN', 'JJ', ':', 'NN', 'NNS', '.']",19
natural_language_inference,96,37,"We start with pairs of temporally adjacent video captions , each with a context and a follow - up event that we know is physically possible .","['We', 'start', 'with', 'pairs', 'of', 'temporally', 'adjacent', 'video', 'captions', ',', 'each', 'with', 'a', 'context', 'and', 'a', 'follow', '-', 'up', 'event', 'that', 'we', 'know', 'is', 'physically', 'possible', '.']","['O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'NNS', 'IN', 'RB', 'JJ', 'NN', 'NNS', ',', 'DT', 'IN', 'DT', 'NN', 'CC', 'DT', 'JJ', ':', 'RB', 'NN', 'IN', 'PRP', 'VBP', 'VBZ', 'RB', 'JJ', '.']",27
natural_language_inference,96,38,We then use a state - of - theart language model fine - tuned on this data to massively oversample a diverse set of possible negative sentence endings ( or counterfactuals ) .,"['We', 'then', 'use', 'a', 'state', '-', 'of', '-', 'theart', 'language', 'model', 'fine', '-', 'tuned', 'on', 'this', 'data', 'to', 'massively', 'oversample', 'a', 'diverse', 'set', 'of', 'possible', 'negative', 'sentence', 'endings', '(', 'or', 'counterfactuals', ')', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', ':', 'IN', ':', 'NN', 'NN', 'NN', 'JJ', ':', 'VBN', 'IN', 'DT', 'NN', 'TO', 'RB', 'VB', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'NNS', '(', 'CC', 'NNS', ')', '.']",33
natural_language_inference,96,39,"Next , we filter these candidate endings aggressively and adversarially using a committee of trained models to obtain a population of de-biased endings with similar stylistic features to the real ones .","['Next', ',', 'we', 'filter', 'these', 'candidate', 'endings', 'aggressively', 'and', 'adversarially', 'using', 'a', 'committee', 'of', 'trained', 'models', 'to', 'obtain', 'a', 'population', 'of', 'de-biased', 'endings', 'with', 'similar', 'stylistic', 'features', 'to', 'the', 'real', 'ones', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['JJ', ',', 'PRP', 'VBP', 'DT', 'VBP', 'NNS', 'RB', 'CC', 'RB', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'JJ', 'NNS', 'TO', 'DT', 'JJ', 'NNS', '.']",32
natural_language_inference,96,40,"Finally , these filtered counterfactuals are validated by crowd workers to further ensure data quality .","['Finally', ',', 'these', 'filtered', 'counterfactuals', 'are', 'validated', 'by', 'crowd', 'workers', 'to', 'further', 'ensure', 'data', 'quality', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['RB', ',', 'DT', 'VBN', 'NNS', 'VBP', 'VBN', 'IN', 'JJ', 'NNS', 'TO', 'JJ', 'VB', 'NNS', 'NN', '.']",16
natural_language_inference,96,236,"The best model that only uses the ending is the LSTM sequence model with ELMo embeddings , which obtains 43.6 % .","['The', 'best', 'model', 'that', 'only', 'uses', 'the', 'ending', 'is', 'the', 'LSTM', 'sequence', 'model', 'with', 'ELMo', 'embeddings', ',', 'which', 'obtains', '43.6', '%', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'JJS', 'NN', 'IN', 'JJ', 'VBZ', 'DT', 'VBG', 'VBZ', 'DT', 'NNP', 'NN', 'NN', 'IN', 'NNP', 'NNS', ',', 'WDT', 'VBZ', 'CD', 'NN', '.']",22
natural_language_inference,96,237,"This model , as with most models studied , greatly improves with more context : by 3.1 % when given the initial noun phrase , and by an ad-ditional 4 % when also given the first sentence .","['This', 'model', ',', 'as', 'with', 'most', 'models', 'studied', ',', 'greatly', 'improves', 'with', 'more', 'context', ':', 'by', '3.1', '%', 'when', 'given', 'the', 'initial', 'noun', 'phrase', ',', 'and', 'by', 'an', 'ad-ditional', '4', '%', 'when', 'also', 'given', 'the', 'first', 'sentence', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', ',', 'IN', 'IN', 'JJS', 'NNS', 'VBN', ',', 'RB', 'VBZ', 'IN', 'RBR', 'NN', ':', 'IN', 'CD', 'NN', 'WRB', 'VBN', 'DT', 'JJ', 'JJ', 'NN', ',', 'CC', 'IN', 'DT', 'JJ', 'CD', 'NN', 'WRB', 'RB', 'VBN', 'DT', 'JJ', 'NN', '.']",38
natural_language_inference,96,238,Further improvement is gained from models that compute pairwise representations of the inputs .,"['Further', 'improvement', 'is', 'gained', 'from', 'models', 'that', 'compute', 'pairwise', 'representations', 'of', 'the', 'inputs', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['JJ', 'NN', 'VBZ', 'VBN', 'IN', 'NNS', 'IN', 'VBP', 'JJ', 'NNS', 'IN', 'DT', 'NNS', '.']",14
natural_language_inference,96,239,"While the simplest such model , Dual - BoW , obtains only 35.1 % accuracy , combining In - fer Sent sentence representations gives 40.5 % accuracy ( InferSent - Bilinear ) .","['While', 'the', 'simplest', 'such', 'model', ',', 'Dual', '-', 'BoW', ',', 'obtains', 'only', '35.1', '%', 'accuracy', ',', 'combining', 'In', '-', 'fer', 'Sent', 'sentence', 'representations', 'gives', '40.5', '%', 'accuracy', '(', 'InferSent', '-', 'Bilinear', ')', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'DT', 'JJS', 'JJ', 'NN', ',', 'NNP', ':', 'NNP', ',', 'VBZ', 'RB', 'CD', 'NN', 'NN', ',', 'VBG', 'IN', ':', 'NN', 'NNP', 'NN', 'NNS', 'VBZ', 'CD', 'NN', 'NN', '(', 'NNP', ':', 'NN', ')', '.']",33
natural_language_inference,96,240,"The best results come from pairwise NLI models : when fully trained on Swag , ESIM + ELMo obtains 59.2 % accuracy .","['The', 'best', 'results', 'come', 'from', 'pairwise', 'NLI', 'models', ':', 'when', 'fully', 'trained', 'on', 'Swag', ',', 'ESIM', '+', 'ELMo', 'obtains', '59.2', '%', 'accuracy', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJS', 'NNS', 'VBP', 'IN', 'NN', 'NNP', 'NNS', ':', 'WRB', 'RB', 'VBN', 'IN', 'NNP', ',', 'NNP', 'NNP', 'NNP', 'VBZ', 'CD', 'NN', 'NN', '.']",23
natural_language_inference,86,2,Multi - Perspective Context Matching for Machine Comprehension,"['Multi', '-', 'Perspective', 'Context', 'Matching', 'for', 'Machine', 'Comprehension']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', ':', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",8
natural_language_inference,86,4,"Previous machine comprehension ( MC ) datasets are either too small to train endto - end deep learning models , or not difficult enough to evaluate the ability of current MC techniques .","['Previous', 'machine', 'comprehension', '(', 'MC', ')', 'datasets', 'are', 'either', 'too', 'small', 'to', 'train', 'endto', '-', 'end', 'deep', 'learning', 'models', ',', 'or', 'not', 'difficult', 'enough', 'to', 'evaluate', 'the', 'ability', 'of', 'current', 'MC', 'techniques', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O']","['JJ', 'NN', 'NN', '(', 'NNP', ')', 'NNS', 'VBP', 'RB', 'RB', 'JJ', 'TO', 'VB', 'JJ', ':', 'NN', 'JJ', 'NN', 'NNS', ',', 'CC', 'RB', 'JJ', 'RB', 'TO', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNP', 'NNS', '.']",33
natural_language_inference,86,27,"In this work , we focus on the SQuAD dataset and propose an end - to - end deep neural network model for machine comprehension .","['In', 'this', 'work', ',', 'we', 'focus', 'on', 'the', 'SQuAD', 'dataset', 'and', 'propose', 'an', 'end', '-', 'to', '-', 'end', 'deep', 'neural', 'network', 'model', 'for', 'machine', 'comprehension', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'IN', 'DT', 'NNP', 'NN', 'CC', 'VB', 'DT', 'NN', ':', 'TO', ':', 'VB', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",26
natural_language_inference,86,29,"Based on this assumption , we design a Multi - Perspective Context Matching ( MPCM ) model to identify the answer span by matching the context of each point in the passage with the question from multiple perspectives .","['Based', 'on', 'this', 'assumption', ',', 'we', 'design', 'a', 'Multi', '-', 'Perspective', 'Context', 'Matching', '(', 'MPCM', ')', 'model', 'to', 'identify', 'the', 'answer', 'span', 'by', 'matching', 'the', 'context', 'of', 'each', 'point', 'in', 'the', 'passage', 'with', 'the', 'question', 'from', 'multiple', 'perspectives', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['VBN', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', ':', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', '.']",39
natural_language_inference,86,30,"Instead of enumerating all the possible spans explicitly and ranking them , our model identifies the answer span by predicting the beginning and ending points individually with globally normalized probability distributions across the whole passage .","['Instead', 'of', 'enumerating', 'all', 'the', 'possible', 'spans', 'explicitly', 'and', 'ranking', 'them', ',', 'our', 'model', 'identifies', 'the', 'answer', 'span', 'by', 'predicting', 'the', 'beginning', 'and', 'ending', 'points', 'individually', 'with', 'globally', 'normalized', 'probability', 'distributions', 'across', 'the', 'whole', 'passage', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', 'IN', 'VBG', 'PDT', 'DT', 'JJ', 'NNS', 'RB', 'CC', 'VBG', 'PRP', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'CC', 'VBG', 'NNS', 'RB', 'IN', 'RB', 'VBN', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",36
natural_language_inference,86,102,We process the corpus with the tokenizer from Stanford CorNLP .,"['We', 'process', 'the', 'corpus', 'with', 'the', 'tokenizer', 'from', 'Stanford', 'CorNLP', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NNP', '.']",11
natural_language_inference,86,104,"To initialize the word embeddings in the word representation layer , we use the 300 - dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus .","['To', 'initialize', 'the', 'word', 'embeddings', 'in', 'the', 'word', 'representation', 'layer', ',', 'we', 'use', 'the', '300', '-', 'dimensional', 'GloVe', 'word', 'vectors', 'pre-trained', 'from', 'the', '840B', 'Common', 'Crawl', 'corpus', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'CD', ':', 'JJ', 'NNP', 'NN', 'NNS', 'VBD', 'IN', 'DT', 'CD', 'NNP', 'NNP', 'NN', '.']",28
natural_language_inference,86,105,"For the out - of - vocabulary ( OOV ) words , we initialize the word embeddings randomly .","['For', 'the', 'out', '-', 'of', '-', 'vocabulary', '(', 'OOV', ')', 'words', ',', 'we', 'initialize', 'the', 'word', 'embeddings', 'randomly', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O']","['IN', 'DT', 'RP', ':', 'IN', ':', 'NN', '(', 'NNP', ')', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'VBZ', 'RB', '.']",19
natural_language_inference,86,106,"We set the hidden size as 100 for all the LSTM layers , and set the number of perspectives l of our multiperspective matching function ( Equation ( 5 ) ) as 50 .","['We', 'set', 'the', 'hidden', 'size', 'as', '100', 'for', 'all', 'the', 'LSTM', 'layers', ',', 'and', 'set', 'the', 'number', 'of', 'perspectives', 'l', 'of', 'our', 'multiperspective', 'matching', 'function', '(', 'Equation', '(', '5', ')', ')', 'as', '50', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'CD', 'IN', 'PDT', 'DT', 'NNP', 'NNS', ',', 'CC', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'NN', 'IN', 'PRP$', 'JJ', 'NN', 'NN', '(', 'NNP', '(', 'CD', ')', ')', 'IN', 'CD', '.']",34
natural_language_inference,86,107,"We apply dropout to every layers in , and set the dropout ratio as 0.2 .","['We', 'apply', 'dropout', 'to', 'every', 'layers', 'in', ',', 'and', 'set', 'the', 'dropout', 'ratio', 'as', '0.2', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'RB', 'TO', 'DT', 'NNS', 'IN', ',', 'CC', 'VBD', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",16
natural_language_inference,86,108,"To train the model , we minimize the cross entropy of the be - ginning and end points , and use the ADAM optimizer to update parameters .","['To', 'train', 'the', 'model', ',', 'we', 'minimize', 'the', 'cross', 'entropy', 'of', 'the', 'be', '-', 'ginning', 'and', 'end', 'points', ',', 'and', 'use', 'the', 'ADAM', 'optimizer', 'to', 'update', 'parameters', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'VB', ':', 'NN', 'CC', 'NN', 'NNS', ',', 'CC', 'VB', 'DT', 'NNP', 'NN', 'TO', 'VB', 'NNS', '.']",28
natural_language_inference,86,109,We set the learning rate as 0.0001 .,"['We', 'set', 'the', 'learning', 'rate', 'as', '0.0001', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",8
natural_language_inference,86,126,We can see that removing any components from the MPCM model decreases the performance significantly .,"['We', 'can', 'see', 'that', 'removing', 'any', 'components', 'from', 'the', 'MPCM', 'model', 'decreases', 'the', 'performance', 'significantly', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'B-n', 'O']","['PRP', 'MD', 'VB', 'IN', 'VBG', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'VBZ', 'DT', 'NN', 'RB', '.']",16
natural_language_inference,86,127,"Among all the layers , the Aggregation Layer is the most crucial layer .","['Among', 'all', 'the', 'layers', ',', 'the', 'Aggregation', 'Layer', 'is', 'the', 'most', 'crucial', 'layer', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'PDT', 'DT', 'NNS', ',', 'DT', 'NNP', 'NNP', 'VBZ', 'DT', 'RBS', 'JJ', 'NN', '.']",14
natural_language_inference,86,128,"Among all the matching strategies , Maxpooling - Matching has the biggest effect .","['Among', 'all', 'the', 'matching', 'strategies', ',', 'Maxpooling', '-', 'Matching', 'has', 'the', 'biggest', 'effect', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']","['IN', 'PDT', 'DT', 'NN', 'NNS', ',', 'VBG', ':', 'NN', 'VBZ', 'DT', 'JJS', 'NN', '.']",14
natural_language_inference,3,2,Modelling Interaction of Sentence Pair with Coupled- LSTMs,"['Modelling', 'Interaction', 'of', 'Sentence', 'Pair', 'with', 'Coupled-', 'LSTMs']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['VBG', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",8
natural_language_inference,3,4,"Recently , there is rising interest in modelling the interactions of two sentences with deep neural networks .","['Recently', ',', 'there', 'is', 'rising', 'interest', 'in', 'modelling', 'the', 'interactions', 'of', 'two', 'sentences', 'with', 'deep', 'neural', 'networks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'EX', 'VBZ', 'VBG', 'NN', 'IN', 'VBG', 'DT', 'NNS', 'IN', 'CD', 'NNS', 'IN', 'JJ', 'JJ', 'NNS', '.']",18
natural_language_inference,3,12,"Among these tasks , a common problem is modelling the relevance / similarity of the sentence pair , which is also called text semantic matching .","['Among', 'these', 'tasks', ',', 'a', 'common', 'problem', 'is', 'modelling', 'the', 'relevance', '/', 'similarity', 'of', 'the', 'sentence', 'pair', ',', 'which', 'is', 'also', 'called', 'text', 'semantic', 'matching', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'DT', 'JJ', 'NN', 'VBZ', 'VBG', 'DT', 'NN', 'NNP', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'WDT', 'VBZ', 'RB', 'VBN', 'NN', 'JJ', 'NN', '.']",26
natural_language_inference,3,28,"In this paper , we propose a new deep neural network architecture to model the strong interactions of two sentences .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'new', 'deep', 'neural', 'network', 'architecture', 'to', 'model', 'the', 'strong', 'interactions', 'of', 'two', 'sentences', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'IN', 'CD', 'NNS', '.']",21
natural_language_inference,3,29,"Different with modelling two sentences with separated LSTMs , we utilize two interdependent LSTMs , called coupled - LSTMs , to fully affect each other at different time steps .","['Different', 'with', 'modelling', 'two', 'sentences', 'with', 'separated', 'LSTMs', ',', 'we', 'utilize', 'two', 'interdependent', 'LSTMs', ',', 'called', 'coupled', '-', 'LSTMs', ',', 'to', 'fully', 'affect', 'each', 'other', 'at', 'different', 'time', 'steps', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NN', 'IN', 'VBG', 'CD', 'NNS', 'IN', 'JJ', 'NNP', ',', 'PRP', 'VBP', 'CD', 'JJ', 'NNP', ',', 'VBD', 'VBN', ':', 'NN', ',', 'TO', 'RB', 'VB', 'DT', 'JJ', 'IN', 'JJ', 'NN', 'NNS', '.']",30
natural_language_inference,3,30,The output of coupled - LSTMs at each step depends on both sentences .,"['The', 'output', 'of', 'coupled', '-', 'LSTMs', 'at', 'each', 'step', 'depends', 'on', 'both', 'sentences', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'VBN', ':', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'IN', 'DT', 'NNS', '.']",14
natural_language_inference,3,31,"Specifically , we propose two interdependent ways for the coupled - LSTMs : loosely coupled model ( LC - LSTMs ) and tightly coupled model ( TC - LSTMs ) .","['Specifically', ',', 'we', 'propose', 'two', 'interdependent', 'ways', 'for', 'the', 'coupled', '-', 'LSTMs', ':', 'loosely', 'coupled', 'model', '(', 'LC', '-', 'LSTMs', ')', 'and', 'tightly', 'coupled', 'model', '(', 'TC', '-', 'LSTMs', ')', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'CD', 'JJ', 'NNS', 'IN', 'DT', 'VBN', ':', 'NN', ':', 'RB', 'VBN', 'NN', '(', 'NNP', ':', 'NNP', ')', 'CC', 'RB', 'VBN', 'NN', '(', 'NNP', ':', 'NN', ')', '.']",31
natural_language_inference,3,33,"To utilize all the information of four directions of coupled - LSTMs , we aggregate them and adopt a dynamic pooling strategy to automatically select the most informative interaction signals .","['To', 'utilize', 'all', 'the', 'information', 'of', 'four', 'directions', 'of', 'coupled', '-', 'LSTMs', ',', 'we', 'aggregate', 'them', 'and', 'adopt', 'a', 'dynamic', 'pooling', 'strategy', 'to', 'automatically', 'select', 'the', 'most', 'informative', 'interaction', 'signals', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'PDT', 'DT', 'NN', 'IN', 'CD', 'NNS', 'IN', 'VBN', ':', 'NN', ',', 'PRP', 'VBP', 'PRP', 'CC', 'VB', 'DT', 'JJ', 'NN', 'NN', 'TO', 'RB', 'VB', 'DT', 'RBS', 'JJ', 'NN', 'NNS', '.']",31
natural_language_inference,3,34,"Finally , we feed them into a fully connected layer , followed by an output layer to compute the matching score .","['Finally', ',', 'we', 'feed', 'them', 'into', 'a', 'fully', 'connected', 'layer', ',', 'followed', 'by', 'an', 'output', 'layer', 'to', 'compute', 'the', 'matching', 'score', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'PRP', 'IN', 'DT', 'RB', 'VBN', 'NN', ',', 'VBN', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', '.']",22
natural_language_inference,3,151,"The word embeddings for all of the models are initialized with the 100d GloVe vectors ( 840B token version , ) and fine - tuned during training to improve the performance .","['The', 'word', 'embeddings', 'for', 'all', 'of', 'the', 'models', 'are', 'initialized', 'with', 'the', '100d', 'GloVe', 'vectors', '(', '840B', 'token', 'version', ',', ')', 'and', 'fine', '-', 'tuned', 'during', 'training', 'to', 'improve', 'the', 'performance', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['DT', 'NN', 'NNS', 'IN', 'DT', 'IN', 'DT', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'CD', 'NNP', 'NNS', '(', 'CD', 'VBN', 'NN', ',', ')', 'CC', 'JJ', ':', 'VBN', 'IN', 'NN', 'TO', 'VB', 'DT', 'NN', '.']",32
natural_language_inference,3,152,"The other parameters are initialized by randomly sampling from uniform distribution in [ ? 0.1 , 0.1 ] .","['The', 'other', 'parameters', 'are', 'initialized', 'by', 'randomly', 'sampling', 'from', 'uniform', 'distribution', 'in', '[', '?', '0.1', ',', '0.1', ']', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'RB', 'VBG', 'IN', 'JJ', 'NN', 'IN', 'NNP', '.', 'CD', ',', 'CD', 'NN', '.']",19
natural_language_inference,3,153,"For each task , we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value","['For', 'each', 'task', ',', 'we', 'take', 'the', 'hyperparameters', 'which', 'achieve', 'the', 'best', 'performance', 'on', 'the', 'development', 'set', 'via', 'an', 'small', 'grid', 'search', 'over', 'combinations', 'of', 'the', 'initial', 'learning', 'rate', '[', '0.05', ',', '0.0005', ',', '0.0001', ']', ',', 'l', '2', 'regularization', '[', '0.0', ',', '5', 'E?', '5', ',', '1E?', '5', ',', '1E?', '6', ']', 'and', 'the', 'threshold', 'value']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNS', 'WDT', 'VBP', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', 'VBD', 'CD', ',', 'CD', ',', 'CD', 'NN', ',', 'VBZ', 'CD', 'NN', 'NN', 'CD', ',', 'CD', 'NNP', 'CD', ',', 'CD', 'CD', ',', 'CD', 'CD', 'NN', 'CC', 'DT', 'NN', 'NN']",57
natural_language_inference,3,155,Neural bag - of - words ( NBOW ) :,"['Neural', 'bag', '-', 'of', '-', 'words', '(', 'NBOW', ')', ':']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'SYM', ':', 'IN', ':', 'NNS', '(', 'NNP', ')', ':']",10
natural_language_inference,3,156,"Each sequence as the sum of the embeddings of the words it contains , then they are concatenated and fed to a MLP .","['Each', 'sequence', 'as', 'the', 'sum', 'of', 'the', 'embeddings', 'of', 'the', 'words', 'it', 'contains', ',', 'then', 'they', 'are', 'concatenated', 'and', 'fed', 'to', 'a', 'MLP', '.']","['B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNS', 'PRP', 'VBZ', ',', 'RB', 'PRP', 'VBP', 'VBN', 'CC', 'VBN', 'TO', 'DT', 'NNP', '.']",24
natural_language_inference,3,157,"Single LSTM : A single LSTM to encode the two sequences , which is used in .","['Single', 'LSTM', ':', 'A', 'single', 'LSTM', 'to', 'encode', 'the', 'two', 'sequences', ',', 'which', 'is', 'used', 'in', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', ':', 'DT', 'JJ', 'NNP', 'TO', 'VB', 'DT', 'CD', 'NNS', ',', 'WDT', 'VBZ', 'VBN', 'IN', '.']",17
natural_language_inference,3,158,"Parallel LSTMs : Two sequences are encoded by two LSTMs separately , then they are concatenated and fed to a MLP .","['Parallel', 'LSTMs', ':', 'Two', 'sequences', 'are', 'encoded', 'by', 'two', 'LSTMs', 'separately', ',', 'then', 'they', 'are', 'concatenated', 'and', 'fed', 'to', 'a', 'MLP', '.']","['B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['NNP', 'NNP', ':', 'CD', 'NNS', 'VBP', 'VBN', 'IN', 'CD', 'NNP', 'RB', ',', 'RB', 'PRP', 'VBP', 'VBN', 'CC', 'VBN', 'TO', 'DT', 'NNP', '.']",22
natural_language_inference,3,159,"Attention LSTMs : An attentive LSTM to encode two sentences into a semantic space , which used in .","['Attention', 'LSTMs', ':', 'An', 'attentive', 'LSTM', 'to', 'encode', 'two', 'sentences', 'into', 'a', 'semantic', 'space', ',', 'which', 'used', 'in', '.']","['B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', ':', 'DT', 'JJ', 'NNP', 'TO', 'VB', 'CD', 'NNS', 'IN', 'DT', 'JJ', 'NN', ',', 'WDT', 'VBD', 'IN', '.']",19
natural_language_inference,3,160,Experiment - I : Recognizing Textual Entailment,"['Experiment', '-', 'I', ':', 'Recognizing', 'Textual', 'Entailment']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'PRP', ':', 'VBG', 'NNP', 'NNP']",7
natural_language_inference,3,169,"Our proposed two C - LSTMs models with four stacked blocks outperform all the competitor models , which indicates that our thinner and deeper network does work effectively .","['Our', 'proposed', 'two', 'C', '-', 'LSTMs', 'models', 'with', 'four', 'stacked', 'blocks', 'outperform', 'all', 'the', 'competitor', 'models', ',', 'which', 'indicates', 'that', 'our', 'thinner', 'and', 'deeper', 'network', 'does', 'work', 'effectively', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'VBN', 'CD', 'NNP', ':', 'NNP', 'NNS', 'IN', 'CD', 'VBD', 'NNS', 'IN', 'PDT', 'DT', 'NN', 'NNS', ',', 'WDT', 'VBZ', 'IN', 'PRP$', 'NN', 'CC', 'JJR', 'NN', 'VBZ', 'VB', 'RB', '.']",29
natural_language_inference,3,172,"Compared with attention LSTMs , our two models achieve comparable results to them using much fewer parameters ( nearly 1 / 5 ) .","['Compared', 'with', 'attention', 'LSTMs', ',', 'our', 'two', 'models', 'achieve', 'comparable', 'results', 'to', 'them', 'using', 'much', 'fewer', 'parameters', '(', 'nearly', '1', '/', '5', ')', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'NN', 'NNP', ',', 'PRP$', 'CD', 'NNS', 'VBP', 'JJ', 'NNS', 'TO', 'PRP', 'VBG', 'JJ', 'JJR', 'NNS', '(', 'RB', 'CD', 'JJ', 'CD', ')', '.']",24
natural_language_inference,3,173,"By stacking C - LSTMs , the performance of them are improved significantly , and the four stacked TC - LSTMs achieve 85.1 % accuracy on this dataset .","['By', 'stacking', 'C', '-', 'LSTMs', ',', 'the', 'performance', 'of', 'them', 'are', 'improved', 'significantly', ',', 'and', 'the', 'four', 'stacked', 'TC', '-', 'LSTMs', 'achieve', '85.1', '%', 'accuracy', 'on', 'this', 'dataset', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBG', 'NNP', ':', 'NNP', ',', 'DT', 'NN', 'IN', 'PRP', 'VBP', 'VBN', 'RB', ',', 'CC', 'DT', 'CD', 'VBD', 'NNP', ':', 'NNP', 'VBP', 'CD', 'NN', 'NN', 'IN', 'DT', 'NN', '.']",29
natural_language_inference,38,2,MEMEN : Multi-layer Embedding with Memory Networks for Machine Comprehension,"['MEMEN', ':', 'Multi-layer', 'Embedding', 'with', 'Memory', 'Networks', 'for', 'Machine', 'Comprehension']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NN', ':', 'JJ', 'VBG', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",10
natural_language_inference,38,4,Machine comprehension ( MC ) style question answering is a representative problem in natural language processing .,"['Machine', 'comprehension', '(', 'MC', ')', 'style', 'question', 'answering', 'is', 'a', 'representative', 'problem', 'in', 'natural', 'language', 'processing', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', '(', 'NNP', ')', 'NN', 'NN', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",17
natural_language_inference,38,12,Machine comprehension ( MC ) has gained significant popularity over the past few years and it is a coveted goal in the field of natural language processing and artificial intelligence .,"['Machine', 'comprehension', '(', 'MC', ')', 'has', 'gained', 'significant', 'popularity', 'over', 'the', 'past', 'few', 'years', 'and', 'it', 'is', 'a', 'coveted', 'goal', 'in', 'the', 'field', 'of', 'natural', 'language', 'processing', 'and', 'artificial', 'intelligence', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', '(', 'NNP', ')', 'VBZ', 'VBN', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NNS', 'CC', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', 'CC', 'JJ', 'NN', '.']",31
natural_language_inference,38,27,"In this paper , we introduce the Multi - layer Embedding with Memory Networks ( MEMEN ) , an end - to - end neural network for machine comprehension task .","['In', 'this', 'paper', ',', 'we', 'introduce', 'the', 'Multi', '-', 'layer', 'Embedding', 'with', 'Memory', 'Networks', '(', 'MEMEN', ')', ',', 'an', 'end', '-', 'to', '-', 'end', 'neural', 'network', 'for', 'machine', 'comprehension', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', ':', 'NN', 'VBG', 'IN', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'DT', 'NN', ':', 'TO', ':', 'VB', 'JJ', 'NN', 'IN', 'NN', 'NN', 'NN', '.']",31
natural_language_inference,38,28,Our model consists of three parts :,"['Our', 'model', 'consists', 'of', 'three', 'parts', ':']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'IN', 'CD', 'NNS', ':']",7
natural_language_inference,38,29,"1 ) the encoding of context and query , in which we add useful syntactic and semantic information in the embedding of every word , 2 ) the high - efficiency multilayer memory network of full - orientation matching to match the question and context , 3 ) the pointer - network based answer boundary prediction layer to get the location of the answer in the passage .","['1', ')', 'the', 'encoding', 'of', 'context', 'and', 'query', ',', 'in', 'which', 'we', 'add', 'useful', 'syntactic', 'and', 'semantic', 'information', 'in', 'the', 'embedding', 'of', 'every', 'word', ',', '2', ')', 'the', 'high', '-', 'efficiency', 'multilayer', 'memory', 'network', 'of', 'full', '-', 'orientation', 'matching', 'to', 'match', 'the', 'question', 'and', 'context', ',', '3', ')', 'the', 'pointer', '-', 'network', 'based', 'answer', 'boundary', 'prediction', 'layer', 'to', 'get', 'the', 'location', 'of', 'the', 'answer', 'in', 'the', 'passage', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']","['CD', ')', 'DT', 'NN', 'IN', 'NN', 'CC', 'NN', ',', 'IN', 'WDT', 'PRP', 'VBP', 'JJ', 'JJ', 'CC', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'CD', ')', 'DT', 'JJ', ':', 'NN', 'NN', 'NN', 'NN', 'IN', 'JJ', ':', 'NN', 'VBG', 'TO', 'VB', 'DT', 'NN', 'CC', 'NN', ',', 'CD', ')', 'DT', 'NN', ':', 'NN', 'VBN', 'NN', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",68
natural_language_inference,38,110,The tokenizers we use in the step of preprocessing data are from Stanford CoreNLP .,"['The', 'tokenizers', 'we', 'use', 'in', 'the', 'step', 'of', 'preprocessing', 'data', 'are', 'from', 'Stanford', 'CoreNLP', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'IN', 'VBG', 'NNS', 'VBP', 'IN', 'NNP', 'NNP', '.']",15
natural_language_inference,38,111,We also use part - of - speech tagger and named - entity recognition tagger in Stanford CoreNLP utilities to transform the passage and question .,"['We', 'also', 'use', 'part', '-', 'of', '-', 'speech', 'tagger', 'and', 'named', '-', 'entity', 'recognition', 'tagger', 'in', 'Stanford', 'CoreNLP', 'utilities', 'to', 'transform', 'the', 'passage', 'and', 'question', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'NN', ':', 'IN', ':', 'NN', 'NN', 'CC', 'VBN', ':', 'NN', 'NN', 'NN', 'IN', 'NNP', 'NNP', 'NNS', 'TO', 'VB', 'DT', 'NN', 'CC', 'NN', '.']",26
natural_language_inference,38,112,"For the skip - gram model , our model refers to the word2 vec module in open source software library , Tensorflow , the skip window is set as 2 .","['For', 'the', 'skip', '-', 'gram', 'model', ',', 'our', 'model', 'refers', 'to', 'the', 'word2', 'vec', 'module', 'in', 'open', 'source', 'software', 'library', ',', 'Tensorflow', ',', 'the', 'skip', 'window', 'is', 'set', 'as', '2', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'DT', 'NN', ':', 'NN', 'NN', ',', 'PRP$', 'NN', 'NNS', 'TO', 'DT', 'NN', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NN', 'NN', ',', 'NNP', ',', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', '.']",31
natural_language_inference,38,114,"To improve the reliability and stabllity , we screen out the sentences whose length are shorter than 9 .","['To', 'improve', 'the', 'reliability', 'and', 'stabllity', ',', 'we', 'screen', 'out', 'the', 'sentences', 'whose', 'length', 'are', 'shorter', 'than', '9', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', 'CC', 'NN', ',', 'PRP', 'VBP', 'RP', 'DT', 'NNS', 'WP$', 'NN', 'VBP', 'JJR', 'IN', 'CD', '.']",19
natural_language_inference,38,115,"We use 100 one dimensional filters for CNN in the character level embedding , with width of 5 for each one .","['We', 'use', '100', 'one', 'dimensional', 'filters', 'for', 'CNN', 'in', 'the', 'character', 'level', 'embedding', ',', 'with', 'width', 'of', '5', 'for', 'each', 'one', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'CD', 'NN', 'NNS', 'IN', 'NNP', 'IN', 'DT', 'NN', 'NN', 'NN', ',', 'IN', 'NN', 'IN', 'CD', 'IN', 'DT', 'CD', '.']",22
natural_language_inference,38,116,We set the hidden size as 100 for all the LSTM and GRU layers and apply dropout between layers with a dropout ratio as 0.2 .,"['We', 'set', 'the', 'hidden', 'size', 'as', '100', 'for', 'all', 'the', 'LSTM', 'and', 'GRU', 'layers', 'and', 'apply', 'dropout', 'between', 'layers', 'with', 'a', 'dropout', 'ratio', 'as', '0.2', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'CD', 'IN', 'PDT', 'DT', 'NNP', 'CC', 'NNP', 'NNS', 'CC', 'VB', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",26
natural_language_inference,38,117,"We use the AdaDelta ( Zeiler , 2012 ) optimizer with a initial learning rate as 0.001 .","['We', 'use', 'the', 'AdaDelta', '(', 'Zeiler', ',', '2012', ')', 'optimizer', 'with', 'a', 'initial', 'learning', 'rate', 'as', '0.001', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', '(', 'NNP', ',', 'CD', ')', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', '.']",18
natural_language_inference,38,118,"For the memory networks , we set the number of layer as 3 .","['For', 'the', 'memory', 'networks', ',', 'we', 'set', 'the', 'number', 'of', 'layer', 'as', '3', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NN', 'NNS', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'IN', 'CD', '.']",14
natural_language_inference,38,129,"As we can see in , our model outperforms all other baselines and achieves the state - of - the - art result on all subsets on TriviaQA .","['As', 'we', 'can', 'see', 'in', ',', 'our', 'model', 'outperforms', 'all', 'other', 'baselines', 'and', 'achieves', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'result', 'on', 'all', 'subsets', 'on', 'TriviaQA', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'PRP', 'MD', 'VB', 'IN', ',', 'PRP$', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'CC', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'DT', 'NNS', 'IN', 'NNP', '.']",29
natural_language_inference,38,132,We also use the Stanford Question Answering Dataset ( SQuAD ) v 1.1 to conduct our experiments .,"['We', 'also', 'use', 'the', 'Stanford', 'Question', 'Answering', 'Dataset', '(', 'SQuAD', ')', 'v', '1.1', 'to', 'conduct', 'our', 'experiments', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'VBD', 'CD', 'TO', 'VB', 'PRP$', 'NNS', '.']",18
natural_language_inference,38,138,"The results of this dataset are all exhibited on a leaderboard , and top methods are almost all ensemble models , our model achieves an exact match score of 75.37 % and an F1 score of 82 . 66 % , which is competitive to state - of - the - art method .","['The', 'results', 'of', 'this', 'dataset', 'are', 'all', 'exhibited', 'on', 'a', 'leaderboard', ',', 'and', 'top', 'methods', 'are', 'almost', 'all', 'ensemble', 'models', ',', 'our', 'model', 'achieves', 'an', 'exact', 'match', 'score', 'of', '75.37', '%', 'and', 'an', 'F1', 'score', 'of', '82', '.', '66', '%', ',', 'which', 'is', 'competitive', 'to', 'state', '-', 'of', '-', 'the', '-', 'art', 'method', '.']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'DT', 'VBN', 'IN', 'DT', 'NN', ',', 'CC', 'JJ', 'NNS', 'VBP', 'RB', 'DT', 'JJ', 'NNS', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'NN', 'CC', 'DT', 'NNP', 'NN', 'IN', 'CD', '.', 'CD', 'NN', ',', 'WDT', 'VBZ', 'JJ', 'TO', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', '.']",54
natural_language_inference,38,156,We also run the ablations of our single model on SQ u AD dev set to evaluate the individual contribution .,"['We', 'also', 'run', 'the', 'ablations', 'of', 'our', 'single', 'model', 'on', 'SQ', 'u', 'AD', 'dev', 'set', 'to', 'evaluate', 'the', 'individual', 'contribution', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NNS', 'IN', 'PRP$', 'JJ', 'NN', 'IN', 'NNP', 'JJ', 'NNP', 'NN', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NN', '.']",21
natural_language_inference,38,157,"As shows , both syntactic embeddings and semantic embeddings contribute towards the model 's performance and the POS tags seem to be more important .","['As', 'shows', ',', 'both', 'syntactic', 'embeddings', 'and', 'semantic', 'embeddings', 'contribute', 'towards', 'the', 'model', ""'s"", 'performance', 'and', 'the', 'POS', 'tags', 'seem', 'to', 'be', 'more', 'important', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'NNS', ',', 'DT', 'JJ', 'NNS', 'CC', 'JJ', 'NNS', 'VBP', 'IN', 'DT', 'NN', 'POS', 'NN', 'CC', 'DT', 'NNP', 'NNS', 'VBP', 'TO', 'VB', 'RBR', 'JJ', '.']",25
natural_language_inference,38,160,"For ablating integral query matching , the result drops about 2 % on both metrics and it shows that the integral information of query for each word in passage is crucial .","['For', 'ablating', 'integral', 'query', 'matching', ',', 'the', 'result', 'drops', 'about', '2', '%', 'on', 'both', 'metrics', 'and', 'it', 'shows', 'that', 'the', 'integral', 'information', 'of', 'query', 'for', 'each', 'word', 'in', 'passage', 'is', 'crucial', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['IN', 'VBG', 'JJ', 'NN', 'NN', ',', 'DT', 'NN', 'VBZ', 'IN', 'CD', 'NN', 'IN', 'DT', 'NNS', 'CC', 'PRP', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'VBZ', 'JJ', '.']",32
natural_language_inference,38,161,"The query - based similarity matching accounts for about 10 % performance degradation , which proves the effectiveness of alignment context words against query .","['The', 'query', '-', 'based', 'similarity', 'matching', 'accounts', 'for', 'about', '10', '%', 'performance', 'degradation', ',', 'which', 'proves', 'the', 'effectiveness', 'of', 'alignment', 'context', 'words', 'against', 'query', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', ':', 'VBN', 'NN', 'VBG', 'NNS', 'IN', 'RB', 'CD', 'NN', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NNS', 'IN', 'NN', '.']",25
natural_language_inference,38,162,"For context - based similarity matching , we simply took out the M 3 from the linear function and it is proved to be contributory to the performance of full - orientation matching .","['For', 'context', '-', 'based', 'similarity', 'matching', ',', 'we', 'simply', 'took', 'out', 'the', 'M', '3', 'from', 'the', 'linear', 'function', 'and', 'it', 'is', 'proved', 'to', 'be', 'contributory', 'to', 'the', 'performance', 'of', 'full', '-', 'orientation', 'matching', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ':', 'VBN', 'NN', 'NN', ',', 'PRP', 'RB', 'VBD', 'RP', 'DT', 'NNP', 'CD', 'IN', 'DT', 'JJ', 'NN', 'CC', 'PRP', 'VBZ', 'VBN', 'TO', 'VB', 'JJ', 'TO', 'DT', 'NN', 'IN', 'JJ', ':', 'NN', 'NN', '.']",34
natural_language_inference,89,2,Read + Verify : Machine Reading Comprehension with Unanswerable Questions,"['Read', '+', 'Verify', ':', 'Machine', 'Reading', 'Comprehension', 'with', 'Unanswerable', 'Questions']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', ':', 'NN', 'VBG', 'NNP', 'IN', 'JJ', 'NNS']",10
natural_language_inference,89,26,"To address the above issue , we propose a read - then - verify system that aims to be robust to unanswerable questions in this paper .","['To', 'address', 'the', 'above', 'issue', ',', 'we', 'propose', 'a', 'read', '-', 'then', '-', 'verify', 'system', 'that', 'aims', 'to', 'be', 'robust', 'to', 'unanswerable', 'questions', 'in', 'this', 'paper', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', ':', 'RB', ':', 'NN', 'NN', 'WDT', 'VBZ', 'TO', 'VB', 'JJ', 'TO', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']",27
natural_language_inference,89,27,"As shown in , our system consists of two components : ( 1 ) a no-answer reader for extracting candidate answers and detecting unanswerable questions , and ( 2 ) an answer verifier for deciding whether or not the extracted candidate is legitimate .","['As', 'shown', 'in', ',', 'our', 'system', 'consists', 'of', 'two', 'components', ':', '(', '1', ')', 'a', 'no-answer', 'reader', 'for', 'extracting', 'candidate', 'answers', 'and', 'detecting', 'unanswerable', 'questions', ',', 'and', '(', '2', ')', 'an', 'answer', 'verifier', 'for', 'deciding', 'whether', 'or', 'not', 'the', 'extracted', 'candidate', 'is', 'legitimate', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'VBN', 'IN', ',', 'PRP$', 'NN', 'VBZ', 'IN', 'CD', 'NNS', ':', '(', 'CD', ')', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'NN', 'NNS', 'CC', 'VBG', 'JJ', 'NNS', ',', 'CC', '(', 'CD', ')', 'DT', 'NN', 'NN', 'IN', 'VBG', 'IN', 'CC', 'RB', 'DT', 'JJ', 'NN', 'VBZ', 'JJ', '.']",44
natural_language_inference,89,29,"First , we augment existing readers with two auxiliary losses , to better handle answer extraction and no - answer detection respectively .","['First', ',', 'we', 'augment', 'existing', 'readers', 'with', 'two', 'auxiliary', 'losses', ',', 'to', 'better', 'handle', 'answer', 'extraction', 'and', 'no', '-', 'answer', 'detection', 'respectively', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'VBG', 'NNS', 'IN', 'CD', 'JJ', 'NNS', ',', 'TO', 'RBR', 'VB', 'JJR', 'NN', 'CC', 'DT', ':', 'NN', 'NN', 'RB', '.']",23
natural_language_inference,89,32,We solve this problem by introducing an independent span loss that aims to concentrate on the answer extraction task regardless of the answerability of the question .,"['We', 'solve', 'this', 'problem', 'by', 'introducing', 'an', 'independent', 'span', 'loss', 'that', 'aims', 'to', 'concentrate', 'on', 'the', 'answer', 'extraction', 'task', 'regardless', 'of', 'the', 'answerability', 'of', 'the', 'question', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'TO', 'VB', 'IN', 'DT', 'NN', 'NN', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",27
natural_language_inference,89,33,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is normalized with the no -answer score and the other is used for our auxiliary loss .","['In', 'order', 'to', 'not', 'conflict', 'with', 'no', '-', 'answer', 'detection', ',', 'we', 'leverage', 'a', 'multi-head', 'pointer', 'network', 'to', 'generate', 'two', 'pairs', 'of', 'span', 'scores', ',', 'where', 'one', 'pair', 'is', 'normalized', 'with', 'the', 'no', '-answer', 'score', 'and', 'the', 'other', 'is', 'used', 'for', 'our', 'auxiliary', 'loss', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'RB', 'VB', 'IN', 'DT', ':', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'CD', 'NNS', 'IN', 'NN', 'NNS', ',', 'WRB', 'CD', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'DT', 'NNP', 'NN', 'CC', 'DT', 'JJ', 'VBZ', 'VBN', 'IN', 'PRP$', 'JJ', 'NN', '.']",45
natural_language_inference,89,34,"Besides , we present another independent noanswer loss to further alleviate the confliction , by focusing on the no-answer detection task without considering the shared normalization of answer extraction .","['Besides', ',', 'we', 'present', 'another', 'independent', 'noanswer', 'loss', 'to', 'further', 'alleviate', 'the', 'confliction', ',', 'by', 'focusing', 'on', 'the', 'no-answer', 'detection', 'task', 'without', 'considering', 'the', 'shared', 'normalization', 'of', 'answer', 'extraction', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJR', 'NN', 'TO', 'JJ', 'VB', 'DT', 'NN', ',', 'IN', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'VBG', 'DT', 'VBN', 'NN', 'IN', 'JJR', 'NN', '.']",30
natural_language_inference,89,35,"Second , in addition to the standard reading phase , we introduce an additional answer verifying phase , which aims at finding local entailment that supports the answer by comparing the answer sentence with the question .","['Second', ',', 'in', 'addition', 'to', 'the', 'standard', 'reading', 'phase', ',', 'we', 'introduce', 'an', 'additional', 'answer', 'verifying', 'phase', ',', 'which', 'aims', 'at', 'finding', 'local', 'entailment', 'that', 'supports', 'the', 'answer', 'by', 'comparing', 'the', 'answer', 'sentence', 'with', 'the', 'question', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', ',', 'IN', 'NN', 'TO', 'DT', 'JJ', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBG', 'NN', ',', 'WDT', 'VBZ', 'IN', 'VBG', 'JJ', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', '.']",37
natural_language_inference,89,39,"Inspired by recent advances in natural language inference ( NLI ) , we investigate three different architectures for the answer verifying task .","['Inspired', 'by', 'recent', 'advances', 'in', 'natural', 'language', 'inference', '(', 'NLI', ')', ',', 'we', 'investigate', 'three', 'different', 'architectures', 'for', 'the', 'answer', 'verifying', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['VBN', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', ',', 'PRP', 'VBP', 'CD', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'VBG', 'NN', '.']",23
natural_language_inference,89,40,"The first one is a sequential model that takes two sentences as along sequence , while the second one attempts to capture interactions between two sentences .","['The', 'first', 'one', 'is', 'a', 'sequential', 'model', 'that', 'takes', 'two', 'sentences', 'as', 'along', 'sequence', ',', 'while', 'the', 'second', 'one', 'attempts', 'to', 'capture', 'interactions', 'between', 'two', 'sentences', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'CD', 'NNS', 'IN', 'JJ', 'NN', ',', 'IN', 'DT', 'JJ', 'CD', 'VBZ', 'TO', 'VB', 'NNS', 'IN', 'CD', 'NNS', '.']",27
natural_language_inference,89,41,The last one is a hybrid model that combines the above two models to test if the performance can be further improved .,"['The', 'last', 'one', 'is', 'a', 'hybrid', 'model', 'that', 'combines', 'the', 'above', 'two', 'models', 'to', 'test', 'if', 'the', 'performance', 'can', 'be', 'further', 'improved', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'CD', 'NNS', 'TO', 'VB', 'IN', 'DT', 'NN', 'MD', 'VB', 'RB', 'VBN', '.']",23
natural_language_inference,89,177,"We run a grid search on ? and ? among [ 0.1 , 0.3 , 0.5 , 0.7 , 1 , 2 ] .","['We', 'run', 'a', 'grid', 'search', 'on', '?', 'and', '?', 'among', '[', '0.1', ',', '0.3', ',', '0.5', ',', '0.7', ',', '1', ',', '2', ']', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', '.', 'CC', '.', 'IN', 'JJ', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', 'NN', '.']",24
natural_language_inference,89,179,"As for answer verifiers , we use the original configuration from for Model - I. For Model - II , the Adam optimizer ( Kingma and Ba 2014 ) with a learning rate of 0.0008 is used , the hidden size is set as 300 , and a dropout ) of 0.3 is applied for preventing overfitting .","['As', 'for', 'answer', 'verifiers', ',', 'we', 'use', 'the', 'original', 'configuration', 'from', 'for', 'Model', '-', 'I.', 'For', 'Model', '-', 'II', ',', 'the', 'Adam', 'optimizer', '(', 'Kingma', 'and', 'Ba', '2014', ')', 'with', 'a', 'learning', 'rate', 'of', '0.0008', 'is', 'used', ',', 'the', 'hidden', 'size', 'is', 'set', 'as', '300', ',', 'and', 'a', 'dropout', ')', 'of', '0.3', 'is', 'applied', 'for', 'preventing', 'overfitting', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'IN', 'NN', 'NNS', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'IN', 'NNP', ':', 'NN', 'IN', 'NNP', ':', 'NNP', ',', 'DT', 'NNP', 'NN', '(', 'NNP', 'CC', 'NNP', 'CD', ')', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', 'VBZ', 'VBN', ',', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'CD', ',', 'CC', 'DT', 'NN', ')', 'IN', 'CD', 'VBZ', 'VBN', 'IN', 'VBG', 'NN', '.']",58
natural_language_inference,89,180,"The batch size is 48 for the reader , 64 for Model - II , and 32 for Model - I as well as Model - III .","['The', 'batch', 'size', 'is', '48', 'for', 'the', 'reader', ',', '64', 'for', 'Model', '-', 'II', ',', 'and', '32', 'for', 'Model', '-', 'I', 'as', 'well', 'as', 'Model', '-', 'III', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'CD', 'IN', 'DT', 'NN', ',', 'CD', 'IN', 'NNP', ':', 'NNP', ',', 'CC', 'CD', 'IN', 'NNP', ':', 'PRP', 'RB', 'RB', 'IN', 'NNP', ':', 'NN', '.']",28
natural_language_inference,89,181,"We use the Glo Ve 100D embeddings for the reader , and 300D embeddings for Model - II and Model - III .","['We', 'use', 'the', 'Glo', 'Ve', '100D', 'embeddings', 'for', 'the', 'reader', ',', 'and', '300D', 'embeddings', 'for', 'Model', '-', 'II', 'and', 'Model', '-', 'III', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NNP', 'CD', 'NNS', 'IN', 'DT', 'NN', ',', 'CC', 'CD', 'NNS', 'IN', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NN', '.']",23
natural_language_inference,89,182,"We utilize the nltk tokenizer 3 to preprocess passages and questions , as well as split sentences .","['We', 'utilize', 'the', 'nltk', 'tokenizer', '3', 'to', 'preprocess', 'passages', 'and', 'questions', ',', 'as', 'well', 'as', 'split', 'sentences', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'CD', 'TO', 'VB', 'NNS', 'CC', 'NNS', ',', 'RB', 'RB', 'IN', 'NN', 'NNS', '.']",18
natural_language_inference,89,187,"As we can see , our system obtains state - of the - art results by achieving an EM score of 71.7 and a F 1 score of 74.2 on the test set .","['As', 'we', 'can', 'see', ',', 'our', 'system', 'obtains', 'state', '-', 'of', 'the', '-', 'art', 'results', 'by', 'achieving', 'an', 'EM', 'score', 'of', '71.7', 'and', 'a', 'F', '1', 'score', 'of', '74.2', 'on', 'the', 'test', 'set', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'PRP', 'MD', 'VB', ',', 'PRP$', 'NN', 'VBZ', 'NN', ':', 'IN', 'DT', ':', 'NN', 'NNS', 'IN', 'VBG', 'DT', 'NNP', 'NN', 'IN', 'CD', 'CC', 'DT', 'NNP', 'CD', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', 'NN', '.']",34
natural_language_inference,89,188,Notice that SLQA + has reached a comparable result compared to our approach .,"['Notice', 'that', 'SLQA', '+', 'has', 'reached', 'a', 'comparable', 'result', 'compared', 'to', 'our', 'approach', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['NNP', 'WDT', 'NNP', 'NNP', 'VBZ', 'VBN', 'DT', 'JJ', 'NN', 'VBN', 'TO', 'PRP$', 'NN', '.']",14
natural_language_inference,89,193,"Removing the independent span loss ( indep - I ) results in a performance drop for all answerable questions ( HasAns ) , indicating that this loss helps the model in better identifying the answer boundary .","['Removing', 'the', 'independent', 'span', 'loss', '(', 'indep', '-', 'I', ')', 'results', 'in', 'a', 'performance', 'drop', 'for', 'all', 'answerable', 'questions', '(', 'HasAns', ')', ',', 'indicating', 'that', 'this', 'loss', 'helps', 'the', 'model', 'in', 'better', 'identifying', 'the', 'answer', 'boundary', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'DT', 'JJ', 'JJ', 'NN', '(', 'JJ', ':', 'PRP', ')', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNS', '(', 'NNP', ')', ',', 'VBG', 'IN', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'JJR', 'VBG', 'DT', 'NN', 'NN', '.']",37
natural_language_inference,89,194,"Ablating independent no - answer loss ( indep - II ) , on the other hand , causes little influence on HasAns , but leads to a severe decline on no - answer accuracy ( NoAns ACC ) .","['Ablating', 'independent', 'no', '-', 'answer', 'loss', '(', 'indep', '-', 'II', ')', ',', 'on', 'the', 'other', 'hand', ',', 'causes', 'little', 'influence', 'on', 'HasAns', ',', 'but', 'leads', 'to', 'a', 'severe', 'decline', 'on', 'no', '-', 'answer', 'accuracy', '(', 'NoAns', 'ACC', ')', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'JJ', 'DT', ':', 'NN', 'NN', '(', 'JJ', ':', 'NN', ')', ',', 'IN', 'DT', 'JJ', 'NN', ',', 'VBZ', 'JJ', 'NN', 'IN', 'NNP', ',', 'CC', 'VBZ', 'TO', 'DT', 'JJ', 'NN', 'IN', 'DT', ':', 'NN', 'NN', '(', 'NNP', 'NNP', ')', '.']",39
natural_language_inference,89,196,"Finally , deleting both of two losses causes a degradation of more than 1.5 points on the over all performance in terms of F1 , with or without ELMo embeddings .","['Finally', ',', 'deleting', 'both', 'of', 'two', 'losses', 'causes', 'a', 'degradation', 'of', 'more', 'than', '1.5', 'points', 'on', 'the', 'over', 'all', 'performance', 'in', 'terms', 'of', 'F1', ',', 'with', 'or', 'without', 'ELMo', 'embeddings', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['RB', ',', 'VBG', 'DT', 'IN', 'CD', 'NNS', 'VBZ', 'DT', 'NN', 'IN', 'JJR', 'IN', 'CD', 'NNS', 'IN', 'DT', 'IN', 'DT', 'NN', 'IN', 'NNS', 'IN', 'NNP', ',', 'IN', 'CC', 'IN', 'NNP', 'NNS', '.']",31
natural_language_inference,89,200,"Adding ELMo embeddings , however , does not boost the performance .","['Adding', 'ELMo', 'embeddings', ',', 'however', ',', 'does', 'not', 'boost', 'the', 'performance', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O']","['VBG', 'NNP', 'NNS', ',', 'RB', ',', 'VBZ', 'RB', 'VB', 'DT', 'NN', '.']",12
natural_language_inference,89,204,We find that the improvement on noanswer accuracy is significant .,"['We', 'find', 'that', 'the', 'improvement', 'on', 'noanswer', 'accuracy', 'is', 'significant', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NN', 'IN', 'JJR', 'NN', 'VBZ', 'JJ', '.']",11
natural_language_inference,89,210,We observe that RMR + ELMo + Verifier achieves the best precision when the recall is less than 80 .,"['We', 'observe', 'that', 'RMR', '+', 'ELMo', '+', 'Verifier', 'achieves', 'the', 'best', 'precision', 'when', 'the', 'recall', 'is', 'less', 'than', '80', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJS', 'NN', 'WRB', 'DT', 'NN', 'VBZ', 'JJR', 'IN', 'CD', '.']",20
natural_language_inference,89,212,"Ablating two auxiliary losses , however , leads to an over all degradation on the curve , but it still outperforms the baseline by a large margin .","['Ablating', 'two', 'auxiliary', 'losses', ',', 'however', ',', 'leads', 'to', 'an', 'over', 'all', 'degradation', 'on', 'the', 'curve', ',', 'but', 'it', 'still', 'outperforms', 'the', 'baseline', 'by', 'a', 'large', 'margin', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['VBG', 'CD', 'JJ', 'NNS', ',', 'RB', ',', 'VBZ', 'TO', 'DT', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'CC', 'PRP', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",28
natural_language_inference,80,2,DR- BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference,"['DR-', 'BiLSTM', ':', 'Dependent', 'Reading', 'Bidirectional', 'LSTM', 'for', 'Natural', 'Language', 'Inference']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['JJ', 'NNP', ':', 'JJ', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",11
natural_language_inference,80,4,We present a novel deep learning architecture to address the natural language inference ( NLI ) task .,"['We', 'present', 'a', 'novel', 'deep', 'learning', 'architecture', 'to', 'address', 'the', 'natural', 'language', 'inference', '(', 'NLI', ')', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'NN', '.']",18
natural_language_inference,80,12,"The goal of NLI is to identify the logical relationship ( entailment , neutral , or contradiction ) between a premise and a corresponding hypothesis .","['The', 'goal', 'of', 'NLI', 'is', 'to', 'identify', 'the', 'logical', 'relationship', '(', 'entailment', ',', 'neutral', ',', 'or', 'contradiction', ')', 'between', 'a', 'premise', 'and', 'a', 'corresponding', 'hypothesis', '.']","['O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'NNP', 'VBZ', 'TO', 'VB', 'DT', 'JJ', 'NN', '(', 'NN', ',', 'JJ', ',', 'CC', 'NN', ')', 'IN', 'DT', 'NN', 'CC', 'DT', 'JJ', 'NN', '.']",26
natural_language_inference,80,30,We propose a dependent reading bidirectional LSTM ( DR - BiLSTM ) model to address these limitations .,"['We', 'propose', 'a', 'dependent', 'reading', 'bidirectional', 'LSTM', '(', 'DR', '-', 'BiLSTM', ')', 'model', 'to', 'address', 'these', 'limitations', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'JJ', 'NNP', '(', 'NNP', ':', 'NNP', ')', 'NN', 'TO', 'VB', 'DT', 'NNS', '.']",18
natural_language_inference,80,31,"Given a premise u and a hypothesis v , our model first encodes them considering dependency on each other .","['Given', 'a', 'premise', 'u', 'and', 'a', 'hypothesis', 'v', ',', 'our', 'model', 'first', 'encodes', 'them', 'considering', 'dependency', 'on', 'each', 'other', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['VBN', 'DT', 'NN', 'NN', 'CC', 'DT', 'NN', 'NN', ',', 'PRP$', 'NN', 'RB', 'VBZ', 'PRP', 'VBG', 'NN', 'IN', 'DT', 'JJ', '.']",20
natural_language_inference,80,32,"Next , the model employs a soft attention mechanism to extract relevant information from these encodings .","['Next', ',', 'the', 'model', 'employs', 'a', 'soft', 'attention', 'mechanism', 'to', 'extract', 'relevant', 'information', 'from', 'these', 'encodings', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'JJ', 'NN', 'IN', 'DT', 'NNS', '.']",17
natural_language_inference,80,33,"The augmented sentence representations are then passed to the inference stage , which uses a similar dependent reading strategy in both directions , i.e. u ? v and v ? u .","['The', 'augmented', 'sentence', 'representations', 'are', 'then', 'passed', 'to', 'the', 'inference', 'stage', ',', 'which', 'uses', 'a', 'similar', 'dependent', 'reading', 'strategy', 'in', 'both', 'directions', ',', 'i.e.', 'u', '?', 'v', 'and', 'v', '?', 'u', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'TO', 'DT', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'NN', 'IN', 'DT', 'NNS', ',', 'FW', 'FW', '.', 'NN', 'CC', 'NN', '.', 'UH', '.']",32
natural_language_inference,80,34,"Finally , a decision is made through a multi - layer perceptron ( MLP ) based on the aggregated information .","['Finally', ',', 'a', 'decision', 'is', 'made', 'through', 'a', 'multi', '-', 'layer', 'perceptron', '(', 'MLP', ')', 'based', 'on', 'the', 'aggregated', 'information', '.']","['O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', ':', 'NN', 'NN', '(', 'NNP', ')', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",21
natural_language_inference,80,128,We use pre-trained 300 - D Glove 840B vectors to initialize our word embedding vectors .,"['We', 'use', 'pre-trained', '300', '-', 'D', 'Glove', '840B', 'vectors', 'to', 'initialize', 'our', 'word', 'embedding', 'vectors', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'CD', ':', 'NNP', 'NNP', 'CD', 'NNS', 'TO', 'VB', 'PRP$', 'NN', 'VBG', 'NNS', '.']",16
natural_language_inference,80,129,All hidden states of BiLSTMs during input encoding and inference have 450 dimensions ( r = 300 and d = 450 ) .,"['All', 'hidden', 'states', 'of', 'BiLSTMs', 'during', 'input', 'encoding', 'and', 'inference', 'have', '450', 'dimensions', '(', 'r', '=', '300', 'and', 'd', '=', '450', ')', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NNS', 'IN', 'NNP', 'IN', 'NN', 'VBG', 'CC', 'NN', 'VBP', 'CD', 'NNS', '(', 'VB', '$', 'CD', 'CC', 'VB', 'NNP', 'CD', ')', '.']",23
natural_language_inference,80,130,"The weights are learned by minimizing the log - loss on the training data via the Adam optimizer ( Kingma and Ba , 2014 ) .","['The', 'weights', 'are', 'learned', 'by', 'minimizing', 'the', 'log', '-', 'loss', 'on', 'the', 'training', 'data', 'via', 'the', 'Adam', 'optimizer', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'VBG', 'DT', 'NN', ':', 'NN', 'IN', 'DT', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', '.']",26
natural_language_inference,80,131,The initial learning rate is 0.0004 .,"['The', 'initial', 'learning', 'rate', 'is', '0.0004', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'CD', '.']",7
natural_language_inference,80,132,"To avoid overfitting , we use dropout with the rate of 0.4 for regularization , which is applied to all feedforward connections .","['To', 'avoid', 'overfitting', ',', 'we', 'use', 'dropout', 'with', 'the', 'rate', 'of', '0.4', 'for', 'regularization', ',', 'which', 'is', 'applied', 'to', 'all', 'feedforward', 'connections', '.']","['B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'NN', ',', 'PRP', 'VBP', 'IN', 'IN', 'DT', 'NN', 'IN', 'CD', 'IN', 'NN', ',', 'WDT', 'VBZ', 'VBN', 'TO', 'DT', 'JJ', 'NNS', '.']",23
natural_language_inference,80,133,"During training , the word embeddings are updated to learn effective representations for the NLI task .","['During', 'training', ',', 'the', 'word', 'embeddings', 'are', 'updated', 'to', 'learn', 'effective', 'representations', 'for', 'the', 'NLI', 'task', '.']","['B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'DT', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'DT', 'NNP', 'NN', '.']",17
natural_language_inference,80,134,We use a fairly small batch size of 32 to provide more exploration power to the model .,"['We', 'use', 'a', 'fairly', 'small', 'batch', 'size', 'of', '32', 'to', 'provide', 'more', 'exploration', 'power', 'to', 'the', 'model', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'RB', 'JJ', 'NN', 'NN', 'IN', 'CD', 'TO', 'VB', 'JJR', 'NN', 'NN', 'TO', 'DT', 'NN', '.']",18
natural_language_inference,80,174,DR - BiLSTM ( Single ) achieves 88.5 % accuracy on the test set which is noticeably the best reported result among the existing single models for this task .,"['DR', '-', 'BiLSTM', '(', 'Single', ')', 'achieves', '88.5', '%', 'accuracy', 'on', 'the', 'test', 'set', 'which', 'is', 'noticeably', 'the', 'best', 'reported', 'result', 'among', 'the', 'existing', 'single', 'models', 'for', 'this', 'task', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NNP', ':', 'NNP', '(', 'NNP', ')', 'VBZ', 'CD', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'WDT', 'VBZ', 'RB', 'DT', 'JJS', 'JJ', 'NN', 'IN', 'DT', 'VBG', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']",30
natural_language_inference,80,177,"DR - BiLSTM ( Ensemble ) achieves the accuracy of 89.3 % , the best result observed on SNLI , while DR - BiLSTM ( Single ) obtains the accuracy of 88.5 % , which considerably outperforms the previous non-ensemble models .","['DR', '-', 'BiLSTM', '(', 'Ensemble', ')', 'achieves', 'the', 'accuracy', 'of', '89.3', '%', ',', 'the', 'best', 'result', 'observed', 'on', 'SNLI', ',', 'while', 'DR', '-', 'BiLSTM', '(', 'Single', ')', 'obtains', 'the', 'accuracy', 'of', '88.5', '%', ',', 'which', 'considerably', 'outperforms', 'the', 'previous', 'non-ensemble', 'models', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', '(', 'NNP', ')', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', ',', 'DT', 'JJS', 'NN', 'VBD', 'IN', 'NNP', ',', 'IN', 'NNP', ':', 'NNP', '(', 'NNP', ')', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', ',', 'WDT', 'RB', 'VBZ', 'DT', 'JJ', 'JJ', 'NNS', '.']",42
natural_language_inference,80,178,"Also , utilizing a trivial preprocessing step yields to further improvements of 0.4 % and 0.3 % for single and ensemble DR - BiLSTM models respectively .","['Also', ',', 'utilizing', 'a', 'trivial', 'preprocessing', 'step', 'yields', 'to', 'further', 'improvements', 'of', '0.4', '%', 'and', '0.3', '%', 'for', 'single', 'and', 'ensemble', 'DR', '-', 'BiLSTM', 'models', 'respectively', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['RB', ',', 'VBG', 'DT', 'JJ', 'JJ', 'NN', 'NNS', 'TO', 'JJ', 'NNS', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'JJ', 'CC', 'JJ', 'NNP', ':', 'NNP', 'NNS', 'RB', '.']",27
natural_language_inference,80,183,Our ensemble model considerably outperforms the current state - of - the - art by obtaining 89.3 % accuracy .,"['Our', 'ensemble', 'model', 'considerably', 'outperforms', 'the', 'current', 'state', '-', 'of', '-', 'the', '-', 'art', 'by', 'obtaining', '89.3', '%', 'accuracy', '.']","['O', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'JJ', 'NN', 'RB', 'VBZ', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'VBG', 'CD', 'NN', 'NN', '.']",20
natural_language_inference,80,186,We can see that our preprocessing mechanism leads to further improvements of 0.4 % and 0.3 % on the SNLI test set for our single and ensemble models respectively .,"['We', 'can', 'see', 'that', 'our', 'preprocessing', 'mechanism', 'leads', 'to', 'further', 'improvements', 'of', '0.4', '%', 'and', '0.3', '%', 'on', 'the', 'SNLI', 'test', 'set', 'for', 'our', 'single', 'and', 'ensemble', 'models', 'respectively', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'MD', 'VB', 'IN', 'PRP$', 'VBG', 'NN', 'VBZ', 'TO', 'JJ', 'NNS', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NN', 'IN', 'PRP$', 'JJ', 'CC', 'JJ', 'NNS', 'RB', '.']",30
natural_language_inference,80,187,"In fact , our single model ( "" DR - BiLSTM ( Single ) + Process "" ) obtains the state - of - the - art performance over both reported single and ensemble models by performing a simple preprocessing step .","['In', 'fact', ',', 'our', 'single', 'model', '(', '""', 'DR', '-', 'BiLSTM', '(', 'Single', ')', '+', 'Process', '""', ')', 'obtains', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'over', 'both', 'reported', 'single', 'and', 'ensemble', 'models', 'by', 'performing', 'a', 'simple', 'preprocessing', 'step', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP$', 'JJ', 'NN', '(', 'JJ', 'NNP', ':', 'NNP', '(', 'NNP', ')', 'VBZ', 'NNP', 'NNP', ')', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'DT', 'VBN', 'JJ', 'CC', 'JJ', 'NNS', 'IN', 'VBG', 'DT', 'JJ', 'JJ', 'NN', '.']",42
natural_language_inference,80,188,"Furthermore , "" DR - BiLSTM ( Ensem . ) + Process "" outperforms the existing state - of - the - art remarkably ( 0.7 % improvement ) .","['Furthermore', ',', '""', 'DR', '-', 'BiLSTM', '(', 'Ensem', '.', ')', '+', 'Process', '""', 'outperforms', 'the', 'existing', 'state', '-', 'of', '-', 'the', '-', 'art', 'remarkably', '(', '0.7', '%', 'improvement', ')', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['RB', ',', 'NNP', 'NNP', ':', 'NNP', '(', 'NNP', '.', ')', 'NN', 'NNP', 'NN', 'VBZ', 'DT', 'VBG', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', '(', 'CD', 'NN', 'NN', ')', '.']",30
natural_language_inference,80,195,We can see that all modifications lead to a new model and their differences are statistically significant with a p-value of < 0.001 over Chi square test .,"['We', 'can', 'see', 'that', 'all', 'modifications', 'lead', 'to', 'a', 'new', 'model', 'and', 'their', 'differences', 'are', 'statistically', 'significant', 'with', 'a', 'p-value', 'of', '<', '0.001', 'over', 'Chi', 'square', 'test', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'MD', 'VB', 'IN', 'DT', 'NNS', 'VBP', 'TO', 'DT', 'JJ', 'NN', 'CC', 'PRP$', 'NNS', 'VBP', 'RB', 'JJ', 'IN', 'DT', 'NN', 'IN', '$', 'CD', 'IN', 'NNP', 'JJ', 'NN', '.']",28
natural_language_inference,80,197,"Among all components , three of them have noticeable influences : max pooling , difference in the attention stage , and dependent reading .","['Among', 'all', 'components', ',', 'three', 'of', 'them', 'have', 'noticeable', 'influences', ':', 'max', 'pooling', ',', 'difference', 'in', 'the', 'attention', 'stage', ',', 'and', 'dependent', 'reading', '.']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNS', ',', 'CD', 'IN', 'PRP', 'VBP', 'JJ', 'NNS', ':', 'NN', 'NN', ',', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'CC', 'JJ', 'NN', '.']",24
natural_language_inference,80,199,"They illustrate the importance of our proposed dependent reading strategy which leads to significant improvement , specifically in the encoding stage .","['They', 'illustrate', 'the', 'importance', 'of', 'our', 'proposed', 'dependent', 'reading', 'strategy', 'which', 'leads', 'to', 'significant', 'improvement', ',', 'specifically', 'in', 'the', 'encoding', 'stage', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'PRP$', 'VBN', 'NN', 'VBG', 'NN', 'WDT', 'VBZ', 'TO', 'JJ', 'NN', ',', 'RB', 'IN', 'DT', 'NN', 'NN', '.']",22
natural_language_inference,80,203,demonstrates that we achieve the best performance with 450 - dimensional BiLSTMs .,"['demonstrates', 'that', 'we', 'achieve', 'the', 'best', 'performance', 'with', '450', '-', 'dimensional', 'BiLSTMs', '.']","['B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'IN', 'PRP', 'VBP', 'DT', 'JJS', 'NN', 'IN', 'CD', ':', 'JJ', 'NNP', '.']",13
natural_language_inference,66,2,Learning Natural Language Inference with LSTM,"['Learning', 'Natural', 'Language', 'Inference', 'with', 'LSTM']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['VBG', 'NNP', 'NNP', 'NNP', 'IN', 'NNP']",6
natural_language_inference,66,4,Natural language inference ( NLI ) is a fundamentally important task in natural language processing that has many applications .,"['Natural', 'language', 'inference', '(', 'NLI', ')', 'is', 'a', 'fundamentally', 'important', 'task', 'in', 'natural', 'language', 'processing', 'that', 'has', 'many', 'applications', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', '(', 'NNP', ')', 'VBZ', 'DT', 'RB', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'JJ', 'NNS', '.']",20
natural_language_inference,66,6,"In this paper , we propose a special long short - term memory ( LSTM ) architecture for NLI .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'special', 'long', 'short', '-', 'term', 'memory', '(', 'LSTM', ')', 'architecture', 'for', 'NLI', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'JJ', ':', 'NN', 'NN', '(', 'NNP', ')', 'NN', 'IN', 'NNP', '.']",20
natural_language_inference,66,32,"In this paper , we propose a new LSTM - based architecture for learning natural language inference .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'new', 'LSTM', '-', 'based', 'architecture', 'for', 'learning', 'natural', 'language', 'inference', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNP', ':', 'VBN', 'NN', 'IN', 'VBG', 'JJ', 'NN', 'NN', '.']",18
natural_language_inference,66,34,"Instead , we use an LSTM to perform word - by - word matching of the hypothesis with the premise .","['Instead', ',', 'we', 'use', 'an', 'LSTM', 'to', 'perform', 'word', '-', 'by', '-', 'word', 'matching', 'of', 'the', 'hypothesis', 'with', 'the', 'premise', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NNP', 'TO', 'VB', 'NN', ':', 'IN', ':', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",21
natural_language_inference,66,35,"Our LSTM sequentially processes the hypothesis , and at each position , it tries to match the current word in the hypothesis with an attention - weighted representation of the premise .","['Our', 'LSTM', 'sequentially', 'processes', 'the', 'hypothesis', ',', 'and', 'at', 'each', 'position', ',', 'it', 'tries', 'to', 'match', 'the', 'current', 'word', 'in', 'the', 'hypothesis', 'with', 'an', 'attention', '-', 'weighted', 'representation', 'of', 'the', 'premise', '.']","['B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['PRP$', 'NNP', 'RB', 'VBZ', 'DT', 'NN', ',', 'CC', 'IN', 'DT', 'NN', ',', 'PRP', 'VBZ', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', ':', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",32
natural_language_inference,66,37,"We refer to this architecture a match - LSTM , or m LSTM for short .","['We', 'refer', 'to', 'this', 'architecture', 'a', 'match', '-', 'LSTM', ',', 'or', 'm', 'LSTM', 'for', 'short', '.']","['O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'TO', 'DT', 'NN', 'DT', 'NN', ':', 'NNP', ',', 'CC', 'VB', 'NNP', 'IN', 'JJ', '.']",16
natural_language_inference,66,42,1 https://github.com/shuohangwang/,"['1', 'https://github.com/shuohangwang/']","['O', 'B-n']","['CD', 'NN']",2
natural_language_inference,66,142,"We use the Adam method ( Kingma and Ba , 2014 ) with hyperparameters ?","['We', 'use', 'the', 'Adam', 'method', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', 'with', 'hyperparameters', '?']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'IN', 'NNS', '.']",15
natural_language_inference,66,143,1 set to 0.9 and ?,"['1', 'set', 'to', '0.9', 'and', '?']","['O', 'B-p', 'I-p', 'B-n', 'O', 'O']","['CD', 'NN', 'TO', 'CD', 'CC', '.']",6
natural_language_inference,66,144,2 set to 0.999 for optimization .,"['2', 'set', 'to', '0.999', 'for', 'optimization', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O']","['CD', 'NN', 'TO', 'CD', 'IN', 'NN', '.']",7
natural_language_inference,66,145,The initial learning rate is set to be 0.001 with a decay ratio of 0.95 for each iteration .,"['The', 'initial', 'learning', 'rate', 'is', 'set', 'to', 'be', '0.001', 'with', 'a', 'decay', 'ratio', 'of', '0.95', 'for', 'each', 'iteration', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'CD', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', '.']",19
natural_language_inference,66,146,The batch size is set to be 30 .,"['The', 'batch', 'size', 'is', 'set', 'to', 'be', '30', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'CD', '.']",9
natural_language_inference,66,147,We experiment with d = 150 and d = 300 where d is the dimension of all the hidden states .,"['We', 'experiment', 'with', 'd', '=', '150', 'and', 'd', '=', '300', 'where', 'd', 'is', 'the', 'dimension', 'of', 'all', 'the', 'hidden', 'states', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'NN', '$', 'CD', 'CC', 'VB', 'NN', 'CD', 'WRB', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'PDT', 'DT', 'JJ', 'NNS', '.']",21
natural_language_inference,66,163,"We have the following observations : ( 1 ) First of all , we can see that when we set d to 300 , our model achieves an accuracy of 86.1 % on the test data , which to the best of our knowledge is the highest on and |?| M is the number of parameters excluding the word embeddings .","['We', 'have', 'the', 'following', 'observations', ':', '(', '1', ')', 'First', 'of', 'all', ',', 'we', 'can', 'see', 'that', 'when', 'we', 'set', 'd', 'to', '300', ',', 'our', 'model', 'achieves', 'an', 'accuracy', 'of', '86.1', '%', 'on', 'the', 'test', 'data', ',', 'which', 'to', 'the', 'best', 'of', 'our', 'knowledge', 'is', 'the', 'highest', 'on', 'and', '|?|', 'M', 'is', 'the', 'number', 'of', 'parameters', 'excluding', 'the', 'word', 'embeddings', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NNS', ':', '(', 'CD', ')', 'NNP', 'IN', 'RB', ',', 'PRP', 'MD', 'VB', 'IN', 'WRB', 'PRP', 'VBP', 'JJ', 'TO', 'CD', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', 'IN', 'DT', 'NN', 'NNS', ',', 'WDT', 'TO', 'DT', 'JJS', 'IN', 'PRP$', 'NN', 'VBZ', 'DT', 'JJS', 'IN', 'CC', 'NNP', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'NNS', 'VBG', 'DT', 'NN', 'NNS', '.']",61
natural_language_inference,66,182,"( 2 ) If we compare our m LSTM model with our implementation of the word - by - word attention model by under the same setting with d = 150 , we can see that our performance on the test data ( 85.7 % ) is higher than that of their model ( 82.6 % ) .","['(', '2', ')', 'If', 'we', 'compare', 'our', 'm', 'LSTM', 'model', 'with', 'our', 'implementation', 'of', 'the', 'word', '-', 'by', '-', 'word', 'attention', 'model', 'by', 'under', 'the', 'same', 'setting', 'with', 'd', '=', '150', ',', 'we', 'can', 'see', 'that', 'our', 'performance', 'on', 'the', 'test', 'data', '(', '85.7', '%', ')', 'is', 'higher', 'than', 'that', 'of', 'their', 'model', '(', '82.6', '%', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'IN', 'PRP', 'VBP', 'PRP$', 'NN', 'NNP', 'NN', 'IN', 'PRP$', 'NN', 'IN', 'DT', 'NN', ':', 'IN', ':', 'NN', 'NN', 'NN', 'IN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'CD', ',', 'PRP', 'MD', 'VB', 'IN', 'PRP$', 'NN', 'IN', 'DT', 'NN', 'NNS', '(', 'CD', 'NN', ')', 'VBZ', 'JJR', 'IN', 'DT', 'IN', 'PRP$', 'NN', '(', 'CD', 'NN', ')', '.']",58
natural_language_inference,66,184,"( 3 ) The performance of mLSTM with bi - LSTM sentence modeling compared with the model with standard LSTM sentence modeling when d is set to 150 shows that using bi - LSTM to process the original sentences helps ( 86.0 % vs. 85.7 % on the test data ) , but the difference is small and the complexity of bi - LSTM is much higher than LSTM .","['(', '3', ')', 'The', 'performance', 'of', 'mLSTM', 'with', 'bi', '-', 'LSTM', 'sentence', 'modeling', 'compared', 'with', 'the', 'model', 'with', 'standard', 'LSTM', 'sentence', 'modeling', 'when', 'd', 'is', 'set', 'to', '150', 'shows', 'that', 'using', 'bi', '-', 'LSTM', 'to', 'process', 'the', 'original', 'sentences', 'helps', '(', '86.0', '%', 'vs.', '85.7', '%', 'on', 'the', 'test', 'data', ')', ',', 'but', 'the', 'difference', 'is', 'small', 'and', 'the', 'complexity', 'of', 'bi', '-', 'LSTM', 'is', 'much', 'higher', 'than', 'LSTM', '.']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'CD', ')', 'DT', 'NN', 'IN', 'NN', 'IN', 'JJ', ':', 'JJ', 'NN', 'NN', 'VBN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNP', 'NN', 'VBG', 'WRB', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'NNS', 'IN', 'VBG', 'SYM', ':', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'VBZ', '(', 'CD', 'NN', 'FW', 'CD', 'NN', 'IN', 'DT', 'NN', 'NN', ')', ',', 'CC', 'DT', 'NN', 'VBZ', 'JJ', 'CC', 'DT', 'NN', 'IN', 'NN', ':', 'NNP', 'VBZ', 'RB', 'JJR', 'IN', 'NNP', '.']",70
natural_language_inference,66,186,"( 4 ) Interestingly , when we experimented with the m LSTM model using the pre-trained word embeddings instead of LSTMgenerated hidden states as initial representations of the premise and the hypothesis , we were able to achieve an accuracy of 85.3 % on the test data , which is still better than previously reported state of the art .","['(', '4', ')', 'Interestingly', ',', 'when', 'we', 'experimented', 'with', 'the', 'm', 'LSTM', 'model', 'using', 'the', 'pre-trained', 'word', 'embeddings', 'instead', 'of', 'LSTMgenerated', 'hidden', 'states', 'as', 'initial', 'representations', 'of', 'the', 'premise', 'and', 'the', 'hypothesis', ',', 'we', 'were', 'able', 'to', 'achieve', 'an', 'accuracy', 'of', '85.3', '%', 'on', 'the', 'test', 'data', ',', 'which', 'is', 'still', 'better', 'than', 'previously', 'reported', 'state', 'of', 'the', 'art', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'RB', ',', 'WRB', 'PRP', 'VBD', 'IN', 'DT', 'NN', 'NNP', 'NN', 'VBG', 'DT', 'JJ', 'NN', 'NNS', 'RB', 'IN', 'NNP', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CC', 'DT', 'NN', ',', 'PRP', 'VBD', 'JJ', 'TO', 'VB', 'DT', 'NN', 'IN', 'CD', 'NN', 'IN', 'DT', 'NN', 'NNS', ',', 'WDT', 'VBZ', 'RB', 'JJR', 'IN', 'RB', 'VBN', 'NN', 'IN', 'DT', 'NN', '.']",60
natural_language_inference,81,2,COARSE - GRAIN FINE - GRAIN COATTENTION NET - WORK FOR MULTI - EVIDENCE QUESTION ANSWERING,"['COARSE', '-', 'GRAIN', 'FINE', '-', 'GRAIN', 'COATTENTION', 'NET', '-', 'WORK', 'FOR', 'MULTI', '-', 'EVIDENCE', 'QUESTION', 'ANSWERING']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'NNP', ':', 'NN', 'NNP', 'NNP', ':', 'JJ', 'NNP', 'NN']",16
natural_language_inference,81,4,"End - to - end neural models have made significant progress in question answering , however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document .","['End', '-', 'to', '-', 'end', 'neural', 'models', 'have', 'made', 'significant', 'progress', 'in', 'question', 'answering', ',', 'however', 'recent', 'studies', 'show', 'that', 'these', 'models', 'implicitly', 'assume', 'that', 'the', 'answer', 'and', 'evidence', 'appear', 'close', 'together', 'in', 'a', 'single', 'document', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', ':', 'TO', ':', 'VB', 'JJ', 'NNS', 'VBP', 'VBN', 'JJ', 'NN', 'IN', 'NN', 'NN', ',', 'RB', 'JJ', 'NNS', 'VBP', 'IN', 'DT', 'NNS', 'RB', 'VBP', 'IN', 'DT', 'NN', 'CC', 'NN', 'VBP', 'JJ', 'RB', 'IN', 'DT', 'JJ', 'NN', '.']",37
natural_language_inference,81,42,A requirement of scalable and practical question answering ( QA ) systems is the ability to reason over multiple documents and combine their information to answer questions .,"['A', 'requirement', 'of', 'scalable', 'and', 'practical', 'question', 'answering', '(', 'QA', ')', 'systems', 'is', 'the', 'ability', 'to', 'reason', 'over', 'multiple', 'documents', 'and', 'combine', 'their', 'information', 'to', 'answer', 'questions', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'JJ', 'CC', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'NNS', 'VBZ', 'DT', 'NN', 'TO', 'NN', 'IN', 'JJ', 'NNS', 'CC', 'VB', 'PRP$', 'NN', 'TO', 'VB', 'NNS', '.']",28
natural_language_inference,81,43,"Although existing datasets enabled the development of effective end - to - end neural question answering systems , they tend to focus on reasoning over localized sections of a single document .","['Although', 'existing', 'datasets', 'enabled', 'the', 'development', 'of', 'effective', 'end', '-', 'to', '-', 'end', 'neural', 'question', 'answering', 'systems', ',', 'they', 'tend', 'to', 'focus', 'on', 'reasoning', 'over', 'localized', 'sections', 'of', 'a', 'single', 'document', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBG', 'NNS', 'VBD', 'DT', 'NN', 'IN', 'JJ', 'NN', ':', 'TO', ':', 'VB', 'JJ', 'NN', 'VBG', 'NNS', ',', 'PRP', 'VBP', 'TO', 'VB', 'IN', 'VBG', 'RP', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",32
natural_language_inference,81,46,"Our multi-evidence QA model , the Coarse - grain Fine - grain Coattention Network ( CFC ) , selects among a set of candidate answers given a set of support documents and a query .","['Our', 'multi-evidence', 'QA', 'model', ',', 'the', 'Coarse', '-', 'grain', 'Fine', '-', 'grain', 'Coattention', 'Network', '(', 'CFC', ')', ',', 'selects', 'among', 'a', 'set', 'of', 'candidate', 'answers', 'given', 'a', 'set', 'of', 'support', 'documents', 'and', 'a', 'query', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O']","['PRP$', 'NN', 'NNP', 'NN', ',', 'DT', 'NNP', ':', 'NN', 'NNP', ':', 'NN', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', 'VBN', 'DT', 'NN', 'IN', 'NN', 'NNS', 'CC', 'DT', 'NN', '.']",35
natural_language_inference,81,47,The CFC is inspired by coarse - grain reasoning and fine - grain reasoning .,"['The', 'CFC', 'is', 'inspired', 'by', 'coarse', '-', 'grain', 'reasoning', 'and', 'fine', '-', 'grain', 'reasoning', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'VBZ', 'VBN', 'IN', 'NN', ':', 'NN', 'NN', 'CC', 'JJ', ':', 'NN', 'NN', '.']",15
natural_language_inference,81,48,"In coarse - grain reasoning , the model builds a coarse summary of support documents conditioned on the query without knowing what candidates are available , then scores each candidate .","['In', 'coarse', '-', 'grain', 'reasoning', ',', 'the', 'model', 'builds', 'a', 'coarse', 'summary', 'of', 'support', 'documents', 'conditioned', 'on', 'the', 'query', 'without', 'knowing', 'what', 'candidates', 'are', 'available', ',', 'then', 'scores', 'each', 'candidate', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', ':', 'NN', 'NN', ',', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'IN', 'VBG', 'WP', 'NNS', 'VBP', 'JJ', ',', 'RB', 'VBZ', 'DT', 'NN', '.']",31
natural_language_inference,81,49,"In fine - grain reasoning , the model matches specific finegrain contexts in which the candidate is mentioned with the query in order to gauge the relevance of the candidate .","['In', 'fine', '-', 'grain', 'reasoning', ',', 'the', 'model', 'matches', 'specific', 'finegrain', 'contexts', 'in', 'which', 'the', 'candidate', 'is', 'mentioned', 'with', 'the', 'query', 'in', 'order', 'to', 'gauge', 'the', 'relevance', 'of', 'the', 'candidate', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'O', 'O']","['IN', 'JJ', ':', 'NN', 'NN', ',', 'DT', 'NN', 'NNS', 'JJ', 'VBP', 'NN', 'IN', 'WDT', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",31
natural_language_inference,81,51,Each module employs a novel hierarchical attention - a hierarchy of coattention and self - attention - to combine information from the support documents conditioned on the query and candidates .,"['Each', 'module', 'employs', 'a', 'novel', 'hierarchical', 'attention', '-', 'a', 'hierarchy', 'of', 'coattention', 'and', 'self', '-', 'attention', '-', 'to', 'combine', 'information', 'from', 'the', 'support', 'documents', 'conditioned', 'on', 'the', 'query', 'and', 'candidates', '.']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', ':', 'DT', 'NN', 'IN', 'NN', 'CC', 'PRP', ':', 'NN', ':', 'TO', 'VB', 'NN', 'IN', 'DT', 'NN', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'CC', 'NNS', '.']",31
natural_language_inference,81,111,MULTI - EVIDENCE QUESTION ANSWERING ON WIKIHOP,"['MULTI', '-', 'EVIDENCE', 'QUESTION', 'ANSWERING', 'ON', 'WIKIHOP']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n']","['NNP', ':', 'JJ', 'NNP', 'NNP', 'NNP', 'NNP']",7
natural_language_inference,81,121,We tokenize the data using Stanford CoreNLP .,"['We', 'tokenize', 'the', 'data', 'using', 'Stanford', 'CoreNLP', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNS', 'VBG', 'NNP', 'NNP', '.']",8
natural_language_inference,81,122,We use fixed Glo Ve embeddings as well as character ngram embeddings .,"['We', 'use', 'fixed', 'Glo', 'Ve', 'embeddings', 'as', 'well', 'as', 'character', 'ngram', 'embeddings', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'VBN', 'NNP', 'NNP', 'NNS', 'RB', 'RB', 'IN', 'NN', 'JJ', 'NNS', '.']",13
natural_language_inference,81,123,We split symbolic query relations into words .,"['We', 'split', 'symbolic', 'query', 'relations', 'into', 'words', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'NNS', 'IN', 'NNS', '.']",8
natural_language_inference,81,124,All models are trained using ADAM .,"['All', 'models', 'are', 'trained', 'using', 'ADAM', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'VBG', 'NNP', '.']",7
natural_language_inference,81,127,The CFC achieves state - of - the - art results on both the masked and unmasked versions of WikiHop .,"['The', 'CFC', 'achieves', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'on', 'both', 'the', 'masked', 'and', 'unmasked', 'versions', 'of', 'WikiHop', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NNP', 'VBZ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'DT', 'NNS', 'CC', 'JJ', 'NNS', 'IN', 'NNP', '.']",21
natural_language_inference,81,128,"In particular , on the blind , held - out WikiHop test set , the CFC achieves a new best accuracy of 70.6 % .","['In', 'particular', ',', 'on', 'the', 'blind', ',', 'held', '-', 'out', 'WikiHop', 'test', 'set', ',', 'the', 'CFC', 'achieves', 'a', 'new', 'best', 'accuracy', 'of', '70.6', '%', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'JJ', ',', 'IN', 'DT', 'NN', ',', 'VBD', ':', 'RP', 'NNP', 'NN', 'NN', ',', 'DT', 'NNP', 'VBZ', 'DT', 'JJ', 'JJS', 'NN', 'IN', 'CD', 'NN', '.']",25
natural_language_inference,81,134,RERANKING EXTRACTIVE QUESTION ANSWERING ON TRIVIAQA,"['RERANKING', 'EXTRACTIVE', 'QUESTION', 'ANSWERING', 'ON', 'TRIVIAQA']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n']","['NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",6
natural_language_inference,81,141,Our experimental results in show that reranking using the CFC provides consistent performance gains over only using the span extraction question answering model .,"['Our', 'experimental', 'results', 'in', 'show', 'that', 'reranking', 'using', 'the', 'CFC', 'provides', 'consistent', 'performance', 'gains', 'over', 'only', 'using', 'the', 'span', 'extraction', 'question', 'answering', 'model', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'JJ', 'NNS', 'IN', 'NN', 'IN', 'VBG', 'VBG', 'DT', 'NNP', 'VBZ', 'JJ', 'NN', 'NNS', 'IN', 'RB', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'VBG', 'NN', '.']",24
natural_language_inference,81,142,"In particular , reranking using the CFC improves performance regardless of whether the candidate answer set obtained from the span extraction model contains correct answers .","['In', 'particular', ',', 'reranking', 'using', 'the', 'CFC', 'improves', 'performance', 'regardless', 'of', 'whether', 'the', 'candidate', 'answer', 'set', 'obtained', 'from', 'the', 'span', 'extraction', 'model', 'contains', 'correct', 'answers', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', ',', 'VBG', 'VBG', 'DT', 'NNP', 'VBZ', 'NN', 'NN', 'IN', 'IN', 'DT', 'NN', 'NN', 'VBN', 'VBN', 'IN', 'DT', 'NN', 'NN', 'NN', 'NNS', 'VBP', 'NNS', '.']",26
natural_language_inference,81,143,"On the whole Trivia QA dev set , reranking using the CFC results in again of 3.1 % EM and 3.0 % F1 , which suggests that the CFC can be used to further refine the outputs produced by span extraction question answering models .","['On', 'the', 'whole', 'Trivia', 'QA', 'dev', 'set', ',', 'reranking', 'using', 'the', 'CFC', 'results', 'in', 'again', 'of', '3.1', '%', 'EM', 'and', '3.0', '%', 'F1', ',', 'which', 'suggests', 'that', 'the', 'CFC', 'can', 'be', 'used', 'to', 'further', 'refine', 'the', 'outputs', 'produced', 'by', 'span', 'extraction', 'question', 'answering', 'models', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NNP', 'NNP', 'NN', 'NN', ',', 'VBG', 'VBG', 'DT', 'NNP', 'NNS', 'IN', 'RB', 'IN', 'CD', 'NN', 'NNP', 'CC', 'CD', 'NN', 'NNP', ',', 'WDT', 'VBZ', 'IN', 'DT', 'NNP', 'MD', 'VB', 'VBN', 'TO', 'RBR', 'VB', 'DT', 'NNS', 'VBN', 'IN', 'JJ', 'NN', 'NN', 'VBG', 'NNS', '.']",45
natural_language_inference,81,145,Both the coarse - grain module and the fine - grain module significantly contribute to model performance .,"['Both', 'the', 'coarse', '-', 'grain', 'module', 'and', 'the', 'fine', '-', 'grain', 'module', 'significantly', 'contribute', 'to', 'model', 'performance', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'DT', 'JJ', ':', 'NN', 'NN', 'CC', 'DT', 'JJ', ':', 'NN', 'NN', 'RB', 'VBZ', 'TO', 'VB', 'NN', '.']",18
natural_language_inference,81,146,Replacing selfattention layers with mean - pooling and the bidirectional GRUs with unidirectional GRUs result in less performance degradation .,"['Replacing', 'selfattention', 'layers', 'with', 'mean', '-', 'pooling', 'and', 'the', 'bidirectional', 'GRUs', 'with', 'unidirectional', 'GRUs', 'result', 'in', 'less', 'performance', 'degradation', '.']","['B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['VBG', 'NN', 'NNS', 'IN', 'JJ', ':', 'NN', 'CC', 'DT', 'JJ', 'NNP', 'IN', 'JJ', 'NNP', 'NN', 'IN', 'JJR', 'NN', 'NN', '.']",20
natural_language_inference,81,147,"Replacing the encoder with a projection over word embeddings result in significant performance drop , which suggests that contextual encodings that capture positional information is crucial to this task .","['Replacing', 'the', 'encoder', 'with', 'a', 'projection', 'over', 'word', 'embeddings', 'result', 'in', 'significant', 'performance', 'drop', ',', 'which', 'suggests', 'that', 'contextual', 'encodings', 'that', 'capture', 'positional', 'information', 'is', 'crucial', 'to', 'this', 'task', '.']","['B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', 'VBP', 'IN', 'JJ', 'NN', 'NN', ',', 'WDT', 'VBZ', 'IN', 'JJ', 'NNS', 'IN', 'NN', 'JJ', 'NN', 'VBZ', 'JJ', 'TO', 'DT', 'NN', '.']",30
natural_language_inference,81,149,The fine - grain - only model under-performs the coarse - grain - only model consistently across almost all length measures .,"['The', 'fine', '-', 'grain', '-', 'only', 'model', 'under-performs', 'the', 'coarse', '-', 'grain', '-', 'only', 'model', 'consistently', 'across', 'almost', 'all', 'length', 'measures', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', ':', 'NN', ':', 'RB', 'VBZ', 'JJ', 'DT', 'JJ', ':', 'NN', ':', 'RB', 'VBZ', 'RB', 'IN', 'RB', 'DT', 'NN', 'NNS', '.']",22
natural_language_inference,81,151,"However , the fine - grain - only model matches or outperforms the coarse - grain - only model on examples with a large number of support documents or with long support documents .","['However', ',', 'the', 'fine', '-', 'grain', '-', 'only', 'model', 'matches', 'or', 'outperforms', 'the', 'coarse', '-', 'grain', '-', 'only', 'model', 'on', 'examples', 'with', 'a', 'large', 'number', 'of', 'support', 'documents', 'or', 'with', 'long', 'support', 'documents', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'JJ', ':', 'NN', ':', 'RB', 'NN', 'NNS', 'CC', 'VBZ', 'DT', 'JJ', ':', 'NN', ':', 'RB', 'NN', 'IN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', 'CC', 'IN', 'JJ', 'NN', 'NNS', '.']",34
natural_language_inference,15,2,Semantic Sentence Matching with Densely - connected Recurrent and Co - attentive Information,"['Semantic', 'Sentence', 'Matching', 'with', 'Densely', '-', 'connected', 'Recurrent', 'and', 'Co', '-', 'attentive', 'Information']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', ':', 'VBN', 'NNP', 'CC', 'NNP', ':', 'JJ', 'NN']",13
natural_language_inference,15,4,"Sentence matching is widely used in various natural language tasks such as natural language inference , paraphrase identification , and question answering .","['Sentence', 'matching', 'is', 'widely', 'used', 'in', 'various', 'natural', 'language', 'tasks', 'such', 'as', 'natural', 'language', 'inference', ',', 'paraphrase', 'identification', ',', 'and', 'question', 'answering', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'JJ', 'JJ', 'NN', 'NNS', 'JJ', 'IN', 'JJ', 'NN', 'NN', ',', 'NN', 'NN', ',', 'CC', 'NN', 'NN', '.']",23
natural_language_inference,15,31,"Inspired by Densenet ) , we propose a densely - connected recurrent network where the recurrent hidden features are retained to the uppermost layer .","['Inspired', 'by', 'Densenet', ')', ',', 'we', 'propose', 'a', 'densely', '-', 'connected', 'recurrent', 'network', 'where', 'the', 'recurrent', 'hidden', 'features', 'are', 'retained', 'to', 'the', 'uppermost', 'layer', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['VBN', 'IN', 'NNP', ')', ',', 'PRP', 'VBP', 'DT', 'RB', ':', 'VBN', 'NN', 'NN', 'WRB', 'DT', 'NN', 'NN', 'NNS', 'VBP', 'VBN', 'TO', 'DT', 'JJ', 'NN', '.']",25
natural_language_inference,15,32,"In addition , instead of the conventional summation operation , the concatenation operation is used in combination with the attention mechanism to preserve co-attentive information better .","['In', 'addition', ',', 'instead', 'of', 'the', 'conventional', 'summation', 'operation', ',', 'the', 'concatenation', 'operation', 'is', 'used', 'in', 'combination', 'with', 'the', 'attention', 'mechanism', 'to', 'preserve', 'co-attentive', 'information', 'better', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-n', 'O']","['IN', 'NN', ',', 'RB', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'NN', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', 'JJ', 'NN', 'RBR', '.']",27
natural_language_inference,15,33,The proposed architecture shown in is called DRCN which is an abbreviation for Densely - connected Recurrent and Co -attentive neural Network .,"['The', 'proposed', 'architecture', 'shown', 'in', 'is', 'called', 'DRCN', 'which', 'is', 'an', 'abbreviation', 'for', 'Densely', '-', 'connected', 'Recurrent', 'and', 'Co', '-attentive', 'neural', 'Network', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBN', 'NN', 'VBN', 'IN', 'VBZ', 'VBN', 'NNP', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'NNP', ':', 'VBN', 'NNP', 'CC', 'NNP', 'NNP', 'JJ', 'NNP', '.']",23
natural_language_inference,15,34,The proposed DRCN can utilize the increased representational power of deeper recurrent networks and attentive information .,"['The', 'proposed', 'DRCN', 'can', 'utilize', 'the', 'increased', 'representational', 'power', 'of', 'deeper', 'recurrent', 'networks', 'and', 'attentive', 'information', '.']","['O', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['DT', 'VBN', 'NNP', 'MD', 'VB', 'DT', 'VBN', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NNS', 'CC', 'JJ', 'NN', '.']",17
natural_language_inference,15,35,"Furthermore , to alleviate the problem of an ever- increasing feature vector size due to concatenation operations , we adopted an autoencoder and forwarded a fixed length vector to the higher layer recurrent module as shown in the figure .","['Furthermore', ',', 'to', 'alleviate', 'the', 'problem', 'of', 'an', 'ever-', 'increasing', 'feature', 'vector', 'size', 'due', 'to', 'concatenation', 'operations', ',', 'we', 'adopted', 'an', 'autoencoder', 'and', 'forwarded', 'a', 'fixed', 'length', 'vector', 'to', 'the', 'higher', 'layer', 'recurrent', 'module', 'as', 'shown', 'in', 'the', 'figure', '.']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', 'NN', 'JJ', 'TO', 'VB', 'NNS', ',', 'PRP', 'VBD', 'DT', 'NN', 'CC', 'VBD', 'DT', 'JJ', 'NN', 'NN', 'TO', 'DT', 'JJR', 'NN', 'NN', 'NN', 'IN', 'VBN', 'IN', 'DT', 'NN', '.']",40
natural_language_inference,15,113,"We initialized word embedding with 300d Glo Ve vectors pre-trained from the 840B Common Crawl corpus ( Pennington , Socher , and Manning 2014 ) , while the word embeddings for the out - of - vocabulary words were initialized randomly .","['We', 'initialized', 'word', 'embedding', 'with', '300d', 'Glo', 'Ve', 'vectors', 'pre-trained', 'from', 'the', '840B', 'Common', 'Crawl', 'corpus', '(', 'Pennington', ',', 'Socher', ',', 'and', 'Manning', '2014', ')', ',', 'while', 'the', 'word', 'embeddings', 'for', 'the', 'out', '-', 'of', '-', 'vocabulary', 'words', 'were', 'initialized', 'randomly', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O']","['PRP', 'VBD', 'NN', 'VBG', 'IN', 'CD', 'NNP', 'NNP', 'NNS', 'VBD', 'IN', 'DT', 'CD', 'NNP', 'NNP', 'NN', '(', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', 'CD', ')', ',', 'IN', 'DT', 'NN', 'VBZ', 'IN', 'DT', 'RP', ':', 'IN', ':', 'JJ', 'NNS', 'VBD', 'VBN', 'RB', '.']",42
natural_language_inference,15,114,We also randomly initialized character embedding with a 16d vector and extracted 32d character representation with a convolutional network .,"['We', 'also', 'randomly', 'initialized', 'character', 'embedding', 'with', 'a', '16d', 'vector', 'and', 'extracted', '32d', 'character', 'representation', 'with', 'a', 'convolutional', 'network', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'RB', 'VBN', 'NN', 'VBG', 'IN', 'DT', 'CD', 'NN', 'CC', 'VBD', 'CD', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",20
natural_language_inference,15,115,"For the densely - connected recurrent layers , we stacked 5 layers each of which have 100 hidden units .","['For', 'the', 'densely', '-', 'connected', 'recurrent', 'layers', ',', 'we', 'stacked', '5', 'layers', 'each', 'of', 'which', 'have', '100', 'hidden', 'units', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'RB', ':', 'VBN', 'NN', 'NNS', ',', 'PRP', 'VBD', 'CD', 'NNS', 'DT', 'IN', 'WDT', 'VBP', 'CD', 'JJ', 'NNS', '.']",20
natural_language_inference,15,116,We set 1000 hidden units with respect to the fullyconnected layers .,"['We', 'set', '1000', 'hidden', 'units', 'with', 'respect', 'to', 'the', 'fullyconnected', 'layers', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', 'JJ', 'NNS', 'IN', 'NN', 'TO', 'DT', 'JJ', 'NNS', '.']",12
natural_language_inference,15,117,The dropout was applied after the word and character embedding layers with a keep rate of 0.5 .,"['The', 'dropout', 'was', 'applied', 'after', 'the', 'word', 'and', 'character', 'embedding', 'layers', 'with', 'a', 'keep', 'rate', 'of', '0.5', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'VBD', 'VBN', 'IN', 'DT', 'NN', 'CC', 'NN', 'VBG', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', '.']",18
natural_language_inference,15,118,It was also applied before the fully - connected layers with a keep rate of 0.8 .,"['It', 'was', 'also', 'applied', 'before', 'the', 'fully', '-', 'connected', 'layers', 'with', 'a', 'keep', 'rate', 'of', '0.8', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBD', 'RB', 'VBN', 'IN', 'DT', 'RB', ':', 'VBN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', '.']",17
natural_language_inference,15,119,"For the bottleneck component , we set 200 hidden units as encoded features of the autoencoder with a dropout rate of 0.2 .","['For', 'the', 'bottleneck', 'component', ',', 'we', 'set', '200', 'hidden', 'units', 'as', 'encoded', 'features', 'of', 'the', 'autoencoder', 'with', 'a', 'dropout', 'rate', 'of', '0.2', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NN', 'NN', ',', 'PRP', 'VBP', 'CD', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', '.']",23
natural_language_inference,15,120,"The batch normalization was applied on the fully - connected layers , only for the one - way type datasets .","['The', 'batch', 'normalization', 'was', 'applied', 'on', 'the', 'fully', '-', 'connected', 'layers', ',', 'only', 'for', 'the', 'one', '-', 'way', 'type', 'datasets', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBD', 'VBN', 'IN', 'DT', 'RB', ':', 'VBN', 'NNS', ',', 'RB', 'IN', 'DT', 'CD', ':', 'NN', 'JJ', 'NNS', '.']",21
natural_language_inference,15,121,The RMSProp optimizer with an initial learning rate of 0.001 was applied .,"['The', 'RMSProp', 'optimizer', 'with', 'an', 'initial', 'learning', 'rate', 'of', '0.001', 'was', 'applied', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O']","['DT', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'VBD', 'VBN', '.']",13
natural_language_inference,15,122,The learning rate was decreased by a factor of 0.85 when the dev accuracy does not improve .,"['The', 'learning', 'rate', 'was', 'decreased', 'by', 'a', 'factor', 'of', '0.85', 'when', 'the', 'dev', 'accuracy', 'does', 'not', 'improve', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBD', 'VBN', 'IN', 'DT', 'NN', 'IN', 'CD', 'WRB', 'DT', 'NN', 'NN', 'VBZ', 'RB', 'VB', '.']",18
natural_language_inference,15,123,All weights except embedding matrices are constrained by L2 regularization with a regularization constant ? = 10 ?6 .,"['All', 'weights', 'except', 'embedding', 'matrices', 'are', 'constrained', 'by', 'L2', 'regularization', 'with', 'a', 'regularization', 'constant', '?', '=', '10', '?6', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'VBG', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'NN', 'IN', 'DT', 'NN', 'NN', '.', '$', 'CD', 'NN', '.']",19
natural_language_inference,15,124,"The sequence lengths of the sentence are all different for each dataset : 35 for SNLI , 55 for MultiNLI , 25 for Quora question pair and 50 for TrecQA .","['The', 'sequence', 'lengths', 'of', 'the', 'sentence', 'are', 'all', 'different', 'for', 'each', 'dataset', ':', '35', 'for', 'SNLI', ',', '55', 'for', 'MultiNLI', ',', '25', 'for', 'Quora', 'question', 'pair', 'and', '50', 'for', 'TrecQA', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'VBP', 'DT', 'JJ', 'IN', 'DT', 'NN', ':', 'CD', 'IN', 'NNP', ',', 'CD', 'IN', 'NNP', ',', 'CD', 'IN', 'NNP', 'NN', 'NN', 'CC', 'CD', 'IN', 'NNP', '.']",31
natural_language_inference,15,133,The proposed DRCN obtains an accuracy of 88.9 % which is a competitive score although we do not use any external knowledge like ESIM + ELMo and LM - Transformer .,"['The', 'proposed', 'DRCN', 'obtains', 'an', 'accuracy', 'of', '88.9', '%', 'which', 'is', 'a', 'competitive', 'score', 'although', 'we', 'do', 'not', 'use', 'any', 'external', 'knowledge', 'like', 'ESIM', '+', 'ELMo', 'and', 'LM', '-', 'Transformer', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'VBN', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'PRP', 'VBP', 'RB', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', ':', 'NN', '.']",31
natural_language_inference,15,134,"The ensemble model achieves an accuracy of 90.1 % , which sets the new state - of the - art performance .","['The', 'ensemble', 'model', 'achieves', 'an', 'accuracy', 'of', '90.1', '%', ',', 'which', 'sets', 'the', 'new', 'state', '-', 'of', 'the', '-', 'art', 'performance', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', ':', 'IN', 'DT', ':', 'NN', 'NN', '.']",22
natural_language_inference,15,135,Our ensemble model with 53 m parameters ( 6.7 m 8 ) outperforms the LM - Transformer whose the number of parameters is 85 m .,"['Our', 'ensemble', 'model', 'with', '53', 'm', 'parameters', '(', '6.7', 'm', '8', ')', 'outperforms', 'the', 'LM', '-', 'Transformer', 'whose', 'the', 'number', 'of', 'parameters', 'is', '85', 'm', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP$', 'JJ', 'NN', 'IN', 'CD', 'NN', 'NNS', '(', 'CD', 'RB', 'CD', ')', 'VBZ', 'DT', 'NNP', ':', 'NN', 'WP$', 'DT', 'NN', 'IN', 'NNS', 'VBZ', 'CD', 'NN', '.']",26
natural_language_inference,15,136,"Furthermore , in case of the encoding - based method , we obtain the best performance of 86.5 % without the co-attention and exact match flag .","['Furthermore', ',', 'in', 'case', 'of', 'the', 'encoding', '-', 'based', 'method', ',', 'we', 'obtain', 'the', 'best', 'performance', 'of', '86.5', '%', 'without', 'the', 'co-attention', 'and', 'exact', 'match', 'flag', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'IN', 'NN', 'IN', 'DT', 'VBG', ':', 'VBN', 'NN', ',', 'PRP', 'VB', 'DT', 'JJS', 'NN', 'IN', 'CD', 'NN', 'IN', 'DT', 'NN', 'CC', 'JJ', 'NN', 'NN', '.']",27
natural_language_inference,15,137,shows the results on MATCHED and MISMATCHED problems of MultiNLI dataset .,"['shows', 'the', 'results', 'on', 'MATCHED', 'and', 'MISMATCHED', 'problems', 'of', 'MultiNLI', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['VBZ', 'DT', 'NNS', 'IN', 'NNP', 'CC', 'NNP', 'NNS', 'IN', 'NNP', 'NN', '.']",12
natural_language_inference,15,138,Our plain DRCN has a competitive performance without any contextualized knowledge .,"['Our', 'plain', 'DRCN', 'has', 'a', 'competitive', 'performance', 'without', 'any', 'contextualized', 'knowledge', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",12
natural_language_inference,15,139,"And , by combining DRCN with the ELMo , one of the contextualized embeddings from language models , our model outperforms the LM - Transformer which has 85 m parameters with fewer parameters of 61 m .","['And', ',', 'by', 'combining', 'DRCN', 'with', 'the', 'ELMo', ',', 'one', 'of', 'the', 'contextualized', 'embeddings', 'from', 'language', 'models', ',', 'our', 'model', 'outperforms', 'the', 'LM', '-', 'Transformer', 'which', 'has', '85', 'm', 'parameters', 'with', 'fewer', 'parameters', 'of', '61', 'm', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['CC', ',', 'IN', 'VBG', 'NNP', 'IN', 'DT', 'NNP', ',', 'CD', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NN', 'NNS', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'NNP', ':', 'NN', 'WDT', 'VBZ', 'CD', 'NN', 'NNS', 'IN', 'JJR', 'NNS', 'IN', 'CD', 'NN', '.']",37
natural_language_inference,15,145,Pair shows our results on the Quora question pair dataset .,"['Pair', 'shows', 'our', 'results', 'on', 'the', 'Quora', 'question', 'pair', 'dataset', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'VBZ', 'PRP$', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'NN', 'NN', '.']",11
natural_language_inference,15,147,"We obtained accuracies of 90.15 % and 91.30 % in single and ensemble methods , respectively , surpassing the previous state - of - the - art model of DIIN .","['We', 'obtained', 'accuracies', 'of', '90.15', '%', 'and', '91.30', '%', 'in', 'single', 'and', 'ensemble', 'methods', ',', 'respectively', ',', 'surpassing', 'the', 'previous', 'state', '-', 'of', '-', 'the', '-', 'art', 'model', 'of', 'DIIN', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBD', 'NNS', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'JJ', 'CC', 'JJ', 'NNS', ',', 'RB', ',', 'VBG', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'NNP', '.']",31
natural_language_inference,15,148,TrecQA and SelQA shows the performance of different models on TrecQA and SelQA datasets for answer sentence selection task that aims to select a set of candidate answer sentences given a question .,"['TrecQA', 'and', 'SelQA', 'shows', 'the', 'performance', 'of', 'different', 'models', 'on', 'TrecQA', 'and', 'SelQA', 'datasets', 'for', 'answer', 'sentence', 'selection', 'task', 'that', 'aims', 'to', 'select', 'a', 'set', 'of', 'candidate', 'answer', 'sentences', 'given', 'a', 'question', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'CC', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NNP', 'CC', 'NNP', 'NNS', 'IN', 'JJR', 'NN', 'NN', 'NN', 'WDT', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', 'NN', 'NNS', 'VBN', 'DT', 'NN', '.']",33
natural_language_inference,15,150,"However , the proposed DRCN using collective attentions over multiple layers , achieves the new state - of the - art performance , exceeding the current state - of - the - art performance significantly on both datasets .","['However', ',', 'the', 'proposed', 'DRCN', 'using', 'collective', 'attentions', 'over', 'multiple', 'layers', ',', 'achieves', 'the', 'new', 'state', '-', 'of', 'the', '-', 'art', 'performance', ',', 'exceeding', 'the', 'current', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'significantly', 'on', 'both', 'datasets', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O']","['RB', ',', 'DT', 'VBN', 'NNP', 'VBG', 'JJ', 'NNS', 'IN', 'JJ', 'NNS', ',', 'VBZ', 'DT', 'JJ', 'NN', ':', 'IN', 'DT', ':', 'NN', 'NN', ',', 'VBG', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'RB', 'IN', 'DT', 'NNS', '.']",39
natural_language_inference,15,156,"Although the number of parameters in the DRCN significantly decreased as shown in , we could see that the performance was rather higher because of the regularization effect .","['Although', 'the', 'number', 'of', 'parameters', 'in', 'the', 'DRCN', 'significantly', 'decreased', 'as', 'shown', 'in', ',', 'we', 'could', 'see', 'that', 'the', 'performance', 'was', 'rather', 'higher', 'because', 'of', 'the', 'regularization', 'effect', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NNP', 'RB', 'VBD', 'IN', 'VBN', 'IN', ',', 'PRP', 'MD', 'VB', 'IN', 'DT', 'NN', 'VBD', 'RB', 'JJR', 'IN', 'IN', 'DT', 'NN', 'NN', '.']",29
natural_language_inference,15,162,The result shows that the dense connections over attentive features are more effective .,"['The', 'result', 'shows', 'that', 'the', 'dense', 'connections', 'over', 'attentive', 'features', 'are', 'more', 'effective', '.']","['O', 'B-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'NNS', 'IN', 'JJ', 'NNS', 'VBP', 'RBR', 'JJ', '.']",14
natural_language_inference,15,163,"In , we removed dense connections over both co-attentive and recurrent features , and the performance degraded to 88.5 % .","['In', ',', 'we', 'removed', 'dense', 'connections', 'over', 'both', 'co-attentive', 'and', 'recurrent', 'features', ',', 'and', 'the', 'performance', 'degraded', 'to', '88.5', '%', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', ',', 'PRP', 'VBD', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'CC', 'JJ', 'NNS', ',', 'CC', 'DT', 'NN', 'VBD', 'TO', 'CD', 'NN', '.']",21
natural_language_inference,15,167,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over deeper layers , has more powerful capability retaining collective knowledge to learn textual semantics .","['The', 'results', 'of', '(', '8', '-', '9', ')', 'demonstrate', 'that', 'the', 'dense', 'connection', 'using', 'concatenation', 'operation', 'over', 'deeper', 'layers', ',', 'has', 'more', 'powerful', 'capability', 'retaining', 'collective', 'knowledge', 'to', 'learn', 'textual', 'semantics', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'IN', '(', 'CD', ':', 'CD', ')', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBG', 'NN', 'NN', 'IN', 'JJR', 'NNS', ',', 'VBZ', 'JJR', 'JJ', 'NN', 'VBG', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'NNS', '.']",32
natural_language_inference,15,169,The result of ( 10 ) shows that the connections among the layers are important to help gradient flow .,"['The', 'result', 'of', '(', '10', ')', 'shows', 'that', 'the', 'connections', 'among', 'the', 'layers', 'are', 'important', 'to', 'help', 'gradient', 'flow', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'IN', '(', 'CD', ')', 'VBZ', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNS', 'VBP', 'JJ', 'TO', 'VB', 'NN', 'NN', '.']",20
natural_language_inference,15,170,"And , the result of ( 11 ) shows that the attentive information functioning as a soft - alignment is significantly effective in semantic sentence matching .","['And', ',', 'the', 'result', 'of', '(', '11', ')', 'shows', 'that', 'the', 'attentive', 'information', 'functioning', 'as', 'a', 'soft', '-', 'alignment', 'is', 'significantly', 'effective', 'in', 'semantic', 'sentence', 'matching', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['CC', ',', 'DT', 'NN', 'IN', '(', 'CD', ')', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', ':', 'NN', 'VBZ', 'RB', 'JJ', 'IN', 'JJ', 'NN', 'NN', '.']",27
natural_language_inference,15,172,"The models ( 5 - 9 ) which have connections between layers , are more robust to the increased depth of network , however , the performances of ( 10 - 11 ) tend to degrade as layers get deeper .","['The', 'models', '(', '5', '-', '9', ')', 'which', 'have', 'connections', 'between', 'layers', ',', 'are', 'more', 'robust', 'to', 'the', 'increased', 'depth', 'of', 'network', ',', 'however', ',', 'the', 'performances', 'of', '(', '10', '-', '11', ')', 'tend', 'to', 'degrade', 'as', 'layers', 'get', 'deeper', '.']","['O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['DT', 'NNS', '(', 'CD', ':', 'CD', ')', 'WDT', 'VBP', 'NNS', 'IN', 'NNS', ',', 'VBP', 'RBR', 'JJ', 'TO', 'DT', 'VBN', 'NN', 'IN', 'NN', ',', 'RB', ',', 'DT', 'NNS', 'IN', '(', 'CD', ':', 'CD', ')', 'VBP', 'TO', 'VB', 'IN', 'NNS', 'VBP', 'JJR', '.']",41
natural_language_inference,15,173,"In addition , the models with dense connections rather than residual connections , have higher performance in general .","['In', 'addition', ',', 'the', 'models', 'with', 'dense', 'connections', 'rather', 'than', 'residual', 'connections', ',', 'have', 'higher', 'performance', 'in', 'general', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O']","['IN', 'NN', ',', 'DT', 'NNS', 'IN', 'JJ', 'NNS', 'RB', 'IN', 'JJ', 'NNS', ',', 'VBP', 'JJR', 'NN', 'IN', 'JJ', '.']",19
natural_language_inference,15,174,"shows that the connection between layers is essential , especially in deep models , endowing more representational power , and the dense connection is more effective than the residual connection .","['shows', 'that', 'the', 'connection', 'between', 'layers', 'is', 'essential', ',', 'especially', 'in', 'deep', 'models', ',', 'endowing', 'more', 'representational', 'power', ',', 'and', 'the', 'dense', 'connection', 'is', 'more', 'effective', 'than', 'the', 'residual', 'connection', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNS', 'IN', 'DT', 'NN', 'IN', 'NNS', 'VBZ', 'JJ', ',', 'RB', 'IN', 'JJ', 'NNS', ',', 'VBG', 'RBR', 'JJ', 'NN', ',', 'CC', 'DT', 'JJ', 'NN', 'VBZ', 'RBR', 'JJ', 'IN', 'DT', 'JJ', 'NN', '.']",31
natural_language_inference,46,2,The NarrativeQA Reading Comprehension Challenge,"['The', 'NarrativeQA', 'Reading', 'Comprehension', 'Challenge']","['O', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNP', 'NNP', 'NNP', 'NNP']",5
natural_language_inference,46,4,"Reading comprehension ( RC ) - in contrast to information retrieval - requires integrating information and reasoning about events , entities , and their relations across a full document .","['Reading', 'comprehension', '(', 'RC', ')', '-', 'in', 'contrast', 'to', 'information', 'retrieval', '-', 'requires', 'integrating', 'information', 'and', 'reasoning', 'about', 'events', ',', 'entities', ',', 'and', 'their', 'relations', 'across', 'a', 'full', 'document', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NN', '(', 'NNP', ')', ':', 'IN', 'NN', 'TO', 'NN', 'NN', ':', 'VBZ', 'VBG', 'NN', 'CC', 'VBG', 'IN', 'NNS', ',', 'NNS', ',', 'CC', 'PRP$', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",30
natural_language_inference,46,5,"Question answering is conventionally used to assess RC ability , in both artificial agents and children learning to read .","['Question', 'answering', 'is', 'conventionally', 'used', 'to', 'assess', 'RC', 'ability', ',', 'in', 'both', 'artificial', 'agents', 'and', 'children', 'learning', 'to', 'read', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'VBZ', 'RB', 'VBN', 'TO', 'VB', 'NNP', 'NN', ',', 'IN', 'DT', 'JJ', 'NNS', 'CC', 'NNS', 'VBG', 'TO', 'VB', '.']",20
natural_language_inference,46,39,"We present a new task and dataset , which we call NarrativeQA , which will test and reward artificial agents approaching this level of competence ( Section 3 ) .","['We', 'present', 'a', 'new', 'task', 'and', 'dataset', ',', 'which', 'we', 'call', 'NarrativeQA', ',', 'which', 'will', 'test', 'and', 'reward', 'artificial', 'agents', 'approaching', 'this', 'level', 'of', 'competence', '(', 'Section', '3', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'CC', 'NN', ',', 'WDT', 'PRP', 'VBP', 'NNP', ',', 'WDT', 'MD', 'VB', 'CC', 'VB', 'JJ', 'NNS', 'VBG', 'DT', 'NN', 'IN', 'NN', '(', 'NNP', 'CD', ')', '.']",30
natural_language_inference,46,40,"The dataset consists of stories , which are books and movie scripts , with human written questions and answers based solely on human - generated abstractive summaries .","['The', 'dataset', 'consists', 'of', 'stories', ',', 'which', 'are', 'books', 'and', 'movie', 'scripts', ',', 'with', 'human', 'written', 'questions', 'and', 'answers', 'based', 'solely', 'on', 'human', '-', 'generated', 'abstractive', 'summaries', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'IN', 'NNS', ',', 'WDT', 'VBP', 'NNS', 'CC', 'NN', 'NNS', ',', 'IN', 'JJ', 'VBN', 'NNS', 'CC', 'NNS', 'VBN', 'RB', 'IN', 'JJ', ':', 'VBD', 'JJ', 'NNS', '.']",28
natural_language_inference,46,41,"For the RC tasks , questions maybe answered using just the summaries or the full story text .","['For', 'the', 'RC', 'tasks', ',', 'questions', 'maybe', 'answered', 'using', 'just', 'the', 'summaries', 'or', 'the', 'full', 'story', 'text', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNP', 'NNS', ',', 'NNS', 'RB', 'VBD', 'VBG', 'RB', 'DT', 'NNS', 'CC', 'DT', 'JJ', 'NN', 'NN', '.']",18
natural_language_inference,46,196,Reading Summaries Only,"['Reading', 'Summaries', 'Only']","['B-n', 'I-n', 'I-n']","['VBG', 'NNS', 'RB']",3
natural_language_inference,46,201,"This is indeed the case , with the neural span prediction model significantly outperforming all other proposed methods .","['This', 'is', 'indeed', 'the', 'case', ',', 'with', 'the', 'neural', 'span', 'prediction', 'model', 'significantly', 'outperforming', 'all', 'other', 'proposed', 'methods', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'RB', 'DT', 'NN', ',', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', 'RB', 'VBG', 'DT', 'JJ', 'VBN', 'NNS', '.']",19
natural_language_inference,46,203,"Both the plain sequence to sequence model and the AS Reader , successfully applied to the CNN / DailyMail reading comprehension task , also perform well on this task .","['Both', 'the', 'plain', 'sequence', 'to', 'sequence', 'model', 'and', 'the', 'AS', 'Reader', ',', 'successfully', 'applied', 'to', 'the', 'CNN', '/', 'DailyMail', 'reading', 'comprehension', 'task', ',', 'also', 'perform', 'well', 'on', 'this', 'task', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['DT', 'DT', 'NN', 'NN', 'TO', 'VB', 'NN', 'CC', 'DT', 'NNP', 'NNP', ',', 'RB', 'VBN', 'TO', 'DT', 'NNP', 'NNP', 'NNP', 'VBG', 'NN', 'NN', ',', 'RB', 'VBP', 'RB', 'IN', 'DT', 'NN', '.']",30
natural_language_inference,46,205,An additional inductive bias results in higher performance for the span prediction model .,"['An', 'additional', 'inductive', 'bias', 'results', 'in', 'higher', 'performance', 'for', 'the', 'span', 'prediction', 'model', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'NNS', 'IN', 'JJR', 'NN', 'IN', 'DT', 'NN', 'NN', 'NN', '.']",14
natural_language_inference,46,208,"summarizes the results on the full Narra - tive QA task , where the context documents are full stories .","['summarizes', 'the', 'results', 'on', 'the', 'full', 'Narra', '-', 'tive', 'QA', 'task', ',', 'where', 'the', 'context', 'documents', 'are', 'full', 'stories', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['VBZ', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NNP', ':', 'JJ', 'NNP', 'NN', ',', 'WRB', 'DT', 'NN', 'NNS', 'VBP', 'JJ', 'NNS', '.']",20
natural_language_inference,46,209,"As expected ( and desired ) , we observe a decline in performance of the span- selection oracle IR model , compared with the results on summaries .","['As', 'expected', '(', 'and', 'desired', ')', ',', 'we', 'observe', 'a', 'decline', 'in', 'performance', 'of', 'the', 'span-', 'selection', 'oracle', 'IR', 'model', ',', 'compared', 'with', 'the', 'results', 'on', 'summaries', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBN', '(', 'CC', 'VBN', ')', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NNP', 'NN', ',', 'VBN', 'IN', 'DT', 'NNS', 'IN', 'NNS', '.']",28
natural_language_inference,46,215,Reading Full Stories Only,"['Reading', 'Full', 'Stories', 'Only']","['B-n', 'I-n', 'I-n', 'I-n']","['VBG', 'NNP', 'NNP', 'RB']",4
natural_language_inference,46,224,"The AS Reader , which was the better - performing model on the summaries task , underperforms the simple no -context Seq2Seq baseline ( shown in ) in terms of MRR .","['The', 'AS', 'Reader', ',', 'which', 'was', 'the', 'better', '-', 'performing', 'model', 'on', 'the', 'summaries', 'task', ',', 'underperforms', 'the', 'simple', 'no', '-context', 'Seq2Seq', 'baseline', '(', 'shown', 'in', ')', 'in', 'terms', 'of', 'MRR', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['DT', 'NNP', 'NNP', ',', 'WDT', 'VBD', 'DT', 'JJR', ':', 'VBG', 'NN', 'IN', 'DT', 'NNS', 'NN', ',', 'VBZ', 'DT', 'JJ', 'DT', 'JJ', 'NNP', 'NN', '(', 'VBN', 'IN', ')', 'IN', 'NNS', 'IN', 'NNP', '.']",32
natural_language_inference,46,229,"As with the AS Reader , we observed no significant differences for varying number of chunks .","['As', 'with', 'the', 'AS', 'Reader', ',', 'we', 'observed', 'no', 'significant', 'differences', 'for', 'varying', 'number', 'of', 'chunks', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'IN', 'DT', 'NNP', 'NNP', ',', 'PRP', 'VBD', 'DT', 'JJ', 'NNS', 'IN', 'VBG', 'NN', 'IN', 'NNS', '.']",17
natural_language_inference,36,2,Explicit Contextual Semantics for Text Comprehension,"['Explicit', 'Contextual', 'Semantics', 'for', 'Text', 'Comprehension']","['O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",6
natural_language_inference,36,12,"This paper focuses on two core text comprehension ( TC ) tasks , machine reading comprehension ( MRC ) and textual entailment ( TE ) .","['This', 'paper', 'focuses', 'on', 'two', 'core', 'text', 'comprehension', '(', 'TC', ')', 'tasks', ',', 'machine', 'reading', 'comprehension', '(', 'MRC', ')', 'and', 'textual', 'entailment', '(', 'TE', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'IN', 'CD', 'NN', 'NN', 'NN', '(', 'NNP', ')', 'NNS', ',', 'NN', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'JJ', 'NN', '(', 'NNP', ')', '.']",26
natural_language_inference,36,26,"In this work , to alleviate such an obvious shortcoming about semantics , we make attempt to explore integrative models for finer - grained text comprehension and inference .","['In', 'this', 'work', ',', 'to', 'alleviate', 'such', 'an', 'obvious', 'shortcoming', 'about', 'semantics', ',', 'we', 'make', 'attempt', 'to', 'explore', 'integrative', 'models', 'for', 'finer', '-', 'grained', 'text', 'comprehension', 'and', 'inference', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'TO', 'VB', 'PDT', 'DT', 'JJ', 'VBG', 'IN', 'NNS', ',', 'PRP', 'VBP', 'JJ', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'JJR', ':', 'VBN', 'JJ', 'NN', 'CC', 'NN', '.']",29
natural_language_inference,36,27,"In this work , we propose a semantics enhancement framework for TC tasks , which boosts the strong baselines effectively .","['In', 'this', 'work', ',', 'we', 'propose', 'a', 'semantics', 'enhancement', 'framework', 'for', 'TC', 'tasks', ',', 'which', 'boosts', 'the', 'strong', 'baselines', 'effectively', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNS', 'JJ', 'NN', 'IN', 'NNP', 'NNS', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NNS', 'RB', '.']",21
natural_language_inference,36,28,We implement an easy and feasible scheme to integrate semantic signals in downstream neural models in end - to - end manner to boost strong baselines effectively .,"['We', 'implement', 'an', 'easy', 'and', 'feasible', 'scheme', 'to', 'integrate', 'semantic', 'signals', 'in', 'downstream', 'neural', 'models', 'in', 'end', '-', 'to', '-', 'end', 'manner', 'to', 'boost', 'strong', 'baselines', 'effectively', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'CC', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'JJ', 'JJ', 'NNS', 'IN', 'NN', ':', 'TO', ':', 'NN', 'NN', 'TO', 'VB', 'JJ', 'NNS', 'RB', '.']",28
natural_language_inference,36,168,Textual Entailment,"['Textual', 'Entailment']","['B-n', 'I-n']","['JJ', 'NN']",2
natural_language_inference,36,172,Results in show that SRL embedding can boost the ESIM + ELMo model by + 0.7 % improvement .,"['Results', 'in', 'show', 'that', 'SRL', 'embedding', 'can', 'boost', 'the', 'ESIM', '+', 'ELMo', 'model', 'by', '+', '0.7', '%', 'improvement', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNS', 'IN', 'NN', 'IN', 'NNP', 'VBG', 'MD', 'VB', 'DT', 'NNP', 'NNP', 'NNP', 'NN', 'IN', 'JJ', 'CD', 'NN', 'NN', '.']",19
natural_language_inference,36,173,"With the semantic cues , the simple sequential encoding model yields substantial gains , and our single BERT LARGE model also achieves a new stateof - the - art , even outperforms all the ensemble models in the leaderboard 8 .","['With', 'the', 'semantic', 'cues', ',', 'the', 'simple', 'sequential', 'encoding', 'model', 'yields', 'substantial', 'gains', ',', 'and', 'our', 'single', 'BERT', 'LARGE', 'model', 'also', 'achieves', 'a', 'new', 'stateof', '-', 'the', '-', 'art', ',', 'even', 'outperforms', 'all', 'the', 'ensemble', 'models', 'in', 'the', 'leaderboard', '8', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O']","['IN', 'DT', 'JJ', 'NNS', ',', 'DT', 'JJ', 'JJ', 'VBG', 'NN', 'NNS', 'JJ', 'NNS', ',', 'CC', 'PRP$', 'JJ', 'NNP', 'NNP', 'NN', 'RB', 'VBZ', 'DT', 'JJ', 'NN', ':', 'DT', ':', 'NN', ',', 'RB', 'VBZ', 'PDT', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CD', '.']",41
natural_language_inference,36,181,Machine Reading Comprehension,"['Machine', 'Reading', 'Comprehension']","['B-n', 'I-n', 'I-n']","['NN', 'NNP', 'NN']",3
natural_language_inference,36,186,"Our baseline includes MQAN for single task and multi-task with SRL , BiDAF + ELMo , R.M. Reader and BERT .","['Our', 'baseline', 'includes', 'MQAN', 'for', 'single', 'task', 'and', 'multi-task', 'with', 'SRL', ',', 'BiDAF', '+', 'ELMo', ',', 'R.M.', 'Reader', 'and', 'BERT', '.']","['O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['PRP$', 'NN', 'VBZ', 'NNP', 'IN', 'JJ', 'NN', 'CC', 'NN', 'IN', 'NNP', ',', 'NNP', 'NNP', 'NNP', ',', 'NNP', 'NNP', 'CC', 'NNP', '.']",21
natural_language_inference,36,188,"9 . The SRL embeddings give substantial performance gains over all the strong baselines , showing it is also quite effective for more complex document and question encoding .","['9', '.', 'The', 'SRL', 'embeddings', 'give', 'substantial', 'performance', 'gains', 'over', 'all', 'the', 'strong', 'baselines', ',', 'showing', 'it', 'is', 'also', 'quite', 'effective', 'for', 'more', 'complex', 'document', 'and', 'question', 'encoding', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['CD', '.', 'DT', 'NNP', 'NNS', 'VBP', 'JJ', 'NN', 'NNS', 'IN', 'PDT', 'DT', 'JJ', 'NNS', ',', 'VBG', 'PRP', 'VBZ', 'RB', 'RB', 'JJ', 'IN', 'RBR', 'JJ', 'NN', 'CC', 'NN', 'NN', '.']",29
natural_language_inference,22,4,"To answer the question in machine comprehension ( MC ) task , the models need to establish the interaction between the question and the context .","['To', 'answer', 'the', 'question', 'in', 'machine', 'comprehension', '(', 'MC', ')', 'task', ',', 'the', 'models', 'need', 'to', 'establish', 'the', 'interaction', 'between', 'the', 'question', 'and', 'the', 'context', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'DT', 'NN', 'IN', 'NN', 'NN', '(', 'NNP', ')', 'NN', ',', 'DT', 'NNS', 'VBP', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'CC', 'DT', 'NN', '.']",26
natural_language_inference,22,10,1 The latest results are listed at https://rajpurkar.github.io/SQuAD -explorer/,"['1', 'The', 'latest', 'results', 'are', 'listed', 'at', 'https://rajpurkar.github.io/SQuAD', '-explorer/']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['CD', 'DT', 'JJS', 'NNS', 'VBP', 'VBN', 'IN', 'NN', 'NN']",9
natural_language_inference,22,14,Machine comprehension ( MC ) - especially in the form of question answering ( QA ) - is therefore attracting a significant amount of attention from the machine learning community .,"['Machine', 'comprehension', '(', 'MC', ')', '-', 'especially', 'in', 'the', 'form', 'of', 'question', 'answering', '(', 'QA', ')', '-', 'is', 'therefore', 'attracting', 'a', 'significant', 'amount', 'of', 'attention', 'from', 'the', 'machine', 'learning', 'community', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', '(', 'NNP', ')', ':', 'RB', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', '(', 'NNP', ')', ':', 'VBZ', 'RB', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'DT', 'NN', 'VBG', 'NN', '.']",31
natural_language_inference,22,23,"We propose an extension of BIDAF , called Ruminating Reader , which uses a second pass of reading and reasoning to allow it to learn to avoid mistakes and to ensure that it is able to effectively use the full context when selecting an answer .","['We', 'propose', 'an', 'extension', 'of', 'BIDAF', ',', 'called', 'Ruminating', 'Reader', ',', 'which', 'uses', 'a', 'second', 'pass', 'of', 'reading', 'and', 'reasoning', 'to', 'allow', 'it', 'to', 'learn', 'to', 'avoid', 'mistakes', 'and', 'to', 'ensure', 'that', 'it', 'is', 'able', 'to', 'effectively', 'use', 'the', 'full', 'context', 'when', 'selecting', 'an', 'answer', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'NNP', ',', 'VBD', 'NNP', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'CC', 'VBG', 'TO', 'VB', 'PRP', 'TO', 'VB', 'TO', 'VB', 'NNS', 'CC', 'TO', 'VB', 'IN', 'PRP', 'VBZ', 'JJ', 'TO', 'RB', 'VB', 'DT', 'JJ', 'NN', 'WRB', 'VBG', 'DT', 'NN', '.']",46
natural_language_inference,22,24,"In addition to adding a second pass , we also introduce two novel layer types , the ruminate layers , which use gating mechanisms to fuse the obtained from the first and second passes .","['In', 'addition', 'to', 'adding', 'a', 'second', 'pass', ',', 'we', 'also', 'introduce', 'two', 'novel', 'layer', 'types', ',', 'the', 'ruminate', 'layers', ',', 'which', 'use', 'gating', 'mechanisms', 'to', 'fuse', 'the', 'obtained', 'from', 'the', 'first', 'and', 'second', 'passes', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'VBG', 'DT', 'JJ', 'NN', ',', 'PRP', 'RB', 'VBP', 'CD', 'JJ', 'NN', 'NNS', ',', 'DT', 'NN', 'NNS', ',', 'WDT', 'VBP', 'VBG', 'NNS', 'TO', 'VB', 'DT', 'VBN', 'IN', 'DT', 'JJ', 'CC', 'JJ', 'NNS', '.']",35
natural_language_inference,22,26,"In addition , we introduce an answer-question similarity loss to penalize overlap between question and predicted answer , a common feature in the errors of our base model .","['In', 'addition', ',', 'we', 'introduce', 'an', 'answer-question', 'similarity', 'loss', 'to', 'penalize', 'overlap', 'between', 'question', 'and', 'predicted', 'answer', ',', 'a', 'common', 'feature', 'in', 'the', 'errors', 'of', 'our', 'base', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'NNS', 'IN', 'NN', 'CC', 'VBD', 'NN', ',', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNS', 'IN', 'PRP$', 'NN', 'NN', '.']",29
natural_language_inference,22,148,"In the character encoding layer , we use 100 filters of width 5 .","['In', 'the', 'character', 'encoding', 'layer', ',', 'we', 'use', '100', 'filters', 'of', 'width', '5', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'VBG', 'NN', ',', 'PRP', 'VBP', 'CD', 'NNS', 'IN', 'JJ', 'CD', '.']",14
natural_language_inference,22,149,"In the remainder of the model , we set the hidden layer dimension ( d ) to 100 .","['In', 'the', 'remainder', 'of', 'the', 'model', ',', 'we', 'set', 'the', 'hidden', 'layer', 'dimension', '(', 'd', ')', 'to', '100', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', '(', 'NN', ')', 'TO', 'CD', '.']",19
natural_language_inference,22,150,We use pretrained 100D Glo Ve vectors ( 6B - token version ) as word embeddings .,"['We', 'use', 'pretrained', '100D', 'Glo', 'Ve', 'vectors', '(', '6B', '-', 'token', 'version', ')', 'as', 'word', 'embeddings', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'VBD', 'CD', 'NNP', 'NNP', 'NNS', '(', 'CD', ':', 'JJ', 'NN', ')', 'IN', 'NN', 'NNS', '.']",17
natural_language_inference,22,151,"Out - of - vocobulary tokens are represented by an UNK symbol in the word embedding layer , but treated normally by the character embedding layer .","['Out', '-', 'of', '-', 'vocobulary', 'tokens', 'are', 'represented', 'by', 'an', 'UNK', 'symbol', 'in', 'the', 'word', 'embedding', 'layer', ',', 'but', 'treated', 'normally', 'by', 'the', 'character', 'embedding', 'layer', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', ':', 'IN', ':', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'DT', 'NN', 'VBG', 'NN', ',', 'CC', 'VBD', 'RB', 'IN', 'DT', 'NN', 'VBG', 'NN', '.']",27
natural_language_inference,22,153,"We use the AdaDelta optimizer ( Zeiler , 2012 ) for optimization .","['We', 'use', 'the', 'AdaDelta', 'optimizer', '(', 'Zeiler', ',', '2012', ')', 'for', 'optimization', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', '(', 'NNP', ',', 'CD', ')', 'IN', 'NN', '.']",13
natural_language_inference,22,154,We selected hyperparameter values through random search .,"['We', 'selected', 'hyperparameter', 'values', 'through', 'random', 'search', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBD', 'NN', 'NNS', 'IN', 'JJ', 'NN', '.']",8
natural_language_inference,22,155,Batch size is 30 .,"['Batch', 'size', 'is', '30', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'O']","['NN', 'NN', 'VBZ', 'CD', '.']",5
natural_language_inference,22,156,"Learning rate starts at 0.5 , and decreases to 0.2 once the model stops improving .","['Learning', 'rate', 'starts', 'at', '0.5', ',', 'and', 'decreases', 'to', '0.2', 'once', 'the', 'model', 'stops', 'improving', '.']","['B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'B-n', 'I-n', 'O']","['VBG', 'NN', 'NNS', 'IN', 'CD', ',', 'CC', 'VBZ', 'TO', 'CD', 'RB', 'DT', 'NN', 'VBZ', 'VBG', '.']",16
natural_language_inference,22,157,"The L2-regularization weight is 1 e - 4 , AQSL weight is 1 and dropout with a drop rate of 0.2 is A typical model run converges in about 40 k steps .","['The', 'L2-regularization', 'weight', 'is', '1', 'e', '-', '4', ',', 'AQSL', 'weight', 'is', '1', 'and', 'dropout', 'with', 'a', 'drop', 'rate', 'of', '0.2', 'is', 'A', 'typical', 'model', 'run', 'converges', 'in', 'about', '40', 'k', 'steps', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'NN', 'VBZ', 'CD', 'NN', ':', 'CD', ',', 'NNP', 'NN', 'VBZ', 'CD', 'CC', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'NNS', 'IN', 'RB', 'CD', 'JJ', 'NNS', '.']",33
natural_language_inference,22,158,This takes two days using Tensorflow and a single NVIDIA K80 GPU . provide an official evaluation script that allows us to measure F 1 score and EM score by comparing the prediction and ground truth answers .,"['This', 'takes', 'two', 'days', 'using', 'Tensorflow', 'and', 'a', 'single', 'NVIDIA', 'K80', 'GPU', '.', 'provide', 'an', 'official', 'evaluation', 'script', 'that', 'allows', 'us', 'to', 'measure', 'F', '1', 'score', 'and', 'EM', 'score', 'by', 'comparing', 'the', 'prediction', 'and', 'ground', 'truth', 'answers', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'VBZ', 'CD', 'NNS', 'VBG', 'NNP', 'CC', 'DT', 'JJ', 'NNP', 'NNP', 'NNP', '.', 'VB', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'PRP', 'TO', 'VB', 'NNP', 'CD', 'NN', 'CC', 'NNP', 'NN', 'IN', 'VBG', 'DT', 'NN', 'CC', 'NN', 'NN', 'NNS', '.']",38
natural_language_inference,22,169,"At the time of submission , our model is tied in accuracy on the hidden test set with the bestperforming published single model .","['At', 'the', 'time', 'of', 'submission', ',', 'our', 'model', 'is', 'tied', 'in', 'accuracy', 'on', 'the', 'hidden', 'test', 'set', 'with', 'the', 'bestperforming', 'published', 'single', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'IN', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'VBN', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBN', 'IN', 'DT', 'NN', 'VBN', 'JJ', 'NN', '.']",24
natural_language_inference,22,170,We achieve an F 1 score of 79.5 and EM score of 70.6 .,"['We', 'achieve', 'an', 'F', '1', 'score', 'of', '79.5', 'and', 'EM', 'score', 'of', '70.6', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'CD', 'NN', 'IN', 'CD', 'CC', 'NNP', 'NN', 'IN', 'CD', '.']",14
natural_language_inference,22,180,Experiments 3 and 4 show that the two ruminate layers are both important and helpful in contributing performance .,"['Experiments', '3', 'and', '4', 'show', 'that', 'the', 'two', 'ruminate', 'layers', 'are', 'both', 'important', 'and', 'helpful', 'in', 'contributing', 'performance', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['NNS', 'CD', 'CC', 'CD', 'NN', 'IN', 'DT', 'CD', 'NN', 'NNS', 'VBP', 'DT', 'JJ', 'CC', 'JJ', 'IN', 'VBG', 'NN', '.']",19
natural_language_inference,22,181,It is worth noting that the BiLSTM in the context ruminate layer contributes substantially to model performance .,"['It', 'is', 'worth', 'noting', 'that', 'the', 'BiLSTM', 'in', 'the', 'context', 'ruminate', 'layer', 'contributes', 'substantially', 'to', 'model', 'performance', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBZ', 'JJ', 'VBG', 'IN', 'DT', 'NNP', 'IN', 'DT', 'NN', 'NN', 'NN', 'VBZ', 'RB', 'TO', 'VB', 'NN', '.']",18
natural_language_inference,94,2,Commonsense for Generative Multi - Hop Question Answering Tasks,"['Commonsense', 'for', 'Generative', 'Multi', '-', 'Hop', 'Question', 'Answering', 'Tasks']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NN', 'IN', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'NNP']",9
natural_language_inference,94,4,"Reading comprehension QA tasks have seen a recent surge in popularity , yet most works have focused on fact - finding extractive QA .","['Reading', 'comprehension', 'QA', 'tasks', 'have', 'seen', 'a', 'recent', 'surge', 'in', 'popularity', ',', 'yet', 'most', 'works', 'have', 'focused', 'on', 'fact', '-', 'finding', 'extractive', 'QA', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NN', 'NNP', 'NNS', 'VBP', 'VBN', 'DT', 'JJ', 'NN', 'IN', 'NN', ',', 'RB', 'JJS', 'NNS', 'VBP', 'VBN', 'IN', 'NN', ':', 'VBG', 'JJ', 'NNP', '.']",24
natural_language_inference,94,14,"Reading comprehension QA tasks have seen a recent surge in popularity , yet most works have focused on fact - finding extractive QA .","['Reading', 'comprehension', 'QA', 'tasks', 'have', 'seen', 'a', 'recent', 'surge', 'in', 'popularity', ',', 'yet', 'most', 'works', 'have', 'focused', 'on', 'fact', '-', 'finding', 'extractive', 'QA', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NN', 'NNP', 'NNS', 'VBP', 'VBN', 'DT', 'JJ', 'NN', 'IN', 'NN', ',', 'RB', 'JJS', 'NNS', 'VBP', 'VBN', 'IN', 'NN', ':', 'VBG', 'JJ', 'NNP', '.']",24
natural_language_inference,94,24,"In this paper , we explore the task of machine reading comprehension ( MRC ) based QA .","['In', 'this', 'paper', ',', 'we', 'explore', 'the', 'task', 'of', 'machine', 'reading', 'comprehension', '(', 'MRC', ')', 'based', 'QA', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'NN', 'NN', '(', 'NNP', ')', 'VBN', 'NNP', '.']",18
natural_language_inference,94,27,https://github.com/yicheng-w/CommonSenseMultiHopQA task tests a model 's natural language understanding capabilities by asking it to answer a question based on a passage of relevant content .,"['https://github.com/yicheng-w/CommonSenseMultiHopQA', 'task', 'tests', 'a', 'model', ""'s"", 'natural', 'language', 'understanding', 'capabilities', 'by', 'asking', 'it', 'to', 'answer', 'a', 'question', 'based', 'on', 'a', 'passage', 'of', 'relevant', 'content', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'VBZ', 'DT', 'NN', 'POS', 'JJ', 'NN', 'VBG', 'NNS', 'IN', 'VBG', 'PRP', 'TO', 'VB', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NN', '.']",25
natural_language_inference,94,28,"Much progress has been made in reasoning - based MRC - QA on the bAbI dataset , which contains questions that require the combination of multiple disjoint pieces of evidence in the context .","['Much', 'progress', 'has', 'been', 'made', 'in', 'reasoning', '-', 'based', 'MRC', '-', 'QA', 'on', 'the', 'bAbI', 'dataset', ',', 'which', 'contains', 'questions', 'that', 'require', 'the', 'combination', 'of', 'multiple', 'disjoint', 'pieces', 'of', 'evidence', 'in', 'the', 'context', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'VBZ', 'VBN', 'VBN', 'IN', 'VBG', ':', 'VBN', 'NNP', ':', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'WDT', 'VBZ', 'NNS', 'WDT', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NNS', 'IN', 'NN', 'IN', 'DT', 'NN', '.']",34
natural_language_inference,94,37,"In this paper , we first propose the Multi - Hop Pointer - Generator Model ( MHPGM ) , a strong baseline model that uses multiple hops of bidirectional attention , self - attention , and a pointer - generator decoder to effectively read and reason within along passage and synthesize a coherent response .","['In', 'this', 'paper', ',', 'we', 'first', 'propose', 'the', 'Multi', '-', 'Hop', 'Pointer', '-', 'Generator', 'Model', '(', 'MHPGM', ')', ',', 'a', 'strong', 'baseline', 'model', 'that', 'uses', 'multiple', 'hops', 'of', 'bidirectional', 'attention', ',', 'self', '-', 'attention', ',', 'and', 'a', 'pointer', '-', 'generator', 'decoder', 'to', 'effectively', 'read', 'and', 'reason', 'within', 'along', 'passage', 'and', 'synthesize', 'a', 'coherent', 'response', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'RB', 'VBD', 'DT', 'NNP', ':', 'NNP', 'NNP', ':', 'NN', 'NNP', '(', 'NNP', ')', ',', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'JJ', 'NNS', 'IN', 'JJ', 'NN', ',', 'PRP', ':', 'NN', ',', 'CC', 'DT', 'NN', ':', 'NN', 'NN', 'TO', 'RB', 'VB', 'CC', 'NN', 'IN', 'JJ', 'NN', 'CC', 'VB', 'DT', 'NN', 'NN', '.']",55
natural_language_inference,94,39,"Next , to address the issue that understanding human - generated text and performing longdistance reasoning on it often involves intermittent access to missing hops of external commonsense ( background ) knowledge , we present an algorithm for selecting useful , grounded multi-hop relational knowledge paths from ConceptNet ) via a pointwise mutual information ( PMI ) and term - frequency - based scoring function .","['Next', ',', 'to', 'address', 'the', 'issue', 'that', 'understanding', 'human', '-', 'generated', 'text', 'and', 'performing', 'longdistance', 'reasoning', 'on', 'it', 'often', 'involves', 'intermittent', 'access', 'to', 'missing', 'hops', 'of', 'external', 'commonsense', '(', 'background', ')', 'knowledge', ',', 'we', 'present', 'an', 'algorithm', 'for', 'selecting', 'useful', ',', 'grounded', 'multi-hop', 'relational', 'knowledge', 'paths', 'from', 'ConceptNet', ')', 'via', 'a', 'pointwise', 'mutual', 'information', '(', 'PMI', ')', 'and', 'term', '-', 'frequency', '-', 'based', 'scoring', 'function', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'TO', 'VB', 'DT', 'NN', 'IN', 'VBG', 'JJ', ':', 'VBN', 'NN', 'CC', 'VBG', 'NN', 'VBG', 'IN', 'PRP', 'RB', 'VBZ', 'JJ', 'NN', 'TO', 'VBG', 'NNS', 'IN', 'JJ', 'NN', '(', 'NN', ')', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'JJ', ',', 'VBD', 'JJ', 'JJ', 'NN', 'NNS', 'IN', 'NNP', ')', 'IN', 'DT', 'NN', 'JJ', 'NN', '(', 'NNP', ')', 'CC', 'NN', ':', 'NN', ':', 'VBN', 'VBG', 'NN', '.']",66
natural_language_inference,94,40,"We then present a novel method of inserting these selected commonsense paths between the hops of document - context reasoning within our model , via the Necessary and Optional Information Cell ( NOIC ) , which employs a selectivelygated attention mechanism that utilizes commonsense information to effectively fill in gaps of inference .","['We', 'then', 'present', 'a', 'novel', 'method', 'of', 'inserting', 'these', 'selected', 'commonsense', 'paths', 'between', 'the', 'hops', 'of', 'document', '-', 'context', 'reasoning', 'within', 'our', 'model', ',', 'via', 'the', 'Necessary', 'and', 'Optional', 'Information', 'Cell', '(', 'NOIC', ')', ',', 'which', 'employs', 'a', 'selectivelygated', 'attention', 'mechanism', 'that', 'utilizes', 'commonsense', 'information', 'to', 'effectively', 'fill', 'in', 'gaps', 'of', 'inference', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'VBN', 'NN', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'NN', ':', 'NN', 'NN', 'IN', 'PRP$', 'NN', ',', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'JJ', 'NN', 'TO', 'RB', 'VB', 'IN', 'NNS', 'IN', 'NN', '.']",53
natural_language_inference,94,214,"We see empirically that our model outperforms all generative models on NarrativeQA , and is competitive with the top span prediction models .","['We', 'see', 'empirically', 'that', 'our', 'model', 'outperforms', 'all', 'generative', 'models', 'on', 'NarrativeQA', ',', 'and', 'is', 'competitive', 'with', 'the', 'top', 'span', 'prediction', 'models', '.']","['O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'RB', 'IN', 'PRP$', 'NN', 'VBZ', 'DT', 'JJ', 'NNS', 'IN', 'NNP', ',', 'CC', 'VBZ', 'JJ', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NNS', '.']",23
natural_language_inference,94,215,"Furthermore , with the NOIC commonsense integration , we were able to further improve performance ( p < 0.001 on all metrics 5 ) , establishing a new state - of - the - art for the task .","['Furthermore', ',', 'with', 'the', 'NOIC', 'commonsense', 'integration', ',', 'we', 'were', 'able', 'to', 'further', 'improve', 'performance', '(', 'p', '<', '0.001', 'on', 'all', 'metrics', '5', ')', ',', 'establishing', 'a', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'for', 'the', 'task', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['RB', ',', 'IN', 'DT', 'NNP', 'NN', 'NN', ',', 'PRP', 'VBD', 'JJ', 'TO', 'JJ', 'VB', 'NN', '(', 'JJ', 'VBP', 'CD', 'IN', 'DT', 'NNS', 'CD', ')', ',', 'VBG', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'DT', 'NN', '.']",39
natural_language_inference,94,216,"We also see that our model performs reasonably well on WikiHop , and further achieves promising initial improvements via the addition of commonsense , hinting at the generalizability of our approaches .","['We', 'also', 'see', 'that', 'our', 'model', 'performs', 'reasonably', 'well', 'on', 'WikiHop', ',', 'and', 'further', 'achieves', 'promising', 'initial', 'improvements', 'via', 'the', 'addition', 'of', 'commonsense', ',', 'hinting', 'at', 'the', 'generalizability', 'of', 'our', 'approaches', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'IN', 'PRP$', 'NN', 'VBZ', 'RB', 'RB', 'IN', 'NNP', ',', 'CC', 'JJ', 'NNS', 'VBG', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NN', ',', 'VBG', 'IN', 'DT', 'NN', 'IN', 'PRP$', 'NNS', '.']",32
natural_language_inference,94,217,"We speculate that the improvement is smaller on Wikihop because only approximately 11 % of WikiHop data points require commonsense and because WikiHop data requires more fact - based commonsense ( e.g. , from Freebase ) as opposed to semantics - based commonsense ( e.g. , from Con-ceptNet ( Speer and Havasi , 2012 ) ) .","['We', 'speculate', 'that', 'the', 'improvement', 'is', 'smaller', 'on', 'Wikihop', 'because', 'only', 'approximately', '11', '%', 'of', 'WikiHop', 'data', 'points', 'require', 'commonsense', 'and', 'because', 'WikiHop', 'data', 'requires', 'more', 'fact', '-', 'based', 'commonsense', '(', 'e.g.', ',', 'from', 'Freebase', ')', 'as', 'opposed', 'to', 'semantics', '-', 'based', 'commonsense', '(', 'e.g.', ',', 'from', 'Con-ceptNet', '(', 'Speer', 'and', 'Havasi', ',', '2012', ')', ')', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NN', 'VBZ', 'JJR', 'IN', 'NNP', 'IN', 'RB', 'RB', 'CD', 'NN', 'IN', 'NNP', 'NN', 'NNS', 'VBP', 'NN', 'CC', 'IN', 'NNP', 'NN', 'VBZ', 'JJR', 'NN', ':', 'VBN', 'NN', '(', 'JJ', ',', 'IN', 'NNP', ')', 'IN', 'VBN', 'TO', 'NNS', ':', 'VBN', 'NN', '(', 'JJ', ',', 'IN', 'NNP', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', ')', '.']",57
natural_language_inference,94,224,Experiment 1 and 5 are our models presented in were also important for the model 's performance and that self - attention is able to contribute significantly to performance on top of other components of the model .,"['Experiment', '1', 'and', '5', 'are', 'our', 'models', 'presented', 'in', 'were', 'also', 'important', 'for', 'the', 'model', ""'s"", 'performance', 'and', 'that', 'self', '-', 'attention', 'is', 'able', 'to', 'contribute', 'significantly', 'to', 'performance', 'on', 'top', 'of', 'other', 'components', 'of', 'the', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', 'CD', 'CC', 'CD', 'VBP', 'PRP$', 'NNS', 'VBN', 'IN', 'VBD', 'RB', 'JJ', 'IN', 'DT', 'NN', 'POS', 'NN', 'CC', 'IN', 'PRP', ':', 'NN', 'VBZ', 'JJ', 'TO', 'VB', 'RB', 'TO', 'NN', 'IN', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']",38
natural_language_inference,94,225,"Finally , we see that effectively introducing external knowledge via our commonsense selection algorithm and NOIC can improve performance even further on top of our strong baseline .","['Finally', ',', 'we', 'see', 'that', 'effectively', 'introducing', 'external', 'knowledge', 'via', 'our', 'commonsense', 'selection', 'algorithm', 'and', 'NOIC', 'can', 'improve', 'performance', 'even', 'further', 'on', 'top', 'of', 'our', 'strong', 'baseline', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'IN', 'RB', 'VBG', 'JJ', 'NN', 'IN', 'PRP$', 'NN', 'NN', 'NN', 'CC', 'NNP', 'MD', 'VB', 'NN', 'RB', 'RBR', 'IN', 'NN', 'IN', 'PRP$', 'JJ', 'NN', '.']",28
natural_language_inference,94,232,"The results of these are shown in , where we see that neither NumberBatch nor random - relationships nor single - hop common - sense offer statistically significant improvements 7 , whereas our commonsense selection and incorporation mechanism improves performance significantly across all metrics .","['The', 'results', 'of', 'these', 'are', 'shown', 'in', ',', 'where', 'we', 'see', 'that', 'neither', 'NumberBatch', 'nor', 'random', '-', 'relationships', 'nor', 'single', '-', 'hop', 'common', '-', 'sense', 'offer', 'statistically', 'significant', 'improvements', '7', ',', 'whereas', 'our', 'commonsense', 'selection', 'and', 'incorporation', 'mechanism', 'improves', 'performance', 'significantly', 'across', 'all', 'metrics', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'DT', 'VBP', 'VBN', 'IN', ',', 'WRB', 'PRP', 'VBP', 'IN', 'DT', 'NNP', 'CC', 'JJ', ':', 'NNS', 'CC', 'JJ', ':', 'NN', 'JJ', ':', 'NN', 'NN', 'RB', 'JJ', 'NNS', 'CD', ',', 'IN', 'PRP$', 'NN', 'NN', 'CC', 'NN', 'NN', 'VBZ', 'NN', 'RB', 'IN', 'DT', 'NNS', '.']",45
natural_language_inference,91,2,A Fast Unified Model for Parsing and Sentence Understanding,"['A', 'Fast', 'Unified', 'Model', 'for', 'Parsing', 'and', 'Sentence', 'Understanding']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n']","['DT', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'CC', 'NNP', 'NNP']",9
natural_language_inference,91,21,"This paper introduces a new model to address both these issues : the Stack - augmented Parser - Interpreter Neural Network , or SPINN , shown in .","['This', 'paper', 'introduces', 'a', 'new', 'model', 'to', 'address', 'both', 'these', 'issues', ':', 'the', 'Stack', '-', 'augmented', 'Parser', '-', 'Interpreter', 'Neural', 'Network', ',', 'or', 'SPINN', ',', 'shown', 'in', '.']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'DT', 'NNS', ':', 'DT', 'NNP', ':', 'VBD', 'NNP', ':', 'NNP', 'NNP', 'NNP', ',', 'CC', 'NNP', ',', 'VBN', 'IN', '.']",28
natural_language_inference,91,22,"SPINN executes the computations of a tree - structured model in a linearized sequence , and can incorporate a neural network parser that produces the required parse structure on the fly .","['SPINN', 'executes', 'the', 'computations', 'of', 'a', 'tree', '-', 'structured', 'model', 'in', 'a', 'linearized', 'sequence', ',', 'and', 'can', 'incorporate', 'a', 'neural', 'network', 'parser', 'that', 'produces', 'the', 'required', 'parse', 'structure', 'on', 'the', 'fly', '.']","['B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'DT', 'NNS', 'IN', 'DT', 'JJ', ':', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'CC', 'MD', 'VB', 'DT', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', '.']",32
natural_language_inference,91,23,This design improves upon the TreeRNN architecture in three ways :,"['This', 'design', 'improves', 'upon', 'the', 'TreeRNN', 'architecture', 'in', 'three', 'ways', ':']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'IN', 'DT', 'NNP', 'NN', 'IN', 'CD', 'NNS', ':']",11
natural_language_inference,91,24,"At test time , it can simultaneously parse and interpret unparsed sentences , removing the dependence on an external parser at nearly no additional computational cost .","['At', 'test', 'time', ',', 'it', 'can', 'simultaneously', 'parse', 'and', 'interpret', 'unparsed', 'sentences', ',', 'removing', 'the', 'dependence', 'on', 'an', 'external', 'parser', 'at', 'nearly', 'no', 'additional', 'computational', 'cost', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', 'NN', ',', 'PRP', 'MD', 'RB', 'VB', 'CC', 'VB', 'JJ', 'NNS', ',', 'VBG', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'RB', 'DT', 'JJ', 'NN', 'NN', '.']",27
natural_language_inference,91,25,"Secondly , it supports batched computation for both parsed and unparsed sentences , yielding dramatic speedups over standard TreeRNNs .","['Secondly', ',', 'it', 'supports', 'batched', 'computation', 'for', 'both', 'parsed', 'and', 'unparsed', 'sentences', ',', 'yielding', 'dramatic', 'speedups', 'over', 'standard', 'TreeRNNs', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBZ', 'VBN', 'NN', 'IN', 'DT', 'JJ', 'CC', 'JJ', 'NNS', ',', 'VBG', 'JJ', 'NNS', 'IN', 'JJ', 'NNP', '.']",20
natural_language_inference,91,26,"Finally , it supports a novel tree - sequence hybrid architecture for handling local linear context in sentence interpretation .","['Finally', ',', 'it', 'supports', 'a', 'novel', 'tree', '-', 'sequence', 'hybrid', 'architecture', 'for', 'handling', 'local', 'linear', 'context', 'in', 'sentence', 'interpretation', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', ':', 'NN', 'JJ', 'NN', 'IN', 'VBG', 'JJ', 'JJ', 'NN', 'IN', 'NN', 'NN', '.']",20
natural_language_inference,91,143,Our optimized C ++/ CUDA models and the Theano source code for the full SPINN are available at https://github.com / stanfordnlp/spinn. 30 tokens or fewer .,"['Our', 'optimized', 'C', '++/', 'CUDA', 'models', 'and', 'the', 'Theano', 'source', 'code', 'for', 'the', 'full', 'SPINN', 'are', 'available', 'at', 'https://github.com', '/', 'stanfordnlp/spinn.', '30', 'tokens', 'or', 'fewer', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'JJ', 'NNP', 'NNP', 'NNP', 'NNS', 'CC', 'DT', 'NNP', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'VBP', 'JJ', 'IN', 'NN', 'NNP', 'VBD', 'CD', 'NNS', 'CC', 'JJR', '.']",26
natural_language_inference,91,144,We fix the model dimension D and the word embedding dimension at 300 .,"['We', 'fix', 'the', 'model', 'dimension', 'D', 'and', 'the', 'word', 'embedding', 'dimension', 'at', '300', '.']","['O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'NNP', 'CC', 'DT', 'NN', 'VBG', 'NN', 'IN', 'CD', '.']",14
natural_language_inference,91,145,We run the CPU performance test on a 2.20 GHz 16 core Intel Xeon E5-2660 processor with hyperthreading enabled .,"['We', 'run', 'the', 'CPU', 'performance', 'test', 'on', 'a', '2.20', 'GHz', '16', 'core', 'Intel', 'Xeon', 'E5-2660', 'processor', 'with', 'hyperthreading', 'enabled', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', 'NN', 'IN', 'DT', 'CD', 'NNP', 'CD', 'NN', 'NNP', 'NNP', 'NNP', 'NN', 'IN', 'VBG', 'VBN', '.']",20
natural_language_inference,91,146,We test our thin - stack implementation and the RNN model on an NVIDIA Titan X GPU .,"['We', 'test', 'our', 'thin', '-', 'stack', 'implementation', 'and', 'the', 'RNN', 'model', 'on', 'an', 'NVIDIA', 'Titan', 'X', 'GPU', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'JJ', ':', 'NN', 'NN', 'CC', 'DT', 'NNP', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', '.']",18
natural_language_inference,91,189,"We find that the bare SPINN - PI - NT model performs little better than the RNN baseline , but that SPINN - PI with the added tracking LSTM performs well .","['We', 'find', 'that', 'the', 'bare', 'SPINN', '-', 'PI', '-', 'NT', 'model', 'performs', 'little', 'better', 'than', 'the', 'RNN', 'baseline', ',', 'but', 'that', 'SPINN', '-', 'PI', 'with', 'the', 'added', 'tracking', 'LSTM', 'performs', 'well', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NN', 'NNP', ':', 'NNP', ':', 'NNP', 'NN', 'NNS', 'RB', 'JJR', 'IN', 'DT', 'NNP', 'NN', ',', 'CC', 'IN', 'NNP', ':', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'VBZ', 'RB', '.']",32
natural_language_inference,91,191,"The full SPINN model with its relatively weak internal parser performs slightly less well , but nonetheless robustly exceeds the performance of the RNN baseline .","['The', 'full', 'SPINN', 'model', 'with', 'its', 'relatively', 'weak', 'internal', 'parser', 'performs', 'slightly', 'less', 'well', ',', 'but', 'nonetheless', 'robustly', 'exceeds', 'the', 'performance', 'of', 'the', 'RNN', 'baseline', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NNP', 'NN', 'IN', 'PRP$', 'RB', 'JJ', 'JJ', 'NN', 'NNS', 'RB', 'RBR', 'RB', ',', 'CC', 'RB', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', '.']",26
natural_language_inference,91,192,Both SPINN - PI and the full SPINN significantly outperform all previous sentence - encoding models .,"['Both', 'SPINN', '-', 'PI', 'and', 'the', 'full', 'SPINN', 'significantly', 'outperform', 'all', 'previous', 'sentence', '-', 'encoding', 'models', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', ':', 'NN', 'CC', 'DT', 'JJ', 'NNP', 'RB', 'VBZ', 'DT', 'JJ', 'NN', ':', 'NN', 'NNS', '.']",17
natural_language_inference,91,193,"Most notably , these models outperform the tree - based CNN of , which also uses tree - structured composition for local feature extraction , but uses simpler pooling techniques to build sentence features in the interest of efficiency .","['Most', 'notably', ',', 'these', 'models', 'outperform', 'the', 'tree', '-', 'based', 'CNN', 'of', ',', 'which', 'also', 'uses', 'tree', '-', 'structured', 'composition', 'for', 'local', 'feature', 'extraction', ',', 'but', 'uses', 'simpler', 'pooling', 'techniques', 'to', 'build', 'sentence', 'features', 'in', 'the', 'interest', 'of', 'efficiency', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['JJS', 'RB', ',', 'DT', 'NNS', 'VBP', 'DT', 'NN', ':', 'VBN', 'NNP', 'IN', ',', 'WDT', 'RB', 'VBZ', 'JJ', ':', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', ',', 'CC', 'VBZ', 'NN', 'VBG', 'NNS', 'TO', 'VB', 'NN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NN', '.']",40
natural_language_inference,91,194,"Our results show that a model that uses tree - structured composition fully ( SPINN ) outper - forms one which uses it only partially ( tree - based CNN ) , which in turn outperforms one which does not use it at all ( RNN ) .","['Our', 'results', 'show', 'that', 'a', 'model', 'that', 'uses', 'tree', '-', 'structured', 'composition', 'fully', '(', 'SPINN', ')', 'outper', '-', 'forms', 'one', 'which', 'uses', 'it', 'only', 'partially', '(', 'tree', '-', 'based', 'CNN', ')', ',', 'which', 'in', 'turn', 'outperforms', 'one', 'which', 'does', 'not', 'use', 'it', 'at', 'all', '(', 'RNN', ')', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NNS', 'VBP', 'IN', 'DT', 'NN', 'WDT', 'VBZ', 'JJ', ':', 'JJ', 'NN', 'RB', '(', 'NNP', ')', 'IN', ':', 'NNS', 'CD', 'WDT', 'VBZ', 'PRP', 'RB', 'RB', '(', 'JJ', ':', 'VBN', 'NNP', ')', ',', 'WDT', 'IN', 'NN', 'NNS', 'CD', 'WDT', 'VBZ', 'RB', 'VB', 'PRP', 'IN', 'DT', '(', 'NNP', ')', '.']",48
natural_language_inference,91,195,"The full SPINN performed moderately well at reproducing the Stanford Parser 's parses of the SNLI data at a transition - by - transition level , with 92.4 % accuracy at test time .","['The', 'full', 'SPINN', 'performed', 'moderately', 'well', 'at', 'reproducing', 'the', 'Stanford', 'Parser', ""'s"", 'parses', 'of', 'the', 'SNLI', 'data', 'at', 'a', 'transition', '-', 'by', '-', 'transition', 'level', ',', 'with', '92.4', '%', 'accuracy', 'at', 'test', 'time', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NNP', 'VBD', 'RB', 'RB', 'IN', 'VBG', 'DT', 'NNP', 'NNP', 'POS', 'NNS', 'IN', 'DT', 'NNP', 'NNS', 'IN', 'DT', 'NN', ':', 'IN', ':', 'NN', 'NN', ',', 'IN', 'CD', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",34
natural_language_inference,30,2,Open Question Answering with Weakly Supervised Embedding Models,"['Open', 'Question', 'Answering', 'with', 'Weakly', 'Supervised', 'Embedding', 'Models']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', 'IN', 'NNP', 'VBD', 'NNP', 'NNP']",8
natural_language_inference,30,12,"This paper addresses the challenging problem of open - domain question answering , which consists of building systems able to answer questions from any domain .","['This', 'paper', 'addresses', 'the', 'challenging', 'problem', 'of', 'open', '-', 'domain', 'question', 'answering', ',', 'which', 'consists', 'of', 'building', 'systems', 'able', 'to', 'answer', 'questions', 'from', 'any', 'domain', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'DT', 'VBG', 'NN', 'IN', 'JJ', ':', 'NN', 'NN', 'NN', ',', 'WDT', 'VBZ', 'IN', 'VBG', 'NNS', 'JJ', 'TO', 'VB', 'NNS', 'IN', 'DT', 'NN', '.']",26
natural_language_inference,30,16,Question answering is then defined as the task of retrieving the correct entity or set of entities from a KB given a query expressed as a question in natural language .,"['Question', 'answering', 'is', 'then', 'defined', 'as', 'the', 'task', 'of', 'retrieving', 'the', 'correct', 'entity', 'or', 'set', 'of', 'entities', 'from', 'a', 'KB', 'given', 'a', 'query', 'expressed', 'as', 'a', 'question', 'in', 'natural', 'language', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'DT', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'CC', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NNP', 'VBN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NN', '.']",31
natural_language_inference,30,23,"In this paper , we instead take the approach of converting questions to ( uninterpretable ) vectorial representations which require no pre-defined grammars or lexicons and can query any KB independent of its schema .","['In', 'this', 'paper', ',', 'we', 'instead', 'take', 'the', 'approach', 'of', 'converting', 'questions', 'to', '(', 'uninterpretable', ')', 'vectorial', 'representations', 'which', 'require', 'no', 'pre-defined', 'grammars', 'or', 'lexicons', 'and', 'can', 'query', 'any', 'KB', 'independent', 'of', 'its', 'schema', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'RB', 'VBP', 'DT', 'NN', 'IN', 'VBG', 'NNS', 'TO', '(', 'JJ', ')', 'NN', 'NNS', 'WDT', 'VBP', 'DT', 'JJ', 'NNS', 'CC', 'NNS', 'CC', 'MD', 'VB', 'DT', 'NNP', 'JJ', 'IN', 'PRP$', 'NN', '.']",35
natural_language_inference,30,24,"Following , we focus on answering simple factual questions on a broad range of topics , more specifically , those for which single KB triples stand for both the question and an answer ( of which there maybe many ) .","['Following', ',', 'we', 'focus', 'on', 'answering', 'simple', 'factual', 'questions', 'on', 'a', 'broad', 'range', 'of', 'topics', ',', 'more', 'specifically', ',', 'those', 'for', 'which', 'single', 'KB', 'triples', 'stand', 'for', 'both', 'the', 'question', 'and', 'an', 'answer', '(', 'of', 'which', 'there', 'maybe', 'many', ')', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', ',', 'PRP', 'VBP', 'IN', 'VBG', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', ',', 'RBR', 'RB', ',', 'DT', 'IN', 'WDT', 'JJ', 'NNP', 'NNS', 'VBP', 'IN', 'DT', 'DT', 'NN', 'CC', 'DT', 'NN', '(', 'IN', 'WDT', 'EX', 'RB', 'JJ', ')', '.']",41
natural_language_inference,30,31,Our approach is based on learning low - dimensional vector embeddings of words and of KB triples so that representations of questions and corresponding answers end up being similar in the embedding space .,"['Our', 'approach', 'is', 'based', 'on', 'learning', 'low', '-', 'dimensional', 'vector', 'embeddings', 'of', 'words', 'and', 'of', 'KB', 'triples', 'so', 'that', 'representations', 'of', 'questions', 'and', 'corresponding', 'answers', 'end', 'up', 'being', 'similar', 'in', 'the', 'embedding', 'space', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'VBN', 'IN', 'VBG', 'JJ', ':', 'JJ', 'NN', 'NNS', 'IN', 'NNS', 'CC', 'IN', 'NNP', 'NNS', 'RB', 'IN', 'NNS', 'IN', 'NNS', 'CC', 'VBG', 'NNS', 'VBP', 'RP', 'VBG', 'JJ', 'IN', 'DT', 'JJ', 'NN', '.']",34
natural_language_inference,30,33,"In order to avoid transferring the cost of manual intervention to the one of labeling large amounts of data , we make use of weak supervision .","['In', 'order', 'to', 'avoid', 'transferring', 'the', 'cost', 'of', 'manual', 'intervention', 'to', 'the', 'one', 'of', 'labeling', 'large', 'amounts', 'of', 'data', ',', 'we', 'make', 'use', 'of', 'weak', 'supervision', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'VB', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'NN', 'TO', 'DT', 'CD', 'IN', 'VBG', 'JJ', 'NNS', 'IN', 'NNS', ',', 'PRP', 'VBP', 'NN', 'IN', 'JJ', 'NN', '.']",27
natural_language_inference,30,34,We show empirically that our model is able to take advantage of noisy and indirect supervision by ( i ) automatically generating questions from KB triples and treating this as training data ; and ( ii ) supplementing this with a data set of questions collaboratively marked as paraphrases but with no associated answers .,"['We', 'show', 'empirically', 'that', 'our', 'model', 'is', 'able', 'to', 'take', 'advantage', 'of', 'noisy', 'and', 'indirect', 'supervision', 'by', '(', 'i', ')', 'automatically', 'generating', 'questions', 'from', 'KB', 'triples', 'and', 'treating', 'this', 'as', 'training', 'data', ';', 'and', '(', 'ii', ')', 'supplementing', 'this', 'with', 'a', 'data', 'set', 'of', 'questions', 'collaboratively', 'marked', 'as', 'paraphrases', 'but', 'with', 'no', 'associated', 'answers', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'RB', 'IN', 'PRP$', 'NN', 'VBZ', 'JJ', 'TO', 'VB', 'NN', 'IN', 'NN', 'CC', 'JJ', 'NN', 'IN', '(', 'NN', ')', 'RB', 'VBG', 'NNS', 'IN', 'NNP', 'NNS', 'CC', 'VBG', 'DT', 'IN', 'NN', 'NNS', ':', 'CC', '(', 'NN', ')', 'VBG', 'DT', 'IN', 'DT', 'NN', 'NN', 'IN', 'NNS', 'RB', 'VBD', 'IN', 'NNS', 'CC', 'IN', 'DT', 'JJ', 'NNS', '.']",55
natural_language_inference,30,35,We end up learning meaningful vectorial representations for questions involving up to 800 k words and for triples of an mostly automatically created KB with 2.4 M entities and 600 k relationships .,"['We', 'end', 'up', 'learning', 'meaningful', 'vectorial', 'representations', 'for', 'questions', 'involving', 'up', 'to', '800', 'k', 'words', 'and', 'for', 'triples', 'of', 'an', 'mostly', 'automatically', 'created', 'KB', 'with', '2.4', 'M', 'entities', 'and', '600', 'k', 'relationships', '.']","['O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'RP', 'VBG', 'JJ', 'JJ', 'NNS', 'IN', 'NNS', 'VBG', 'RB', 'TO', 'CD', 'NN', 'NNS', 'CC', 'IN', 'NNS', 'IN', 'DT', 'RB', 'RB', 'VBN', 'NNP', 'IN', 'CD', 'NNP', 'NNS', 'CC', 'CD', 'NNS', 'NNS', '.']",33
natural_language_inference,30,201,"First , we can see that multitasking with paraphrase data is essential since it improves F1 from 0.60 to 0.68 .","['First', ',', 'we', 'can', 'see', 'that', 'multitasking', 'with', 'paraphrase', 'data', 'is', 'essential', 'since', 'it', 'improves', 'F1', 'from', '0.60', 'to', '0.68', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['RB', ',', 'PRP', 'MD', 'VB', 'IN', 'VBG', 'IN', 'NN', 'NNS', 'VBZ', 'JJ', 'IN', 'PRP', 'VBZ', 'NNP', 'IN', 'CD', 'TO', 'CD', '.']",21
natural_language_inference,30,207,Fine - tuning the embedding model is very beneficial to optimize the top of the list and grants a bump of 5 points of F1 : carefully tuning the similarity makes a clear difference .,"['Fine', '-', 'tuning', 'the', 'embedding', 'model', 'is', 'very', 'beneficial', 'to', 'optimize', 'the', 'top', 'of', 'the', 'list', 'and', 'grants', 'a', 'bump', 'of', '5', 'points', 'of', 'F1', ':', 'carefully', 'tuning', 'the', 'similarity', 'makes', 'a', 'clear', 'difference', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'VBG', 'DT', 'VBG', 'NN', 'VBZ', 'RB', 'JJ', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'CC', 'NNS', 'DT', 'NN', 'IN', 'CD', 'NNS', 'IN', 'NNP', ':', 'RB', 'VBG', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', '.']",35
natural_language_inference,30,208,"All versions of our system greatly outperform paralex : the fine - tuned model improves the F1 - score by almost 20 points and , according to , is better in precision for all levels of recall .","['All', 'versions', 'of', 'our', 'system', 'greatly', 'outperform', 'paralex', ':', 'the', 'fine', '-', 'tuned', 'model', 'improves', 'the', 'F1', '-', 'score', 'by', 'almost', '20', 'points', 'and', ',', 'according', 'to', ',', 'is', 'better', 'in', 'precision', 'for', 'all', 'levels', 'of', 'recall', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'PRP$', 'NN', 'RB', 'JJ', 'NN', ':', 'DT', 'JJ', ':', 'JJ', 'NN', 'VBZ', 'DT', 'NNP', ':', 'NN', 'IN', 'RB', 'CD', 'NNS', 'CC', ',', 'VBG', 'TO', ',', 'VBZ', 'RBR', 'IN', 'NN', 'IN', 'DT', 'NNS', 'IN', 'NN', '.']",38
natural_language_inference,30,231,"As expected , string matching greatly improves results , both in precision and recall , and also significantly reduces evaluation time .","['As', 'expected', ',', 'string', 'matching', 'greatly', 'improves', 'results', ',', 'both', 'in', 'precision', 'and', 'recall', ',', 'and', 'also', 'significantly', 'reduces', 'evaluation', 'time', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O']","['IN', 'VBN', ',', 'VBG', 'VBG', 'RB', 'VBZ', 'NNS', ',', 'DT', 'IN', 'NN', 'CC', 'NN', ',', 'CC', 'RB', 'RB', 'VBZ', 'NN', 'NN', '.']",22
natural_language_inference,30,232,"The final F1 obtained by our fine - tuned model is even better then the result of paralex in reranking , which is pretty remarkable , because this time , this setting advantages it quite a lot .","['The', 'final', 'F1', 'obtained', 'by', 'our', 'fine', '-', 'tuned', 'model', 'is', 'even', 'better', 'then', 'the', 'result', 'of', 'paralex', 'in', 'reranking', ',', 'which', 'is', 'pretty', 'remarkable', ',', 'because', 'this', 'time', ',', 'this', 'setting', 'advantages', 'it', 'quite', 'a', 'lot', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NNP', 'VBN', 'IN', 'PRP$', 'JJ', ':', 'JJ', 'NN', 'VBZ', 'RB', 'RBR', 'RB', 'DT', 'NN', 'IN', 'NN', 'IN', 'NN', ',', 'WDT', 'VBZ', 'JJ', 'JJ', ',', 'IN', 'DT', 'NN', ',', 'DT', 'NN', 'VBZ', 'PRP', 'RB', 'DT', 'NN', '.']",38
natural_language_inference,31,2,Simple and Effective Text Matching with Richer Alignment Features,"['Simple', 'and', 'Effective', 'Text', 'Matching', 'with', 'Richer', 'Alignment', 'Features']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['NN', 'CC', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNS']",9
natural_language_inference,31,21,"This paper presents RE2 , a fast and strong neural architecture with multiple alignment processes for general purpose text matching .","['This', 'paper', 'presents', 'RE2', ',', 'a', 'fast', 'and', 'strong', 'neural', 'architecture', 'with', 'multiple', 'alignment', 'processes', 'for', 'general', 'purpose', 'text', 'matching', '.']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'NNP', ',', 'DT', 'NN', 'CC', 'JJ', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NNS', 'IN', 'JJ', 'NN', 'NN', 'NN', '.']",21
natural_language_inference,31,25,"These components , which the name RE2 stands for , are previous aligned features ( Residual vectors ) , original point - wise features ( Embedding vectors ) , and contextual features ( Encoded vectors ) .","['These', 'components', ',', 'which', 'the', 'name', 'RE2', 'stands', 'for', ',', 'are', 'previous', 'aligned', 'features', '(', 'Residual', 'vectors', ')', ',', 'original', 'point', '-', 'wise', 'features', '(', 'Embedding', 'vectors', ')', ',', 'and', 'contextual', 'features', '(', 'Encoded', 'vectors', ')', '.']","['O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O']","['DT', 'NNS', ',', 'WDT', 'DT', 'NN', 'NNP', 'VBZ', 'IN', ',', 'VBP', 'JJ', 'VBN', 'NNS', '(', 'JJ', 'NNS', ')', ',', 'JJ', 'NN', ':', 'NN', 'NNS', '(', 'VBG', 'NNS', ')', ',', 'CC', 'JJ', 'NNS', '(', 'NNP', 'NNS', ')', '.']",37
natural_language_inference,31,28,An embedding layer first embeds discrete tokens .,"['An', 'embedding', 'layer', 'first', 'embeds', 'discrete', 'tokens', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'VBG', 'NN', 'RB', 'VBZ', 'JJ', 'NNS', '.']",8
natural_language_inference,31,29,"Several same - structured blocks consisting of encoding , alignment and fusion layers then process the sequences consecutively .","['Several', 'same', '-', 'structured', 'blocks', 'consisting', 'of', 'encoding', ',', 'alignment', 'and', 'fusion', 'layers', 'then', 'process', 'the', 'sequences', 'consecutively', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-n', 'O']","['JJ', 'JJ', ':', 'VBD', 'NNS', 'VBG', 'IN', 'VBG', ',', 'NN', 'CC', 'NN', 'NNS', 'RB', 'IN', 'DT', 'NNS', 'RB', '.']",19
natural_language_inference,31,30,These blocks are connected by an augmented version of residual connections ( see section 2.1 ) .,"['These', 'blocks', 'are', 'connected', 'by', 'an', 'augmented', 'version', 'of', 'residual', 'connections', '(', 'see', 'section', '2.1', ')', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', '(', 'VB', 'NN', 'CD', ')', '.']",17
natural_language_inference,31,31,A pooling layer aggregates sequential representations into vectors which are finally processed by a prediction layer to give the final prediction .,"['A', 'pooling', 'layer', 'aggregates', 'sequential', 'representations', 'into', 'vectors', 'which', 'are', 'finally', 'processed', 'by', 'a', 'prediction', 'layer', 'to', 'give', 'the', 'final', 'prediction', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'JJ', 'NNS', 'IN', 'NNS', 'WDT', 'VBP', 'RB', 'VBN', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', '.']",22
natural_language_inference,31,32,"The implementation of each layer is kept as simple as possible , and the whole model , as a well - organized combination , is quite powerful and lightweight at the same time .","['The', 'implementation', 'of', 'each', 'layer', 'is', 'kept', 'as', 'simple', 'as', 'possible', ',', 'and', 'the', 'whole', 'model', ',', 'as', 'a', 'well', '-', 'organized', 'combination', ',', 'is', 'quite', 'powerful', 'and', 'lightweight', 'at', 'the', 'same', 'time', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'VBN', 'RB', 'JJ', 'IN', 'JJ', ',', 'CC', 'DT', 'JJ', 'NN', ',', 'IN', 'DT', 'NN', ':', 'VBN', 'NN', ',', 'VBZ', 'RB', 'JJ', 'CC', 'JJ', 'IN', 'DT', 'JJ', 'NN', '.']",34
natural_language_inference,31,127,We implement our model with TensorFlow and train on Nvidia P100 GPUs .,"['We', 'implement', 'our', 'model', 'with', 'TensorFlow', 'and', 'train', 'on', 'Nvidia', 'P100', 'GPUs', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'IN', 'NNP', 'CC', 'NN', 'IN', 'NNP', 'NNP', 'NNP', '.']",13
natural_language_inference,31,128,"We tokenize sentences with the NLTK toolkit , convert them to lower cases and remove all punctuations .","['We', 'tokenize', 'sentences', 'with', 'the', 'NLTK', 'toolkit', ',', 'convert', 'them', 'to', 'lower', 'cases', 'and', 'remove', 'all', 'punctuations', '.']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'NNS', 'IN', 'DT', 'NNP', 'NN', ',', 'VB', 'PRP', 'TO', 'VB', 'NNS', 'CC', 'VB', 'DT', 'NNS', '.']",18
natural_language_inference,31,130,Word embeddings are initialized with 840B - 300d,"['Word', 'embeddings', 'are', 'initialized', 'with', '840B', '-', '300d']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n']","['NNP', 'NNS', 'VBP', 'VBN', 'IN', 'CD', ':', 'CD']",8
natural_language_inference,31,131,Glo Ve word vectors and fixed during training .,"['Glo', 'Ve', 'word', 'vectors', 'and', 'fixed', 'during', 'training', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['NNP', 'NNP', 'NN', 'NNS', 'CC', 'VBN', 'IN', 'NN', '.']",9
natural_language_inference,31,132,Embeddings of out - ofvocabulary words are initialized to zeros and fixed as well .,"['Embeddings', 'of', 'out', '-', 'ofvocabulary', 'words', 'are', 'initialized', 'to', 'zeros', 'and', 'fixed', 'as', 'well', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['NNS', 'IN', 'IN', ':', 'JJ', 'NNS', 'VBP', 'VBN', 'TO', 'NNS', 'CC', 'VBN', 'RB', 'RB', '.']",15
natural_language_inference,31,133,All other parameters are initialized with He initialization and normalized by weight normalization .,"['All', 'other', 'parameters', 'are', 'initialized', 'with', 'He', 'initialization', 'and', 'normalized', 'by', 'weight', 'normalization', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'PRP', 'NN', 'CC', 'VBN', 'IN', 'JJ', 'NN', '.']",14
natural_language_inference,31,134,Dropout with a keep probability of 0.8 is applied before every fully - connected or convolutional layer .,"['Dropout', 'with', 'a', 'keep', 'probability', 'of', '0.8', 'is', 'applied', 'before', 'every', 'fully', '-', 'connected', 'or', 'convolutional', 'layer', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', 'VBZ', 'VBN', 'IN', 'DT', 'RB', ':', 'VBD', 'CC', 'JJ', 'NN', '.']",18
natural_language_inference,31,135,The kernel size of the convolutional encoder is set to 3 .,"['The', 'kernel', 'size', 'of', 'the', 'convolutional', 'encoder', 'is', 'set', 'to', '3', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",12
natural_language_inference,31,136,The prediction layer is a two - layer feed - forward network .,"['The', 'prediction', 'layer', 'is', 'a', 'two', '-', 'layer', 'feed', '-', 'forward', 'network', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'DT', 'CD', ':', 'NN', 'NN', ':', 'NN', 'NN', '.']",13
natural_language_inference,31,137,The hidden size is set to 150 in all experiments .,"['The', 'hidden', 'size', 'is', 'set', 'to', '150', 'in', 'all', 'experiments', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'CD', 'IN', 'DT', 'NNS', '.']",11
natural_language_inference,31,138,"Activations in all feed - forward networks are GeLU activations , and we use ?","['Activations', 'in', 'all', 'feed', '-', 'forward', 'networks', 'are', 'GeLU', 'activations', ',', 'and', 'we', 'use', '?']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NNS', 'IN', 'DT', 'NN', ':', 'NN', 'NNS', 'VBP', 'NNP', 'NNS', ',', 'CC', 'PRP', 'VBP', '.']",15
natural_language_inference,31,140,We scale the summation in augmented residual connections by 1 / ? 2 when n ? 3 to preserve the variance under the assumption that the two addends have the same variance .,"['We', 'scale', 'the', 'summation', 'in', 'augmented', 'residual', 'connections', 'by', '1', '/', '?', '2', 'when', 'n', '?', '3', 'to', 'preserve', 'the', 'variance', 'under', 'the', 'assumption', 'that', 'the', 'two', 'addends', 'have', 'the', 'same', 'variance', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'JJ', 'NNS', 'IN', 'CD', 'NNS', '.', 'CD', 'WRB', 'NN', '.', 'CD', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'CD', 'NNS', 'VBP', 'DT', 'JJ', 'NN', '.']",33
natural_language_inference,31,141,The number of blocks is tuned in a range from 1 to 3 .,"['The', 'number', 'of', 'blocks', 'is', 'tuned', 'in', 'a', 'range', 'from', '1', 'to', '3', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'NNS', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'IN', 'CD', 'TO', 'CD', '.']",14
natural_language_inference,31,142,The number of layers of the convolutional encoder is tuned from 1 to 3 .,"['The', 'number', 'of', 'layers', 'of', 'the', 'convolutional', 'encoder', 'is', 'tuned', 'from', '1', 'to', '3', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'CD', 'TO', 'CD', '.']",15
natural_language_inference,31,144,"We use the Adam optimizer ( Kingma and Ba , 2015 ) and an exponentially decaying learning rate with a linear warmup .","['We', 'use', 'the', 'Adam', 'optimizer', '(', 'Kingma', 'and', 'Ba', ',', '2015', ')', 'and', 'an', 'exponentially', 'decaying', 'learning', 'rate', 'with', 'a', 'linear', 'warmup', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'CC', 'DT', 'RB', 'VBG', 'VBG', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",23
natural_language_inference,31,145,The initial learning rate is tuned from 0.0001 to 0.003 .,"['The', 'initial', 'learning', 'rate', 'is', 'tuned', 'from', '0.0001', 'to', '0.003', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', 'TO', 'CD', '.']",11
natural_language_inference,31,146,The batch size is tuned from 64 to 512 .,"['The', 'batch', 'size', 'is', 'tuned', 'from', '64', 'to', '512', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', 'TO', 'CD', '.']",10
natural_language_inference,31,147,The threshold for gradient clipping is set to 5 .,"['The', 'threshold', 'for', 'gradient', 'clipping', 'is', 'set', 'to', '5', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'CD', '.']",10
natural_language_inference,31,160,Results on WikiQA dataset are listed in .,"['Results', 'on', 'WikiQA', 'dataset', 'are', 'listed', 'in', '.']","['O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['NNS', 'IN', 'NNP', 'NN', 'VBP', 'VBN', 'IN', '.']",8
natural_language_inference,31,163,We obtain a result on par with the state - of - the - art reported on this dataset .,"['We', 'obtain', 'a', 'result', 'on', 'par', 'with', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'reported', 'on', 'this', 'dataset', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VB', 'DT', 'NN', 'IN', 'NN', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'VBD', 'IN', 'DT', 'NN', '.']",20
natural_language_inference,31,165,Our method can perform well in the answer selection task without any taskspecific modifications .,"['Our', 'method', 'can', 'perform', 'well', 'in', 'the', 'answer', 'selection', 'task', 'without', 'any', 'taskspecific', 'modifications', '.']","['O', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'MD', 'VB', 'RB', 'IN', 'DT', 'NN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNS', '.']",15
natural_language_inference,31,198,"The first ablation baseline shows that without richer features as the alignment input , the performance on all datasets degrades significantly .","['The', 'first', 'ablation', 'baseline', 'shows', 'that', 'without', 'richer', 'features', 'as', 'the', 'alignment', 'input', ',', 'the', 'performance', 'on', 'all', 'datasets', 'degrades', 'significantly', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'IN', 'IN', 'JJR', 'NNS', 'IN', 'DT', 'NN', 'NN', ',', 'DT', 'NN', 'IN', 'DT', 'NNS', 'VBZ', 'RB', '.']",22
natural_language_inference,31,200,The results of the second baseline show that vanilla residual connections without direct access to the original pointwise features are not enough to model the relations in many text matching tasks .,"['The', 'results', 'of', 'the', 'second', 'baseline', 'show', 'that', 'vanilla', 'residual', 'connections', 'without', 'direct', 'access', 'to', 'the', 'original', 'pointwise', 'features', 'are', 'not', 'enough', 'to', 'model', 'the', 'relations', 'in', 'many', 'text', 'matching', 'tasks', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NN', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'TO', 'DT', 'JJ', 'NN', 'NNS', 'VBP', 'RB', 'RB', 'TO', 'VB', 'DT', 'NNS', 'IN', 'JJ', 'NN', 'VBG', 'NNS', '.']",32
natural_language_inference,31,201,"The simpler implementation of the fusion layer leads to evidently worse performance , indicating that the fu- sion layer can not be further simplified .","['The', 'simpler', 'implementation', 'of', 'the', 'fusion', 'layer', 'leads', 'to', 'evidently', 'worse', 'performance', ',', 'indicating', 'that', 'the', 'fu-', 'sion', 'layer', 'can', 'not', 'be', 'further', 'simplified', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'TO', 'RB', 'JJR', 'NN', ',', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NN', 'MD', 'RB', 'VB', 'RB', 'VBN', '.']",25
natural_language_inference,31,203,"In the last ablation study , we can see that parallel blocks perform worse than stacked blocks , which supports the preference for deeper models over wider ones .","['In', 'the', 'last', 'ablation', 'study', ',', 'we', 'can', 'see', 'that', 'parallel', 'blocks', 'perform', 'worse', 'than', 'stacked', 'blocks', ',', 'which', 'supports', 'the', 'preference', 'for', 'deeper', 'models', 'over', 'wider', 'ones', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'NN', ',', 'PRP', 'MD', 'VB', 'DT', 'JJ', 'NNS', 'VBP', 'JJR', 'IN', 'VBD', 'NNS', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'JJR', 'NNS', 'IN', 'JJR', 'NNS', '.']",29
natural_language_inference,12,2,Shortcut - Stacked Sentence Encoders for Multi- Domain Inference,"['Shortcut', '-', 'Stacked', 'Sentence', 'Encoders', 'for', 'Multi-', 'Domain', 'Inference']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', ':', 'VBD', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",9
natural_language_inference,12,4,We present a simple sequential sentence encoder for multi-domain natural language inference .,"['We', 'present', 'a', 'simple', 'sequential', 'sentence', 'encoder', 'for', 'multi-domain', 'natural', 'language', 'inference', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'NN', '.']",13
natural_language_inference,12,10,Natural language inference ( NLI ) or recognizing textual entailment ( RTE ) is a fundamental semantic task in the field of natural language processing .,"['Natural', 'language', 'inference', '(', 'NLI', ')', 'or', 'recognizing', 'textual', 'entailment', '(', 'RTE', ')', 'is', 'a', 'fundamental', 'semantic', 'task', 'in', 'the', 'field', 'of', 'natural', 'language', 'processing', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'VBG', 'JJ', 'NN', '(', 'NNP', ')', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",26
natural_language_inference,12,15,"In this paper , we follow the former approach of encoding - based models , and propose a novel yet simple sequential sentence encoder for the Multi - NLI problem .","['In', 'this', 'paper', ',', 'we', 'follow', 'the', 'former', 'approach', 'of', 'encoding', '-', 'based', 'models', ',', 'and', 'propose', 'a', 'novel', 'yet', 'simple', 'sequential', 'sentence', 'encoder', 'for', 'the', 'Multi', '-', 'NLI', 'problem', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'VBG', ':', 'VBN', 'NNS', ',', 'CC', 'VB', 'DT', 'NN', 'RB', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NNP', ':', 'NN', 'NN', '.']",31
natural_language_inference,12,18,It is basically a stacked ( multi-layered ) bidirectional LSTM - RNN with shortcut connections ( feeding all previous layers ' outputs and word embeddings to each layer ) and word embedding fine - tuning .,"['It', 'is', 'basically', 'a', 'stacked', '(', 'multi-layered', ')', 'bidirectional', 'LSTM', '-', 'RNN', 'with', 'shortcut', 'connections', '(', 'feeding', 'all', 'previous', 'layers', ""'"", 'outputs', 'and', 'word', 'embeddings', 'to', 'each', 'layer', ')', 'and', 'word', 'embedding', 'fine', '-', 'tuning', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'RB', 'DT', 'VBN', '(', 'JJ', ')', 'JJ', 'NNP', ':', 'NN', 'IN', 'JJ', 'NNS', '(', 'VBG', 'DT', 'JJ', 'NNS', 'POS', 'NNS', 'CC', 'NN', 'NNS', 'TO', 'DT', 'NN', ')', 'CC', 'NN', 'VBG', 'JJ', ':', 'NN', '.']",36
natural_language_inference,12,19,"The over all supervised model uses these shortcutstacked encoders to encode two input sentences into two vectors , and then we use a classifier over the vector combination to label the relationship between these two sentences as that of entailment , contradiction , or neural ( similar to the classifier setup of and ) .","['The', 'over', 'all', 'supervised', 'model', 'uses', 'these', 'shortcutstacked', 'encoders', 'to', 'encode', 'two', 'input', 'sentences', 'into', 'two', 'vectors', ',', 'and', 'then', 'we', 'use', 'a', 'classifier', 'over', 'the', 'vector', 'combination', 'to', 'label', 'the', 'relationship', 'between', 'these', 'two', 'sentences', 'as', 'that', 'of', 'entailment', ',', 'contradiction', ',', 'or', 'neural', '(', 'similar', 'to', 'the', 'classifier', 'setup', 'of', 'and', ')', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NNS', 'TO', 'VB', 'CD', 'NN', 'NNS', 'IN', 'CD', 'NNS', ',', 'CC', 'RB', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'CD', 'NNS', 'IN', 'DT', 'IN', 'NN', ',', 'NN', ',', 'CC', 'JJ', '(', 'JJ', 'TO', 'DT', 'JJR', 'NN', 'IN', 'CC', ')', '.']",55
natural_language_inference,12,22,Github Code Link : https://github.com/ easonnie/multiNLI_encoder,"['Github', 'Code', 'Link', ':', 'https://github.com/', 'easonnie/multiNLI_encoder']","['O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', 'NNP', 'NNP', ':', 'NN', 'NN']",6
natural_language_inference,12,52,We use cross - entropy loss as the training objective with Adam - based opti-Model Accuracy SNLI Multi - NLI Matched Multi - NLI Mismatched CBOW 80.6 65.2 64.6 biLSTM Encoder 81.5 67.5 67.1 300D Tree - CNN Encoder 82.1 --300D SPINN - PI Encoder 83.2 --300D NSE Encoder 84.6 --biLSTM -Max Encoder 84 . mization with 32 batch size .,"['We', 'use', 'cross', '-', 'entropy', 'loss', 'as', 'the', 'training', 'objective', 'with', 'Adam', '-', 'based', 'opti-Model', 'Accuracy', 'SNLI', 'Multi', '-', 'NLI', 'Matched', 'Multi', '-', 'NLI', 'Mismatched', 'CBOW', '80.6', '65.2', '64.6', 'biLSTM', 'Encoder', '81.5', '67.5', '67.1', '300D', 'Tree', '-', 'CNN', 'Encoder', '82.1', '--300D', 'SPINN', '-', 'PI', 'Encoder', '83.2', '--300D', 'NSE', 'Encoder', '84.6', '--biLSTM', '-Max', 'Encoder', '84', '.', 'mization', 'with', '32', 'batch', 'size', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', ':', 'JJ', 'NN', 'IN', 'DT', 'NN', 'JJ', 'IN', 'NNP', ':', 'VBN', 'JJ', 'NNP', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'CD', 'CD', 'CD', 'NN', 'NNP', 'CD', 'CD', 'CD', 'CD', 'NNP', ':', 'NNP', 'NNP', 'CD', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'CD', 'NNP', 'NNP', 'NNP', 'CD', 'NNP', 'NNP', 'NNP', 'CD', '.', 'NN', 'IN', 'CD', 'NN', 'NN', '.']",61
natural_language_inference,12,53,The starting learning rate is 0.0002 with half decay every two epochs .,"['The', 'starting', 'learning', 'rate', 'is', '0.0002', 'with', 'half', 'decay', 'every', 'two', 'epochs', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'VBG', 'NN', 'NN', 'VBZ', 'CD', 'IN', 'JJ', 'JJ', 'DT', 'CD', 'NNS', '.']",13
natural_language_inference,12,54,The number of hidden units for MLP in classifier is 1600 .,"['The', 'number', 'of', 'hidden', 'units', 'for', 'MLP', 'in', 'classifier', 'is', '1600', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NNP', 'IN', 'NN', 'VBZ', 'CD', '.']",12
natural_language_inference,12,55,"Dropout layer is also applied on the output of each layer of MLP , with dropout rate set to 0.1 .","['Dropout', 'layer', 'is', 'also', 'applied', 'on', 'the', 'output', 'of', 'each', 'layer', 'of', 'MLP', ',', 'with', 'dropout', 'rate', 'set', 'to', '0.1', '.']","['B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['NNP', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'IN', 'NN', 'NN', 'VBN', 'TO', 'CD', '.']",21
natural_language_inference,12,56,We used pre-trained 300D Glove 840B vectors to initialize the word embeddings .,"['We', 'used', 'pre-trained', '300D', 'Glove', '840B', 'vectors', 'to', 'initialize', 'the', 'word', 'embeddings', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBD', 'JJ', 'CD', 'NNP', 'CD', 'NNS', 'TO', 'VB', 'DT', 'NN', 'NNS', '.']",13
natural_language_inference,12,61,"These ablation results are shown in and 4 , all based on the Multi - NLI development sets .","['These', 'ablation', 'results', 'are', 'shown', 'in', 'and', '4', ',', 'all', 'based', 'on', 'the', 'Multi', '-', 'NLI', 'development', 'sets', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'CC', 'CD', ',', 'DT', 'VBN', 'IN', 'DT', 'NNP', ':', 'NNP', 'NN', 'NNS', '.']",19
natural_language_inference,12,65,"As shown , each added layer model improves the accuracy and we achieve a substantial improvement in accuracy ( around 2 % ) on both matched and mismatched settings , compared to the single - layer biLSTM in .","['As', 'shown', ',', 'each', 'added', 'layer', 'model', 'improves', 'the', 'accuracy', 'and', 'we', 'achieve', 'a', 'substantial', 'improvement', 'in', 'accuracy', '(', 'around', '2', '%', ')', 'on', 'both', 'matched', 'and', 'mismatched', 'settings', ',', 'compared', 'to', 'the', 'single', '-', 'layer', 'biLSTM', 'in', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'VBN', ',', 'DT', 'VBD', 'NN', 'NN', 'VBZ', 'DT', 'NN', 'CC', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NN', '(', 'IN', 'CD', 'NN', ')', 'IN', 'DT', 'VBN', 'CC', 'VBN', 'NNS', ',', 'VBN', 'TO', 'DT', 'JJ', ':', 'NN', 'NN', 'IN', '.']",39
natural_language_inference,12,67,"Next , in , we show that the shortcut connections among the biLSTM layers is also an important contributor to accuracy improvement ( around 1.5 % on top of the full 3 - layered stacked - RNN model ) .","['Next', ',', 'in', ',', 'we', 'show', 'that', 'the', 'shortcut', 'connections', 'among', 'the', 'biLSTM', 'layers', 'is', 'also', 'an', 'important', 'contributor', 'to', 'accuracy', 'improvement', '(', 'around', '1.5', '%', 'on', 'top', 'of', 'the', 'full', '3', '-', 'layered', 'stacked', '-', 'RNN', 'model', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['JJ', ',', 'IN', ',', 'PRP', 'VBP', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NNS', 'VBZ', 'RB', 'DT', 'JJ', 'NN', 'TO', 'VB', 'NN', '(', 'IN', 'CD', 'NN', 'IN', 'NN', 'IN', 'DT', 'JJ', 'CD', ':', 'VBN', 'VBD', ':', 'NNP', 'NN', ')', '.']",40
natural_language_inference,12,69,"Next , in , we show that fine - tuning the word embeddings also improves results , again for both the in - domain task and cross - domain tasks ( the ablation results are based on a smaller model with a 128 +256 2 - layer biLSTM ) .","['Next', ',', 'in', ',', 'we', 'show', 'that', 'fine', '-', 'tuning', 'the', 'word', 'embeddings', 'also', 'improves', 'results', ',', 'again', 'for', 'both', 'the', 'in', '-', 'domain', 'task', 'and', 'cross', '-', 'domain', 'tasks', '(', 'the', 'ablation', 'results', 'are', 'based', 'on', 'a', 'smaller', 'model', 'with', 'a', '128', '+256', '2', '-', 'layer', 'biLSTM', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', ',', 'IN', ',', 'PRP', 'VBP', 'IN', 'JJ', ':', 'VBG', 'DT', 'NN', 'VBZ', 'RB', 'VBZ', 'NNS', ',', 'RB', 'IN', 'DT', 'DT', 'IN', ':', 'NN', 'NN', 'CC', 'NN', ':', 'NN', 'NNS', '(', 'DT', 'NN', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'JJR', 'NN', 'IN', 'DT', 'CD', 'NN', 'CD', ':', 'NN', 'NN', ')', '.']",50
natural_language_inference,12,71,The last ablation in shows that a classifier with two layers of relu is preferable than other options .,"['The', 'last', 'ablation', 'in', 'shows', 'that', 'a', 'classifier', 'with', 'two', 'layers', 'of', 'relu', 'is', 'preferable', 'than', 'other', 'options', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'CD', 'NNS', 'IN', 'NN', 'VBZ', 'JJ', 'IN', 'JJ', 'NNS', '.']",19
natural_language_inference,12,75,"First for Multi - NLI , we improve substantially over the CBOW and biL - STM Encoder baselines reported in the dataset paper .","['First', 'for', 'Multi', '-', 'NLI', ',', 'we', 'improve', 'substantially', 'over', 'the', 'CBOW', 'and', 'biL', '-', 'STM', 'Encoder', 'baselines', 'reported', 'in', 'the', 'dataset', 'paper', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'IN', 'NNP', ':', 'NN', ',', 'PRP', 'VBP', 'RB', 'IN', 'DT', 'NNP', 'CC', 'SYM', ':', 'JJ', 'NNP', 'NNS', 'VBD', 'IN', 'DT', 'JJ', 'NN', '.']",24
natural_language_inference,12,76,We also show that our final shortcut - based stacked encoder achieves around 3 % improvement as compared to the 1 layer biLSTM - Max Encoder in the second last row ( using the exact same classifier and optimizer settings ) .,"['We', 'also', 'show', 'that', 'our', 'final', 'shortcut', '-', 'based', 'stacked', 'encoder', 'achieves', 'around', '3', '%', 'improvement', 'as', 'compared', 'to', 'the', '1', 'layer', 'biLSTM', '-', 'Max', 'Encoder', 'in', 'the', 'second', 'last', 'row', '(', 'using', 'the', 'exact', 'same', 'classifier', 'and', 'optimizer', 'settings', ')', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'IN', 'PRP$', 'JJ', 'NN', ':', 'VBN', 'JJ', 'NN', 'NNS', 'IN', 'CD', 'NN', 'NN', 'IN', 'VBN', 'TO', 'DT', 'CD', 'NN', 'SYM', ':', 'NNP', 'NNP', 'IN', 'DT', 'JJ', 'JJ', 'NN', '(', 'VBG', 'DT', 'JJ', 'JJ', 'NN', 'CC', 'JJ', 'NNS', ')', '.']",42
natural_language_inference,12,77,Our shortcut - encoder was also the top singe - model ( non-ensemble ) result on the EMNLP RepEval Shared Task leaderboard .,"['Our', 'shortcut', '-', 'encoder', 'was', 'also', 'the', 'top', 'singe', '-', 'model', '(', 'non-ensemble', ')', 'result', 'on', 'the', 'EMNLP', 'RepEval', 'Shared', 'Task', 'leaderboard', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', ':', 'NN', 'VBD', 'RB', 'DT', 'JJ', 'NN', ':', 'NN', '(', 'JJ', ')', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', 'NN', '.']",23
natural_language_inference,12,78,"Next , for SNLI , we compare our shortcutstacked encoder with the current state - of - the - art encoders from the SNLI leaderboard ( https :// nlp.stanford.edu/projects/snli/ ) .","['Next', ',', 'for', 'SNLI', ',', 'we', 'compare', 'our', 'shortcutstacked', 'encoder', 'with', 'the', 'current', 'state', '-', 'of', '-', 'the', '-', 'art', 'encoders', 'from', 'the', 'SNLI', 'leaderboard', '(', 'https', '://', 'nlp.stanford.edu/projects/snli/', ')', '.']","['O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', ',', 'IN', 'NNP', ',', 'PRP', 'VBP', 'PRP$', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'NN', '(', 'JJ', 'NNP', 'NN', ')', '.']",31
natural_language_inference,12,79,"We also compare to the recent biLSTM - Max Encoder of , which served as our model 's 1 - layer starting point .","['We', 'also', 'compare', 'to', 'the', 'recent', 'biLSTM', '-', 'Max', 'Encoder', 'of', ',', 'which', 'served', 'as', 'our', 'model', ""'s"", '1', '-', 'layer', 'starting', 'point', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'TO', 'DT', 'JJ', 'NN', ':', 'NNP', 'NNP', 'IN', ',', 'WDT', 'VBD', 'IN', 'PRP$', 'NN', 'POS', 'CD', ':', 'NN', 'VBG', 'NN', '.']",24
natural_language_inference,12,80,"The results indicate that ' Our Shortcut - Stacked Encoder ' sur-passes all the previous state - of - the - art encoders , and achieves the new best encoding - based result on SNLI , suggesting the general effectiveness of simple shortcut - connected stacked layers in sentence encoders .","['The', 'results', 'indicate', 'that', ""'"", 'Our', 'Shortcut', '-', 'Stacked', 'Encoder', ""'"", 'sur-passes', 'all', 'the', 'previous', 'state', '-', 'of', '-', 'the', '-', 'art', 'encoders', ',', 'and', 'achieves', 'the', 'new', 'best', 'encoding', '-', 'based', 'result', 'on', 'SNLI', ',', 'suggesting', 'the', 'general', 'effectiveness', 'of', 'simple', 'shortcut', '-', 'connected', 'stacked', 'layers', 'in', 'sentence', 'encoders', '.']","['O', 'B-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'VBP', 'IN', ""''"", 'PRP$', 'NNP', ':', 'VBD', 'NNP', 'POS', 'NNS', 'PDT', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', ',', 'CC', 'VBZ', 'DT', 'JJ', 'JJS', 'VBG', ':', 'VBN', 'NN', 'IN', 'NNP', ',', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', ':', 'VBN', 'JJ', 'NNS', 'IN', 'NN', 'NNS', '.']",51
natural_language_inference,55,2,Question Answering with Subgraph Embeddings,"['Question', 'Answering', 'with', 'Subgraph', 'Embeddings']","['B-n', 'I-n', 'O', 'O', 'O']","['NN', 'VBG', 'IN', 'NNP', 'NNS']",5
natural_language_inference,55,4,This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few handcrafted features .,"['This', 'paper', 'presents', 'a', 'system', 'which', 'learns', 'to', 'answer', 'questions', 'on', 'a', 'broad', 'range', 'of', 'topics', 'from', 'a', 'knowledge', 'base', 'using', 'few', 'handcrafted', 'features', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'DT', 'NN', 'WDT', 'VBZ', 'TO', 'VB', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'VBG', 'JJ', 'JJ', 'NNS', '.']",25
natural_language_inference,55,8,Teaching machines how to automatically answer questions asked in natural language on any topic or in any domain has always been along standing goal in Artificial Intelligence .,"['Teaching', 'machines', 'how', 'to', 'automatically', 'answer', 'questions', 'asked', 'in', 'natural', 'language', 'on', 'any', 'topic', 'or', 'in', 'any', 'domain', 'has', 'always', 'been', 'along', 'standing', 'goal', 'in', 'Artificial', 'Intelligence', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NNS', 'WRB', 'TO', 'RB', 'VB', 'NNS', 'VBN', 'IN', 'JJ', 'NN', 'IN', 'DT', 'NN', 'CC', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'VBG', 'NN', 'IN', 'JJ', 'NNP', '.']",28
natural_language_inference,55,9,"With the rise of large scale structured knowledge bases ( KBs ) , this problem , known as open - domain question answering ( or open QA ) , boils down to being able to query efficiently such databases with natural language .","['With', 'the', 'rise', 'of', 'large', 'scale', 'structured', 'knowledge', 'bases', '(', 'KBs', ')', ',', 'this', 'problem', ',', 'known', 'as', 'open', '-', 'domain', 'question', 'answering', '(', 'or', 'open', 'QA', ')', ',', 'boils', 'down', 'to', 'being', 'able', 'to', 'query', 'efficiently', 'such', 'databases', 'with', 'natural', 'language', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'IN', 'JJ', 'NN', 'VBD', 'NN', 'NNS', '(', 'NNP', ')', ',', 'DT', 'NN', ',', 'VBN', 'IN', 'JJ', ':', 'NN', 'NN', 'VBG', '(', 'CC', 'VB', 'NNP', ')', ',', 'VBZ', 'RB', 'TO', 'VBG', 'JJ', 'TO', 'VB', 'RB', 'JJ', 'NNS', 'IN', 'JJ', 'NN', '.']",43
natural_language_inference,55,10,"These KBs , such as Freebase encompass huge ever growing amounts of information and ease open QA by organizing a great variety of answers in a structured format .","['These', 'KBs', ',', 'such', 'as', 'Freebase', 'encompass', 'huge', 'ever', 'growing', 'amounts', 'of', 'information', 'and', 'ease', 'open', 'QA', 'by', 'organizing', 'a', 'great', 'variety', 'of', 'answers', 'in', 'a', 'structured', 'format', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', ',', 'JJ', 'IN', 'NNP', 'NN', 'JJ', 'RB', 'VBG', 'NNS', 'IN', 'NN', 'CC', 'NN', 'JJ', 'NNP', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",29
natural_language_inference,55,23,"In this paper , we improve the model of by providing the ability to answer more complicated questions .","['In', 'this', 'paper', ',', 'we', 'improve', 'the', 'model', 'of', 'by', 'providing', 'the', 'ability', 'to', 'answer', 'more', 'complicated', 'questions', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'IN', 'VBG', 'DT', 'NN', 'TO', 'VB', 'RBR', 'JJ', 'NNS', '.']",19
natural_language_inference,55,24,s The main contributions of the paper are : ( 1 ) a more sophisticated inference procedure that is both efficient and can consider longer paths ( considered only answers directly connected to the question in the graph ) ; and ( 2 ) a richer representation of the answers which encodes the question - answer path and surrounding subgraph of the KB .,"['s', 'The', 'main', 'contributions', 'of', 'the', 'paper', 'are', ':', '(', '1', ')', 'a', 'more', 'sophisticated', 'inference', 'procedure', 'that', 'is', 'both', 'efficient', 'and', 'can', 'consider', 'longer', 'paths', '(', 'considered', 'only', 'answers', 'directly', 'connected', 'to', 'the', 'question', 'in', 'the', 'graph', ')', ';', 'and', '(', '2', ')', 'a', 'richer', 'representation', 'of', 'the', 'answers', 'which', 'encodes', 'the', 'question', '-', 'answer', 'path', 'and', 'surrounding', 'subgraph', 'of', 'the', 'KB', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O']","['VB', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'VBP', ':', '(', 'CD', ')', 'DT', 'RBR', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'CC', 'MD', 'VB', 'JJR', 'NNS', '(', 'VBN', 'RB', 'NNS', 'RB', 'VBN', 'TO', 'DT', 'NN', 'IN', 'DT', 'NN', ')', ':', 'CC', '(', 'CD', ')', 'DT', 'JJR', 'NN', 'IN', 'DT', 'NNS', 'WDT', 'VBP', 'DT', 'NN', ':', 'NN', 'NN', 'CC', 'VBG', 'NN', 'IN', 'DT', 'NNP', '.']",64
natural_language_inference,55,136,Replacing C 2 by C 1 induces a large drop in performance because many questions do not have answers that are directly connected to their inluded entity ( not in C 1 ) .,"['Replacing', 'C', '2', 'by', 'C', '1', 'induces', 'a', 'large', 'drop', 'in', 'performance', 'because', 'many', 'questions', 'do', 'not', 'have', 'answers', 'that', 'are', 'directly', 'connected', 'to', 'their', 'inluded', 'entity', '(', 'not', 'in', 'C', '1', ')', '.']","['B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NNP', 'CD', 'IN', 'NNP', 'CD', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'JJ', 'NNS', 'VBP', 'RB', 'VB', 'NNS', 'WDT', 'VBP', 'RB', 'VBN', 'TO', 'PRP$', 'JJ', 'NN', '(', 'RB', 'IN', 'NNP', 'CD', ')', '.']",34
natural_language_inference,55,137,"However , using all 2 - hops connections as a candidate set is also detrimental , because the larger number of candidates confuses ( and slows a lot ) our ranking based inference .","['However', ',', 'using', 'all', '2', '-', 'hops', 'connections', 'as', 'a', 'candidate', 'set', 'is', 'also', 'detrimental', ',', 'because', 'the', 'larger', 'number', 'of', 'candidates', 'confuses', '(', 'and', 'slows', 'a', 'lot', ')', 'our', 'ranking', 'based', 'inference', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'VBG', 'DT', 'CD', ':', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'RB', 'JJ', ',', 'IN', 'DT', 'JJR', 'NN', 'IN', 'NNS', 'NNS', '(', 'CC', 'VBZ', 'DT', 'NN', ')', 'PRP$', 'VBG', 'VBN', 'NN', '.']",34
natural_language_inference,55,138,"Our results also verify our hypothesis of Section 3.1 , that a richer representation for answers ( using the local subgraph ) can store more pertinent information .","['Our', 'results', 'also', 'verify', 'our', 'hypothesis', 'of', 'Section', '3.1', ',', 'that', 'a', 'richer', 'representation', 'for', 'answers', '(', 'using', 'the', 'local', 'subgraph', ')', 'can', 'store', 'more', 'pertinent', 'information', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP$', 'NNS', 'RB', 'VBP', 'PRP$', 'NN', 'IN', 'NNP', 'CD', ',', 'IN', 'DT', 'JJR', 'NN', 'IN', 'NNS', '(', 'VBG', 'DT', 'JJ', 'NN', ')', 'MD', 'VB', 'RBR', 'JJ', 'NN', '.']",28
natural_language_inference,55,139,"Finally , we demonstrate that we greatly improve upon the model of , which actually corresponds to a setting with the Path representation and C 1 as candidate set .","['Finally', ',', 'we', 'demonstrate', 'that', 'we', 'greatly', 'improve', 'upon', 'the', 'model', 'of', ',', 'which', 'actually', 'corresponds', 'to', 'a', 'setting', 'with', 'the', 'Path', 'representation', 'and', 'C', '1', 'as', 'candidate', 'set', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'IN', 'PRP', 'RB', 'VB', 'IN', 'DT', 'NN', 'IN', ',', 'WDT', 'RB', 'VBZ', 'TO', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', 'CC', 'NNP', 'CD', 'IN', 'NN', 'NN', '.']",30
natural_language_inference,55,145,"The ensemble improves the state - of - the - art , and indicates that our models are significantly different in their design .","['The', 'ensemble', 'improves', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', ',', 'and', 'indicates', 'that', 'our', 'models', 'are', 'significantly', 'different', 'in', 'their', 'design', '.']","['O', 'B-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', ',', 'CC', 'VBZ', 'IN', 'PRP$', 'NNS', 'VBP', 'RB', 'JJ', 'IN', 'PRP$', 'NN', '.']",24
natural_language_inference,67,2,End - to - End Answer Chunk Extraction and Ranking for Reading Comprehension,"['End', '-', 'to', '-', 'End', 'Answer', 'Chunk', 'Extraction', 'and', 'Ranking', 'for', 'Reading', 'Comprehension']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NN', ':', 'TO', ':', 'NN', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', 'IN', 'NNP', 'NNP']",13
natural_language_inference,67,4,"This paper proposes dynamic chunk reader ( DCR ) , an end - toend neural reading comprehension ( RC ) model that is able to extract and rank a set of answer candidates from a given document to answer questions .","['This', 'paper', 'proposes', 'dynamic', 'chunk', 'reader', '(', 'DCR', ')', ',', 'an', 'end', '-', 'toend', 'neural', 'reading', 'comprehension', '(', 'RC', ')', 'model', 'that', 'is', 'able', 'to', 'extract', 'and', 'rank', 'a', 'set', 'of', 'answer', 'candidates', 'from', 'a', 'given', 'document', 'to', 'answer', 'questions', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'JJ', 'NN', 'NN', '(', 'NNP', ')', ',', 'DT', 'NN', ':', 'JJ', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'NN', 'WDT', 'VBZ', 'JJ', 'TO', 'VB', 'CC', 'VB', 'DT', 'NN', 'IN', 'NN', 'NNS', 'IN', 'DT', 'VBN', 'NN', 'TO', 'VB', 'NNS', '.']",41
natural_language_inference,67,9,Reading comprehension - based question answering ( RCQA ) is the task of answering a question with a chunk of text taken from related document ( s ) .,"['Reading', 'comprehension', '-', 'based', 'question', 'answering', '(', 'RCQA', ')', 'is', 'the', 'task', 'of', 'answering', 'a', 'question', 'with', 'a', 'chunk', 'of', 'text', 'taken', 'from', 'related', 'document', '(', 's', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NN', ':', 'VBN', 'NN', 'VBG', '(', 'NNP', ')', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'VBN', 'IN', 'JJ', 'NN', '(', 'NN', ')', '.']",29
natural_language_inference,67,12,"Different from the above two assumptions for RCQA , in the real - world QA scenario , people may ask questions about both entities ( factoid ) and non-entities such as explanations and reasons ( non -factoid ) ( see for examples ) .","['Different', 'from', 'the', 'above', 'two', 'assumptions', 'for', 'RCQA', ',', 'in', 'the', 'real', '-', 'world', 'QA', 'scenario', ',', 'people', 'may', 'ask', 'questions', 'about', 'both', 'entities', '(', 'factoid', ')', 'and', 'non-entities', 'such', 'as', 'explanations', 'and', 'reasons', '(', 'non', '-factoid', ')', '(', 'see', 'for', 'examples', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'IN', 'DT', 'JJ', 'CD', 'NNS', 'IN', 'NNP', ',', 'IN', 'DT', 'JJ', ':', 'NN', 'NNP', 'NN', ',', 'NNS', 'MD', 'VB', 'NNS', 'IN', 'DT', 'NNS', '(', 'NN', ')', 'CC', 'NNS', 'JJ', 'IN', 'NNS', 'CC', 'NNS', '(', 'JJ', 'NN', ')', '(', 'VB', 'IN', 'NNS', ')', '.']",44
natural_language_inference,67,25,"Our proposed model , called dynamic chunk reader ( DCR ) , not only significantly differs from both the above systems in the way that answer candidates are generated and ranked , but also shares merits with both works .","['Our', 'proposed', 'model', ',', 'called', 'dynamic', 'chunk', 'reader', '(', 'DCR', ')', ',', 'not', 'only', 'significantly', 'differs', 'from', 'both', 'the', 'above', 'systems', 'in', 'the', 'way', 'that', 'answer', 'candidates', 'are', 'generated', 'and', 'ranked', ',', 'but', 'also', 'shares', 'merits', 'with', 'both', 'works', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'VBN', 'NN', ',', 'VBN', 'JJ', 'NN', 'NN', '(', 'NNP', ')', ',', 'RB', 'RB', 'RB', 'NNS', 'IN', 'DT', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'WDT', 'VBP', 'NNS', 'VBP', 'VBN', 'CC', 'VBN', ',', 'CC', 'RB', 'NNS', 'NNS', 'IN', 'DT', 'NNS', '.']",40
natural_language_inference,67,26,"First , our model uses deep networks to learn better representations for candidate answer chunks , instead of using fixed feature representations as in .","['First', ',', 'our', 'model', 'uses', 'deep', 'networks', 'to', 'learn', 'better', 'representations', 'for', 'candidate', 'answer', 'chunks', ',', 'instead', 'of', 'using', 'fixed', 'feature', 'representations', 'as', 'in', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['RB', ',', 'PRP$', 'NN', 'VBZ', 'JJ', 'NNS', 'TO', 'VB', 'JJR', 'NNS', 'IN', 'NN', 'NN', 'NNS', ',', 'RB', 'IN', 'VBG', 'JJ', 'NN', 'NNS', 'IN', 'IN', '.']",25
natural_language_inference,67,27,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .","['Second', ',', 'it', 'represents', 'answer', 'candidates', 'as', 'chunks', ',', 'as', 'in', '(', 'Rajpurkar', 'et', 'al.', ')', ',', 'instead', 'of', 'word', '-', 'level', 'representations', ',', 'to', 'make', 'the', 'model', 'aware', 'of', 'the', 'subtle', 'differences', 'among', 'candidates', '(', 'importantly', ',', 'overlapping', 'candidates', ')', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', ',', 'PRP', 'VBZ', 'JJR', 'NNS', 'IN', 'NNS', ',', 'IN', 'IN', '(', 'NNP', 'RB', 'RB', ')', ',', 'RB', 'IN', 'NN', ':', 'NN', 'NNS', ',', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NNS', '(', 'RB', ',', 'VBG', 'NNS', ')', '.']",42
natural_language_inference,67,136,We pre-processed the SQuAD dataset using Stanford CoreNLP tool 5 with its default setting to tokenize the text and obtain the POS and NE annotations .,"['We', 'pre-processed', 'the', 'SQuAD', 'dataset', 'using', 'Stanford', 'CoreNLP', 'tool', '5', 'with', 'its', 'default', 'setting', 'to', 'tokenize', 'the', 'text', 'and', 'obtain', 'the', 'POS', 'and', 'NE', 'annotations', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'DT', 'NNP', 'NN', 'VBG', 'NNP', 'NNP', 'VBD', 'CD', 'IN', 'PRP$', 'NN', 'VBG', 'TO', 'VB', 'DT', 'NN', 'CC', 'VB', 'DT', 'NNP', 'CC', 'NNP', 'NNS', '.']",26
natural_language_inference,67,137,"To train our model , we used stochastic gradient descent with the ADAM optimizer , with an initial learning rate of 0.001 .","['To', 'train', 'our', 'model', ',', 'we', 'used', 'stochastic', 'gradient', 'descent', 'with', 'the', 'ADAM', 'optimizer', ',', 'with', 'an', 'initial', 'learning', 'rate', 'of', '0.001', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['TO', 'VB', 'PRP$', 'NN', ',', 'PRP', 'VBD', 'JJ', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', ',', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', '.']",23
natural_language_inference,67,138,"All GRU weights were initialized from a uniform distribution between ( - 0.01 , 0.01 ) .","['All', 'GRU', 'weights', 'were', 'initialized', 'from', 'a', 'uniform', 'distribution', 'between', '(', '-', '0.01', ',', '0.01', ')', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNP', 'NNS', 'VBD', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', '(', ':', 'CD', ',', 'CD', ')', '.']",17
natural_language_inference,67,139,"The hidden state size , d , was set to 300 for all GRUs .","['The', 'hidden', 'state', 'size', ',', 'd', ',', 'was', 'set', 'to', '300', 'for', 'all', 'GRUs', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', ',', 'NN', ',', 'VBD', 'VBN', 'TO', 'CD', 'IN', 'DT', 'NNP', '.']",15
natural_language_inference,67,142,"We also applied dropout of rate 0.2 to the embedding layer of input bi - GRU encoder , and gradient clipping when the norm of gradients exceeded 10 .","['We', 'also', 'applied', 'dropout', 'of', 'rate', '0.2', 'to', 'the', 'embedding', 'layer', 'of', 'input', 'bi', '-', 'GRU', 'encoder', ',', 'and', 'gradient', 'clipping', 'when', 'the', 'norm', 'of', 'gradients', 'exceeded', '10', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'RB', 'VBD', 'IN', 'IN', 'NN', 'CD', 'TO', 'DT', 'VBG', 'NN', 'IN', 'NN', 'SYM', ':', 'NNP', 'NN', ',', 'CC', 'NN', 'NN', 'WRB', 'DT', 'NN', 'IN', 'NNS', 'VBD', 'CD', '.']",29
natural_language_inference,67,143,We trained in mini-batch style ( mini - batch size is 180 ) and applied zero - padding to the passage and question inputs in each batch .,"['We', 'trained', 'in', 'mini-batch', 'style', '(', 'mini', '-', 'batch', 'size', 'is', '180', ')', 'and', 'applied', 'zero', '-', 'padding', 'to', 'the', 'passage', 'and', 'question', 'inputs', 'in', 'each', 'batch', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBD', 'IN', 'JJ', 'NN', '(', 'SYM', ':', 'NN', 'NN', 'VBZ', 'CD', ')', 'CC', 'VBN', 'CD', ':', 'NN', 'TO', 'DT', 'NN', 'CC', 'NN', 'NNS', 'IN', 'DT', 'NN', '.']",28
natural_language_inference,67,144,"We also set the maximum passage length to be 300 tokens , and pruned all the tokens after the 300 - th token in the training set to save memory and speedup the training process .","['We', 'also', 'set', 'the', 'maximum', 'passage', 'length', 'to', 'be', '300', 'tokens', ',', 'and', 'pruned', 'all', 'the', 'tokens', 'after', 'the', '300', '-', 'th', 'token', 'in', 'the', 'training', 'set', 'to', 'save', 'memory', 'and', 'speedup', 'the', 'training', 'process', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBD', 'DT', 'JJ', 'NN', 'NN', 'TO', 'VB', 'CD', 'NNS', ',', 'CC', 'VBD', 'PDT', 'DT', 'NNS', 'IN', 'DT', 'CD', ':', 'NN', 'NN', 'IN', 'DT', 'NN', 'VBN', 'TO', 'VB', 'NN', 'CC', 'VB', 'DT', 'NN', 'NN', '.']",36
natural_language_inference,67,147,"We trained the model for at most 30 epochs , and in case the accuracy did not improve for 10 epochs , we stopped training .","['We', 'trained', 'the', 'model', 'for', 'at', 'most', '30', 'epochs', ',', 'and', 'in', 'case', 'the', 'accuracy', 'did', 'not', 'improve', 'for', '10', 'epochs', ',', 'we', 'stopped', 'training', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBD', 'DT', 'NN', 'IN', 'IN', 'JJS', 'CD', 'NN', ',', 'CC', 'IN', 'NN', 'DT', 'NN', 'VBD', 'RB', 'VB', 'IN', 'CD', 'NNS', ',', 'PRP', 'VBD', 'VBG', '.']",26
natural_language_inference,67,148,"For the feature ranking - based system , we used jforest ranker ( Ganjis affar , Caruana , and Lopes 2011 ) with Lambda MART - Regression Tree algorithm and the ranking metric was NDCG @ 10 .","['For', 'the', 'feature', 'ranking', '-', 'based', 'system', ',', 'we', 'used', 'jforest', 'ranker', '(', 'Ganjis', 'affar', ',', 'Caruana', ',', 'and', 'Lopes', '2011', ')', 'with', 'Lambda', 'MART', '-', 'Regression', 'Tree', 'algorithm', 'and', 'the', 'ranking', 'metric', 'was', 'NDCG', '@', '10', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'VBG', ':', 'VBN', 'NN', ',', 'PRP', 'VBD', 'JJS', 'NN', '(', 'NNP', 'RB', ',', 'NNP', ',', 'CC', 'NNP', 'CD', ')', 'IN', 'NNP', 'NNP', ':', 'NN', 'NNP', 'NN', 'CC', 'DT', 'JJ', 'JJ', 'VBD', 'NNP', 'NNP', 'CD', '.']",38
natural_language_inference,67,150,Results shows our main results on the SQuAD dataset .,"['Results', 'shows', 'our', 'main', 'results', 'on', 'the', 'SQuAD', 'dataset', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNS', 'VBZ', 'PRP$', 'JJ', 'NNS', 'IN', 'DT', 'NNP', 'NN', '.']",10
natural_language_inference,67,151,"Compared to the scores reported in , our exact match ( EM ) and F1 on the development set and EM score on the test set are better , and F1 on the test set is comparable .","['Compared', 'to', 'the', 'scores', 'reported', 'in', ',', 'our', 'exact', 'match', '(', 'EM', ')', 'and', 'F1', 'on', 'the', 'development', 'set', 'and', 'EM', 'score', 'on', 'the', 'test', 'set', 'are', 'better', ',', 'and', 'F1', 'on', 'the', 'test', 'set', 'is', 'comparable', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']","['VBN', 'TO', 'DT', 'NNS', 'VBN', 'IN', ',', 'PRP$', 'JJ', 'NN', '(', 'NNP', ')', 'CC', 'NNP', 'IN', 'DT', 'NN', 'NN', 'CC', 'NNP', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBP', 'JJR', ',', 'CC', 'NNP', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'JJ', '.']",38
natural_language_inference,67,154,"As the first row of shows , our baseline system improves 10 % ( EM ) over , row 1 ) , the feature - based ranking system .","['As', 'the', 'first', 'row', 'of', 'shows', ',', 'our', 'baseline', 'system', 'improves', '10', '%', '(', 'EM', ')', 'over', ',', 'row', '1', ')', ',', 'the', 'feature', '-', 'based', 'ranking', 'system', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', ',', 'PRP$', 'NN', 'NN', 'VBZ', 'CD', 'NN', '(', 'NNP', ')', 'IN', ',', 'VB', 'CD', ')', ',', 'DT', 'NN', ':', 'VBN', 'NN', 'NN', '.']",29
natural_language_inference,67,155,"However when compared to our DCR model , row 2 ) , the baseline ( row 1 ) is more than 12 % ( EM ) behind even though it is based on the state - of - the - art model for cloze - style RC tasks .","['However', 'when', 'compared', 'to', 'our', 'DCR', 'model', ',', 'row', '2', ')', ',', 'the', 'baseline', '(', 'row', '1', ')', 'is', 'more', 'than', '12', '%', '(', 'EM', ')', 'behind', 'even', 'though', 'it', 'is', 'based', 'on', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'model', 'for', 'cloze', '-', 'style', 'RC', 'tasks', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'WRB', 'VBN', 'TO', 'PRP$', 'NNP', 'NN', ',', 'VB', 'CD', ')', ',', 'DT', 'NN', '(', 'VB', 'CD', ')', 'VBZ', 'JJR', 'IN', 'CD', 'NN', '(', 'NNP', ')', 'IN', 'RB', 'IN', 'PRP', 'VBZ', 'VBN', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'JJ', ':', 'NN', 'NNP', 'NNS', '.']",49
natural_language_inference,67,159,"First , replacing the word - by - word attention with Attentive Reader style attention decreases the EM score by about 4.5 % , showing the strength of our proposed attention mechanism .","['First', ',', 'replacing', 'the', 'word', '-', 'by', '-', 'word', 'attention', 'with', 'Attentive', 'Reader', 'style', 'attention', 'decreases', 'the', 'EM', 'score', 'by', 'about', '4.5', '%', ',', 'showing', 'the', 'strength', 'of', 'our', 'proposed', 'attention', 'mechanism', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'VBG', 'DT', 'NN', ':', 'IN', ':', 'NN', 'NN', 'IN', 'JJ', 'NNP', 'NN', 'NN', 'VBZ', 'DT', 'NNP', 'NN', 'IN', 'IN', 'CD', 'NN', ',', 'VBG', 'DT', 'NN', 'IN', 'PRP$', 'VBN', 'NN', 'NN', '.']",33
natural_language_inference,67,161,The result shows that POS feature ( 1 ) and question - word feature ( 3 ) are the two most important features .,"['The', 'result', 'shows', 'that', 'POS', 'feature', '(', '1', ')', 'and', 'question', '-', 'word', 'feature', '(', '3', ')', 'are', 'the', 'two', 'most', 'important', 'features', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'IN', 'NNP', 'NN', '(', 'CD', ')', 'CC', 'NN', ':', 'NN', 'NN', '(', 'CD', ')', 'VBP', 'DT', 'CD', 'RBS', 'JJ', 'NNS', '.']",24
natural_language_inference,67,162,"Finally , combining the DCR model with the proposed POS - trie constraints yields a score similar to the one obtained using the DCR model with all possible n-gram chunks .","['Finally', ',', 'combining', 'the', 'DCR', 'model', 'with', 'the', 'proposed', 'POS', '-', 'trie', 'constraints', 'yields', 'a', 'score', 'similar', 'to', 'the', 'one', 'obtained', 'using', 'the', 'DCR', 'model', 'with', 'all', 'possible', 'n-gram', 'chunks', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'VBG', 'DT', 'NNP', 'NN', 'IN', 'DT', 'VBN', 'NNP', ':', 'NN', 'NNS', 'VBZ', 'DT', 'NN', 'JJ', 'TO', 'DT', 'NN', 'VBN', 'VBG', 'DT', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NNS', '.']",31
natural_language_inference,73,14,Equipping deep neural networks ( DNN ) with attention mechanisms provides an effective and parallelizable approach for context fusion and sequence compression .,"['Equipping', 'deep', 'neural', 'networks', '(', 'DNN', ')', 'with', 'attention', 'mechanisms', 'provides', 'an', 'effective', 'and', 'parallelizable', 'approach', 'for', 'context', 'fusion', 'and', 'sequence', 'compression', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', 'JJ', 'NNS', '(', 'NNP', ')', 'IN', 'NN', 'NNS', 'VBZ', 'DT', 'JJ', 'CC', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'CC', 'NN', 'NN', '.']",23
natural_language_inference,73,36,"In this paper , we first propose a novel hard attention mechanism called "" reinforced sequence sampling ( RSS ) "" , which selects tokens from an input sequence in parallel , and differs from existing ones in that it is highly parallelizable without any recurrent structure .","['In', 'this', 'paper', ',', 'we', 'first', 'propose', 'a', 'novel', 'hard', 'attention', 'mechanism', 'called', '""', 'reinforced', 'sequence', 'sampling', '(', 'RSS', ')', '""', ',', 'which', 'selects', 'tokens', 'from', 'an', 'input', 'sequence', 'in', 'parallel', ',', 'and', 'differs', 'from', 'existing', 'ones', 'in', 'that', 'it', 'is', 'highly', 'parallelizable', 'without', 'any', 'recurrent', 'structure', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'RB', 'VBD', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'VBN', 'NNP', 'VBD', 'NN', 'NN', '(', 'NNP', ')', 'NN', ',', 'WDT', 'VBZ', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'NN', ',', 'CC', 'NNS', 'IN', 'VBG', 'NNS', 'IN', 'DT', 'PRP', 'VBZ', 'RB', 'JJ', 'IN', 'DT', 'NN', 'NN', '.']",48
natural_language_inference,73,37,"We then develop a model , "" reinforced self - attention ( ReSA ) "" , which naturally combines the RSS with a soft self - attention .","['We', 'then', 'develop', 'a', 'model', ',', '""', 'reinforced', 'self', '-', 'attention', '(', 'ReSA', ')', '""', ',', 'which', 'naturally', 'combines', 'the', 'RSS', 'with', 'a', 'soft', 'self', '-', 'attention', '.']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VB', 'DT', 'NN', ',', 'NNP', 'VBD', 'PRP', ':', 'NN', '(', 'NNP', ')', 'NN', ',', 'WDT', 'RB', 'VBZ', 'DT', 'NNP', 'IN', 'DT', 'JJ', 'NN', ':', 'NN', '.']",28
natural_language_inference,73,38,"In ReSA , two parameter - untied RSS are respectively applied to two copies of the input sequence , where the tokens from one and another are called dependent and head tokens , respectively .","['In', 'ReSA', ',', 'two', 'parameter', '-', 'untied', 'RSS', 'are', 'respectively', 'applied', 'to', 'two', 'copies', 'of', 'the', 'input', 'sequence', ',', 'where', 'the', 'tokens', 'from', 'one', 'and', 'another', 'are', 'called', 'dependent', 'and', 'head', 'tokens', ',', 'respectively', '.']","['B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ',', 'CD', 'NN', ':', 'JJ', 'NNP', 'VBP', 'RB', 'VBN', 'TO', 'CD', 'NNS', 'IN', 'DT', 'NN', 'NN', ',', 'WRB', 'DT', 'NNS', 'IN', 'CD', 'CC', 'DT', 'VBP', 'VBN', 'NN', 'CC', 'NN', 'NNS', ',', 'RB', '.']",35
natural_language_inference,73,39,Re SA only models the sparse dependencies between the head and dependent tokens selected by the two RSS modules .,"['Re', 'SA', 'only', 'models', 'the', 'sparse', 'dependencies', 'between', 'the', 'head', 'and', 'dependent', 'tokens', 'selected', 'by', 'the', 'two', 'RSS', 'modules', '.']","['B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'RB', 'NNS', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CC', 'JJ', 'NNS', 'VBN', 'IN', 'DT', 'CD', 'NNP', 'NNS', '.']",20
natural_language_inference,73,40,"Finally , we build an sentence - encoding model , "" reinforced self - attention network ( ReSAN ) "" , based on ReSA without any CNN / RNN structure .","['Finally', ',', 'we', 'build', 'an', 'sentence', '-', 'encoding', 'model', ',', '""', 'reinforced', 'self', '-', 'attention', 'network', '(', 'ReSAN', ')', '""', ',', 'based', 'on', 'ReSA', 'without', 'any', 'CNN', '/', 'RNN', 'structure', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', ':', 'NN', 'NN', ',', 'NNP', 'VBD', 'PRP', ':', 'NN', 'NN', '(', 'NNP', ')', 'NN', ',', 'VBN', 'IN', 'NNP', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NN', '.']",31
natural_language_inference,73,44,All the experiments codes are released at https://github.com/ taoshen58/DiSAN /tree/master/ReSAN .,"['All', 'the', 'experiments', 'codes', 'are', 'released', 'at', 'https://github.com/', 'taoshen58/DiSAN', '/tree/master/ReSAN', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PDT', 'DT', 'NNS', 'NNS', 'VBP', 'VBN', 'IN', 'JJ', 'NN', 'NN', '.']",11
natural_language_inference,73,177,All experiments are conducted in Python with Tensorflow and run on a Nvidia GTX 1080 Ti .,"['All', 'experiments', 'are', 'conducted', 'in', 'Python', 'with', 'Tensorflow', 'and', 'run', 'on', 'a', 'Nvidia', 'GTX', '1080', 'Ti', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'IN', 'NNP', 'CC', 'VB', 'IN', 'DT', 'NNP', 'NNP', 'CD', 'NNP', '.']",17
natural_language_inference,73,178,"We use Adadelta as optimizer , which performs more stable than Adam on ReSAN .","['We', 'use', 'Adadelta', 'as', 'optimizer', ',', 'which', 'performs', 'more', 'stable', 'than', 'Adam', 'on', 'ReSAN', '.']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'NNP', 'IN', 'NN', ',', 'WDT', 'VBZ', 'RBR', 'JJ', 'IN', 'NNP', 'IN', 'NNP', '.']",15
natural_language_inference,73,180,We use 300D Glo Ve 6B pre-trained vectors,"['We', 'use', '300D', 'Glo', 'Ve', '6B', 'pre-trained', 'vectors']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['PRP', 'VBP', 'CD', 'NNP', 'NNP', 'CD', 'JJ', 'NNS']",8
natural_language_inference,73,185,Natural Language Inference,"['Natural', 'Language', 'Inference']","['B-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NN']",3
natural_language_inference,73,193,"Compared to the methods from official leaderboard , ReSAN outperforms all the sentence encoding based methods and achieves the best test accuracy .","['Compared', 'to', 'the', 'methods', 'from', 'official', 'leaderboard', ',', 'ReSAN', 'outperforms', 'all', 'the', 'sentence', 'encoding', 'based', 'methods', 'and', 'achieves', 'the', 'best', 'test', 'accuracy', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['VBN', 'TO', 'DT', 'NNS', 'IN', 'JJ', 'NN', ',', 'NNP', 'VBZ', 'PDT', 'DT', 'NN', 'VBG', 'VBN', 'NNS', 'CC', 'VBZ', 'DT', 'JJS', 'NN', 'NN', '.']",23
natural_language_inference,73,194,"Specifically , compared to the last best models , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , ReSAN uses far fewer parameters with better performance .","['Specifically', ',', 'compared', 'to', 'the', 'last', 'best', 'models', ',', 'i.e.', ',', '600D', 'Gumbel', 'TreeLSTM', 'encoders', 'and', '600D', 'Residual', 'stacked', 'encoders', ',', 'ReSAN', 'uses', 'far', 'fewer', 'parameters', 'with', 'better', 'performance', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'VBN', 'TO', 'DT', 'JJ', 'NN', 'NNS', ',', 'FW', ',', 'CD', 'NNP', 'NNP', 'NNS', 'CC', 'CD', 'NNP', 'VBD', 'NNS', ',', 'NNP', 'VBZ', 'RB', 'JJR', 'NNS', 'IN', 'JJR', 'NN', '.']",30
natural_language_inference,73,196,"Furthermore , ReSAN even outperforms the 300D SPINN - PI encoders by 3.1 %. , which is a recursive model and uses the result of an external semantic parsing tree as an extra input .","['Furthermore', ',', 'ReSAN', 'even', 'outperforms', 'the', '300D', 'SPINN', '-', 'PI', 'encoders', 'by', '3.1', '%.', ',', 'which', 'is', 'a', 'recursive', 'model', 'and', 'uses', 'the', 'result', 'of', 'an', 'external', 'semantic', 'parsing', 'tree', 'as', 'an', 'extra', 'input', '.']","['O', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'NNP', 'RB', 'VBZ', 'DT', 'CD', 'NNP', ':', 'NN', 'NNS', 'IN', 'CD', 'NNS', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'CC', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",35
natural_language_inference,73,198,"Compared to the recurrent models ( e.g. , Bi - LSTM and Bi - GRU ) , ReSAN shows better prediction quality and more compelling efficiency due to parallelizable computations .","['Compared', 'to', 'the', 'recurrent', 'models', '(', 'e.g.', ',', 'Bi', '-', 'LSTM', 'and', 'Bi', '-', 'GRU', ')', ',', 'ReSAN', 'shows', 'better', 'prediction', 'quality', 'and', 'more', 'compelling', 'efficiency', 'due', 'to', 'parallelizable', 'computations', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['VBN', 'TO', 'DT', 'NN', 'NNS', '(', 'NN', ',', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NNP', ')', ',', 'NNP', 'VBZ', 'JJR', 'NN', 'NN', 'CC', 'JJR', 'JJ', 'NN', 'JJ', 'TO', 'JJ', 'NNS', '.']",31
natural_language_inference,73,199,"Compared to the convolutional models ( i.e. , Multiwindow CNN and Hierarchical CNN ) , ReSAN significantly outperforms them by 3.1 % and 2.4 % respectively due to the weakness of CNNs in modeling long - range dependencies .","['Compared', 'to', 'the', 'convolutional', 'models', '(', 'i.e.', ',', 'Multiwindow', 'CNN', 'and', 'Hierarchical', 'CNN', ')', ',', 'ReSAN', 'significantly', 'outperforms', 'them', 'by', '3.1', '%', 'and', '2.4', '%', 'respectively', 'due', 'to', 'the', 'weakness', 'of', 'CNNs', 'in', 'modeling', 'long', '-', 'range', 'dependencies', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'TO', 'DT', 'JJ', 'NNS', '(', 'FW', ',', 'NNP', 'NNP', 'CC', 'NNP', 'NNP', ')', ',', 'NNP', 'RB', 'VBZ', 'PRP', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'RB', 'JJ', 'TO', 'DT', 'NN', 'IN', 'NNP', 'IN', 'VBG', 'JJ', ':', 'NN', 'NNS', '.']",39
natural_language_inference,73,200,"Compared to the attention - based models , multi-head attention and DiSAN , ReSAN uses a similar number of parameters with better test performance and less time cost .","['Compared', 'to', 'the', 'attention', '-', 'based', 'models', ',', 'multi-head', 'attention', 'and', 'DiSAN', ',', 'ReSAN', 'uses', 'a', 'similar', 'number', 'of', 'parameters', 'with', 'better', 'test', 'performance', 'and', 'less', 'time', 'cost', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['VBN', 'TO', 'DT', 'NN', ':', 'VBN', 'NNS', ',', 'JJ', 'NN', 'CC', 'NNP', ',', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'JJR', 'NN', 'NN', 'CC', 'JJR', 'NN', 'NN', '.']",29
natural_language_inference,73,204,"In terms of prediction quality , the results show that 1 ) the unselected head tokens do contribute to the prediction , bringing 0.2 % improvement ; 2 ) using separate RSS modules to select the head and dependent tokens improves accuracy by 0.5 % ; and 3 ) hard attention and soft self - attention modules improve the accuracy by 0.3 % and 2.9 % respectively .","['In', 'terms', 'of', 'prediction', 'quality', ',', 'the', 'results', 'show', 'that', '1', ')', 'the', 'unselected', 'head', 'tokens', 'do', 'contribute', 'to', 'the', 'prediction', ',', 'bringing', '0.2', '%', 'improvement', ';', '2', ')', 'using', 'separate', 'RSS', 'modules', 'to', 'select', 'the', 'head', 'and', 'dependent', 'tokens', 'improves', 'accuracy', 'by', '0.5', '%', ';', 'and', '3', ')', 'hard', 'attention', 'and', 'soft', 'self', '-', 'attention', 'modules', 'improve', 'the', 'accuracy', 'by', '0.3', '%', 'and', '2.9', '%', 'respectively', '.']","['B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'NNS', 'IN', 'NN', 'NN', ',', 'DT', 'NNS', 'VBP', 'IN', 'CD', ')', 'DT', 'JJ', 'NN', 'NNS', 'VBP', 'VB', 'TO', 'DT', 'NN', ',', 'VBG', 'CD', 'NN', 'NN', ':', 'CD', ')', 'VBG', 'JJ', 'NNP', 'NNS', 'TO', 'VB', 'DT', 'NN', 'CC', 'JJ', 'NNS', 'NNS', 'NN', 'IN', 'CD', 'NN', ':', 'CC', 'CD', ')', 'JJ', 'NN', 'CC', 'JJ', 'NN', ':', 'NN', 'NNS', 'VBP', 'DT', 'NN', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'RB', '.']",68
natural_language_inference,35,2,Multi - Style Generative Reading Comprehension,"['Multi', '-', 'Style', 'Generative', 'Reading', 'Comprehension']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', ':', 'JJ', 'NNP', 'NNP', 'NNP']",6
natural_language_inference,35,4,"This study tackles generative reading comprehension ( RC ) , which consists of answering questions based on textual evidence and natural language generation ( NLG ) .","['This', 'study', 'tackles', 'generative', 'reading', 'comprehension', '(', 'RC', ')', ',', 'which', 'consists', 'of', 'answering', 'questions', 'based', 'on', 'textual', 'evidence', 'and', 'natural', 'language', 'generation', '(', 'NLG', ')', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'JJ', 'NN', 'NN', '(', 'NNP', ')', ',', 'WDT', 'VBZ', 'IN', 'VBG', 'NNS', 'VBN', 'IN', 'JJ', 'NN', 'CC', 'JJ', 'NN', 'NN', '(', 'NNP', ')', '.']",27
natural_language_inference,35,15,"Recently , reading comprehension ( RC ) , a challenge to answer a question given textual evidence provided in a document set , has received much attention .","['Recently', ',', 'reading', 'comprehension', '(', 'RC', ')', ',', 'a', 'challenge', 'to', 'answer', 'a', 'question', 'given', 'textual', 'evidence', 'provided', 'in', 'a', 'document', 'set', ',', 'has', 'received', 'much', 'attention', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'VBG', 'NN', '(', 'NNP', ')', ',', 'DT', 'NN', 'TO', 'VB', 'DT', 'NN', 'VBN', 'JJ', 'NN', 'VBN', 'IN', 'DT', 'NN', 'NN', ',', 'VBZ', 'VBN', 'JJ', 'NN', '.']",28
natural_language_inference,35,16,"Current mainstream studies have treated RC as a process of extracting an answer span from one passage or multiple passages , which is usually done by predicting the start and end positions of the answer .","['Current', 'mainstream', 'studies', 'have', 'treated', 'RC', 'as', 'a', 'process', 'of', 'extracting', 'an', 'answer', 'span', 'from', 'one', 'passage', 'or', 'multiple', 'passages', ',', 'which', 'is', 'usually', 'done', 'by', 'predicting', 'the', 'start', 'and', 'end', 'positions', 'of', 'the', 'answer', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'NNS', 'VBP', 'VBN', 'NNP', 'IN', 'DT', 'NN', 'IN', 'VBG', 'DT', 'NN', 'NN', 'IN', 'CD', 'NN', 'CC', 'JJ', 'NNS', ',', 'WDT', 'VBZ', 'RB', 'VBN', 'IN', 'VBG', 'DT', 'NN', 'CC', 'NN', 'NNS', 'IN', 'DT', 'NN', '.']",36
natural_language_inference,35,24,"In this study , we propose Masque , a generative model for multi-passage RC .","['In', 'this', 'study', ',', 'we', 'propose', 'Masque', ',', 'a', 'generative', 'model', 'for', 'multi-passage', 'RC', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'NNP', ',', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNP', '.']",15
natural_language_inference,35,28,"We introduce the pointer - generator mechanism for generating an abstractive answer from the question and multiple passages , which covers various answer styles .","['We', 'introduce', 'the', 'pointer', '-', 'generator', 'mechanism', 'for', 'generating', 'an', 'abstractive', 'answer', 'from', 'the', 'question', 'and', 'multiple', 'passages', ',', 'which', 'covers', 'various', 'answer', 'styles', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', ':', 'NN', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'CC', 'JJ', 'NNS', ',', 'WDT', 'VBZ', 'JJ', 'NN', 'NNS', '.']",25
natural_language_inference,35,29,We extend the mechanism to a Transformer based one that allows words to be generated from a vocabulary and to be copied from the question and passages .,"['We', 'extend', 'the', 'mechanism', 'to', 'a', 'Transformer', 'based', 'one', 'that', 'allows', 'words', 'to', 'be', 'generated', 'from', 'a', 'vocabulary', 'and', 'to', 'be', 'copied', 'from', 'the', 'question', 'and', 'passages', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'TO', 'DT', 'NNP', 'VBN', 'CD', 'WDT', 'VBZ', 'NNS', 'TO', 'VB', 'VBN', 'IN', 'DT', 'JJ', 'CC', 'TO', 'VB', 'VBN', 'IN', 'DT', 'NN', 'CC', 'NNS', '.']",28
natural_language_inference,35,31,We introduce multi-style learning that enables our model to control answer styles and improves RC for all styles involved .,"['We', 'introduce', 'multi-style', 'learning', 'that', 'enables', 'our', 'model', 'to', 'control', 'answer', 'styles', 'and', 'improves', 'RC', 'for', 'all', 'styles', 'involved', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'VBZ', 'PRP$', 'NN', 'TO', 'VB', 'NN', 'NNS', 'CC', 'NNS', 'NNP', 'IN', 'DT', 'NNS', 'VBN', '.']",20
natural_language_inference,35,32,"We also extend the pointer - generator to a conditional decoder by introducing an artificial token corresponding to each style , as in .","['We', 'also', 'extend', 'the', 'pointer', '-', 'generator', 'to', 'a', 'conditional', 'decoder', 'by', 'introducing', 'an', 'artificial', 'token', 'corresponding', 'to', 'each', 'style', ',', 'as', 'in', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', ':', 'NN', 'TO', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'VBG', 'TO', 'DT', 'NN', ',', 'IN', 'IN', '.']",24
natural_language_inference,35,33,"For each decoding step , it controls the mixture weights over three distributions with the given style ( ) .","['For', 'each', 'decoding', 'step', ',', 'it', 'controls', 'the', 'mixture', 'weights', 'over', 'three', 'distributions', 'with', 'the', 'given', 'style', '(', ')', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'VBG', 'NN', ',', 'PRP', 'VBZ', 'DT', 'NN', 'NNS', 'IN', 'CD', 'NNS', 'IN', 'DT', 'VBN', 'NN', '(', ')', '.']",20
natural_language_inference,35,182,"shows that our single model , trained with two styles and controlled with the NQA style , pushed forward the state - of - the - art by a significant margin .","['shows', 'that', 'our', 'single', 'model', ',', 'trained', 'with', 'two', 'styles', 'and', 'controlled', 'with', 'the', 'NQA', 'style', ',', 'pushed', 'forward', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'by', 'a', 'significant', 'margin', '.']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNS', 'WDT', 'PRP$', 'JJ', 'NN', ',', 'VBN', 'IN', 'CD', 'NNS', 'CC', 'VBN', 'IN', 'DT', 'NNP', 'NN', ',', 'VBN', 'RB', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",32
natural_language_inference,35,183,The evaluation scores of the model controlled with the NLG style were low because the two styles are different .,"['The', 'evaluation', 'scores', 'of', 'the', 'model', 'controlled', 'with', 'the', 'NLG', 'style', 'were', 'low', 'because', 'the', 'two', 'styles', 'are', 'different', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NNS', 'IN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NNP', 'NN', 'VBD', 'JJ', 'IN', 'DT', 'CD', 'NNS', 'VBP', 'JJ', '.']",20
natural_language_inference,35,184,"Also , our model without multi-style learning ( trained with only the NQA style ) outperformed the baselines in terms of ROUGE - L .","['Also', ',', 'our', 'model', 'without', 'multi-style', 'learning', '(', 'trained', 'with', 'only', 'the', 'NQA', 'style', ')', 'outperformed', 'the', 'baselines', 'in', 'terms', 'of', 'ROUGE', '-', 'L', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'NN', 'IN', 'JJ', 'NN', '(', 'VBN', 'IN', 'RB', 'DT', 'NNP', 'NN', ')', 'VBD', 'DT', 'NNS', 'IN', 'NNS', 'IN', 'NNP', ':', 'NN', '.']",25
natural_language_inference,35,186,Experiments on NarrativeQA,"['Experiments', 'on', 'NarrativeQA']","['B-n', 'O', 'O']","['NNS', 'IN', 'NNP']",3
natural_language_inference,60,2,Dynamic Meta - Embeddings for Improved Sentence Representations,"['Dynamic', 'Meta', '-', 'Embeddings', 'for', 'Improved', 'Sentence', 'Representations']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NNP', 'NNP', ':', 'NNS', 'IN', 'NNP', 'NNP', 'NNP']",8
natural_language_inference,60,18,"In this work , we explore the supervised learning of task - specific , dynamic meta-embeddings , and apply the technique to sentence representations .","['In', 'this', 'work', ',', 'we', 'explore', 'the', 'supervised', 'learning', 'of', 'task', '-', 'specific', ',', 'dynamic', 'meta-embeddings', ',', 'and', 'apply', 'the', 'technique', 'to', 'sentence', 'representations', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NN', ':', 'NN', ',', 'JJ', 'NNS', ',', 'CC', 'VB', 'DT', 'NN', 'TO', 'VB', 'NNS', '.']",25
natural_language_inference,60,22,"First , it is embedding - agnostic , meaning that one of the main ( and perhaps most important ) hyperparameters in NLP pipelines is made obsolete .","['First', ',', 'it', 'is', 'embedding', '-', 'agnostic', ',', 'meaning', 'that', 'one', 'of', 'the', 'main', '(', 'and', 'perhaps', 'most', 'important', ')', 'hyperparameters', 'in', 'NLP', 'pipelines', 'is', 'made', 'obsolete', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBZ', 'VBG', ':', 'JJ', ',', 'VBG', 'IN', 'CD', 'IN', 'DT', 'JJ', '(', 'CC', 'RB', 'RBS', 'JJ', ')', 'NNS', 'IN', 'NNP', 'NNS', 'VBZ', 'VBN', 'JJ', '.']",28
natural_language_inference,0,2,Gated - Attention Readers for Text Comprehension,"['Gated', '-', 'Attention', 'Readers', 'for', 'Text', 'Comprehension']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['VBN', ':', 'NN', 'NNS', 'IN', 'NNP', 'NNP']",7
natural_language_inference,0,11,Source code is available on github : https:// github.com/bdhingra/ga-reader,"['Source', 'code', 'is', 'available', 'on', 'github', ':', 'https://', 'github.com/bdhingra/ga-reader']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NN', 'NN', 'VBZ', 'JJ', 'IN', 'NN', ':', 'NN', 'NN']",9
natural_language_inference,0,13,A recent trend to measure progress towards machine reading is to test a system 's ability to answer questions about a document it has to comprehend .,"['A', 'recent', 'trend', 'to', 'measure', 'progress', 'towards', 'machine', 'reading', 'is', 'to', 'test', 'a', 'system', ""'s"", 'ability', 'to', 'answer', 'questions', 'about', 'a', 'document', 'it', 'has', 'to', 'comprehend', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'TO', 'VB', 'NN', 'NNS', 'NN', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'POS', 'NN', 'TO', 'VB', 'NNS', 'IN', 'DT', 'NN', 'PRP', 'VBZ', 'TO', 'VB', '.']",27
natural_language_inference,0,23,"More specifically , unlike existing models where the query attention is applied either token - wise or sentence - wise to allow weighted aggregation , the Gated - Attention ( GA ) module proposed in this work allows the query to directly interact with each dimension of the token embeddings at the semantic - level , and is applied layer - wise as information filters during the multi-hop representation learning process .","['More', 'specifically', ',', 'unlike', 'existing', 'models', 'where', 'the', 'query', 'attention', 'is', 'applied', 'either', 'token', '-', 'wise', 'or', 'sentence', '-', 'wise', 'to', 'allow', 'weighted', 'aggregation', ',', 'the', 'Gated', '-', 'Attention', '(', 'GA', ')', 'module', 'proposed', 'in', 'this', 'work', 'allows', 'the', 'query', 'to', 'directly', 'interact', 'with', 'each', 'dimension', 'of', 'the', 'token', 'embeddings', 'at', 'the', 'semantic', '-', 'level', ',', 'and', 'is', 'applied', 'layer', '-', 'wise', 'as', 'information', 'filters', 'during', 'the', 'multi-hop', 'representation', 'learning', 'process', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RBR', 'RB', ',', 'IN', 'VBG', 'NNS', 'WRB', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'RB', 'JJ', ':', 'NN', 'CC', 'NN', ':', 'NN', 'TO', 'VB', 'JJ', 'NN', ',', 'DT', 'NNP', ':', 'NN', '(', 'NNP', ')', 'NN', 'VBN', 'IN', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'TO', 'RB', 'VB', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'JJ', ':', 'NN', ',', 'CC', 'VBZ', 'VBN', 'JJ', ':', 'NN', 'IN', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'VBG', 'NN', '.']",72
natural_language_inference,0,24,"Such a fine - grained attention enables our model to learn conditional token representations w.r.t. the given question , leading to accurate answer selections .","['Such', 'a', 'fine', '-', 'grained', 'attention', 'enables', 'our', 'model', 'to', 'learn', 'conditional', 'token', 'representations', 'w.r.t.', 'the', 'given', 'question', ',', 'leading', 'to', 'accurate', 'answer', 'selections', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'DT', 'JJ', ':', 'JJ', 'NN', 'VBZ', 'PRP$', 'NN', 'TO', 'VB', 'JJ', 'JJ', 'NNS', 'VBP', 'DT', 'VBN', 'NN', ',', 'VBG', 'TO', 'VB', 'NN', 'NNS', '.']",25
natural_language_inference,0,137,"Interestingly , we observe that feature engineering leads to significant improvements for WDW and CBT datasets , but not for CNN and Daily Mail datasets .","['Interestingly', ',', 'we', 'observe', 'that', 'feature', 'engineering', 'leads', 'to', 'significant', 'improvements', 'for', 'WDW', 'and', 'CBT', 'datasets', ',', 'but', 'not', 'for', 'CNN', 'and', 'Daily', 'Mail', 'datasets', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'VBZ', 'TO', 'JJ', 'NNS', 'IN', 'NNP', 'CC', 'NNP', 'NNS', ',', 'CC', 'RB', 'IN', 'NNP', 'CC', 'NNP', 'NNP', 'NNS', '.']",26
natural_language_inference,0,140,"Similarly , fixing the word embeddings provides an improvement for the WDW and CBT , but not for CNN and Daily Mail .","['Similarly', ',', 'fixing', 'the', 'word', 'embeddings', 'provides', 'an', 'improvement', 'for', 'the', 'WDW', 'and', 'CBT', ',', 'but', 'not', 'for', 'CNN', 'and', 'Daily', 'Mail', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'VBG', 'DT', 'NN', 'VBZ', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NNP', 'CC', 'NNP', ',', 'CC', 'RB', 'IN', 'NNP', 'CC', 'NNP', 'NNP', '.']",23
natural_language_inference,0,142,"Comparing with prior work , on the WDW dataset the basic version of the GA Reader outperforms all previously published models when trained on the Strict setting .","['Comparing', 'with', 'prior', 'work', ',', 'on', 'the', 'WDW', 'dataset', 'the', 'basic', 'version', 'of', 'the', 'GA', 'Reader', 'outperforms', 'all', 'previously', 'published', 'models', 'when', 'trained', 'on', 'the', 'Strict', 'setting', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['VBG', 'IN', 'JJ', 'NN', ',', 'IN', 'DT', 'NNP', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'VBZ', 'DT', 'RB', 'VBN', 'NNS', 'WRB', 'VBN', 'IN', 'DT', 'NNP', 'NN', '.']",28
natural_language_inference,0,143,By adding the qecomm feature the performance increases by 3.2 % and 3.5 % on the Strict and Relaxed settings respectively to set a new state of the art on this dataset .,"['By', 'adding', 'the', 'qecomm', 'feature', 'the', 'performance', 'increases', 'by', '3.2', '%', 'and', '3.5', '%', 'on', 'the', 'Strict', 'and', 'Relaxed', 'settings', 'respectively', 'to', 'set', 'a', 'new', 'state', 'of', 'the', 'art', 'on', 'this', 'dataset', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBG', 'DT', 'NN', 'NN', 'DT', 'NN', 'NNS', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NNS', 'RB', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",33
natural_language_inference,0,144,On the CNN and Daily Mail datasets the GA Reader leads to an improvement of 3.2 % and 4.3 % respectively over the best previous single models .,"['On', 'the', 'CNN', 'and', 'Daily', 'Mail', 'datasets', 'the', 'GA', 'Reader', 'leads', 'to', 'an', 'improvement', 'of', '3.2', '%', 'and', '4.3', '%', 'respectively', 'over', 'the', 'best', 'previous', 'single', 'models', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NNP', 'CC', 'NNP', 'NNP', 'VBZ', 'DT', 'NNP', 'NNP', 'VBZ', 'TO', 'DT', 'NN', 'IN', 'CD', 'NN', 'CC', 'CD', 'NN', 'RB', 'IN', 'DT', 'JJS', 'JJ', 'JJ', 'NNS', '.']",28
natural_language_inference,0,146,"For CBT - NE , GA Reader with the qecomm feature outperforms all previous single and ensemble models except the AS Reader trained on the much larger BookTest Corpus .","['For', 'CBT', '-', 'NE', ',', 'GA', 'Reader', 'with', 'the', 'qecomm', 'feature', 'outperforms', 'all', 'previous', 'single', 'and', 'ensemble', 'models', 'except', 'the', 'AS', 'Reader', 'trained', 'on', 'the', 'much', 'larger', 'BookTest', 'Corpus', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NNP', ':', 'NN', ',', 'NNP', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'JJ', 'CC', 'JJ', 'NNS', 'IN', 'DT', 'NNP', 'NNP', 'VBD', 'IN', 'DT', 'JJ', 'JJR', 'NNP', 'NNP', '.']",30
natural_language_inference,0,147,"Lastly , on CBT - CN the GA Reader with the qe-comm feature outperforms all previously published single models except the NSE , and AS Reader trained on a larger corpus .","['Lastly', ',', 'on', 'CBT', '-', 'CN', 'the', 'GA', 'Reader', 'with', 'the', 'qe-comm', 'feature', 'outperforms', 'all', 'previously', 'published', 'single', 'models', 'except', 'the', 'NSE', ',', 'and', 'AS', 'Reader', 'trained', 'on', 'a', 'larger', 'corpus', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'IN', 'NNP', ':', 'NNP', 'DT', 'NNP', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'DT', 'RB', 'VBN', 'JJ', 'NNS', 'IN', 'DT', 'NNP', ',', 'CC', 'NNP', 'NNP', 'VBD', 'IN', 'DT', 'JJR', 'NN', '.']",32
natural_language_inference,0,182,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the over all query representation .","['Next', ',', 'we', 'observe', 'a', 'substantial', 'drop', 'when', 'removing', 'tokenspecific', 'attentions', 'over', 'the', 'query', 'in', 'the', 'GA', 'module', ',', 'which', 'allow', 'gating', 'individual', 'tokens', 'in', 'the', 'document', 'only', 'by', 'parts', 'of', 'the', 'query', 'relevant', 'to', 'that', 'token', 'rather', 'than', 'the', 'over', 'all', 'query', 'representation', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'WRB', 'VBG', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', ',', 'WDT', 'VBP', 'VBG', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'RB', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'TO', 'DT', 'VBZ', 'RB', 'IN', 'DT', 'IN', 'DT', 'JJ', 'NN', '.']",45
natural_language_inference,0,183,"Finally , removing the character embeddings , which were only used for WDW and CBT , leads to a reduction of about 1 % in the performance .","['Finally', ',', 'removing', 'the', 'character', 'embeddings', ',', 'which', 'were', 'only', 'used', 'for', 'WDW', 'and', 'CBT', ',', 'leads', 'to', 'a', 'reduction', 'of', 'about', '1', '%', 'in', 'the', 'performance', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'VBG', 'DT', 'NN', 'NNS', ',', 'WDT', 'VBD', 'RB', 'VBN', 'IN', 'NNP', 'CC', 'NNP', ',', 'VBZ', 'TO', 'DT', 'NN', 'IN', 'RB', 'CD', 'NN', 'IN', 'DT', 'NN', '.']",28
natural_language_inference,52,2,Tell Me Why : Using Question Answering as Distant Supervision for Answer Justification,"['Tell', 'Me', 'Why', ':', 'Using', 'Question', 'Answering', 'as', 'Distant', 'Supervision', 'for', 'Answer', 'Justification']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['VB', 'NNP', 'WRB', ':', 'NN', 'NN', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",13
natural_language_inference,52,11,"Developing interpretable machine learning ( ML ) models , that is , models where a human user can understand what the model is learning , is considered by many to be crucial for ensuring usability and accelerating progress .","['Developing', 'interpretable', 'machine', 'learning', '(', 'ML', ')', 'models', ',', 'that', 'is', ',', 'models', 'where', 'a', 'human', 'user', 'can', 'understand', 'what', 'the', 'model', 'is', 'learning', ',', 'is', 'considered', 'by', 'many', 'to', 'be', 'crucial', 'for', 'ensuring', 'usability', 'and', 'accelerating', 'progress', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'NNS', ',', 'DT', 'VBZ', ',', 'NNS', 'WRB', 'DT', 'JJ', 'NN', 'MD', 'VB', 'WP', 'DT', 'NN', 'VBZ', 'VBG', ',', 'VBZ', 'VBN', 'IN', 'JJ', 'TO', 'VB', 'JJ', 'IN', 'VBG', 'NN', 'CC', 'VBG', 'NN', '.']",39
natural_language_inference,52,32,"Within this domain , we propose an approach that learns to both select and explain answers , when the only supervision available is for which answer is correct ( but not how to explain it ) .","['Within', 'this', 'domain', ',', 'we', 'propose', 'an', 'approach', 'that', 'learns', 'to', 'both', 'select', 'and', 'explain', 'answers', ',', 'when', 'the', 'only', 'supervision', 'available', 'is', 'for', 'which', 'answer', 'is', 'correct', '(', 'but', 'not', 'how', 'to', 'explain', 'it', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'WDT', 'VBZ', 'TO', 'DT', 'JJ', 'CC', 'JJ', 'NNS', ',', 'WRB', 'DT', 'JJ', 'NN', 'JJ', 'VBZ', 'IN', 'WDT', 'NN', 'VBZ', 'JJ', '(', 'CC', 'RB', 'WRB', 'TO', 'VB', 'PRP', ')', '.']",37
natural_language_inference,52,33,"Intuitively , our approach chooses the justifications that provide the most help towards ranking the correct answers higher than incorrect ones .","['Intuitively', ',', 'our', 'approach', 'chooses', 'the', 'justifications', 'that', 'provide', 'the', 'most', 'help', 'towards', 'ranking', 'the', 'correct', 'answers', 'higher', 'than', 'incorrect', 'ones', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'NNS', 'WDT', 'VBP', 'DT', 'RBS', 'JJ', 'NNS', 'VBG', 'DT', 'NN', 'VBZ', 'JJR', 'IN', 'JJ', 'NNS', '.']",22
natural_language_inference,52,34,"More formally , our neural network approach alternates between using the current model with max - pooling to choose the highest scoring justifications for correct answers , and optimizing the answer ranking model given these justifications .","['More', 'formally', ',', 'our', 'neural', 'network', 'approach', 'alternates', 'between', 'using', 'the', 'current', 'model', 'with', 'max', '-', 'pooling', 'to', 'choose', 'the', 'highest', 'scoring', 'justifications', 'for', 'correct', 'answers', ',', 'and', 'optimizing', 'the', 'answer', 'ranking', 'model', 'given', 'these', 'justifications', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['RBR', 'RB', ',', 'PRP$', 'JJ', 'NN', 'NN', 'VBZ', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'JJ', ':', 'NN', 'TO', 'VB', 'DT', 'JJS', 'NN', 'NNS', 'IN', 'JJ', 'NNS', ',', 'CC', 'VBG', 'DT', 'NN', 'VBG', 'NN', 'VBN', 'DT', 'NNS', '.']",37
natural_language_inference,52,148,IR Baseline :,"['IR', 'Baseline', ':']","['B-n', 'I-n', 'O']","['NNP', 'NNP', ':']",3
natural_language_inference,52,149,"For this baseline , we rank answer candidates by the maximum tf .idf document retrieval score using an unboosted query of question and answer terms ( see Section 4.1 for retrieval details ) .","['For', 'this', 'baseline', ',', 'we', 'rank', 'answer', 'candidates', 'by', 'the', 'maximum', 'tf', '.idf', 'document', 'retrieval', 'score', 'using', 'an', 'unboosted', 'query', 'of', 'question', 'and', 'answer', 'terms', '(', 'see', 'Section', '4.1', 'for', 'retrieval', 'details', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'VB', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'NN', 'NN', 'NN', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'NN', 'CC', 'NN', 'NNS', '(', 'VB', 'NNP', 'CD', 'IN', 'NN', 'NNS', ')', '.']",34
natural_language_inference,52,150,IR ++ :,"['IR', '++', ':']","['B-n', 'I-n', 'O']","['NNP', 'NN', ':']",3
natural_language_inference,52,151,"This baseline uses the same architecture as the full model , as described in Section 4.3 , but with only the IR ++ feature group .","['This', 'baseline', 'uses', 'the', 'same', 'architecture', 'as', 'the', 'full', 'model', ',', 'as', 'described', 'in', 'Section', '4.3', ',', 'but', 'with', 'only', 'the', 'IR', '++', 'feature', 'group', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'IN', 'VBN', 'IN', 'NNP', 'CD', ',', 'CC', 'IN', 'RB', 'DT', 'NNP', 'NNP', 'NN', 'NN', '.']",26
natural_language_inference,52,171,QA Performance,"['QA', 'Performance']","['B-n', 'I-n']","['NNP', 'NNP']",2
natural_language_inference,52,177,"Our full model that combines IR ++ , lexical overlap , discourse , and embeddings - based features , has a P@1 of 53.3 % ( line 7 ) , an absolute gain of 6.3 % over the strong IR baseline despite using the same background knowledge .","['Our', 'full', 'model', 'that', 'combines', 'IR', '++', ',', 'lexical', 'overlap', ',', 'discourse', ',', 'and', 'embeddings', '-', 'based', 'features', ',', 'has', 'a', 'P@1', 'of', '53.3', '%', '(', 'line', '7', ')', ',', 'an', 'absolute', 'gain', 'of', '6.3', '%', 'over', 'the', 'strong', 'IR', 'baseline', 'despite', 'using', 'the', 'same', 'background', 'knowledge', '.']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'JJ', 'NN', 'WDT', 'VBZ', 'NNP', 'NNP', ',', 'JJ', 'NN', ',', 'NN', ',', 'CC', 'NNS', ':', 'VBN', 'NNS', ',', 'VBZ', 'DT', 'NNP', 'IN', 'CD', 'NN', '(', 'NN', 'CD', ')', ',', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'NN', '.']",48
natural_language_inference,52,183,also tackle the AI2 Kaggle question set with an approach that learns alignments between questions and structured and semistructured KB data .,"['also', 'tackle', 'the', 'AI2', 'Kaggle', 'question', 'set', 'with', 'an', 'approach', 'that', 'learns', 'alignments', 'between', 'questions', 'and', 'structured', 'and', 'semistructured', 'KB', 'data', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'VBZ', 'DT', 'NNP', 'NNP', 'NN', 'NN', 'IN', 'DT', 'NN', 'WDT', 'VBZ', 'NNS', 'IN', 'NNS', 'CC', 'JJ', 'CC', 'JJ', 'NNP', 'NN', '.']",22
natural_language_inference,52,185,"By way of a loose comparison ( since we are evaluating on different data partitions ) , our model has approximately 5 % higher performance despite our simpler set of features and unstructured KB .","['By', 'way', 'of', 'a', 'loose', 'comparison', '(', 'since', 'we', 'are', 'evaluating', 'on', 'different', 'data', 'partitions', ')', ',', 'our', 'model', 'has', 'approximately', '5', '%', 'higher', 'performance', 'despite', 'our', 'simpler', 'set', 'of', 'features', 'and', 'unstructured', 'KB', '.']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', 'IN', 'DT', 'JJ', 'NN', '(', 'IN', 'PRP', 'VBP', 'VBG', 'IN', 'JJ', 'NNS', 'NNS', ')', ',', 'PRP$', 'NN', 'VBZ', 'RB', 'CD', 'NN', 'JJR', 'NN', 'IN', 'PRP$', 'NN', 'NN', 'IN', 'NNS', 'CC', 'JJ', 'NNP', '.']",35
natural_language_inference,52,192,"In comparison to other systems that competed in the Kaggle challenge , our system comes in in 7th place out of 170 competitors ( top 4 % ) .","['In', 'comparison', 'to', 'other', 'systems', 'that', 'competed', 'in', 'the', 'Kaggle', 'challenge', ',', 'our', 'system', 'comes', 'in', 'in', '7th', 'place', 'out', 'of', '170', 'competitors', '(', 'top', '4', '%', ')', '.']","['B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', 'TO', 'JJ', 'NNS', 'WDT', 'VBD', 'IN', 'DT', 'NNP', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'IN', 'IN', 'CD', 'NN', 'IN', 'IN', 'CD', 'NNS', '(', 'JJ', 'CD', 'NN', ')', '.']",29
natural_language_inference,52,197,Justification Performance,"['Justification', 'Performance']","['B-n', 'I-n']","['NN', 'NN']",2
natural_language_inference,52,228,"Note that 61 % of the top - ranked justifications from our system were rated as Good as compared to 52 % from the IR baseline ( a gain of 9 % ) , despite the systems using identical corpora .","['Note', 'that', '61', '%', 'of', 'the', 'top', '-', 'ranked', 'justifications', 'from', 'our', 'system', 'were', 'rated', 'as', 'Good', 'as', 'compared', 'to', '52', '%', 'from', 'the', 'IR', 'baseline', '(', 'a', 'gain', 'of', '9', '%', ')', ',', 'despite', 'the', 'systems', 'using', 'identical', 'corpora', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'IN', 'CD', 'NN', 'IN', 'DT', 'JJ', ':', 'JJ', 'NNS', 'IN', 'PRP$', 'NN', 'VBD', 'VBN', 'RB', 'JJ', 'IN', 'VBN', 'TO', 'CD', 'NN', 'IN', 'DT', 'NNP', 'NN', '(', 'DT', 'NN', 'IN', 'CD', 'NN', ')', ',', 'IN', 'DT', 'NNS', 'VBG', 'JJ', 'NN', '.']",41
natural_language_inference,40,2,Linguistic Knowledge as Memory for Recurrent Neural Networks,"['Linguistic', 'Knowledge', 'as', 'Memory', 'for', 'Recurrent', 'Neural', 'Networks']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['JJ', 'NNP', 'IN', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",8
natural_language_inference,40,4,Training recurrent neural networks to model long term dependencies is difficult .,"['Training', 'recurrent', 'neural', 'networks', 'to', 'model', 'long', 'term', 'dependencies', 'is', 'difficult', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['VBG', 'JJ', 'JJ', 'NNS', 'TO', 'VB', 'JJ', 'NN', 'NNS', 'VBZ', 'JJ', '.']",12
natural_language_inference,40,40,"Instead , we utilize the order inherent in the the unaugmented sequence to decompose the graph into two Directed Acyclic Graphs ( DAGs ) with a topological ordering .","['Instead', ',', 'we', 'utilize', 'the', 'order', 'inherent', 'in', 'the', 'the', 'unaugmented', 'sequence', 'to', 'decompose', 'the', 'graph', 'into', 'two', 'Directed', 'Acyclic', 'Graphs', '(', 'DAGs', ')', 'with', 'a', 'topological', 'ordering', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'CD', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'IN', 'DT', 'JJ', 'NN', '.']",29
natural_language_inference,40,41,"We introduce the Memory as Acyclic Graph Encoding RNN ( MAGE - RNN ) framework to compute the representation of such graphs while touching every node only once , and implement a GRU version of it called MAGE - GRU .","['We', 'introduce', 'the', 'Memory', 'as', 'Acyclic', 'Graph', 'Encoding', 'RNN', '(', 'MAGE', '-', 'RNN', ')', 'framework', 'to', 'compute', 'the', 'representation', 'of', 'such', 'graphs', 'while', 'touching', 'every', 'node', 'only', 'once', ',', 'and', 'implement', 'a', 'GRU', 'version', 'of', 'it', 'called', 'MAGE', '-', 'GRU', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ':', 'NN', ')', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'VBG', 'DT', 'NN', 'RB', 'RB', ',', 'CC', 'VB', 'DT', 'NNP', 'NN', 'IN', 'PRP', 'VBD', 'NNP', ':', 'NNP', '.']",41
natural_language_inference,40,42,"MAGE - RNN learns separate representations for propagation along each edge type , which leads to superior performance empirically .","['MAGE', '-', 'RNN', 'learns', 'separate', 'representations', 'for', 'propagation', 'along', 'each', 'edge', 'type', ',', 'which', 'leads', 'to', 'superior', 'performance', 'empirically', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NNP', 'VBZ', 'JJ', 'NNS', 'IN', 'NN', 'IN', 'DT', 'NN', 'NN', ',', 'WDT', 'VBZ', 'TO', 'VB', 'NN', 'RB', '.']",20
natural_language_inference,40,44,"We use MAGE - RNN to model coreference relations for text comprehension tasks , where answers to a query have to be extracted from a context document .","['We', 'use', 'MAGE', '-', 'RNN', 'to', 'model', 'coreference', 'relations', 'for', 'text', 'comprehension', 'tasks', ',', 'where', 'answers', 'to', 'a', 'query', 'have', 'to', 'be', 'extracted', 'from', 'a', 'context', 'document', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', ':', 'NN', 'TO', 'VB', 'NN', 'NNS', 'IN', 'JJ', 'NN', 'NNS', ',', 'WRB', 'NNS', 'TO', 'DT', 'NN', 'VBP', 'TO', 'VB', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",28
natural_language_inference,40,45,Tokens in a document are connected by a coreference relation if they refer to the same underlying entity .,"['Tokens', 'in', 'a', 'document', 'are', 'connected', 'by', 'a', 'coreference', 'relation', 'if', 'they', 'refer', 'to', 'the', 'same', 'underlying', 'entity', '.']","['B-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNS', 'IN', 'DT', 'NN', 'VBP', 'VBN', 'IN', 'DT', 'NN', 'NN', 'IN', 'PRP', 'VBP', 'TO', 'DT', 'JJ', 'JJ', 'NN', '.']",19
natural_language_inference,40,171,Story Based,"['Story', 'Based']","['B-n', 'I-n']","['NN', 'VBD']",2
natural_language_inference,40,182,"Our model achieves new state - of - the - art results , outperforming strong baselines such as QRNs .","['Our', 'model', 'achieves', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', ',', 'outperforming', 'strong', 'baselines', 'such', 'as', 'QRNs', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['PRP$', 'NN', 'VBZ', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', ',', 'VBG', 'JJ', 'NNS', 'JJ', 'IN', 'NNP', '.']",20
natural_language_inference,40,183,"Moreover , we observe that the proposed MAGE architecture can substantially improve the performance for both bi - GRUs and GAs .","['Moreover', ',', 'we', 'observe', 'that', 'the', 'proposed', 'MAGE', 'architecture', 'can', 'substantially', 'improve', 'the', 'performance', 'for', 'both', 'bi', '-', 'GRUs', 'and', 'GAs', '.']","['O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'IN', 'DT', 'VBN', 'NNP', 'NN', 'MD', 'RB', 'VB', 'DT', 'NN', 'IN', 'DT', 'SYM', ':', 'NNP', 'CC', 'NNP', '.']",22
natural_language_inference,40,184,"Adding the same information as one - hot features fails to improve the performance , which indicates that the inductive bias we employ on MAGE is useful .","['Adding', 'the', 'same', 'information', 'as', 'one', '-', 'hot', 'features', 'fails', 'to', 'improve', 'the', 'performance', ',', 'which', 'indicates', 'that', 'the', 'inductive', 'bias', 'we', 'employ', 'on', 'MAGE', 'is', 'useful', '.']","['B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'DT', 'JJ', 'NN', 'IN', 'CD', ':', 'JJ', 'NNS', 'VBZ', 'TO', 'VB', 'DT', 'NN', ',', 'WDT', 'VBZ', 'IN', 'DT', 'JJ', 'NN', 'PRP', 'VBP', 'IN', 'NNP', 'VBZ', 'JJ', '.']",28
natural_language_inference,40,185,"The DAG - RNN baseline from and the shared version of MAGE ( where edge representations are tied ) also perform worse , showing that our proposed architecture is superior .","['The', 'DAG', '-', 'RNN', 'baseline', 'from', 'and', 'the', 'shared', 'version', 'of', 'MAGE', '(', 'where', 'edge', 'representations', 'are', 'tied', ')', 'also', 'perform', 'worse', ',', 'showing', 'that', 'our', 'proposed', 'architecture', 'is', 'superior', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NNP', ':', 'NNP', 'NN', 'IN', 'CC', 'DT', 'VBN', 'NN', 'IN', 'NNP', '(', 'WRB', 'NN', 'NNS', 'VBP', 'VBN', ')', 'RB', 'JJ', 'JJR', ',', 'VBG', 'IN', 'PRP$', 'VBN', 'NN', 'VBZ', 'JJ', '.']",31
natural_language_inference,40,203,"Both variants of MAGE substantially outperform QRNs , which are the current state - of - the - art models on the bAbi dataset .","['Both', 'variants', 'of', 'MAGE', 'substantially', 'outperform', 'QRNs', ',', 'which', 'are', 'the', 'current', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', 'on', 'the', 'bAbi', 'dataset', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'NNP', 'RB', 'VBZ', 'NNP', ',', 'WDT', 'VBP', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",25
natural_language_inference,40,220,Broad Context Language Modeling :,"['Broad', 'Context', 'Language', 'Modeling', ':']","['B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNP', 'NNP', 'NNP', ':']",5
natural_language_inference,40,221,"For our second benchmark we pick the LAMBADA dataset from , where the task is to predict the last word in a given passage .","['For', 'our', 'second', 'benchmark', 'we', 'pick', 'the', 'LAMBADA', 'dataset', 'from', ',', 'where', 'the', 'task', 'is', 'to', 'predict', 'the', 'last', 'word', 'in', 'a', 'given', 'passage', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PRP$', 'JJ', 'NN', 'PRP', 'VBP', 'DT', 'NNP', 'NN', 'IN', ',', 'WRB', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'VBN', 'NN', '.']",25
natural_language_inference,40,232,"Our implementation of GA gave higher performance than that reported by , without the use of linguistic features .","['Our', 'implementation', 'of', 'GA', 'gave', 'higher', 'performance', 'than', 'that', 'reported', 'by', ',', 'without', 'the', 'use', 'of', 'linguistic', 'features', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'IN', 'NNP', 'VBD', 'JJR', 'NN', 'IN', 'DT', 'VBN', 'IN', ',', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', '.']",19
natural_language_inference,40,234,"On the simple bi - GRU architecture we see an improvement of 1.7 % by incorporating coreference edges in the graph , whereas the one - hot baseline does not lead to any improvement .","['On', 'the', 'simple', 'bi', '-', 'GRU', 'architecture', 'we', 'see', 'an', 'improvement', 'of', '1.7', '%', 'by', 'incorporating', 'coreference', 'edges', 'in', 'the', 'graph', ',', 'whereas', 'the', 'one', '-', 'hot', 'baseline', 'does', 'not', 'lead', 'to', 'any', 'improvement', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', ':', 'JJ', 'NN', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'CD', 'NN', 'IN', 'VBG', 'NN', 'NNS', 'IN', 'DT', 'NN', ',', 'IN', 'DT', 'CD', ':', 'JJ', 'NN', 'VBZ', 'RB', 'VB', 'TO', 'DT', 'NN', '.']",35
natural_language_inference,40,236,"On the multi - layer GA architecture , the coreference edges again lead to an improvement of 2 % , setting a new state - of - theart on this dataset .","['On', 'the', 'multi', '-', 'layer', 'GA', 'architecture', ',', 'the', 'coreference', 'edges', 'again', 'lead', 'to', 'an', 'improvement', 'of', '2', '%', ',', 'setting', 'a', 'new', 'state', '-', 'of', '-', 'theart', 'on', 'this', 'dataset', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ':', 'NN', 'NNP', 'NN', ',', 'DT', 'NN', 'VBZ', 'RB', 'JJ', 'TO', 'DT', 'NN', 'IN', 'CD', 'NN', ',', 'VBG', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'NN', 'IN', 'DT', 'NN', '.']",32
natural_language_inference,40,245,"Cloze - style QA : Lastly , we test our models on the CNN dataset from , which consists of pairs of news articles and a cloze - style question over the contents .","['Cloze', '-', 'style', 'QA', ':', 'Lastly', ',', 'we', 'test', 'our', 'models', 'on', 'the', 'CNN', 'dataset', 'from', ',', 'which', 'consists', 'of', 'pairs', 'of', 'news', 'articles', 'and', 'a', 'cloze', '-', 'style', 'question', 'over', 'the', 'contents', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'NN', 'NN', ':', 'RB', ',', 'PRP', 'VBP', 'PRP$', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'IN', ',', 'WDT', 'VBZ', 'IN', 'NNS', 'IN', 'NN', 'NNS', 'CC', 'DT', 'JJ', ':', 'NN', 'NN', 'IN', 'DT', 'NNS', '.']",34
natural_language_inference,40,257,Augmenting the bi - GRU model with MAGE leads to an improvement of 2.5 % on the test set .,"['Augmenting', 'the', 'bi', '-', 'GRU', 'model', 'with', 'MAGE', 'leads', 'to', 'an', 'improvement', 'of', '2.5', '%', 'on', 'the', 'test', 'set', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['VBG', 'DT', 'NN', ':', 'NNP', 'NN', 'IN', 'NNP', 'VBZ', 'TO', 'DT', 'NN', 'IN', 'CD', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",20
natural_language_inference,40,258,"The previous best results for this dataset were achieved by the GA Reader , and we see that adding MAGE to it leads to a further improvement of 0.7 % , setting a new state of the art .","['The', 'previous', 'best', 'results', 'for', 'this', 'dataset', 'were', 'achieved', 'by', 'the', 'GA', 'Reader', ',', 'and', 'we', 'see', 'that', 'adding', 'MAGE', 'to', 'it', 'leads', 'to', 'a', 'further', 'improvement', 'of', '0.7', '%', ',', 'setting', 'a', 'new', 'state', 'of', 'the', 'art', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'JJS', 'NNS', 'IN', 'DT', 'NN', 'VBD', 'VBN', 'IN', 'DT', 'NNP', 'NNP', ',', 'CC', 'PRP', 'VBP', 'IN', 'VBG', 'NNP', 'TO', 'PRP', 'VBZ', 'TO', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NN', ',', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",39
natural_language_inference,47,2,A large annotated corpus for learning natural language inference,"['A', 'large', 'annotated', 'corpus', 'for', 'learning', 'natural', 'language', 'inference']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['DT', 'JJ', 'JJ', 'NN', 'IN', 'VBG', 'JJ', 'NN', 'NN']",9
natural_language_inference,47,11,"Thus , natural language inference ( NLI ) - characterizing and using these relations in computational systems ) - is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning .","['Thus', ',', 'natural', 'language', 'inference', '(', 'NLI', ')', '-', 'characterizing', 'and', 'using', 'these', 'relations', 'in', 'computational', 'systems', ')', '-', 'is', 'essential', 'in', 'tasks', 'ranging', 'from', 'information', 'retrieval', 'to', 'semantic', 'parsing', 'to', 'commonsense', 'reasoning', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'JJ', 'NN', 'NN', '(', 'NNP', ')', ':', 'NN', 'CC', 'VBG', 'DT', 'NNS', 'IN', 'JJ', 'NNS', ')', ':', 'VBZ', 'JJ', 'IN', 'NNS', 'VBG', 'IN', 'NN', 'NN', 'TO', 'JJ', 'NN', 'TO', 'VB', 'NN', '.']",34
natural_language_inference,47,12,"NLI has been addressed using a variety of techniques , including those based on symbolic logic , knowledge bases , and neural networks .","['NLI', 'has', 'been', 'addressed', 'using', 'a', 'variety', 'of', 'techniques', ',', 'including', 'those', 'based', 'on', 'symbolic', 'logic', ',', 'knowledge', 'bases', ',', 'and', 'neural', 'networks', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'VBN', 'VBN', 'VBG', 'DT', 'NN', 'IN', 'NNS', ',', 'VBG', 'DT', 'VBN', 'IN', 'JJ', 'NN', ',', 'NN', 'NNS', ',', 'CC', 'JJ', 'NNS', '.']",24
natural_language_inference,47,19,"To address this , this paper introduces the Stanford Natural Language Inference ( SNLI ) corpus , a collection of sentence pairs labeled for entailment , contradiction , and semantic independence .","['To', 'address', 'this', ',', 'this', 'paper', 'introduces', 'the', 'Stanford', 'Natural', 'Language', 'Inference', '(', 'SNLI', ')', 'corpus', ',', 'a', 'collection', 'of', 'sentence', 'pairs', 'labeled', 'for', 'entailment', ',', 'contradiction', ',', 'and', 'semantic', 'independence', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O']","['TO', 'VB', 'DT', ',', 'DT', 'NN', 'VBZ', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', ',', 'DT', 'NN', 'IN', 'NN', 'NNS', 'VBD', 'IN', 'NN', ',', 'NN', ',', 'CC', 'JJ', 'NN', '.']",32
natural_language_inference,47,20,"At 570,152 sentence pairs , SNLI is two orders of magnitude larger than all other resources of its type .","['At', '570,152', 'sentence', 'pairs', ',', 'SNLI', 'is', 'two', 'orders', 'of', 'magnitude', 'larger', 'than', 'all', 'other', 'resources', 'of', 'its', 'type', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'CD', 'NN', 'NNS', ',', 'NNP', 'VBZ', 'CD', 'NNS', 'IN', 'NN', 'JJR', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'PRP$', 'NN', '.']",20
natural_language_inference,47,21,"And , in contrast to many such resources , all of its sentences and labels were written by humans in a grounded , naturalistic context .","['And', ',', 'in', 'contrast', 'to', 'many', 'such', 'resources', ',', 'all', 'of', 'its', 'sentences', 'and', 'labels', 'were', 'written', 'by', 'humans', 'in', 'a', 'grounded', ',', 'naturalistic', 'context', '.']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['CC', ',', 'IN', 'NN', 'TO', 'JJ', 'JJ', 'NNS', ',', 'DT', 'IN', 'PRP$', 'NNS', 'CC', 'NNS', 'VBD', 'VBN', 'IN', 'NNS', 'IN', 'DT', 'JJ', ',', 'JJ', 'NN', '.']",26
natural_language_inference,47,22,"In a separate validation phase , we collected four additional judgments for each label for 56,941 of the examples .","['In', 'a', 'separate', 'validation', 'phase', ',', 'we', 'collected', 'four', 'additional', 'judgments', 'for', 'each', 'label', 'for', '56,941', 'of', 'the', 'examples', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'NN', ',', 'PRP', 'VBD', 'CD', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'CD', 'IN', 'DT', 'NNS', '.']",20
natural_language_inference,47,23,"Of these , 98 % of cases emerge with a threeannotator consensus , and 58 % see a unanimous consensus from all five annotators .","['Of', 'these', ',', '98', '%', 'of', 'cases', 'emerge', 'with', 'a', 'threeannotator', 'consensus', ',', 'and', '58', '%', 'see', 'a', 'unanimous', 'consensus', 'from', 'all', 'five', 'annotators', '.']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', ',', 'CD', 'NN', 'IN', 'NNS', 'VBP', 'IN', 'DT', 'NN', 'NN', ',', 'CC', 'CD', 'NN', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'CD', 'NNS', '.']",25
natural_language_inference,47,163,"The sum of words model performed slightly worse than the fundamentally similar lexicalized classifier while the sum of words model can use pretrained word embeddings to better handle rare words , it lacks even the rudimentary sensitivity to word order that the lexicalized model 's bigram features provide .","['The', 'sum', 'of', 'words', 'model', 'performed', 'slightly', 'worse', 'than', 'the', 'fundamentally', 'similar', 'lexicalized', 'classifier', 'while', 'the', 'sum', 'of', 'words', 'model', 'can', 'use', 'pretrained', 'word', 'embeddings', 'to', 'better', 'handle', 'rare', 'words', ',', 'it', 'lacks', 'even', 'the', 'rudimentary', 'sensitivity', 'to', 'word', 'order', 'that', 'the', 'lexicalized', 'model', ""'s"", 'bigram', 'features', 'provide', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'NNS', 'NN', 'VBD', 'RB', 'JJR', 'IN', 'DT', 'RB', 'JJ', 'VBN', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNS', 'NN', 'MD', 'VB', 'JJ', 'NN', 'NNS', 'TO', 'RBR', 'VB', 'JJ', 'NNS', ',', 'PRP', 'VBZ', 'RB', 'DT', 'JJ', 'NN', 'TO', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'POS', 'NN', 'NNS', 'VBP', '.']",49
natural_language_inference,47,164,"Of the two RNN models , the LSTM 's more robust ability to learn long - term dependencies serves it well , giving it a substantial advantage over the plain RNN , and resulting in performance that is essentially equivalent to the lexicalized classifier on the test set ( LSTM performance near the stopping iteration varies by up to 0.5 % between evaluation steps ) .","['Of', 'the', 'two', 'RNN', 'models', ',', 'the', 'LSTM', ""'s"", 'more', 'robust', 'ability', 'to', 'learn', 'long', '-', 'term', 'dependencies', 'serves', 'it', 'well', ',', 'giving', 'it', 'a', 'substantial', 'advantage', 'over', 'the', 'plain', 'RNN', ',', 'and', 'resulting', 'in', 'performance', 'that', 'is', 'essentially', 'equivalent', 'to', 'the', 'lexicalized', 'classifier', 'on', 'the', 'test', 'set', '(', 'LSTM', 'performance', 'near', 'the', 'stopping', 'iteration', 'varies', 'by', 'up', 'to', '0.5', '%', 'between', 'evaluation', 'steps', ')', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'CD', 'NNP', 'NNS', ',', 'DT', 'NNP', 'POS', 'JJR', 'JJ', 'NN', 'TO', 'VB', 'JJ', ':', 'NN', 'VBZ', 'VBZ', 'PRP', 'RB', ',', 'VBG', 'PRP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NNP', ',', 'CC', 'VBG', 'IN', 'NN', 'WDT', 'VBZ', 'RB', 'JJ', 'TO', 'DT', 'VBN', 'NN', 'IN', 'DT', 'NN', 'NN', '(', 'NNP', 'NN', 'IN', 'DT', 'VBG', 'NN', 'NNS', 'IN', 'IN', 'TO', 'CD', 'NN', 'IN', 'NN', 'NNS', ')', '.']",66
natural_language_inference,47,165,"While the lexicalized model fits the training set almost perfectly , the gap between train and test set accuracy is relatively small for all three neural network models , suggesting that research into significantly higher capacity versions of these models would be productive .","['While', 'the', 'lexicalized', 'model', 'fits', 'the', 'training', 'set', 'almost', 'perfectly', ',', 'the', 'gap', 'between', 'train', 'and', 'test', 'set', 'accuracy', 'is', 'relatively', 'small', 'for', 'all', 'three', 'neural', 'network', 'models', ',', 'suggesting', 'that', 'research', 'into', 'significantly', 'higher', 'capacity', 'versions', 'of', 'these', 'models', 'would', 'be', 'productive', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', 'VBZ', 'DT', 'NN', 'VBN', 'RB', 'RB', ',', 'DT', 'NN', 'IN', 'NN', 'CC', 'NN', 'NN', 'NN', 'VBZ', 'RB', 'JJ', 'IN', 'DT', 'CD', 'JJ', 'NN', 'NNS', ',', 'VBG', 'IN', 'NN', 'IN', 'RB', 'JJR', 'NN', 'NNS', 'IN', 'DT', 'NNS', 'MD', 'VB', 'JJ', '.']",44
natural_language_inference,47,168,"In addition , though the LSTM and the lexicalized model show similar performance when trained on the current full corpus , the somewhat steeper slope for the LSTM hints that its ability to learn arbitrarily structured representations of sentence meaning may give it an advantage over the more constrained lexicalized model on still larger datasets .","['In', 'addition', ',', 'though', 'the', 'LSTM', 'and', 'the', 'lexicalized', 'model', 'show', 'similar', 'performance', 'when', 'trained', 'on', 'the', 'current', 'full', 'corpus', ',', 'the', 'somewhat', 'steeper', 'slope', 'for', 'the', 'LSTM', 'hints', 'that', 'its', 'ability', 'to', 'learn', 'arbitrarily', 'structured', 'representations', 'of', 'sentence', 'meaning', 'may', 'give', 'it', 'an', 'advantage', 'over', 'the', 'more', 'constrained', 'lexicalized', 'model', 'on', 'still', 'larger', 'datasets', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'IN', 'DT', 'NNP', 'CC', 'DT', 'JJ', 'NN', 'NN', 'JJ', 'NN', 'WRB', 'VBN', 'IN', 'DT', 'JJ', 'JJ', 'NN', ',', 'DT', 'RB', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NNS', 'IN', 'PRP$', 'NN', 'TO', 'VB', 'RB', 'VBN', 'NNS', 'IN', 'NN', 'NN', 'MD', 'VB', 'PRP', 'DT', 'NN', 'IN', 'DT', 'RBR', 'JJ', 'JJ', 'NN', 'IN', 'RB', 'JJR', 'NNS', '.']",56
natural_language_inference,82,2,Dynamic Entity Representation with Max - pooling Improves Machine Reading,"['Dynamic', 'Entity', 'Representation', 'with', 'Max', '-', 'pooling', 'Improves', 'Machine', 'Reading']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['NNP', 'NNP', 'NNP', 'IN', 'NNP', ':', 'VBG', 'VBZ', 'NNP', 'NNP']",10
natural_language_inference,82,7,Our code for the model is available at https://github.com/soskek/der-network,"['Our', 'code', 'for', 'the', 'model', 'is', 'available', 'at', 'https://github.com/soskek/der-network']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']","['PRP$', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'JJ', 'IN', 'NN']",9
natural_language_inference,82,23,"We , however , take it as a strong motivation to implement a reader that dynamically builds meaning representations for each entity , by gathering and accumulating information on that entity as it reads a document ( Section 2 ) .","['We', ',', 'however', ',', 'take', 'it', 'as', 'a', 'strong', 'motivation', 'to', 'implement', 'a', 'reader', 'that', 'dynamically', 'builds', 'meaning', 'representations', 'for', 'each', 'entity', ',', 'by', 'gathering', 'and', 'accumulating', 'information', 'on', 'that', 'entity', 'as', 'it', 'reads', 'a', 'document', '(', 'Section', '2', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O']","['PRP', ',', 'RB', ',', 'VBP', 'PRP', 'IN', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'WDT', 'RB', 'VBZ', 'VBG', 'NNS', 'IN', 'DT', 'NN', ',', 'IN', 'VBG', 'CC', 'VBG', 'NN', 'IN', 'DT', 'NN', 'IN', 'PRP', 'VBZ', 'DT', 'NN', '(', 'NNP', 'CD', ')', '.']",41
natural_language_inference,82,76,"For preprocessing , we segment sentences at punctuation marks "" . "" , "" ! "" , and "" ? "" .","['For', 'preprocessing', ',', 'we', 'segment', 'sentences', 'at', 'punctuation', 'marks', '""', '.', '""', ',', '""', '!', '""', ',', 'and', '""', '?', '""', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O']","['IN', 'VBG', ',', 'PRP', 'NN', 'NNS', 'IN', 'NN', 'NNS', 'NNP', '.', 'NN', ',', 'NN', '.', 'NN', ',', 'CC', 'VB', '.', 'NN', '.']",22
natural_language_inference,82,78,"We train our model 8 with hyper - parameters lightly tuned on the validation set 9 , and we conduct ablation test on several techniques that improve our basic model .","['We', 'train', 'our', 'model', '8', 'with', 'hyper', '-', 'parameters', 'lightly', 'tuned', 'on', 'the', 'validation', 'set', '9', ',', 'and', 'we', 'conduct', 'ablation', 'test', 'on', 'several', 'techniques', 'that', 'improve', 'our', 'basic', 'model', '.']","['O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'CD', 'IN', 'JJ', ':', 'NNS', 'RB', 'VBN', 'IN', 'DT', 'NN', 'VBD', 'CD', ',', 'CC', 'PRP', 'VBP', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'WDT', 'VBP', 'PRP$', 'JJ', 'NN', '.']",31
natural_language_inference,82,80,"As shown in , Max - pooling described in Section 2.2 drastically improves performance , showing the effect of accumulating information on entities .","['As', 'shown', 'in', ',', 'Max', '-', 'pooling', 'described', 'in', 'Section', '2.2', 'drastically', 'improves', 'performance', ',', 'showing', 'the', 'effect', 'of', 'accumulating', 'information', 'on', 'entities', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBN', 'IN', ',', 'NNP', ':', 'VBG', 'VBN', 'IN', 'NN', 'CD', 'RB', 'VBZ', 'NN', ',', 'VBG', 'DT', 'NN', 'IN', 'VBG', 'NN', 'IN', 'NNS', '.']",24
natural_language_inference,82,100,"Further , we note that initializing our model with pre-trained word vectors 10 is helpful , though world knowledge of entities has been prevented by the anonymization process .","['Further', ',', 'we', 'note', 'that', 'initializing', 'our', 'model', 'with', 'pre-trained', 'word', 'vectors', '10', 'is', 'helpful', ',', 'though', 'world', 'knowledge', 'of', 'entities', 'has', 'been', 'prevented', 'by', 'the', 'anonymization', 'process', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'IN', 'VBG', 'PRP$', 'NN', 'IN', 'JJ', 'NN', 'NNS', 'CD', 'VBZ', 'JJ', ',', 'IN', 'NN', 'NN', 'IN', 'NNS', 'VBZ', 'VBN', 'VBN', 'IN', 'DT', 'NN', 'NN', '.']",29
natural_language_inference,82,102,"Finally , we note that our model , full DER Network , shows the best results compared to several previous reader models , endorsing our approach as promising .","['Finally', ',', 'we', 'note', 'that', 'our', 'model', ',', 'full', 'DER', 'Network', ',', 'shows', 'the', 'best', 'results', 'compared', 'to', 'several', 'previous', 'reader', 'models', ',', 'endorsing', 'our', 'approach', 'as', 'promising', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'IN', 'PRP$', 'NN', ',', 'JJ', 'NNP', 'NNP', ',', 'VBZ', 'DT', 'JJS', 'NNS', 'VBN', 'TO', 'JJ', 'JJ', 'NN', 'NNS', ',', 'VBG', 'PRP$', 'NN', 'IN', 'NN', '.']",29
natural_language_inference,82,103,"The 99 % confidence intervals of the results of full DER Network and the one initialized by word2vec on the test set were [ 0.700 , 0.740 ] and [ 0.708 , 0.749 ] , respectively ( measured by bootstrap tests ) .","['The', '99', '%', 'confidence', 'intervals', 'of', 'the', 'results', 'of', 'full', 'DER', 'Network', 'and', 'the', 'one', 'initialized', 'by', 'word2vec', 'on', 'the', 'test', 'set', 'were', '[', '0.700', ',', '0.740', ']', 'and', '[', '0.708', ',', '0.749', ']', ',', 'respectively', '(', 'measured', 'by', 'bootstrap', 'tests', ')', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'CD', 'NN', 'NN', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'JJ', 'NNP', 'NNP', 'CC', 'DT', 'CD', 'VBN', 'IN', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBD', 'JJ', 'CD', ',', 'CD', 'NN', 'CC', '$', 'CD', ',', 'CD', 'NN', ',', 'RB', '(', 'VBN', 'IN', 'NN', 'NNS', ')', '.']",43
natural_language_inference,92,2,A Discrete Hard EM Approach for Weakly Supervised Question Answering,"['A', 'Discrete', 'Hard', 'EM', 'Approach', 'for', 'Weakly', 'Supervised', 'Question', 'Answering']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']","['DT', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'VBD', 'NNP', 'NNP']",10
natural_language_inference,92,4,Many question answering ( QA ) tasks only provide weak supervision for how the answer should be computed .,"['Many', 'question', 'answering', '(', 'QA', ')', 'tasks', 'only', 'provide', 'weak', 'supervision', 'for', 'how', 'the', 'answer', 'should', 'be', 'computed', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'VBG', '(', 'NNP', ')', 'VBZ', 'RB', 'JJ', 'JJ', 'NN', 'IN', 'WRB', 'DT', 'NN', 'MD', 'VB', 'VBN', '.']",19
natural_language_inference,92,8,"Despite its simplicity , we show that this approach significantly outperforms previous methods on six QA tasks , including absolute gains of 2 - 10 % , and achieves the stateof - the - art on five of them .","['Despite', 'its', 'simplicity', ',', 'we', 'show', 'that', 'this', 'approach', 'significantly', 'outperforms', 'previous', 'methods', 'on', 'six', 'QA', 'tasks', ',', 'including', 'absolute', 'gains', 'of', '2', '-', '10', '%', ',', 'and', 'achieves', 'the', 'stateof', '-', 'the', '-', 'art', 'on', 'five', 'of', 'them', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PRP$', 'NN', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'RB', 'VBZ', 'JJ', 'NNS', 'IN', 'CD', 'NNP', 'NNS', ',', 'VBG', 'JJ', 'NNS', 'IN', 'CD', ':', 'CD', 'NN', ',', 'CC', 'VBZ', 'DT', 'NN', ':', 'DT', ':', 'NN', 'IN', 'CD', 'IN', 'PRP', '.']",40
natural_language_inference,92,20,"In this paper , we show it is possible to formulate a wide range of weakly supervised QA tasks as discrete latent - variable learning problems .","['In', 'this', 'paper', ',', 'we', 'show', 'it', 'is', 'possible', 'to', 'formulate', 'a', 'wide', 'range', 'of', 'weakly', 'supervised', 'QA', 'tasks', 'as', 'discrete', 'latent', '-', 'variable', 'learning', 'problems', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'PRP', 'VBZ', 'JJ', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'VBN', 'NNP', 'NNS', 'IN', 'JJ', 'NN', ':', 'JJ', 'NN', 'NNS', '.']",27
natural_language_inference,92,25,"We demonstrate that for many recently introduced tasks , which we group into three categories as given in , it is relatively easy to precompute a discrete , task - specific set of possible solutions that contains the correct solution along with a modest number of spurious options .","['We', 'demonstrate', 'that', 'for', 'many', 'recently', 'introduced', 'tasks', ',', 'which', 'we', 'group', 'into', 'three', 'categories', 'as', 'given', 'in', ',', 'it', 'is', 'relatively', 'easy', 'to', 'precompute', 'a', 'discrete', ',', 'task', '-', 'specific', 'set', 'of', 'possible', 'solutions', 'that', 'contains', 'the', 'correct', 'solution', 'along', 'with', 'a', 'modest', 'number', 'of', 'spurious', 'options', '.']","['O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'IN', 'JJ', 'RB', 'VBD', 'NNS', ',', 'WDT', 'PRP', 'NN', 'IN', 'CD', 'NNS', 'IN', 'VBN', 'IN', ',', 'PRP', 'VBZ', 'RB', 'JJ', 'TO', 'VB', 'DT', 'NN', ',', 'NN', ':', 'JJ', 'NN', 'IN', 'JJ', 'NNS', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', '.']",49
natural_language_inference,92,26,"The learning challenge is then to determine which solution in the set is the correct one , while estimating a complete QA model .","['The', 'learning', 'challenge', 'is', 'then', 'to', 'determine', 'which', 'solution', 'in', 'the', 'set', 'is', 'the', 'correct', 'one', ',', 'while', 'estimating', 'a', 'complete', 'QA', 'model', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'RB', 'TO', 'VB', 'WDT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'CD', ',', 'IN', 'VBG', 'DT', 'JJ', 'NNP', 'NN', '.']",24
natural_language_inference,92,27,"We model the set of possible solutions as a discrete latent variable , and develop a learning strategy that uses hard - EM - style parameter updates .","['We', 'model', 'the', 'set', 'of', 'possible', 'solutions', 'as', 'a', 'discrete', 'latent', 'variable', ',', 'and', 'develop', 'a', 'learning', 'strategy', 'that', 'uses', 'hard', '-', 'EM', '-', 'style', 'parameter', 'updates', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'CC', 'VB', 'DT', 'NN', 'NN', 'WDT', 'VBZ', 'JJ', ':', 'SYM', ':', 'NN', 'NN', 'NNS', '.']",28
natural_language_inference,92,28,"This algorithm repeatedly ( i ) predicts the most likely solution according to the current model from the precomputed set , and ( ii ) updates the model parameters to further encourage its own prediction .","['This', 'algorithm', 'repeatedly', '(', 'i', ')', 'predicts', 'the', 'most', 'likely', 'solution', 'according', 'to', 'the', 'current', 'model', 'from', 'the', 'precomputed', 'set', ',', 'and', '(', 'ii', ')', 'updates', 'the', 'model', 'parameters', 'to', 'further', 'encourage', 'its', 'own', 'prediction', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'VBZ', 'RB', '(', 'NN', ')', 'VBZ', 'DT', 'RBS', 'JJ', 'NN', 'VBG', 'TO', 'DT', 'JJ', 'NN', 'IN', 'DT', 'VBN', 'NN', ',', 'CC', '(', 'NN', ')', 'VBZ', 'DT', 'NN', 'NNS', 'TO', 'JJ', 'VB', 'PRP$', 'JJ', 'NN', '.']",36
natural_language_inference,92,29,"Intuitively , these hard updates more strongly enforce our prior beliefs that there is a single correct solution .","['Intuitively', ',', 'these', 'hard', 'updates', 'more', 'strongly', 'enforce', 'our', 'prior', 'beliefs', 'that', 'there', 'is', 'a', 'single', 'correct', 'solution', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'JJ', 'NNS', 'RBR', 'RB', 'VB', 'PRP$', 'JJ', 'NN', 'IN', 'EX', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', '.']",19
natural_language_inference,92,155,Multi-mention Reading Comprehension,"['Multi-mention', 'Reading', 'Comprehension']","['B-n', 'I-n', 'I-n']","['NN', 'NNP', 'NNP']",3
natural_language_inference,92,164,We use uncased version of BERT base .,"['We', 'use', 'uncased', 'version', 'of', 'BERT', 'base', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'NNP', 'NN', '.']",8
natural_language_inference,92,166,We use batch size of 20 for two reading comprehension tasks and 192 for two open - domain QA tasks .,"['We', 'use', 'batch', 'size', 'of', '20', 'for', 'two', 'reading', 'comprehension', 'tasks', 'and', '192', 'for', 'two', 'open', '-', 'domain', 'QA', 'tasks', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'CD', 'IN', 'CD', 'VBG', 'NN', 'NNS', 'CC', 'CD', 'IN', 'CD', 'JJ', ':', 'NN', 'NNP', 'NNS', '.']",21
natural_language_inference,92,168,"For opendomain QA tasks , we retrieve 50 Wikipedia articles through TF - IDF ( Chen et al. , 2017 ) and further run to retrieve 20 ( for train ) or 80 ( for development and test ) paragraphs .","['For', 'opendomain', 'QA', 'tasks', ',', 'we', 'retrieve', '50', 'Wikipedia', 'articles', 'through', 'TF', '-', 'IDF', '(', 'Chen', 'et', 'al.', ',', '2017', ')', 'and', 'further', 'run', 'to', 'retrieve', '20', '(', 'for', 'train', ')', 'or', '80', '(', 'for', 'development', 'and', 'test', ')', 'paragraphs', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', 'NNP', 'NNS', ',', 'PRP', 'VBP', 'CD', 'NNP', 'NNS', 'IN', 'NNP', ':', 'NNP', '(', 'NNP', 'VBZ', 'RB', ',', 'CD', ')', 'CC', 'JJ', 'NN', 'TO', 'VB', 'CD', '(', 'IN', 'NN', ')', 'CC', 'CD', '(', 'IN', 'NN', 'CC', 'NN', ')', 'NN', '.']",41
natural_language_inference,92,169,"We try 10 , 20 , 40 and 80 paragraphs on the development set to choose the number of paragraphs to use on the test set .","['We', 'try', '10', ',', '20', ',', '40', 'and', '80', 'paragraphs', 'on', 'the', 'development', 'set', 'to', 'choose', 'the', 'number', 'of', 'paragraphs', 'to', 'use', 'on', 'the', 'test', 'set', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'CD', ',', 'CD', ',', 'CD', 'CC', 'CD', 'NN', 'IN', 'DT', 'NN', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', 'TO', 'VB', 'IN', 'DT', 'NN', 'NN', '.']",27
natural_language_inference,92,170,"To avoid local optima , we perform annealing : at training step t , the model optimizes on MML objective with a probability of min ( t / ? , 1 ) and otherwise use our objective , where ?","['To', 'avoid', 'local', 'optima', ',', 'we', 'perform', 'annealing', ':', 'at', 'training', 'step', 't', ',', 'the', 'model', 'optimizes', 'on', 'MML', 'objective', 'with', 'a', 'probability', 'of', 'min', '(', 't', '/', '?', ',', '1', ')', 'and', 'otherwise', 'use', 'our', 'objective', ',', 'where', '?']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['TO', 'VB', 'JJ', 'NN', ',', 'PRP', 'VBP', 'VBG', ':', 'IN', 'VBG', 'NN', 'RB', ',', 'DT', 'NN', 'VBZ', 'IN', 'NNP', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', '(', 'JJ', 'NN', '.', ',', 'CD', ')', 'CC', 'RB', 'VB', 'PRP$', 'NN', ',', 'WRB', '.']",40
natural_language_inference,92,176,"6 First of all , we observe that First - Only is a strong baseline across all the datasets .","['6', 'First', 'of', 'all', ',', 'we', 'observe', 'that', 'First', '-', 'Only', 'is', 'a', 'strong', 'baseline', 'across', 'all', 'the', 'datasets', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['CD', 'NNP', 'IN', 'RB', ',', 'PRP', 'VBP', 'IN', 'NNP', ':', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'PDT', 'DT', 'NNS', '.']",20
natural_language_inference,92,178,"Second , while MML achieves comparable result to the First - Only baseline , our learning method outperforms others by 2 + F1 / ROUGE - L / EM consistently on all datasets .","['Second', ',', 'while', 'MML', 'achieves', 'comparable', 'result', 'to', 'the', 'First', '-', 'Only', 'baseline', ',', 'our', 'learning', 'method', 'outperforms', 'others', 'by', '2', '+', 'F1', '/', 'ROUGE', '-', 'L', '/', 'EM', 'consistently', 'on', 'all', 'datasets', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['JJ', ',', 'IN', 'NNP', 'VBZ', 'JJ', 'NN', 'TO', 'DT', 'NNP', ':', 'RB', 'NN', ',', 'PRP$', 'VBG', 'NN', 'NNS', 'NNS', 'IN', 'CD', 'NNS', 'NNP', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'RB', 'IN', 'DT', 'NNS', '.']",34
natural_language_inference,92,179,"Lastly , our method achieves the new state - of the - art on NARRATIVEQA , TRIVIAQA - OPEN and NATURALQUESTIONS - OPEN , and is comparable to the state - of - the - art on TRIVIAQA , despite our aggressive truncation of documents .","['Lastly', ',', 'our', 'method', 'achieves', 'the', 'new', 'state', '-', 'of', 'the', '-', 'art', 'on', 'NARRATIVEQA', ',', 'TRIVIAQA', '-', 'OPEN', 'and', 'NATURALQUESTIONS', '-', 'OPEN', ',', 'and', 'is', 'comparable', 'to', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'on', 'TRIVIAQA', ',', 'despite', 'our', 'aggressive', 'truncation', 'of', 'documents', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP$', 'NN', 'VBZ', 'DT', 'JJ', 'NN', ':', 'IN', 'DT', ':', 'NN', 'IN', 'NNP', ',', 'NNP', ':', 'NNP', 'CC', 'NNP', ':', 'NNP', ',', 'CC', 'VBZ', 'JJ', 'TO', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'NNP', ',', 'IN', 'PRP$', 'JJ', 'NN', 'IN', 'NNS', '.']",46
natural_language_inference,64,2,TANDA : Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection,"['TANDA', ':', 'Transfer', 'and', 'Adapt', 'Pre-Trained', 'Transformer', 'Models', 'for', 'Answer', 'Sentence', 'Selection']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NN', ':', 'NN', 'CC', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",12
natural_language_inference,64,7,"We demonstrate the benefits of our approach for answer sentence selection , which is a well - known inference task in Question Answering .","['We', 'demonstrate', 'the', 'benefits', 'of', 'our', 'approach', 'for', 'answer', 'sentence', 'selection', ',', 'which', 'is', 'a', 'well', '-', 'known', 'inference', 'task', 'in', 'Question', 'Answering', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNS', 'IN', 'PRP$', 'NN', 'IN', 'JJR', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'RB', ':', 'VBN', 'NN', 'NN', 'IN', 'NNP', 'NNP', '.']",24
natural_language_inference,64,17,"This has renewed the research interest in Question Answering ( QA ) and , in particular , in two main tasks :","['This', 'has', 'renewed', 'the', 'research', 'interest', 'in', 'Question', 'Answering', '(', 'QA', ')', 'and', ',', 'in', 'particular', ',', 'in', 'two', 'main', 'tasks', ':']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'VBZ', 'VBN', 'DT', 'NN', 'NN', 'IN', 'NNP', 'NNP', '(', 'NNP', ')', 'CC', ',', 'IN', 'JJ', ',', 'IN', 'CD', 'JJ', 'NNS', ':']",22
natural_language_inference,64,18,"( i ) answer sentence selection ( AS2 ) , which , given a question and a set of answer sentence candidates , consists in selecting sentences ( e.g. , retrieved by a search engine ) correctly answering the question ; and ( ii ) machine reading ( MR ) or reading comprehension , which , given a question and a reference text , consists in finding a text span answering it .","['(', 'i', ')', 'answer', 'sentence', 'selection', '(', 'AS2', ')', ',', 'which', ',', 'given', 'a', 'question', 'and', 'a', 'set', 'of', 'answer', 'sentence', 'candidates', ',', 'consists', 'in', 'selecting', 'sentences', '(', 'e.g.', ',', 'retrieved', 'by', 'a', 'search', 'engine', ')', 'correctly', 'answering', 'the', 'question', ';', 'and', '(', 'ii', ')', 'machine', 'reading', '(', 'MR', ')', 'or', 'reading', 'comprehension', ',', 'which', ',', 'given', 'a', 'question', 'and', 'a', 'reference', 'text', ',', 'consists', 'in', 'finding', 'a', 'text', 'span', 'answering', 'it', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'NN', ')', 'NN', 'NN', 'NN', '(', 'NNP', ')', ',', 'WDT', ',', 'VBN', 'DT', 'NN', 'CC', 'DT', 'NN', 'IN', 'JJR', 'NN', 'NNS', ',', 'VBZ', 'IN', 'VBG', 'NNS', '(', 'NN', ',', 'VBN', 'IN', 'DT', 'NN', 'NN', ')', 'RB', 'VBG', 'DT', 'NN', ':', 'CC', '(', 'NN', ')', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'VBG', 'NN', ',', 'WDT', ',', 'VBN', 'DT', 'NN', 'CC', 'DT', 'NN', 'NN', ',', 'VBZ', 'IN', 'VBG', 'DT', 'NN', 'NN', 'VBG', 'PRP', '.']",73
natural_language_inference,64,19,"Even though the latter is gaining more and more popularity , AS2 is more relevant to a production scenario since , a combination of a search engine and an AS2 model already implements an initial QA system .","['Even', 'though', 'the', 'latter', 'is', 'gaining', 'more', 'and', 'more', 'popularity', ',', 'AS2', 'is', 'more', 'relevant', 'to', 'a', 'production', 'scenario', 'since', ',', 'a', 'combination', 'of', 'a', 'search', 'engine', 'and', 'an', 'AS2', 'model', 'already', 'implements', 'an', 'initial', 'QA', 'system', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O']","['RB', 'IN', 'DT', 'NN', 'VBZ', 'VBG', 'JJR', 'CC', 'JJR', 'NN', ',', 'NNP', 'VBZ', 'RBR', 'JJ', 'TO', 'DT', 'NN', 'NN', 'IN', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'DT', 'NNP', 'NN', 'RB', 'VBZ', 'DT', 'JJ', 'NNP', 'NN', '.']",38
natural_language_inference,64,31,"In this paper , we study the use of Transformer - based models for AS2 and provide effective solutions to tackle the data scarceness problem for AS2 and the instability of the finetuning step .","['In', 'this', 'paper', ',', 'we', 'study', 'the', 'use', 'of', 'Transformer', '-', 'based', 'models', 'for', 'AS2', 'and', 'provide', 'effective', 'solutions', 'to', 'tackle', 'the', 'data', 'scarceness', 'problem', 'for', 'AS2', 'and', 'the', 'instability', 'of', 'the', 'finetuning', 'step', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NNP', ':', 'VBN', 'NNS', 'IN', 'NNP', 'CC', 'VB', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'NNS', 'NN', 'NN', 'IN', 'NNP', 'CC', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",35
natural_language_inference,64,33,"We improve stability of Transformer models by adding an intermediate fine - tuning step , which aims at specializing them to the target task ( AS2 ) , i.e. , this step transfers a pretrained language model to a model for the target task .","['We', 'improve', 'stability', 'of', 'Transformer', 'models', 'by', 'adding', 'an', 'intermediate', 'fine', '-', 'tuning', 'step', ',', 'which', 'aims', 'at', 'specializing', 'them', 'to', 'the', 'target', 'task', '(', 'AS2', ')', ',', 'i.e.', ',', 'this', 'step', 'transfers', 'a', 'pretrained', 'language', 'model', 'to', 'a', 'model', 'for', 'the', 'target', 'task', '.']","['O', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'NN', 'IN', 'NNP', 'NNS', 'IN', 'VBG', 'DT', 'JJ', 'JJ', ':', 'VBG', 'NN', ',', 'WDT', 'VBZ', 'IN', 'VBG', 'PRP', 'TO', 'DT', 'NN', 'NN', '(', 'NNP', ')', ',', 'FW', ',', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'TO', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",45
natural_language_inference,64,34,"We show that the transferred model can be effectively adapted to the target domain with a subsequent finetuning step , even when using target data of small size .","['We', 'show', 'that', 'the', 'transferred', 'model', 'can', 'be', 'effectively', 'adapted', 'to', 'the', 'target', 'domain', 'with', 'a', 'subsequent', 'finetuning', 'step', ',', 'even', 'when', 'using', 'target', 'data', 'of', 'small', 'size', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'JJ', 'NN', 'MD', 'VB', 'RB', 'VBN', 'TO', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NN', ',', 'RB', 'WRB', 'VBG', 'NN', 'NNS', 'IN', 'JJ', 'NN', '.']",29
natural_language_inference,64,36,"We built ASNQ , a dataset for AS2 , by transforming the recently released Natural Questions ( NQ ) corpus ) from MR to AS2 task .","['We', 'built', 'ASNQ', ',', 'a', 'dataset', 'for', 'AS2', ',', 'by', 'transforming', 'the', 'recently', 'released', 'Natural', 'Questions', '(', 'NQ', ')', 'corpus', ')', 'from', 'MR', 'to', 'AS2', 'task', '.']","['O', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', ',', 'DT', 'NN', 'IN', 'NNP', ',', 'IN', 'VBG', 'DT', 'RB', 'VBN', 'JJ', 'NNP', '(', 'NNP', ')', 'NN', ')', 'IN', 'NNP', 'TO', 'NNP', 'NN', '.']",27
natural_language_inference,64,151,We adopt Adam optimizer ( Kingma and Ba 2014 ) with a learning rate of 2e - 5 for the transfer step on the ASNQ dataset and a learning rate of 1e - 6 for the adapt step on the target dataset .,"['We', 'adopt', 'Adam', 'optimizer', '(', 'Kingma', 'and', 'Ba', '2014', ')', 'with', 'a', 'learning', 'rate', 'of', '2e', '-', '5', 'for', 'the', 'transfer', 'step', 'on', 'the', 'ASNQ', 'dataset', 'and', 'a', 'learning', 'rate', 'of', '1e', '-', '6', 'for', 'the', 'adapt', 'step', 'on', 'the', 'target', 'dataset', '.']","['O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', 'NN', '(', 'NNP', 'CC', 'NNP', 'CD', ')', 'IN', 'DT', 'VBG', 'NN', 'IN', 'CD', ':', 'CD', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', 'CC', 'DT', 'JJ', 'NN', 'IN', 'CD', ':', 'CD', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",43
natural_language_inference,64,152,We apply early stopping on the dev. set of the target corpus for both steps based on the highest MAP score .,"['We', 'apply', 'early', 'stopping', 'on', 'the', 'dev.', 'set', 'of', 'the', 'target', 'corpus', 'for', 'both', 'steps', 'based', 'on', 'the', 'highest', 'MAP', 'score', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NNS', 'VBN', 'IN', 'DT', 'JJS', 'NNP', 'NN', '.']",22
natural_language_inference,64,153,"We set the max number of epochs equal to 3 and 9 for adapt and transfer steps , respectively .","['We', 'set', 'the', 'max', 'number', 'of', 'epochs', 'equal', 'to', '3', 'and', '9', 'for', 'adapt', 'and', 'transfer', 'steps', ',', 'respectively', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'JJ', 'TO', 'CD', 'CC', 'CD', 'IN', 'NN', 'CC', 'VB', 'NNS', ',', 'RB', '.']",20
natural_language_inference,64,154,We set the maximum sequence length for BERT / RoBERTa to 128 tokens .,"['We', 'set', 'the', 'maximum', 'sequence', 'length', 'for', 'BERT', '/', 'RoBERTa', 'to', '128', 'tokens', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'TO', 'CD', 'NNS', '.']",14
natural_language_inference,64,166,"TANDA provides a large improvement over the state of the art , which has been regularly contributed to by hundreds of researchers .","['TANDA', 'provides', 'a', 'large', 'improvement', 'over', 'the', 'state', 'of', 'the', 'art', ',', 'which', 'has', 'been', 'regularly', 'contributed', 'to', 'by', 'hundreds', 'of', 'researchers', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'WDT', 'VBZ', 'VBN', 'RB', 'VBN', 'TO', 'IN', 'NNS', 'IN', 'NNS', '.']",23
natural_language_inference,64,167,RoBERTa- Large TANDA using ASNQ ?,"['RoBERTa-', 'Large', 'TANDA', 'using', 'ASNQ', '?']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['JJ', 'NNP', 'NNP', 'VBG', 'NNP', '.']",6
natural_language_inference,64,168,"Wiki QA establish an impressive new state of the art for AS2 on WikiQA of 0.920 and 0.933 in MAP and MRR , respectively .","['Wiki', 'QA', 'establish', 'an', 'impressive', 'new', 'state', 'of', 'the', 'art', 'for', 'AS2', 'on', 'WikiQA', 'of', '0.920', 'and', '0.933', 'in', 'MAP', 'and', 'MRR', ',', 'respectively', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['NNP', 'NNP', 'VB', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', 'IN', 'NNP', 'IN', 'CD', 'CC', 'CD', 'IN', 'NNP', 'CC', 'NNP', ',', 'RB', '.']",25
natural_language_inference,64,174,RoBERTa - Large TANDA with ASNQ ?,"['RoBERTa', '-', 'Large', 'TANDA', 'with', 'ASNQ', '?']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['NNP', ':', 'JJ', 'NNP', 'IN', 'NNP', '.']",7
natural_language_inference,64,175,"TREC - QA again establishes an impressive performance of 0.943 in MAP and 0.974 in MRR , outperforming the previous state of the art by .","['TREC', '-', 'QA', 'again', 'establishes', 'an', 'impressive', 'performance', 'of', '0.943', 'in', 'MAP', 'and', '0.974', 'in', 'MRR', ',', 'outperforming', 'the', 'previous', 'state', 'of', 'the', 'art', 'by', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['NNP', ':', 'NN', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'CD', 'IN', 'NNP', 'CC', 'CD', 'IN', 'NNP', ',', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', '.']",26
natural_language_inference,64,177,"TANDA improves all the models : BERT - Base , RoBERTa- Base , BERT - Large and RoBERTa - Large , outperforming the previous state of the art with all of them .","['TANDA', 'improves', 'all', 'the', 'models', ':', 'BERT', '-', 'Base', ',', 'RoBERTa-', 'Base', ',', 'BERT', '-', 'Large', 'and', 'RoBERTa', '-', 'Large', ',', 'outperforming', 'the', 'previous', 'state', 'of', 'the', 'art', 'with', 'all', 'of', 'them', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'PDT', 'DT', 'NNS', ':', 'NNP', ':', 'NN', ',', 'NNP', 'NNP', ',', 'NNP', ':', 'NN', 'CC', 'NNP', ':', 'NN', ',', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'IN', 'PRP', '.']",33
natural_language_inference,97,2,A Parallel - Hierarchical Model for Machine Comprehension on Sparse Data,"['A', 'Parallel', '-', 'Hierarchical', 'Model', 'for', 'Machine', 'Comprehension', 'on', 'Sparse', 'Data']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['DT', 'NNP', ':', 'JJ', 'NN', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",11
natural_language_inference,97,4,Understanding unstructured text is a major goal within natural language processing .,"['Understanding', 'unstructured', 'text', 'is', 'a', 'major', 'goal', 'within', 'natural', 'language', 'processing', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', '.']",12
natural_language_inference,97,16,Machine comprehension ( MC ) is evaluated by posing a set of questions based on a text passage ( akin to the reading tests we all took in school ) .,"['Machine', 'comprehension', '(', 'MC', ')', 'is', 'evaluated', 'by', 'posing', 'a', 'set', 'of', 'questions', 'based', 'on', 'a', 'text', 'passage', '(', 'akin', 'to', 'the', 'reading', 'tests', 'we', 'all', 'took', 'in', 'school', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', '(', 'NNP', ')', 'VBZ', 'VBN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'JJ', 'NN', '(', 'JJ', 'TO', 'DT', 'NN', 'VBZ', 'PRP', 'DT', 'VBD', 'IN', 'NN', ')', '.']",31
natural_language_inference,97,21,We present a parallel - hierarchical approach to machine comprehension designed to work well in a data - limited setting .,"['We', 'present', 'a', 'parallel', '-', 'hierarchical', 'approach', 'to', 'machine', 'comprehension', 'designed', 'to', 'work', 'well', 'in', 'a', 'data', '-', 'limited', 'setting', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', ':', 'JJ', 'NN', 'TO', 'NN', 'NN', 'VBN', 'TO', 'VB', 'RB', 'IN', 'DT', 'NN', ':', 'JJ', 'NN', '.']",21
natural_language_inference,97,26,Our model learns to comprehend at a high level even when data is sparse .,"['Our', 'model', 'learns', 'to', 'comprehend', 'at', 'a', 'high', 'level', 'even', 'when', 'data', 'is', 'sparse', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O']","['PRP$', 'NN', 'VBZ', 'TO', 'VB', 'IN', 'DT', 'JJ', 'NN', 'RB', 'WRB', 'NN', 'VBZ', 'JJ', '.']",15
natural_language_inference,97,27,The key to our model is that it compares the question and answer candidates to the text using several distinct perspectives .,"['The', 'key', 'to', 'our', 'model', 'is', 'that', 'it', 'compares', 'the', 'question', 'and', 'answer', 'candidates', 'to', 'the', 'text', 'using', 'several', 'distinct', 'perspectives', '.']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'TO', 'PRP$', 'NN', 'VBZ', 'IN', 'PRP', 'VBZ', 'DT', 'NN', 'CC', 'NN', 'NNS', 'TO', 'DT', 'NN', 'VBG', 'JJ', 'JJ', 'NNS', '.']",22
natural_language_inference,97,29,"The semantic perspective compares the hypothesis to sentences in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .","['The', 'semantic', 'perspective', 'compares', 'the', 'hypothesis', 'to', 'sentences', 'in', 'the', 'text', 'viewed', 'as', 'single', ',', 'self', '-', 'contained', 'thoughts', ';', 'these', 'are', 'represented', 'using', 'a', 'sum', 'and', 'transformation', 'of', 'word', 'embedding', 'vectors', ',', 'similarly', 'to', 'in', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'NN', 'TO', 'NNS', 'IN', 'DT', 'NN', 'VBD', 'IN', 'JJ', ',', 'PRP', ':', 'VBN', 'NNS', ':', 'DT', 'VBP', 'VBN', 'VBG', 'DT', 'NN', 'CC', 'NN', 'IN', 'NN', 'VBG', 'NNS', ',', 'RB', 'TO', 'IN', '.']",37
natural_language_inference,97,30,"The word - by - word perspective focuses on similarity matches between individual words from hypothesis and text , at various scales .","['The', 'word', '-', 'by', '-', 'word', 'perspective', 'focuses', 'on', 'similarity', 'matches', 'between', 'individual', 'words', 'from', 'hypothesis', 'and', 'text', ',', 'at', 'various', 'scales', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', ':', 'IN', ':', 'NN', 'NN', 'VBZ', 'IN', 'NN', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'NN', 'CC', 'NN', ',', 'IN', 'JJ', 'NNS', '.']",23
natural_language_inference,97,31,"As in the semantic perspective , we consider matches over complete sentences .","['As', 'in', 'the', 'semantic', 'perspective', ',', 'we', 'consider', 'matches', 'over', 'complete', 'sentences', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'IN', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'NNS', 'IN', 'JJ', 'NNS', '.']",13
natural_language_inference,97,32,"We also use a sliding window acting on a subsentential scale ( inspired by the work of ) , which implicitly considers the linear distance between matched words .","['We', 'also', 'use', 'a', 'sliding', 'window', 'acting', 'on', 'a', 'subsentential', 'scale', '(', 'inspired', 'by', 'the', 'work', 'of', ')', ',', 'which', 'implicitly', 'considers', 'the', 'linear', 'distance', 'between', 'matched', 'words', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', 'NN', 'VBG', 'IN', 'DT', 'JJ', 'NN', '(', 'VBN', 'IN', 'DT', 'NN', 'IN', ')', ',', 'WDT', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'VBN', 'NNS', '.']",29
natural_language_inference,97,218,"For word vectors we use Google 's publicly available embeddings , trained with word2vec on the 100 - billion - word News corpus .","['For', 'word', 'vectors', 'we', 'use', 'Google', ""'s"", 'publicly', 'available', 'embeddings', ',', 'trained', 'with', 'word2vec', 'on', 'the', '100', '-', 'billion', '-', 'word', 'News', 'corpus', '.']","['B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'NNS', 'PRP', 'VBP', 'NNP', 'POS', 'RB', 'JJ', 'NNS', ',', 'VBN', 'IN', 'NN', 'IN', 'DT', 'CD', ':', 'CD', ':', 'NN', 'NNP', 'NN', '.']",24
natural_language_inference,97,219,"These vectors are kept fixed throughout training , since we found that training them was not helpful ( likely because of MCTest 's size ) .","['These', 'vectors', 'are', 'kept', 'fixed', 'throughout', 'training', ',', 'since', 'we', 'found', 'that', 'training', 'them', 'was', 'not', 'helpful', '(', 'likely', 'because', 'of', 'MCTest', ""'s"", 'size', ')', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'VBN', 'IN', 'NN', ',', 'IN', 'PRP', 'VBD', 'IN', 'VBG', 'PRP', 'VBD', 'RB', 'JJ', '(', 'JJ', 'IN', 'IN', 'NNP', 'POS', 'NN', ')', '.']",26
natural_language_inference,97,220,The vectors are 300 - dimensional ( d = 300 ) .,"['The', 'vectors', 'are', '300', '-', 'dimensional', '(', 'd', '=', '300', ')', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'VBP', 'CD', ':', 'NN', '(', 'JJ', 'NNP', 'CD', ')', '.']",12
natural_language_inference,97,231,"We found dropout to be particularly effective at improving generalization from the training to the test set , and used 0.5 as the dropout probability .","['We', 'found', 'dropout', 'to', 'be', 'particularly', 'effective', 'at', 'improving', 'generalization', 'from', 'the', 'training', 'to', 'the', 'test', 'set', ',', 'and', 'used', '0.5', 'as', 'the', 'dropout', 'probability', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBD', 'NN', 'TO', 'VB', 'RB', 'JJ', 'IN', 'VBG', 'NN', 'IN', 'DT', 'NN', 'TO', 'DT', 'NN', 'NN', ',', 'CC', 'VBD', 'CD', 'IN', 'DT', 'NN', 'NN', '.']",26
natural_language_inference,97,232,"Dropout occurs after all neural - network transformations , if those transformations are allowed to change with training .","['Dropout', 'occurs', 'after', 'all', 'neural', '-', 'network', 'transformations', ',', 'if', 'those', 'transformations', 'are', 'allowed', 'to', 'change', 'with', 'training', '.']","['B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O']","['NNP', 'NNS', 'IN', 'DT', 'JJ', ':', 'NN', 'NNS', ',', 'IN', 'DT', 'NNS', 'VBP', 'VBN', 'TO', 'VB', 'IN', 'NN', '.']",19
natural_language_inference,97,235,"We used the Adam optimizer with the standard settings ( Kingma and Ba , 2014 ) and a learning rate of 0.003 .","['We', 'used', 'the', 'Adam', 'optimizer', 'with', 'the', 'standard', 'settings', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', 'and', 'a', 'learning', 'rate', 'of', '0.003', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBD', 'DT', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NNS', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'CC', 'DT', 'JJ', 'NN', 'IN', 'CD', '.']",23
natural_language_inference,97,247,"On MCTest - 500 , the Parallel Hierarchical model significantly outperforms these methods on single questions ( > 2 % ) and slightly outperforms the latter two on multi questions ( ? 0.3 % ) and over all ( ?","['On', 'MCTest', '-', '500', ',', 'the', 'Parallel', 'Hierarchical', 'model', 'significantly', 'outperforms', 'these', 'methods', 'on', 'single', 'questions', '(', '>', '2', '%', ')', 'and', 'slightly', 'outperforms', 'the', 'latter', 'two', 'on', 'multi', 'questions', '(', '?', '0.3', '%', ')', 'and', 'over', 'all', '(', '?']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ':', 'CD', ',', 'DT', 'NNP', 'NNP', 'NN', 'RB', 'VBZ', 'DT', 'NNS', 'IN', 'JJ', 'NNS', '(', 'JJ', 'CD', 'NN', ')', 'CC', 'RB', 'VBZ', 'DT', 'JJ', 'CD', 'IN', 'NNS', 'NNS', '(', '.', 'CD', 'NN', ')', 'CC', 'IN', 'DT', '(', '.']",40
natural_language_inference,97,249,The method of achieves the best over all result on MCTest - 160 .,"['The', 'method', 'of', 'achieves', 'the', 'best', 'over', 'all', 'result', 'on', 'MCTest', '-', '160', '.']","['O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'NNS', 'DT', 'JJS', 'IN', 'DT', 'NN', 'IN', 'NNP', ':', 'CD', '.']",14
natural_language_inference,97,253,Here we see our model outperforming the alternatives by a large margin across the board ( > 15 % ) .,"['Here', 'we', 'see', 'our', 'model', 'outperforming', 'the', 'alternatives', 'by', 'a', 'large', 'margin', 'across', 'the', 'board', '(', '>', '15', '%', ')', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', 'PRP', 'VBP', 'PRP$', 'NN', 'VBG', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '(', 'VB', 'CD', 'NN', ')', '.']",21
natural_language_inference,97,260,"Not surprisingly , the n-gram functionality is important , contributing almost 5 % accuracy improvement .","['Not', 'surprisingly', ',', 'the', 'n-gram', 'functionality', 'is', 'important', ',', 'contributing', 'almost', '5', '%', 'accuracy', 'improvement', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', 'RB', ',', 'DT', 'JJ', 'NN', 'VBZ', 'JJ', ',', 'VBG', 'RB', 'CD', 'NN', 'NN', 'NN', '.']",16
natural_language_inference,97,263,"The top N function contributes very little to the over all performance , suggesting that most multi questions have their evidence distributed across contiguous sentences .","['The', 'top', 'N', 'function', 'contributes', 'very', 'little', 'to', 'the', 'over', 'all', 'performance', ',', 'suggesting', 'that', 'most', 'multi', 'questions', 'have', 'their', 'evidence', 'distributed', 'across', 'contiguous', 'sentences', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NNP', 'NN', 'VBZ', 'RB', 'JJ', 'TO', 'DT', 'IN', 'DT', 'NN', ',', 'VBG', 'IN', 'JJS', 'JJ', 'NNS', 'VBP', 'PRP$', 'NN', 'VBN', 'IN', 'JJ', 'NNS', '.']",26
natural_language_inference,97,264,"Ablating the sentential component made the most significant difference , reducing performance by more than 5 % .","['Ablating', 'the', 'sentential', 'component', 'made', 'the', 'most', 'significant', 'difference', ',', 'reducing', 'performance', 'by', 'more', 'than', '5', '%', '.']","['B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'DT', 'JJ', 'NN', 'VBD', 'DT', 'RBS', 'JJ', 'NN', ',', 'VBG', 'NN', 'IN', 'JJR', 'IN', 'CD', 'NN', '.']",18
natural_language_inference,97,265,Simple word - by - word matching is obviously useful on MCTest .,"['Simple', 'word', '-', 'by', '-', 'word', 'matching', 'is', 'obviously', 'useful', 'on', 'MCTest', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O']","['NN', 'NN', ':', 'IN', ':', 'NN', 'NN', 'VBZ', 'RB', 'JJ', 'IN', 'NNP', '.']",13
natural_language_inference,97,266,"The sequential sliding window makes a 3 % contribution , highlighting the importance of word - distance measures .","['The', 'sequential', 'sliding', 'window', 'makes', 'a', '3', '%', 'contribution', ',', 'highlighting', 'the', 'importance', 'of', 'word', '-', 'distance', 'measures', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'DT', 'CD', 'NN', 'NN', ',', 'VBG', 'DT', 'NN', 'IN', 'NN', ':', 'NN', 'NNS', '.']",19
natural_language_inference,97,267,"On the other hand , the dependency - based sliding window makes only a minor contribution .","['On', 'the', 'other', 'hand', ',', 'the', 'dependency', '-', 'based', 'sliding', 'window', 'makes', 'only', 'a', 'minor', 'contribution', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'DT', 'NN', ':', 'VBN', 'VBG', 'NN', 'VBZ', 'RB', 'DT', 'JJ', 'NN', '.']",17
natural_language_inference,97,270,"Finally , the exogenous word weights make a significant contribution of almost 5 % .","['Finally', ',', 'the', 'exogenous', 'word', 'weights', 'make', 'a', 'significant', 'contribution', 'of', 'almost', '5', '%', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'JJ', 'NN', 'NNS', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'RB', 'CD', 'NN', '.']",15
sarcasm_detection,1,2,CASCADE : Contextual Sarcasm Detection in Online Discussion Forums,"['CASCADE', ':', 'Contextual', 'Sarcasm', 'Detection', 'in', 'Online', 'Discussion', 'Forums']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['NN', ':', 'JJ', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP']",9
sarcasm_detection,1,4,"The literature in automated sarcasm detection has mainly focused on lexical , syntactic and semantic - level analysis of text .","['The', 'literature', 'in', 'automated', 'sarcasm', 'detection', 'has', 'mainly', 'focused', 'on', 'lexical', ',', 'syntactic', 'and', 'semantic', '-', 'level', 'analysis', 'of', 'text', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'JJ', 'NN', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'JJ', ',', 'JJ', 'CC', 'JJ', ':', 'NN', 'NN', 'IN', 'NN', '.']",21
sarcasm_detection,1,6,"In this paper , we propose CASCADE ( a ContextuAl SarCasm DEtector ) that adopts a hybrid approach of both content and context - driven modeling for sarcasm detection in online social media discussions .","['In', 'this', 'paper', ',', 'we', 'propose', 'CASCADE', '(', 'a', 'ContextuAl', 'SarCasm', 'DEtector', ')', 'that', 'adopts', 'a', 'hybrid', 'approach', 'of', 'both', 'content', 'and', 'context', '-', 'driven', 'modeling', 'for', 'sarcasm', 'detection', 'in', 'online', 'social', 'media', 'discussions', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'NNP', '(', 'DT', 'NNP', 'NNP', 'NNP', ')', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'CC', 'JJ', ':', 'NN', 'VBG', 'IN', 'JJ', 'NN', 'IN', 'JJ', 'JJ', 'NNS', 'NNS', '.']",35
sarcasm_detection,1,25,"Particularly , we propose a hybrid network , named CASCADE , that utilizes both content and contextual - information required for sarcasm detection .","['Particularly', ',', 'we', 'propose', 'a', 'hybrid', 'network', ',', 'named', 'CASCADE', ',', 'that', 'utilizes', 'both', 'content', 'and', 'contextual', '-', 'information', 'required', 'for', 'sarcasm', 'detection', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', ',', 'VBN', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NN', 'CC', 'JJ', ':', 'NN', 'VBN', 'IN', 'JJ', 'NN', '.']",24
sarcasm_detection,1,27,"First , it performs user profiling to create user embeddings that capture indicative behavioral traits for sarcasm .","['First', ',', 'it', 'performs', 'user', 'profiling', 'to', 'create', 'user', 'embeddings', 'that', 'capture', 'indicative', 'behavioral', 'traits', 'for', 'sarcasm', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['RB', ',', 'PRP', 'VBZ', 'JJ', 'VBG', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'NN', 'JJ', 'JJ', 'NNS', 'IN', 'NN', '.']",18
sarcasm_detection,1,29,"It makes use of users ' historical posts to model their writing style ( stylometry ) and personality indicators , which are then fused into comprehensive user embeddings using a multi-view fusion approach , Canonical Correlation Analysis ( CCA ) .","['It', 'makes', 'use', 'of', 'users', ""'"", 'historical', 'posts', 'to', 'model', 'their', 'writing', 'style', '(', 'stylometry', ')', 'and', 'personality', 'indicators', ',', 'which', 'are', 'then', 'fused', 'into', 'comprehensive', 'user', 'embeddings', 'using', 'a', 'multi-view', 'fusion', 'approach', ',', 'Canonical', 'Correlation', 'Analysis', '(', 'CCA', ')', '.']","['O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'NN', 'IN', 'NNS', 'POS', 'JJ', 'NNS', 'TO', 'VB', 'PRP$', 'NN', 'NN', '(', 'NN', ')', 'CC', 'NN', 'NNS', ',', 'WDT', 'VBP', 'RB', 'VBN', 'IN', 'JJ', 'NN', 'NNS', 'VBG', 'DT', 'JJ', 'NN', 'NN', ',', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', '.']",41
sarcasm_detection,1,30,"Second , it extracts contextual information from the discourse of comments in the discussion forums .","['Second', ',', 'it', 'extracts', 'contextual', 'information', 'from', 'the', 'discourse', 'of', 'comments', 'in', 'the', 'discussion', 'forums', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['JJ', ',', 'PRP', 'VBZ', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NNS', '.']",16
sarcasm_detection,1,31,This is done by document modeling of these consolidated comments belonging to the same forum .,"['This', 'is', 'done', 'by', 'document', 'modeling', 'of', 'these', 'consolidated', 'comments', 'belonging', 'to', 'the', 'same', 'forum', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'VBZ', 'VBN', 'IN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'VBG', 'TO', 'DT', 'JJ', 'NN', '.']",16
sarcasm_detection,1,33,"After the contextual modeling phase , CASCADE is provided with a comment for sarcasm detection .","['After', 'the', 'contextual', 'modeling', 'phase', ',', 'CASCADE', 'is', 'provided', 'with', 'a', 'comment', 'for', 'sarcasm', 'detection', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'NN', ',', 'NNP', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NN', '.']",16
sarcasm_detection,1,34,It performs content - modeling using a Convolutional Neural Network ( CNN ) to extract its syntactic features .,"['It', 'performs', 'content', '-', 'modeling', 'using', 'a', 'Convolutional', 'Neural', 'Network', '(', 'CNN', ')', 'to', 'extract', 'its', 'syntactic', 'features', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBZ', 'JJ', ':', 'VBG', 'VBG', 'DT', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'TO', 'VB', 'PRP$', 'JJ', 'NNS', '.']",19
sarcasm_detection,1,35,This CNN representation is then concatenated with the relevant user embedding and discourse features to get the final representation which is used for classification .,"['This', 'CNN', 'representation', 'is', 'then', 'concatenated', 'with', 'the', 'relevant', 'user', 'embedding', 'and', 'discourse', 'features', 'to', 'get', 'the', 'final', 'representation', 'which', 'is', 'used', 'for', 'classification', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NNP', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'CC', 'NN', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'VBN', 'IN', 'NN', '.']",25
sarcasm_detection,1,247,We holdout 10 % of the training data for validation .,"['We', 'holdout', '10', '%', 'of', 'the', 'training', 'data', 'for', 'validation', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'CD', 'NN', 'IN', 'DT', 'NN', 'NNS', 'IN', 'NN', '.']",11
sarcasm_detection,1,249,"To optimize the parameters , Adam optimizer ( Kingma and Ba , 2014 ) is used , starting with an initial learning rate of 1e ? 4 .","['To', 'optimize', 'the', 'parameters', ',', 'Adam', 'optimizer', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', 'is', 'used', ',', 'starting', 'with', 'an', 'initial', 'learning', 'rate', 'of', '1e', '?', '4', '.']","['B-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NNS', ',', 'NNP', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'VBZ', 'VBN', ',', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', '.', 'CD', '.']",28
sarcasm_detection,1,251,Training termination is decided using early stopping technique with a patience of 12 .,"['Training', 'termination', 'is', 'decided', 'using', 'early', 'stopping', 'technique', 'with', 'a', 'patience', 'of', '12', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['VBG', 'NN', 'VBZ', 'VBN', 'VBG', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'CD', '.']",14
sarcasm_detection,1,252,"For the batched - modeling of comments in CNNs , each comment is either restricted or padded to 100 words for uniformity .","['For', 'the', 'batched', '-', 'modeling', 'of', 'comments', 'in', 'CNNs', ',', 'each', 'comment', 'is', 'either', 'restricted', 'or', 'padded', 'to', '100', 'words', 'for', 'uniformity', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'JJ', ':', 'NN', 'IN', 'NNS', 'IN', 'NNP', ',', 'DT', 'NN', 'VBZ', 'RB', 'VBN', 'CC', 'VBN', 'TO', 'CD', 'NNS', 'IN', 'NN', '.']",23
sarcasm_detection,1,258,Bag - of - Words :,"['Bag', '-', 'of', '-', 'Words', ':']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'IN', ':', 'NNS', ':']",6
sarcasm_detection,1,259,This model uses a comment 's word - counts as features in a vector .,"['This', 'model', 'uses', 'a', 'comment', ""'s"", 'word', '-', 'counts', 'as', 'features', 'in', 'a', 'vector', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'NN', 'POS', 'NN', ':', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'NN', '.']",15
sarcasm_detection,1,261,CNN : We compare our model with this individual CNN version .,"['CNN', ':', 'We', 'compare', 'our', 'model', 'with', 'this', 'individual', 'CNN', 'version', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NN', ':', 'PRP', 'VBP', 'PRP$', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'NN', '.']",12
sarcasm_detection,1,264,CNN - SVM :,"['CNN', '-', 'SVM', ':']","['B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', ':']",4
sarcasm_detection,1,265,"This model proposed by consists of a CNN for content modeling and other pre-trained CNNs for extracting sentiment , emotion and personality features from the given comment .","['This', 'model', 'proposed', 'by', 'consists', 'of', 'a', 'CNN', 'for', 'content', 'modeling', 'and', 'other', 'pre-trained', 'CNNs', 'for', 'extracting', 'sentiment', ',', 'emotion', 'and', 'personality', 'features', 'from', 'the', 'given', 'comment', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBN', 'IN', 'NNS', 'IN', 'DT', 'NNP', 'IN', 'NN', 'NN', 'CC', 'JJ', 'JJ', 'NNP', 'IN', 'VBG', 'NN', ',', 'NN', 'CC', 'NN', 'NNS', 'IN', 'DT', 'VBN', 'NN', '.']",28
sarcasm_detection,1,267,CUE - CNN :,"['CUE', '-', 'CNN', ':']","['B-n', 'I-n', 'I-n', 'O']","['NNP', ':', 'NN', ':']",4
sarcasm_detection,1,268,This method proposed by also models user embeddings with a method akin to ParagraphVector .,"['This', 'method', 'proposed', 'by', 'also', 'models', 'user', 'embeddings', 'with', 'a', 'method', 'akin', 'to', 'ParagraphVector', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'VBN', 'IN', 'RB', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'TO', 'NNP', '.']",15
sarcasm_detection,1,273,CASCADE manages to achieve major improvement across all datasets with statistical significance .,"['CASCADE', 'manages', 'to', 'achieve', 'major', 'improvement', 'across', 'all', 'datasets', 'with', 'statistical', 'significance', '.']","['B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'TO', 'VB', 'JJ', 'NN', 'IN', 'DT', 'NNS', 'IN', 'JJ', 'NN', '.']",13
sarcasm_detection,1,274,The lowest performance is obtained by the Bag - of - words approach whereas all neural architectures outperform it .,"['The', 'lowest', 'performance', 'is', 'obtained', 'by', 'the', 'Bag', '-', 'of', '-', 'words', 'approach', 'whereas', 'all', 'neural', 'architectures', 'outperform', 'it', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJS', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NNP', ':', 'IN', ':', 'NNS', 'VBP', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'PRP', '.']",20
sarcasm_detection,1,275,"Amongst the neural networks , the CNN baseline receives the least performance .","['Amongst', 'the', 'neural', 'networks', ',', 'the', 'CNN', 'baseline', 'receives', 'the', 'least', 'performance', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NNS', ',', 'DT', 'NNP', 'NN', 'VBZ', 'DT', 'JJS', 'NN', '.']",13
sarcasm_detection,1,276,CASCADE comfortably beats the state - of - the - art neural models CNN - SVM and CUE - CNN .,"['CASCADE', 'comfortably', 'beats', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'neural', 'models', 'CNN', '-', 'SVM', 'and', 'CUE', '-', 'CNN', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'RB', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'JJ', 'NNS', 'NNP', ':', 'NN', 'CC', 'NNP', ':', 'NN', '.']",21
sarcasm_detection,1,277,Its improved performance on the Main imbalanced dataset also reflects its robustness towards class imbalance and establishes it as a real - world deployable network .,"['Its', 'improved', 'performance', 'on', 'the', 'Main', 'imbalanced', 'dataset', 'also', 'reflects', 'its', 'robustness', 'towards', 'class', 'imbalance', 'and', 'establishes', 'it', 'as', 'a', 'real', '-', 'world', 'deployable', 'network', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'VBD', 'NN', 'RB', 'VBZ', 'PRP$', 'JJ', 'NNS', 'NN', 'NN', 'CC', 'VBZ', 'PRP', 'IN', 'DT', 'JJ', ':', 'NN', 'JJ', 'NN', '.']",26
sarcasm_detection,1,279,"Since CUE - CNN generates its user embeddings using a method similar to the ParagraphVector , we test the importance of personality features being included in our user profiling .","['Since', 'CUE', '-', 'CNN', 'generates', 'its', 'user', 'embeddings', 'using', 'a', 'method', 'similar', 'to', 'the', 'ParagraphVector', ',', 'we', 'test', 'the', 'importance', 'of', 'personality', 'features', 'being', 'included', 'in', 'our', 'user', 'profiling', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ':', 'NNP', 'VBZ', 'PRP$', 'JJ', 'NNS', 'VBG', 'DT', 'NN', 'JJ', 'TO', 'DT', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'NNS', 'VBG', 'VBN', 'IN', 'PRP$', 'NN', 'NN', '.']",30
sarcasm_detection,1,286,"First , we test performance for the content - based CNN only ( row 1 ) .","['First', ',', 'we', 'test', 'performance', 'for', 'the', 'content', '-', 'based', 'CNN', 'only', '(', 'row', '1', ')', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'NN', 'IN', 'DT', 'NN', ':', 'VBN', 'NNP', 'RB', '(', 'VB', 'CD', ')', '.']",17
sarcasm_detection,1,287,This setting provides the worst relative performance with almost 10 % lesser accuracy than optimal .,"['This', 'setting', 'provides', 'the', 'worst', 'relative', 'performance', 'with', 'almost', '10', '%', 'lesser', 'accuracy', 'than', 'optimal', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJS', 'JJ', 'NN', 'IN', 'RB', 'CD', 'NN', 'JJR', 'NN', 'IN', 'JJ', '.']",16
sarcasm_detection,1,288,"Next , we include contextual features to this network .","['Next', ',', 'we', 'include', 'contextual', 'features', 'to', 'this', 'network', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['JJ', ',', 'PRP', 'VBP', 'JJ', 'NNS', 'TO', 'DT', 'NN', '.']",10
sarcasm_detection,1,289,"Here , the effect of discourse features is primarily seen in the Pol dataset getting an increase of 3 % in F1 ( row 2 ) .","['Here', ',', 'the', 'effect', 'of', 'discourse', 'features', 'is', 'primarily', 'seen', 'in', 'the', 'Pol', 'dataset', 'getting', 'an', 'increase', 'of', '3', '%', 'in', 'F1', '(', 'row', '2', ')', '.']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'DT', 'NN', 'IN', 'NN', 'NNS', 'VBZ', 'RB', 'VBN', 'IN', 'DT', 'NNP', 'NN', 'VBG', 'DT', 'NN', 'IN', 'CD', 'NN', 'IN', 'NNP', '(', 'VB', 'CD', ')', '.']",27
sarcasm_detection,1,290,A major boost in performance is observed ( 8 ? 12 % accuracy and F1 ) when user embeddings are introduced ( row 5 ) .,"['A', 'major', 'boost', 'in', 'performance', 'is', 'observed', '(', '8', '?', '12', '%', 'accuracy', 'and', 'F1', ')', 'when', 'user', 'embeddings', 'are', 'introduced', '(', 'row', '5', ')', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'IN', 'NN', 'VBZ', 'VBN', '(', 'CD', '.', 'CD', 'NN', 'NN', 'CC', 'NNP', ')', 'WRB', 'JJ', 'NNS', 'VBP', 'VBN', '(', 'VB', 'CD', ')', '.']",26
sarcasm_detection,1,292,"Overall , CASCADE consisting of CNN with user embeddings and contextual discourse features provide the best performance in all three datasets ( row 6 ) .","['Overall', ',', 'CASCADE', 'consisting', 'of', 'CNN', 'with', 'user', 'embeddings', 'and', 'contextual', 'discourse', 'features', 'provide', 'the', 'best', 'performance', 'in', 'all', 'three', 'datasets', '(', 'row', '6', ')', '.']","['O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['JJ', ',', 'NNP', 'VBG', 'IN', 'NNP', 'IN', 'JJ', 'NNS', 'CC', 'JJ', 'NN', 'NNS', 'VBP', 'DT', 'JJS', 'NN', 'IN', 'DT', 'CD', 'NNS', '(', 'VB', 'CD', ')', '.']",26
sarcasm_detection,1,293,We challenge the use of CCA for the generation of user embeddings and thus replace it with simple concatenation .,"['We', 'challenge', 'the', 'use', 'of', 'CCA', 'for', 'the', 'generation', 'of', 'user', 'embeddings', 'and', 'thus', 'replace', 'it', 'with', 'simple', 'concatenation', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'NNP', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'CC', 'RB', 'VB', 'PRP', 'IN', 'JJ', 'NN', '.']",20
sarcasm_detection,1,294,This however causes a significant drop in performance ( row 3 ) .,"['This', 'however', 'causes', 'a', 'significant', 'drop', 'in', 'performance', '(', 'row', '3', ')', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['DT', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', '(', 'VB', 'CD', ')', '.']",13
sarcasm_detection,0,4,"We introduce the Self - Annotated Reddit Corpus ( SARC ) , a large corpus for sarcasm research and for training and evaluating systems for sarcasm detection .","['We', 'introduce', 'the', 'Self', '-', 'Annotated', 'Reddit', 'Corpus', '(', 'SARC', ')', ',', 'a', 'large', 'corpus', 'for', 'sarcasm', 'research', 'and', 'for', 'training', 'and', 'evaluating', 'systems', 'for', 'sarcasm', 'detection', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', ':', 'VBD', 'NNP', 'NNP', '(', 'NNP', ')', ',', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'CC', 'IN', 'NN', 'CC', 'VBG', 'NNS', 'IN', 'JJ', 'NN', '.']",28
sarcasm_detection,0,12,"In this work , we make available the first corpus 1 for sarcasm detection that has both unbalanced and self - annotated labels and does not consist of short text snippets from Twitter 2 .","['In', 'this', 'work', ',', 'we', 'make', 'available', 'the', 'first', 'corpus', '1', 'for', 'sarcasm', 'detection', 'that', 'has', 'both', 'unbalanced', 'and', 'self', '-', 'annotated', 'labels', 'and', 'does', 'not', 'consist', 'of', 'short', 'text', 'snippets', 'from', 'Twitter', '2', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'JJ', 'DT', 'JJ', 'NN', 'CD', 'IN', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'CC', 'PRP', ':', 'VBN', 'NNS', 'CC', 'VBZ', 'RB', 'VB', 'IN', 'JJ', 'NN', 'NNS', 'IN', 'NNP', 'CD', '.']",35
sarcasm_detection,0,13,"With more than a million examples of sarcastic statements , each provided with author , topic , and contex information , the dataset exceeds all previous sarcasm corpora by an order of magnitude in size .","['With', 'more', 'than', 'a', 'million', 'examples', 'of', 'sarcastic', 'statements', ',', 'each', 'provided', 'with', 'author', ',', 'topic', ',', 'and', 'contex', 'information', ',', 'the', 'dataset', 'exceeds', 'all', 'previous', 'sarcasm', 'corpora', 'by', 'an', 'order', 'of', 'magnitude', 'in', 'size', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJR', 'IN', 'DT', 'CD', 'NNS', 'IN', 'JJ', 'NNS', ',', 'DT', 'VBN', 'IN', 'NN', ',', 'NN', ',', 'CC', 'JJ', 'NN', ',', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NN', 'IN', 'NN', '.']",36
sarcasm_detection,0,120,Code to reproduce our results is provided at https://github.com/NLPrinceton/,"['Code', 'to', 'reproduce', 'our', 'results', 'is', 'provided', 'at', 'https://github.com/NLPrinceton/']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']","['NNP', 'TO', 'VB', 'PRP$', 'NNS', 'VBZ', 'VBN', 'IN', 'NN']",9
sarcasm_detection,0,169,"The baselines in perform reasonably well and much better than the random baseline , but none of them match human performance on either dataset .","['The', 'baselines', 'in', 'perform', 'reasonably', 'well', 'and', 'much', 'better', 'than', 'the', 'random', 'baseline', ',', 'but', 'none', 'of', 'them', 'match', 'human', 'performance', 'on', 'either', 'dataset', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'NN', 'RB', 'RB', 'CC', 'RB', 'JJR', 'IN', 'DT', 'JJ', 'NN', ',', 'CC', 'NN', 'IN', 'PRP', 'VBP', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",25
sarcasm_detection,0,170,"There is clear scope for improvement for machine learning methods , starting with the use of context provided to make better decisions about sarcasm .","['There', 'is', 'clear', 'scope', 'for', 'improvement', 'for', 'machine', 'learning', 'methods', ',', 'starting', 'with', 'the', 'use', 'of', 'context', 'provided', 'to', 'make', 'better', 'decisions', 'about', 'sarcasm', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['EX', 'VBZ', 'JJ', 'NN', 'IN', 'NN', 'IN', 'NN', 'NN', 'NNS', ',', 'VBG', 'IN', 'DT', 'NN', 'IN', 'NN', 'VBN', 'TO', 'VB', 'JJR', 'NNS', 'IN', 'NN', '.']",25
passage_re-ranking,1,2,PASSAGE RE - RANKING WITH BERT,"['PASSAGE', 'RE', '-', 'RANKING', 'WITH', 'BERT']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['NNP', 'NNP', ':', 'NN', 'NNP', 'NNP']",6
passage_re-ranking,1,5,"In this paper , we describe a simple re-implementation of BERT for query - based passage re-ranking .","['In', 'this', 'paper', ',', 'we', 'describe', 'a', 'simple', 're-implementation', 'of', 'BERT', 'for', 'query', '-', 'based', 'passage', 're-ranking', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'IN', 'NN', ':', 'VBN', 'NN', 'NN', '.']",18
passage_re-ranking,1,16,"In this paper , we describe in detail how we have re-purposed BERT as a passage re-ranker and achieved state - of - the - art results on the MS MARCO passage re-ranking task .","['In', 'this', 'paper', ',', 'we', 'describe', 'in', 'detail', 'how', 'we', 'have', 're-purposed', 'BERT', 'as', 'a', 'passage', 're-ranker', 'and', 'achieved', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'on', 'the', 'MS', 'MARCO', 'passage', 're-ranking', 'task', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'IN', 'NN', 'WRB', 'PRP', 'VBP', 'JJ', 'NNP', 'IN', 'DT', 'NN', 'NN', 'CC', 'VBN', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'NNP', 'NN', 'JJ', 'NN', '.']",35
passage_re-ranking,1,17,PASSAGE RE - RANKING WITH BERT,"['PASSAGE', 'RE', '-', 'RANKING', 'WITH', 'BERT']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['NNP', 'NNP', ':', 'NN', 'NNP', 'NNP']",6
passage_re-ranking,1,35,MS MARCO,"['MS', 'MARCO']","['B-n', 'I-n']","['NNP', 'NNP']",2
passage_re-ranking,1,43,"We fine - tune the model using TPUs 1 with a batch size of 32 ( 32 sequences * 512 tokens = 16,384 tokens / batch ) for 400 k iterations , which takes approximately 70 hours .","['We', 'fine', '-', 'tune', 'the', 'model', 'using', 'TPUs', '1', 'with', 'a', 'batch', 'size', 'of', '32', '(', '32', 'sequences', '*', '512', 'tokens', '=', '16,384', 'tokens', '/', 'batch', ')', 'for', '400', 'k', 'iterations', ',', 'which', 'takes', 'approximately', '70', 'hours', '.']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', ':', 'NN', 'DT', 'NN', 'VBG', 'NNP', 'CD', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', '(', 'CD', 'NNS', 'RB', 'CD', 'NNS', 'JJ', 'CD', 'NNS', 'NNP', 'NN', ')', 'IN', 'CD', 'NN', 'NNS', ',', 'WDT', 'VBZ', 'RB', 'CD', 'NNS', '.']",38
passage_re-ranking,1,46,"We use ADAM ( Kingma & Ba , 2014 ) with the initial learning rate set to 3 10 ?6 , ? 1 = 0.9 , ? 2 = 0.999 , L2 weight decay of 0.01 , learning rate warmup over the first 10,000 steps , and linear decay of the learning rate .","['We', 'use', 'ADAM', '(', 'Kingma', '&', 'Ba', ',', '2014', ')', 'with', 'the', 'initial', 'learning', 'rate', 'set', 'to', '3', '10', '?6', ',', '?', '1', '=', '0.9', ',', '?', '2', '=', '0.999', ',', 'L2', 'weight', 'decay', 'of', '0.01', ',', 'learning', 'rate', 'warmup', 'over', 'the', 'first', '10,000', 'steps', ',', 'and', 'linear', 'decay', 'of', 'the', 'learning', 'rate', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'IN', 'DT', 'JJ', 'NN', 'NN', 'VBN', 'TO', 'CD', 'CD', 'NN', ',', '.', 'CD', 'NN', 'CD', ',', '.', 'CD', 'NN', 'CD', ',', 'NNP', 'VBD', 'NN', 'IN', 'CD', ',', 'VBG', 'NN', 'NN', 'IN', 'DT', 'JJ', 'CD', 'NNS', ',', 'CC', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",54
passage_re-ranking,1,47,We use a dropout probability of 0.1 on all layers .,"['We', 'use', 'a', 'dropout', 'probability', 'of', '0.1', 'on', 'all', 'layers', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CD', 'IN', 'DT', 'NNS', '.']",11
passage_re-ranking,1,48,TREC - CAR,"['TREC', '-', 'CAR']","['B-n', 'I-n', 'I-n']","['NNP', ':', 'NN']",3
passage_re-ranking,1,63,"For the fine - tuning data , we generate our query - passage pairs by retrieving the top ten passages from the entire TREC - CAR corpus using BM25 .","['For', 'the', 'fine', '-', 'tuning', 'data', ',', 'we', 'generate', 'our', 'query', '-', 'passage', 'pairs', 'by', 'retrieving', 'the', 'top', 'ten', 'passages', 'from', 'the', 'entire', 'TREC', '-', 'CAR', 'corpus', 'using', 'BM25', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['IN', 'DT', 'JJ', ':', 'NN', 'NNS', ',', 'PRP', 'VBP', 'PRP$', 'NN', ':', 'NN', 'NNS', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NNP', ':', 'NN', 'NN', 'VBG', 'NNP', '.']",30
passage_re-ranking,1,66,"We train it for 400 k iterations , or 12.8 M examples ( 400 k iterations * 32 pairs / batch ) , which corresponds to only 40 % of the training set .","['We', 'train', 'it', 'for', '400', 'k', 'iterations', ',', 'or', '12.8', 'M', 'examples', '(', '400', 'k', 'iterations', '*', '32', 'pairs', '/', 'batch', ')', ',', 'which', 'corresponds', 'to', 'only', '40', '%', 'of', 'the', 'training', 'set', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'PRP', 'IN', 'CD', 'NN', 'NNS', ',', 'CC', 'CD', 'NNP', 'NNS', '(', 'CD', 'NN', 'NNS', 'VBP', 'CD', 'NNS', 'NNP', 'NN', ')', ',', 'WDT', 'VBZ', 'TO', 'RB', 'CD', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",34
passage_re-ranking,1,70,"Despite training on a fraction of the data available , the proposed BERT - based models surpass the previous state - of - the - art models by a large margin on both of the tasks .","['Despite', 'training', 'on', 'a', 'fraction', 'of', 'the', 'data', 'available', ',', 'the', 'proposed', 'BERT', '-', 'based', 'models', 'surpass', 'the', 'previous', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', 'by', 'a', 'large', 'margin', 'on', 'both', 'of', 'the', 'tasks', '.']","['B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBG', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'JJ', ',', 'DT', 'VBN', 'NNP', ':', 'VBN', 'NNS', 'VBP', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'IN', 'DT', 'NNS', '.']",37
passage_re-ranking,0,2,Document Expansion by Query Prediction,"['Document', 'Expansion', 'by', 'Query', 'Prediction']","['B-n', 'I-n', 'O', 'O', 'O']","['NNP', 'NNP', 'IN', 'NNP', 'NNP']",5
passage_re-ranking,0,24,"In this paper , we explore an alternative approach based on enriching the document representation ( prior to indexing ) .","['In', 'this', 'paper', ',', 'we', 'explore', 'an', 'alternative', 'approach', 'based', 'on', 'enriching', 'the', 'document', 'representation', '(', 'prior', 'to', 'indexing', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'VBN', 'IN', 'VBG', 'DT', 'NN', 'NN', '(', 'JJ', 'TO', 'VBG', ')', '.']",21
passage_re-ranking,0,25,"Focusing on question answering , we train a sequence - to - sequence model , that given a document , generates possible questions that the document might answer .","['Focusing', 'on', 'question', 'answering', ',', 'we', 'train', 'a', 'sequence', '-', 'to', '-', 'sequence', 'model', ',', 'that', 'given', 'a', 'document', ',', 'generates', 'possible', 'questions', 'that', 'the', 'document', 'might', 'answer', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O']","['VBG', 'IN', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', ':', 'TO', ':', 'NN', 'NN', ',', 'IN', 'VBN', 'DT', 'NN', ',', 'VBZ', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'MD', 'VB', '.']",29
passage_re-ranking,0,68,BM25 : We use the Anserini open - source IR toolkit 3 to index the original ( non -expanded ) documents and BM25 to rank the passages .,"['BM25', ':', 'We', 'use', 'the', 'Anserini', 'open', '-', 'source', 'IR', 'toolkit', '3', 'to', 'index', 'the', 'original', '(', 'non', '-expanded', ')', 'documents', 'and', 'BM25', 'to', 'rank', 'the', 'passages', '.']","['B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['NNS', ':', 'PRP', 'VBP', 'DT', 'NNP', 'JJ', ':', 'NN', 'NNP', 'VBZ', 'CD', 'TO', 'NN', 'DT', 'JJ', '(', 'RB', 'VBN', ')', 'NNS', 'CC', 'NNP', 'TO', 'VB', 'DT', 'NNS', '.']",28
passage_re-ranking,0,70,BM25 + Doc2query :,"['BM25', '+', 'Doc2query', ':']","['B-n', 'I-n', 'I-n', 'O']","['NNP', 'JJ', 'NN', ':']",4
passage_re-ranking,0,71,We first expand the documents using the proposed Doc2query method .,"['We', 'first', 'expand', 'the', 'documents', 'using', 'the', 'proposed', 'Doc2query', 'method', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NNS', 'VBG', 'DT', 'VBN', 'NNP', 'NN', '.']",11
passage_re-ranking,0,72,We then index and rank the expanded documents exactly as in the BM25 method above .,"['We', 'then', 'index', 'and', 'rank', 'the', 'expanded', 'documents', 'exactly', 'as', 'in', 'the', 'BM25', 'method', 'above', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O']","['PRP', 'RB', 'NN', 'CC', 'VB', 'DT', 'JJ', 'NNS', 'RB', 'IN', 'IN', 'DT', 'NNP', 'NN', 'IN', '.']",16
passage_re-ranking,0,76,RM3 :,"['RM3', ':']","['B-n', 'O']","['NN', ':']",2
passage_re-ranking,0,77,"To compare document expansion with query expansion , we applied the RM3 query expansion technique .","['To', 'compare', 'document', 'expansion', 'with', 'query', 'expansion', ',', 'we', 'applied', 'the', 'RM3', 'query', 'expansion', 'technique', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'JJ', 'NN', 'IN', 'JJ', 'NN', ',', 'PRP', 'VBD', 'DT', 'NNP', 'NN', 'NN', 'NN', '.']",16
passage_re-ranking,0,79,BM25 + BERT : We index and retrieve documents as in the BM25 condition and further re-rank the documents with BERT as described in .,"['BM25', '+', 'BERT', ':', 'We', 'index', 'and', 'retrieve', 'documents', 'as', 'in', 'the', 'BM25', 'condition', 'and', 'further', 're-rank', 'the', 'documents', 'with', 'BERT', 'as', 'described', 'in', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', ':', 'PRP', 'NN', 'CC', 'NN', 'NNS', 'IN', 'IN', 'DT', 'NNP', 'NN', 'CC', 'JJ', 'NN', 'DT', 'NNS', 'IN', 'NNP', 'IN', 'VBN', 'IN', '.']",25
passage_re-ranking,0,80,"BM25 + Doc2query + BERT : We expand , index , and retrieve documents as in the BM25 + Doc2query condition and further re-rank the documents with BERT .","['BM25', '+', 'Doc2query', '+', 'BERT', ':', 'We', 'expand', ',', 'index', ',', 'and', 'retrieve', 'documents', 'as', 'in', 'the', 'BM25', '+', 'Doc2query', 'condition', 'and', 'further', 're-rank', 'the', 'documents', 'with', 'BERT', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['NNP', 'NNP', 'NNP', 'NNP', 'NNP', ':', 'PRP', 'VBP', ',', 'NN', ',', 'CC', 'VBP', 'NNS', 'IN', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NN', 'CC', 'JJ', 'NN', 'DT', 'NNS', 'IN', 'NNP', '.']",29
passage_re-ranking,0,86,Document expansion with our method ( BM25 + Doc2query ) improves retrieval effectiveness by ? 15 % for both datasets .,"['Document', 'expansion', 'with', 'our', 'method', '(', 'BM25', '+', 'Doc2query', ')', 'improves', 'retrieval', 'effectiveness', 'by', '?', '15', '%', 'for', 'both', 'datasets', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['JJ', 'NN', 'IN', 'PRP$', 'NN', '(', 'NNP', 'NNP', 'NNP', ')', 'VBZ', 'JJ', 'NN', 'IN', '.', 'CD', 'NN', 'IN', 'DT', 'NNS', '.']",21
passage_re-ranking,0,87,"When we combine document expansion with a state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) , we achieve the best - known results to date on TREC CAR ; for MS MARCO , we are near the state of the art .","['When', 'we', 'combine', 'document', 'expansion', 'with', 'a', 'state', '-', 'of', '-', 'the', '-', 'art', 're-ranker', '(', 'BM25', '+', 'Doc2query', '+', 'BERT', ')', ',', 'we', 'achieve', 'the', 'best', '-', 'known', 'results', 'to', 'date', 'on', 'TREC', 'CAR', ';', 'for', 'MS', 'MARCO', ',', 'we', 'are', 'near', 'the', 'state', 'of', 'the', 'art', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['WRB', 'PRP', 'VBP', 'JJ', 'NN', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', '(', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', ')', ',', 'PRP', 'VBP', 'DT', 'JJS', ':', 'VBN', 'NNS', 'TO', 'NN', 'IN', 'NNP', 'NNP', ':', 'IN', 'NNP', 'NNP', ',', 'PRP', 'VBP', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",49
passage_re-ranking,0,89,"Our full re-ranking condition ( BM25 + Doc2query + BERT ) beats BM25 + BERT alone , which verifies that the contribution Input Document : July is the hottest month in Washington DC with an average temperature of 27C ( 80F ) and the coldest is January at 4C ( 38F ) with the most daily sunshine hours at 9 in July .","['Our', 'full', 're-ranking', 'condition', '(', 'BM25', '+', 'Doc2query', '+', 'BERT', ')', 'beats', 'BM25', '+', 'BERT', 'alone', ',', 'which', 'verifies', 'that', 'the', 'contribution', 'Input', 'Document', ':', 'July', 'is', 'the', 'hottest', 'month', 'in', 'Washington', 'DC', 'with', 'an', 'average', 'temperature', 'of', '27C', '(', '80F', ')', 'and', 'the', 'coldest', 'is', 'January', 'at', '4C', '(', '38F', ')', 'with', 'the', 'most', 'daily', 'sunshine', 'hours', 'at', '9', 'in', 'July', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'JJ', 'JJ', 'NN', '(', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', ')', 'VBZ', 'NNP', 'NNP', 'NNP', 'RB', ',', 'WDT', 'VBZ', 'IN', 'DT', 'NN', 'NNP', 'NNP', ':', 'NNP', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'NNP', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', '(', 'CD', ')', 'CC', 'DT', 'NN', 'VBZ', 'NNP', 'IN', 'CD', '(', 'CD', ')', 'IN', 'DT', 'RBS', 'JJ', 'NN', 'NNS', 'IN', 'CD', 'IN', 'NNP', '.']",63
passage_re-ranking,0,99,"We notice that the model tends to copy some words from the input document ( e.g. , Washington DC , River , chromosome ) , meaning that it can effectively perform term re-weighting ( i.e. , increasing the importance of key terms ) .","['We', 'notice', 'that', 'the', 'model', 'tends', 'to', 'copy', 'some', 'words', 'from', 'the', 'input', 'document', '(', 'e.g.', ',', 'Washington', 'DC', ',', 'River', ',', 'chromosome', ')', ',', 'meaning', 'that', 'it', 'can', 'effectively', 'perform', 'term', 're-weighting', '(', 'i.e.', ',', 'increasing', 'the', 'importance', 'of', 'key', 'terms', ')', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', '(', 'NN', ',', 'NNP', 'NNP', ',', 'NNP', ',', 'NN', ')', ',', 'VBG', 'IN', 'PRP', 'MD', 'RB', 'VB', 'NN', 'NN', '(', 'FW', ',', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'NNS', ')', '.']",44
passage_re-ranking,0,100,"Nevertheless , the model also produces words not present in the input document ( e.g. , weather , relationship ) , which can be characterized as expansion by synonyms and other related terms .","['Nevertheless', ',', 'the', 'model', 'also', 'produces', 'words', 'not', 'present', 'in', 'the', 'input', 'document', '(', 'e.g.', ',', 'weather', ',', 'relationship', ')', ',', 'which', 'can', 'be', 'characterized', 'as', 'expansion', 'by', 'synonyms', 'and', 'other', 'related', 'terms', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'NN', 'RB', 'VBZ', 'NNS', 'RB', 'JJ', 'IN', 'DT', 'NN', 'NN', '(', 'UH', ',', 'RB', ',', 'NN', ')', ',', 'WDT', 'MD', 'VB', 'VBN', 'IN', 'NN', 'IN', 'NN', 'CC', 'JJ', 'JJ', 'NNS', '.']",34
passage_re-ranking,0,103,"If we expand MS MARCO documents using only new words and retrieve the development set queries with BM25 , we obtain an MRR@10 of 18.8 ( as opposed to 18.4 when indexing with original documents ) .","['If', 'we', 'expand', 'MS', 'MARCO', 'documents', 'using', 'only', 'new', 'words', 'and', 'retrieve', 'the', 'development', 'set', 'queries', 'with', 'BM25', ',', 'we', 'obtain', 'an', 'MRR@10', 'of', '18.8', '(', 'as', 'opposed', 'to', '18.4', 'when', 'indexing', 'with', 'original', 'documents', ')', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'PRP', 'VBP', 'NNP', 'NNP', 'NNS', 'VBG', 'RB', 'JJ', 'NNS', 'CC', 'VB', 'DT', 'NN', 'NN', 'NNS', 'IN', 'NNP', ',', 'PRP', 'VB', 'DT', 'NNP', 'IN', 'CD', '(', 'IN', 'VBN', 'TO', 'CD', 'WRB', 'VBG', 'IN', 'JJ', 'NNS', ')', '.']",37
passage_re-ranking,0,104,Expanding with copied words gives an MRR@10 of 19.7 .,"['Expanding', 'with', 'copied', 'words', 'gives', 'an', 'MRR@10', 'of', '19.7', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['VBG', 'IN', 'JJ', 'NNS', 'VBZ', 'DT', 'NNP', 'IN', 'CD', '.']",10
passage_re-ranking,0,105,"We achieve a higher MRR@10 of 21.5 when documents are expanded with both types of words , showing that they are complementary .","['We', 'achieve', 'a', 'higher', 'MRR@10', 'of', '21.5', 'when', 'documents', 'are', 'expanded', 'with', 'both', 'types', 'of', 'words', ',', 'showing', 'that', 'they', 'are', 'complementary', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJR', 'NNP', 'IN', 'CD', 'WRB', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'NNS', 'IN', 'NNS', ',', 'VBG', 'IN', 'PRP', 'VBP', 'JJ', '.']",23
passage_re-ranking,0,107,We find that the Recall@1000 of the MS MARCO development set increased from 85.3 ( BM25 ) to 89.3 ( BM25 + Doc2query ) .,"['We', 'find', 'that', 'the', 'Recall@1000', 'of', 'the', 'MS', 'MARCO', 'development', 'set', 'increased', 'from', '85.3', '(', 'BM25', ')', 'to', '89.3', '(', 'BM25', '+', 'Doc2query', ')', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NNP', 'IN', 'DT', 'NNP', 'NNP', 'NN', 'NN', 'VBD', 'IN', 'CD', '(', 'NNP', ')', 'TO', 'CD', '(', 'NNP', 'NNP', 'NNP', ')', '.']",25
passage_re-ranking,0,109,"As a contrastive condition , we find that query expansion with RM3 hurts in both datasets , whether applied to the unexpanded corpus ( BM25 + RM3 ) or the expanded version ( BM25 + Doc2query + RM3 ) .","['As', 'a', 'contrastive', 'condition', ',', 'we', 'find', 'that', 'query', 'expansion', 'with', 'RM3', 'hurts', 'in', 'both', 'datasets', ',', 'whether', 'applied', 'to', 'the', 'unexpanded', 'corpus', '(', 'BM25', '+', 'RM3', ')', 'or', 'the', 'expanded', 'version', '(', 'BM25', '+', 'Doc2query', '+', 'RM3', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NNS', 'IN', 'DT', 'NNS', ',', 'IN', 'VBN', 'TO', 'DT', 'JJ', 'NN', '(', 'NNP', 'NNP', 'NNP', ')', 'CC', 'DT', 'VBN', 'NN', '(', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', ')', '.']",40
passage_re-ranking,0,111,"This result shows that document expansion can be more effective than query expansion , most likely because there are more signals to exploit as documents are much longer .","['This', 'result', 'shows', 'that', 'document', 'expansion', 'can', 'be', 'more', 'effective', 'than', 'query', 'expansion', ',', 'most', 'likely', 'because', 'there', 'are', 'more', 'signals', 'to', 'exploit', 'as', 'documents', 'are', 'much', 'longer', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'IN', 'JJ', 'NN', 'MD', 'VB', 'RBR', 'JJ', 'IN', 'JJ', 'NN', ',', 'RBS', 'JJ', 'IN', 'EX', 'VBP', 'JJR', 'NNS', 'TO', 'VB', 'IN', 'NNS', 'VBP', 'RB', 'RBR', '.']",29
passage_re-ranking,0,113,"Our method without a re-ranker ( BM25 + Doc2query ) adds a small latency increase over baseline BM25 ( 50 ms vs. 90 ms ) but is approximately seven times faster than a neural re-ranker that has a three points higher MRR@10 ( Single Duet v2 , which is presented as a baseline in MS MARCO by the organizers ) .","['Our', 'method', 'without', 'a', 're-ranker', '(', 'BM25', '+', 'Doc2query', ')', 'adds', 'a', 'small', 'latency', 'increase', 'over', 'baseline', 'BM25', '(', '50', 'ms', 'vs.', '90', 'ms', ')', 'but', 'is', 'approximately', 'seven', 'times', 'faster', 'than', 'a', 'neural', 're-ranker', 'that', 'has', 'a', 'three', 'points', 'higher', 'MRR@10', '(', 'Single', 'Duet', 'v2', ',', 'which', 'is', 'presented', 'as', 'a', 'baseline', 'in', 'MS', 'MARCO', 'by', 'the', 'organizers', ')', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP$', 'NN', 'IN', 'DT', 'NN', '(', 'NNP', 'NNP', 'NNP', ')', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'IN', 'JJ', 'NNP', '(', 'CD', 'NN', 'FW', 'CD', 'NN', ')', 'CC', 'VBZ', 'RB', 'CD', 'NNS', 'RBR', 'IN', 'DT', 'JJ', 'NN', 'WDT', 'VBZ', 'DT', 'CD', 'NNS', 'RBR', 'NNP', '(', 'NNP', 'NNP', 'NN', ',', 'WDT', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'IN', 'DT', 'NNS', ')', '.']",61
sentence_compression,1,2,Sentence Compression by Deletion with LSTMs,"['Sentence', 'Compression', 'by', 'Deletion', 'with', 'LSTMs']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['NN', 'NN', 'IN', 'NNP', 'IN', 'NNP']",6
sentence_compression,1,4,"We present an LSTM approach to deletion - based sentence compression where the task is to translate a sentence into a sequence of zeros and ones , corresponding to token deletion decisions .","['We', 'present', 'an', 'LSTM', 'approach', 'to', 'deletion', '-', 'based', 'sentence', 'compression', 'where', 'the', 'task', 'is', 'to', 'translate', 'a', 'sentence', 'into', 'a', 'sequence', 'of', 'zeros', 'and', 'ones', ',', 'corresponding', 'to', 'token', 'deletion', 'decisions', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', 'TO', 'VB', ':', 'VBN', 'NN', 'NN', 'WRB', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'CC', 'NNS', ',', 'VBG', 'TO', 'VB', 'NN', 'NNS', '.']",33
sentence_compression,1,9,Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence .,"['Sentence', 'compression', 'is', 'a', 'standard', 'NLP', 'task', 'where', 'the', 'goal', 'is', 'to', 'generate', 'a', 'shorter', 'paraphrase', 'of', 'a', 'sentence', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'NN', 'VBZ', 'DT', 'JJ', 'NNP', 'NN', 'WRB', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'JJR', 'NN', 'IN', 'DT', 'NN', '.']",20
sentence_compression,1,19,"In particular , we will present a model which benefits from the very recent advances in deep learning and uses word embeddings and Long Short Term Memory models ( LSTMs ) to output surprisingly readable and informative compressions .","['In', 'particular', ',', 'we', 'will', 'present', 'a', 'model', 'which', 'benefits', 'from', 'the', 'very', 'recent', 'advances', 'in', 'deep', 'learning', 'and', 'uses', 'word', 'embeddings', 'and', 'Long', 'Short', 'Term', 'Memory', 'models', '(', 'LSTMs', ')', 'to', 'output', 'surprisingly', 'readable', 'and', 'informative', 'compressions', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'JJ', ',', 'PRP', 'MD', 'VB', 'DT', 'NN', 'WDT', 'NNS', 'IN', 'DT', 'RB', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'CC', 'VBZ', 'NN', 'NNS', 'CC', 'NNP', 'NNP', 'NNP', 'NNP', 'NNS', '(', 'NNP', ')', 'TO', 'NN', 'RB', 'JJ', 'CC', 'JJ', 'NNS', '.']",39
sentence_compression,1,20,"Trained on a corpus of less than two million automatically extracted parallel sentences and using a standard tool to obtain word embeddings , in its best and most simple configuration it achieves 4.5 points out of 5 in readability and 3.8 points in informativeness in an extensive evaluation with human judges .","['Trained', 'on', 'a', 'corpus', 'of', 'less', 'than', 'two', 'million', 'automatically', 'extracted', 'parallel', 'sentences', 'and', 'using', 'a', 'standard', 'tool', 'to', 'obtain', 'word', 'embeddings', ',', 'in', 'its', 'best', 'and', 'most', 'simple', 'configuration', 'it', 'achieves', '4.5', 'points', 'out', 'of', '5', 'in', 'readability', 'and', '3.8', 'points', 'in', 'informativeness', 'in', 'an', 'extensive', 'evaluation', 'with', 'human', 'judges', '.']","['B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'IN', 'DT', 'NN', 'IN', 'JJR', 'IN', 'CD', 'CD', 'RB', 'JJ', 'JJ', 'NNS', 'CC', 'VBG', 'DT', 'JJ', 'NN', 'TO', 'VB', 'NN', 'NNS', ',', 'IN', 'PRP$', 'JJS', 'CC', 'RBS', 'JJ', 'NN', 'PRP', 'VBZ', 'CD', 'NNS', 'IN', 'IN', 'CD', 'IN', 'NN', 'CC', 'CD', 'NNS', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', '.']",52
sentence_compression,1,144,"There is a significant difference in performance of the MIRA baseline and the LSTM models , both in terms of F1 - score and in accuracy .","['There', 'is', 'a', 'significant', 'difference', 'in', 'performance', 'of', 'the', 'MIRA', 'baseline', 'and', 'the', 'LSTM', 'models', ',', 'both', 'in', 'terms', 'of', 'F1', '-', 'score', 'and', 'in', 'accuracy', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['EX', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'DT', 'NNP', 'NN', 'CC', 'DT', 'NNP', 'NNS', ',', 'DT', 'IN', 'NNS', 'IN', 'NNP', ':', 'NN', 'CC', 'IN', 'NN', '.']",27
sentence_compression,1,145,More than 30 % of golden compressions could be fully regenerated by the LSTM systems which is in sharp contrast with the 20 % of MIRA .,"['More', 'than', '30', '%', 'of', 'golden', 'compressions', 'could', 'be', 'fully', 'regenerated', 'by', 'the', 'LSTM', 'systems', 'which', 'is', 'in', 'sharp', 'contrast', 'with', 'the', '20', '%', 'of', 'MIRA', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['JJR', 'IN', 'CD', 'NN', 'IN', 'JJ', 'NNS', 'MD', 'VB', 'RB', 'VBN', 'IN', 'DT', 'NNP', 'NNS', 'WDT', 'VBZ', 'IN', 'JJ', 'NN', 'IN', 'DT', 'CD', 'NN', 'IN', 'NNP', '.']",27
sentence_compression,1,146,"The differences in F- score between the three versions of LSTM are not significant , all scores are close to 0.81 .","['The', 'differences', 'in', 'F-', 'score', 'between', 'the', 'three', 'versions', 'of', 'LSTM', 'are', 'not', 'significant', ',', 'all', 'scores', 'are', 'close', 'to', '0.81', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NNS', 'IN', 'NNP', 'NN', 'IN', 'DT', 'CD', 'NNS', 'IN', 'NNP', 'VBP', 'RB', 'JJ', ',', 'DT', 'NNS', 'VBP', 'JJ', 'TO', 'CD', '.']",22
sentence_compression,2,2,Improving sentence compression by learning to predict gaze,"['Improving', 'sentence', 'compression', 'by', 'learning', 'to', 'predict', 'gaze']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NN', 'NN', 'IN', 'VBG', 'TO', 'VB', 'NN']",8
sentence_compression,2,12,We go beyond this by suggesting that eye - tracking recordings can be used to induce better models for sentence compression for text simplification .,"['We', 'go', 'beyond', 'this', 'by', 'suggesting', 'that', 'eye', '-', 'tracking', 'recordings', 'can', 'be', 'used', 'to', 'induce', 'better', 'models', 'for', 'sentence', 'compression', 'for', 'text', 'simplification', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'IN', 'VBG', 'IN', 'NN', ':', 'JJ', 'NNS', 'MD', 'VB', 'VBN', 'TO', 'VB', 'JJR', 'NNS', 'IN', 'NN', 'NN', 'IN', 'JJ', 'NN', '.']",25
sentence_compression,2,13,"Specifically , we show how to use existing eye - tracking recordings to improve the induction of Long Short - Term Memory models ( LSTMs ) for sentence compression .","['Specifically', ',', 'we', 'show', 'how', 'to', 'use', 'existing', 'eye', '-', 'tracking', 'recordings', 'to', 'improve', 'the', 'induction', 'of', 'Long', 'Short', '-', 'Term', 'Memory', 'models', '(', 'LSTMs', ')', 'for', 'sentence', 'compression', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'WRB', 'TO', 'VB', 'VBG', 'NN', ':', 'VBG', 'NNS', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'NNS', '(', 'NNP', ')', 'IN', 'NN', 'NN', '.']",30
sentence_compression,2,14,Our proposed model does not require that the gaze data and the compression data come from the same source .,"['Our', 'proposed', 'model', 'does', 'not', 'require', 'that', 'the', 'gaze', 'data', 'and', 'the', 'compression', 'data', 'come', 'from', 'the', 'same', 'source', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP$', 'VBN', 'NN', 'VBZ', 'RB', 'VB', 'IN', 'DT', 'NN', 'NNS', 'CC', 'DT', 'NN', 'NNS', 'VBP', 'IN', 'DT', 'JJ', 'NN', '.']",20
sentence_compression,2,15,"Indeed , in this work we use gaze data from readers of the Dundee Corpus to improve sentence compression results on several datasets .","['Indeed', ',', 'in', 'this', 'work', 'we', 'use', 'gaze', 'data', 'from', 'readers', 'of', 'the', 'Dundee', 'Corpus', 'to', 'improve', 'sentence', 'compression', 'results', 'on', 'several', 'datasets', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['RB', ',', 'IN', 'DT', 'NN', 'PRP', 'VBP', 'JJ', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'NNP', 'NNP', 'TO', 'VB', 'NN', 'NN', 'NNS', 'IN', 'JJ', 'NNS', '.']",24
sentence_compression,2,16,"While not explored here , an intriguing potential of this work is in deriving sentence simplification models that are personalized for individual users , based on their reading behavior .","['While', 'not', 'explored', 'here', ',', 'an', 'intriguing', 'potential', 'of', 'this', 'work', 'is', 'in', 'deriving', 'sentence', 'simplification', 'models', 'that', 'are', 'personalized', 'for', 'individual', 'users', ',', 'based', 'on', 'their', 'reading', 'behavior', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'RB', 'VBN', 'RB', ',', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'IN', 'VBG', 'NN', 'NN', 'NNS', 'WDT', 'VBP', 'VBN', 'IN', 'JJ', 'NNS', ',', 'VBN', 'IN', 'PRP$', 'NN', 'NN', '.']",30
sentence_compression,2,85,Both the baseline and our systems are three - layer bi - LSTM models trained for 30 iterations with pretrained ( SENNA ) embeddings .,"['Both', 'the', 'baseline', 'and', 'our', 'systems', 'are', 'three', '-', 'layer', 'bi', '-', 'LSTM', 'models', 'trained', 'for', '30', 'iterations', 'with', 'pretrained', '(', 'SENNA', ')', 'embeddings', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'DT', 'NN', 'CC', 'PRP$', 'NNS', 'VBP', 'CD', ':', 'NN', 'SYM', ':', 'JJ', 'NNS', 'VBN', 'IN', 'CD', 'NNS', 'IN', 'VBN', '(', 'NNP', ')', 'NNS', '.']",25
sentence_compression,2,86,"The input and hidden layers are 50 dimensions , and at the output layer we predict sequences of two labels , indicating whether to delete the labeled word or not .","['The', 'input', 'and', 'hidden', 'layers', 'are', '50', 'dimensions', ',', 'and', 'at', 'the', 'output', 'layer', 'we', 'predict', 'sequences', 'of', 'two', 'labels', ',', 'indicating', 'whether', 'to', 'delete', 'the', 'labeled', 'word', 'or', 'not', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'CC', 'JJ', 'NNS', 'VBP', 'CD', 'NNS', ',', 'CC', 'IN', 'DT', 'NN', 'NN', 'PRP', 'VBP', 'NNS', 'IN', 'CD', 'NNS', ',', 'VBG', 'IN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'CC', 'RB', '.']",31
sentence_compression,2,87,Our baseline ( BASELINE - LSTM ) is a multi - task learning 1 http://groups.inf.ed.ac.uk/ccg/,"['Our', 'baseline', '(', 'BASELINE', '-', 'LSTM', ')', 'is', 'a', 'multi', '-', 'task', 'learning', '1', 'http://groups.inf.ed.ac.uk/ccg/']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PRP$', 'NN', '(', 'NNP', ':', 'NN', ')', 'VBZ', 'DT', 'JJ', ':', 'NN', 'VBG', 'CD', 'NN']",15
sentence_compression,2,88,bi -LSTM predicting both CCG supertags and sentence compression ( word deletion ) at the outer layer .,"['bi', '-LSTM', 'predicting', 'both', 'CCG', 'supertags', 'and', 'sentence', 'compression', '(', 'word', 'deletion', ')', 'at', 'the', 'outer', 'layer', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NN', 'NN', 'VBG', 'DT', 'NNP', 'NNS', 'CC', 'NN', 'NN', '(', 'NN', 'NN', ')', 'IN', 'DT', 'NN', 'NN', '.']",18
sentence_compression,2,93,"We observe that across all three datasets , including all three annotations of BROADCAST , gaze features lead to improvements over our baseline 3 - layer bi - LSTM .","['We', 'observe', 'that', 'across', 'all', 'three', 'datasets', ',', 'including', 'all', 'three', 'annotations', 'of', 'BROADCAST', ',', 'gaze', 'features', 'lead', 'to', 'improvements', 'over', 'our', 'baseline', '3', '-', 'layer', 'bi', '-', 'LSTM', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'IN', 'DT', 'CD', 'NNS', ',', 'VBG', 'DT', 'CD', 'NNS', 'IN', 'NNP', ',', 'NN', 'NNS', 'VBP', 'TO', 'NNS', 'IN', 'PRP$', 'NN', 'CD', ':', 'NN', 'SYM', ':', 'NN', '.']",30
sentence_compression,2,94,"Also , CASCADED - LSTM is consistently better than MULTITASK - LSTM . : Results ( F1 ) .","['Also', ',', 'CASCADED', '-', 'LSTM', 'is', 'consistently', 'better', 'than', 'MULTITASK', '-', 'LSTM', '.', ':', 'Results', '(', 'F1', ')', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'NNP', ':', 'NNP', 'VBZ', 'RB', 'JJR', 'IN', 'NNP', ':', 'NN', '.', ':', 'NNS', '(', 'NNP', ')', '.']",19
sentence_compression,2,95,"For all three datasets , the inclusion of gaze measures ( first pass duration ( FP ) and regression duration ( Regr. ) ) leads to improvements over the baseline .","['For', 'all', 'three', 'datasets', ',', 'the', 'inclusion', 'of', 'gaze', 'measures', '(', 'first', 'pass', 'duration', '(', 'FP', ')', 'and', 'regression', 'duration', '(', 'Regr.', ')', ')', 'leads', 'to', 'improvements', 'over', 'the', 'baseline', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'O']","['IN', 'DT', 'CD', 'NNS', ',', 'DT', 'NN', 'IN', 'JJ', 'NNS', '(', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'NN', 'NN', '(', 'NNP', ')', ')', 'VBZ', 'TO', 'NNS', 'IN', 'DT', 'NN', '.']",31
sentence_compression,2,100,"With the harder datasets , the impact of the gaze information becomes stronger , consistently favouring the cascaded architecture , and with improvements using both first pass duration and regression duration , the late measure associated with interpretation of content .","['With', 'the', 'harder', 'datasets', ',', 'the', 'impact', 'of', 'the', 'gaze', 'information', 'becomes', 'stronger', ',', 'consistently', 'favouring', 'the', 'cascaded', 'architecture', ',', 'and', 'with', 'improvements', 'using', 'both', 'first', 'pass', 'duration', 'and', 'regression', 'duration', ',', 'the', 'late', 'measure', 'associated', 'with', 'interpretation', 'of', 'content', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'JJR', 'NNS', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'JJR', ',', 'RB', 'VBG', 'DT', 'JJ', 'NN', ',', 'CC', 'IN', 'NNS', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'CC', 'NN', 'NN', ',', 'DT', 'JJ', 'NN', 'VBN', 'IN', 'NN', 'IN', 'NN', '.']",41
sentence_compression,3,2,A Language Model based Evaluator for Sentence Compression,"['A', 'Language', 'Model', 'based', 'Evaluator', 'for', 'Sentence', 'Compression']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['DT', 'NNP', 'NNP', 'VBN', 'NNP', 'IN', 'NNP', 'NNP']",8
sentence_compression,3,4,"We herein present a language - modelbased evaluator for deletion - based sentence compression , and viewed this task as a series of deletion - and - evaluation operations using the evaluator .","['We', 'herein', 'present', 'a', 'language', '-', 'modelbased', 'evaluator', 'for', 'deletion', '-', 'based', 'sentence', 'compression', ',', 'and', 'viewed', 'this', 'task', 'as', 'a', 'series', 'of', 'deletion', '-', 'and', '-', 'evaluation', 'operations', 'using', 'the', 'evaluator', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', 'DT', 'NN', ':', 'VBN', 'NN', 'IN', 'NN', ':', 'VBN', 'NN', 'NN', ',', 'CC', 'VBD', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', ':', 'CC', ':', 'NN', 'NNS', 'VBG', 'DT', 'NN', '.']",33
sentence_compression,3,19,"To answer the above questions , a syntax - based neural language model is trained on large - scale datasets as a readability evaluator .","['To', 'answer', 'the', 'above', 'questions', ',', 'a', 'syntax', '-', 'based', 'neural', 'language', 'model', 'is', 'trained', 'on', 'large', '-', 'scale', 'datasets', 'as', 'a', 'readability', 'evaluator', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'JJ', 'NNS', ',', 'DT', 'NN', ':', 'VBN', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', ':', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']",25
sentence_compression,3,20,The neural language model is supposed to learn the correct word collocations in terms of both syntax and semantics .,"['The', 'neural', 'language', 'model', 'is', 'supposed', 'to', 'learn', 'the', 'correct', 'word', 'collocations', 'in', 'terms', 'of', 'both', 'syntax', 'and', 'semantics', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NNS', '.']",20
sentence_compression,3,21,"Subsequently , we formulate the deletionbased sentence compression as a series of trialand - error deletion operations through a reinforcement learning framework .","['Subsequently', ',', 'we', 'formulate', 'the', 'deletionbased', 'sentence', 'compression', 'as', 'a', 'series', 'of', 'trialand', '-', 'error', 'deletion', 'operations', 'through', 'a', 'reinforcement', 'learning', 'framework', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ':', 'NN', 'NN', 'NNS', 'IN', 'DT', 'NN', 'VBG', 'NN', '.']",23
sentence_compression,3,22,"The policy network performs either RETAIN or REMOVE action to form a compression , and receives a reward ( e.g. , readability score ) to update the network .","['The', 'policy', 'network', 'performs', 'either', 'RETAIN', 'or', 'REMOVE', 'action', 'to', 'form', 'a', 'compression', ',', 'and', 'receives', 'a', 'reward', '(', 'e.g.', ',', 'readability', 'score', ')', 'to', 'update', 'the', 'network', '.']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['DT', 'NN', 'NN', 'NNS', 'CC', 'NNP', 'CC', 'NNP', 'NN', 'TO', 'VB', 'DT', 'NN', ',', 'CC', 'VBZ', 'DT', 'NN', '(', 'JJ', ',', 'NN', 'NN', ')', 'TO', 'VB', 'DT', 'NN', '.']",29
sentence_compression,3,72,We choose several strong baselines ; the first one is the dependency - tree - based method that considers the sentence compression task as an optimization problem by using integer linear programming 5 .,"['We', 'choose', 'several', 'strong', 'baselines', ';', 'the', 'first', 'one', 'is', 'the', 'dependency', '-', 'tree', '-', 'based', 'method', 'that', 'considers', 'the', 'sentence', 'compression', 'task', 'as', 'an', 'optimization', 'problem', 'by', 'using', 'integer', 'linear', 'programming', '5', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBP', 'JJ', 'JJ', 'NNS', ':', 'DT', 'JJ', 'NN', 'VBZ', 'DT', 'NN', ':', 'NN', ':', 'VBN', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'IN', 'VBG', 'JJ', 'JJ', 'VBG', 'CD', '.']",34
sentence_compression,3,82,The second method is the long short - term memory networks ( LSTMs ) which showed strong promise in sentence compression by .,"['The', 'second', 'method', 'is', 'the', 'long', 'short', '-', 'term', 'memory', 'networks', '(', 'LSTMs', ')', 'which', 'showed', 'strong', 'promise', 'in', 'sentence', 'compression', 'by', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'JJ', ':', 'NN', 'NN', 'NNS', '(', 'NNP', ')', 'WDT', 'VBD', 'JJ', 'NN', 'IN', 'NN', 'NN', 'IN', '.']",23
sentence_compression,3,90,"The embedding size for word , part - of - speech tag , and the dependency relation is 128 .","['The', 'embedding', 'size', 'for', 'word', ',', 'part', '-', 'of', '-', 'speech', 'tag', ',', 'and', 'the', 'dependency', 'relation', 'is', '128', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'IN', 'NN', ',', 'NN', ':', 'IN', ':', 'NN', 'NN', ',', 'CC', 'DT', 'NN', 'NN', 'VBZ', 'CD', '.']",20
sentence_compression,3,91,We employed the vanilla RNN with a hidden size of 512 for both the policy network and neural language model .,"['We', 'employed', 'the', 'vanilla', 'RNN', 'with', 'a', 'hidden', 'size', 'of', '512', 'for', 'both', 'the', 'policy', 'network', 'and', 'neural', 'language', 'model', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBD', 'DT', 'NN', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', 'IN', 'DT', 'DT', 'NN', 'NN', 'CC', 'JJ', 'NN', 'NN', '.']",21
sentence_compression,3,92,"The mini - batch size was chosen from [ 5 , 50 , 100 ] .","['The', 'mini', '-', 'batch', 'size', 'was', 'chosen', 'from', '[', '5', ',', '50', ',', '100', ']', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', ':', 'NN', 'NN', 'VBD', 'VBN', 'IN', 'JJ', 'CD', ',', 'CD', ',', 'CD', 'NN', '.']",16
sentence_compression,3,93,"Vocabulary size was 50,000 .","['Vocabulary', 'size', 'was', '50,000', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'O']","['JJ', 'NN', 'VBD', 'CD', '.']",5
sentence_compression,3,94,"The learning rate for neural language model is 2.5 e - 4 , and 1e - 05 for the policy network .","['The', 'learning', 'rate', 'for', 'neural', 'language', 'model', 'is', '2.5', 'e', '-', '4', ',', 'and', '1e', '-', '05', 'for', 'the', 'policy', 'network', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NN', 'VBZ', 'CD', 'NN', ':', 'CD', ',', 'CC', 'CD', ':', 'CD', 'IN', 'DT', 'NN', 'NN', '.']",22
sentence_compression,3,95,"For policy learning , we used the REINFORCE algorithm to update the parameters of the policy network and find an policy that maximizes the reward .","['For', 'policy', 'learning', ',', 'we', 'used', 'the', 'REINFORCE', 'algorithm', 'to', 'update', 'the', 'parameters', 'of', 'the', 'policy', 'network', 'and', 'find', 'an', 'policy', 'that', 'maximizes', 'the', 'reward', '.']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'O']","['IN', 'NN', 'NN', ',', 'PRP', 'VBD', 'DT', 'NNP', 'NN', 'TO', 'VB', 'DT', 'NNS', 'IN', 'DT', 'NN', 'NN', 'CC', 'VB', 'DT', 'NN', 'WDT', 'VBZ', 'DT', 'NN', '.']",26
sentence_compression,3,99,6 https://github.com/code4conference/code4sc,"['6', 'https://github.com/code4conference/code4sc']","['O', 'B-n']","['CD', 'NN']",2
sentence_compression,3,107,"( 1 ) As shown in , our Evaluator - SLMbased method yields a large improvement over the baselines , demonstrating that the language - modelbased evaluator is effective as a post-hoc grammar checker for the compressed sentences .","['(', '1', ')', 'As', 'shown', 'in', ',', 'our', 'Evaluator', '-', 'SLMbased', 'method', 'yields', 'a', 'large', 'improvement', 'over', 'the', 'baselines', ',', 'demonstrating', 'that', 'the', 'language', '-', 'modelbased', 'evaluator', 'is', 'effective', 'as', 'a', 'post-hoc', 'grammar', 'checker', 'for', 'the', 'compressed', 'sentences', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'CD', ')', 'IN', 'VBN', 'IN', ',', 'PRP$', 'NNP', ':', 'VBD', 'JJ', 'NNS', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNS', ',', 'VBG', 'IN', 'DT', 'NN', ':', 'VBN', 'NN', 'VBZ', 'JJ', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNS', '.']",39
sentence_compression,3,109,"( 3 ) As for Google news dataset , LSTMs ( LSTM + pos+dep ) ( & 3 ) is a relatively strong baseline , suggesting that incorporating dependency relations and part - of - speech tags may help model learn the syntactic relations and thus make a better prediction .","['(', '3', ')', 'As', 'for', 'Google', 'news', 'dataset', ',', 'LSTMs', '(', 'LSTM', '+', 'pos+dep', ')', '(', '&', '3', ')', 'is', 'a', 'relatively', 'strong', 'baseline', ',', 'suggesting', 'that', 'incorporating', 'dependency', 'relations', 'and', 'part', '-', 'of', '-', 'speech', 'tags', 'may', 'help', 'model', 'learn', 'the', 'syntactic', 'relations', 'and', 'thus', 'make', 'a', 'better', 'prediction', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'CD', ')', 'IN', 'IN', 'NNP', 'NN', 'NN', ',', 'NNP', '(', 'NNP', 'NNP', 'NN', ')', '(', 'CC', 'CD', ')', 'VBZ', 'DT', 'RB', 'JJ', 'NN', ',', 'VBG', 'IN', 'VBG', 'NN', 'NNS', 'CC', 'NN', ':', 'IN', ':', 'NN', 'NN', 'MD', 'VB', 'VB', 'VB', 'DT', 'JJ', 'NNS', 'CC', 'RB', 'VB', 'DT', 'JJR', 'NN', '.']",51
sentence_compression,3,110,"When further applying Evaluator - SLM , only a tiny improvement is observed ( &3 vs & 4 ) , not comparable to the improvement between # 3 and # 5 .","['When', 'further', 'applying', 'Evaluator', '-', 'SLM', ',', 'only', 'a', 'tiny', 'improvement', 'is', 'observed', '(', '&3', 'vs', '&', '4', ')', ',', 'not', 'comparable', 'to', 'the', 'improvement', 'between', '#', '3', 'and', '#', '5', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['WRB', 'RBR', 'VBG', 'NNP', ':', 'NN', ',', 'RB', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', '(', 'NNP', 'NNP', 'CC', 'CD', ')', ',', 'RB', 'JJ', 'TO', 'DT', 'NN', 'IN', '#', 'CD', 'CC', '#', 'CD', '.']",32
sentence_compression,3,112,"For Gigaword dataset with 1.02 million instances , the perplexity of the language model is 20.3 , while for the Google news dataset with 0.2 million instances , the perplexity is 76.5 .","['For', 'Gigaword', 'dataset', 'with', '1.02', 'million', 'instances', ',', 'the', 'perplexity', 'of', 'the', 'language', 'model', 'is', '20.3', ',', 'while', 'for', 'the', 'Google', 'news', 'dataset', 'with', '0.2', 'million', 'instances', ',', 'the', 'perplexity', 'is', '76.5', '.']","['B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O']","['IN', 'NNP', 'NN', 'IN', 'CD', 'CD', 'NNS', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'CD', ',', 'IN', 'IN', 'DT', 'NNP', 'NN', 'NN', 'IN', 'CD', 'CD', 'NNS', ',', 'DT', 'NN', 'VBZ', 'CD', '.']",33
sentence_compression,3,114,"The results shows that small improvements are observed on two datasets ( # 4 vs # 5 ; & 4 vs & 5 ) , suggesting that incorporating syntactic knowledge may help evaluator to encourage more unseen but reasonable word collocations .","['The', 'results', 'shows', 'that', 'small', 'improvements', 'are', 'observed', 'on', 'two', 'datasets', '(', '#', '4', 'vs', '#', '5', ';', '&', '4', 'vs', '&', '5', ')', ',', 'suggesting', 'that', 'incorporating', 'syntactic', 'knowledge', 'may', 'help', 'evaluator', 'to', 'encourage', 'more', 'unseen', 'but', 'reasonable', 'word', 'collocations', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'VBZ', 'IN', 'JJ', 'NNS', 'VBP', 'VBN', 'IN', 'CD', 'NNS', '(', '#', 'CD', 'JJ', '#', 'CD', ':', 'CC', 'CD', 'NN', 'CC', 'CD', ')', ',', 'VBG', 'IN', 'VBG', 'JJ', 'NN', 'MD', 'VB', 'VB', 'TO', 'VB', 'JJR', 'JJ', 'CC', 'JJ', 'NN', 'NNS', '.']",42
sentence_compression,0,2,Can Syntax Help ? Improving an LSTM - based Sentence Compression Model for New Domains,"['Can', 'Syntax', 'Help', '?', 'Improving', 'an', 'LSTM', '-', 'based', 'Sentence', 'Compression', 'Model', 'for', 'New', 'Domains']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['MD', 'VB', 'NNP', '.', 'VBG', 'DT', 'NNP', ':', 'VBN', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",15
sentence_compression,0,28,"To this end , we extend the deletionbased LSTM model for sentence compression by .","['To', 'this', 'end', ',', 'we', 'extend', 'the', 'deletionbased', 'LSTM', 'model', 'for', 'sentence', 'compression', 'by', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O']","['TO', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNP', 'NN', 'IN', 'NN', 'NN', 'IN', '.']",15
sentence_compression,0,35,"Specifically , we propose two major changes to the model by : We explicitly introduce POS embeddings and dependency relation embeddings into the neural network model .","['Specifically', ',', 'we', 'propose', 'two', 'major', 'changes', 'to', 'the', 'model', 'by', ':', 'We', 'explicitly', 'introduce', 'POS', 'embeddings', 'and', 'dependency', 'relation', 'embeddings', 'into', 'the', 'neural', 'network', 'model', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'CD', 'JJ', 'NNS', 'TO', 'DT', 'NN', 'IN', ':', 'PRP', 'RB', 'VBP', 'NNP', 'NNS', 'CC', 'NN', 'NN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",27
sentence_compression,0,36,"( 2 ) Inspired by a previous method , we formulate the final predictions as an Integer Linear Programming problem to incorporate constraints based on syntactic relations between words and expected lengths of the compressed sentences .","['(', '2', ')', 'Inspired', 'by', 'a', 'previous', 'method', ',', 'we', 'formulate', 'the', 'final', 'predictions', 'as', 'an', 'Integer', 'Linear', 'Programming', 'problem', 'to', 'incorporate', 'constraints', 'based', 'on', 'syntactic', 'relations', 'between', 'words', 'and', 'expected', 'lengths', 'of', 'the', 'compressed', 'sentences', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['(', 'CD', ')', 'VBN', 'IN', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NN', 'TO', 'VB', 'NNS', 'VBN', 'IN', 'JJ', 'NNS', 'IN', 'NNS', 'CC', 'VBD', 'NNS', 'IN', 'DT', 'JJ', 'NNS', '.']",37
sentence_compression,0,37,"In addition to the two major changes above , we also use bi-directional LSTM to include contextual information from both directions into the model .","['In', 'addition', 'to', 'the', 'two', 'major', 'changes', 'above', ',', 'we', 'also', 'use', 'bi-directional', 'LSTM', 'to', 'include', 'contextual', 'information', 'from', 'both', 'directions', 'into', 'the', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'DT', 'CD', 'JJ', 'NNS', 'IN', ',', 'PRP', 'RB', 'VBP', 'JJ', 'NNP', 'TO', 'VB', 'JJ', 'NN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NN', '.']",25
sentence_compression,0,165,"In the experiments , our model was trained using the Adam algorithm with a learning rate initialized at 0.001 .","['In', 'the', 'experiments', ',', 'our', 'model', 'was', 'trained', 'using', 'the', 'Adam', 'algorithm', 'with', 'a', 'learning', 'rate', 'initialized', 'at', '0.001', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'DT', 'NNS', ',', 'PRP$', 'NN', 'VBD', 'VBN', 'VBG', 'DT', 'NNP', 'NN', 'IN', 'DT', 'VBG', 'NN', 'VBN', 'IN', 'CD', '.']",20
sentence_compression,0,166,The dimension of the hidden layers of bi - LSTM is 100 .,"['The', 'dimension', 'of', 'the', 'hidden', 'layers', 'of', 'bi', '-', 'LSTM', 'is', '100', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NN', ':', 'NNP', 'VBZ', 'CD', '.']",13
sentence_compression,0,167,Word embeddings are initialized from GloVe 100 dimensional pre-trained embeddings .,"['Word', 'embeddings', 'are', 'initialized', 'from', 'GloVe', '100', 'dimensional', 'pre-trained', 'embeddings', '.']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'CD', 'JJ', 'JJ', 'NNS', '.']",11
sentence_compression,0,168,POS and dependency embeddings are randomly initialized with 40 - dimensional vectors .,"['POS', 'and', 'dependency', 'embeddings', 'are', 'randomly', 'initialized', 'with', '40', '-', 'dimensional', 'vectors', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['NNP', 'CC', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'IN', 'CD', ':', 'JJ', 'NNS', '.']",13
sentence_compression,0,169,The embeddings are all updated during training .,"['The', 'embeddings', 'are', 'all', 'updated', 'during', 'training', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NNS', 'VBP', 'DT', 'VBN', 'IN', 'NN', '.']",8
sentence_compression,0,170,Dropping probability for dropout layers between stacked LSTM layers is 0.5 .,"['Dropping', 'probability', 'for', 'dropout', 'layers', 'between', 'stacked', 'LSTM', 'layers', 'is', '0.5', '.']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['VBG', 'NN', 'IN', 'NN', 'NNS', 'IN', 'VBN', 'NNP', 'NNS', 'VBZ', 'CD', '.']",12
sentence_compression,0,171,The batch size is set as 30 .,"['The', 'batch', 'size', 'is', 'set', 'as', '30', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', '.']",8
sentence_compression,0,174,We utilize an open source ILP solver 4 in our method .,"['We', 'utilize', 'an', 'open', 'source', 'ILP', 'solver', '4', 'in', 'our', 'method', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'NNP', 'RB', 'CD', 'IN', 'PRP$', 'NN', '.']",12
sentence_compression,0,176,LSTM : This is the basic LSTM - based deletion method proposed by .,"['LSTM', ':', 'This', 'is', 'the', 'basic', 'LSTM', '-', 'based', 'deletion', 'method', 'proposed', 'by', '.']","['B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['NN', ':', 'DT', 'VBZ', 'DT', 'JJ', 'NNP', ':', 'VBN', 'NN', 'NN', 'VBN', 'IN', '.']",14
sentence_compression,0,178,"LSTM + : This is advanced version of the model proposed by , where the authors incorporated some dependency parse tree information into the LSTM model and used the prediction on the previous word to help the prediction on the current word .","['LSTM', '+', ':', 'This', 'is', 'advanced', 'version', 'of', 'the', 'model', 'proposed', 'by', ',', 'where', 'the', 'authors', 'incorporated', 'some', 'dependency', 'parse', 'tree', 'information', 'into', 'the', 'LSTM', 'model', 'and', 'used', 'the', 'prediction', 'on', 'the', 'previous', 'word', 'to', 'help', 'the', 'prediction', 'on', 'the', 'current', 'word', '.']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', 'NN', ':', 'DT', 'VBZ', 'JJ', 'NN', 'IN', 'DT', 'NN', 'VBN', 'IN', ',', 'WRB', 'DT', 'NNS', 'VBD', 'DT', 'NN', 'VBD', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NN', 'CC', 'VBD', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",43
sentence_compression,0,179,Traditional ILP :,"['Traditional', 'ILP', ':']","['B-n', 'I-n', 'O']","['JJ', 'NN', ':']",3
sentence_compression,0,180,This is the ILP - based method proposed by .,"['This', 'is', 'the', 'ILP', '-', 'based', 'method', 'proposed', 'by', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['DT', 'VBZ', 'DT', 'NNP', ':', 'VBN', 'NNS', 'VBN', 'IN', '.']",10
sentence_compression,0,183,Abstractive seq2seq :,"['Abstractive', 'seq2seq', ':']","['B-n', 'I-n', 'O']","['JJ', 'NN', ':']",3
sentence_compression,0,184,This is an abstractive sequence - to - sequence model trained on 3.8 million Gigaword title - article pairs as described in Section 1 .,"['This', 'is', 'an', 'abstractive', 'sequence', '-', 'to', '-', 'sequence', 'model', 'trained', 'on', '3.8', 'million', 'Gigaword', 'title', '-', 'article', 'pairs', 'as', 'described', 'in', 'Section', '1', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'VBZ', 'DT', 'JJ', 'NN', ':', 'TO', ':', 'NN', 'NN', 'VBD', 'IN', 'CD', 'CD', 'NNP', 'NN', ':', 'NN', 'NNS', 'IN', 'VBN', 'IN', 'NN', 'CD', '.']",25
sentence_compression,0,197,We can see that indeed this abstractive method performed poorly in cross - domain settings .,"['We', 'can', 'see', 'that', 'indeed', 'this', 'abstractive', 'method', 'performed', 'poorly', 'in', 'cross', '-', 'domain', 'settings', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'DT', 'RB', 'DT', 'JJ', 'NN', 'VBD', 'RB', 'IN', 'NN', ':', 'NN', 'NNS', '.']",16
sentence_compression,0,198,"( 2 ) In the in - domain setting , with the same amount of training data ( 8,000 ) , our BiLSTM method with syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) performs similarly to or better than the LSTM + method proposed by , in terms of both F1 and accuracy .","['(', '2', ')', 'In', 'the', 'in', '-', 'domain', 'setting', ',', 'with', 'the', 'same', 'amount', 'of', 'training', 'data', '(', '8,000', ')', ',', 'our', 'BiLSTM', 'method', 'with', 'syntactic', 'features', '(', 'BiLSTM', '+', 'SynFeat', 'and', 'BiL', '-', 'STM', '+', 'SynFeat', '+', 'ILP', ')', 'performs', 'similarly', 'to', 'or', 'better', 'than', 'the', 'LSTM', '+', 'method', 'proposed', 'by', ',', 'in', 'terms', 'of', 'both', 'F1', 'and', 'accuracy', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'IN', 'DT', 'IN', ':', 'NN', 'NN', ',', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', '(', 'CD', ')', ',', 'PRP$', 'NNP', 'NN', 'IN', 'JJ', 'NNS', '(', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', ')', 'VBZ', 'JJ', 'TO', 'CC', 'JJR', 'IN', 'DT', 'NNP', 'NNP', 'NN', 'VBN', 'IN', ',', 'IN', 'NNS', 'IN', 'DT', 'NNP', 'CC', 'NN', '.']",61
sentence_compression,0,199,This shows that our method is comparable to the LSTM + method in the in - domain setting .,"['This', 'shows', 'that', 'our', 'method', 'is', 'comparable', 'to', 'the', 'LSTM', '+', 'method', 'in', 'the', 'in', '-', 'domain', 'setting', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'IN', 'PRP$', 'NN', 'VBZ', 'JJ', 'TO', 'DT', 'NNP', 'NNP', 'NN', 'IN', 'DT', 'IN', ':', 'NN', 'NN', '.']",19
sentence_compression,0,202,"( 4 ) In the out - of - domain setting , our BiLSTM + SynFeat and BiLSTM+SynFeat+ILP methods clearly outperform the LSTM and LSTM + methods .","['(', '4', ')', 'In', 'the', 'out', '-', 'of', '-', 'domain', 'setting', ',', 'our', 'BiLSTM', '+', 'SynFeat', 'and', 'BiLSTM+SynFeat+ILP', 'methods', 'clearly', 'outperform', 'the', 'LSTM', 'and', 'LSTM', '+', 'methods', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'IN', 'DT', 'RP', ':', 'IN', ':', 'NN', 'NN', ',', 'PRP$', 'NNP', 'NNP', 'NNP', 'CC', 'NNP', 'NNS', 'RB', 'VBP', 'DT', 'NNP', 'CC', 'NNP', 'NNP', 'NNS', '.']",28
sentence_compression,0,204,( 5 ) The Traditional ILP method also works better than the LSTM and LSTM + methods in the out - of - domain setting .,"['(', '5', ')', 'The', 'Traditional', 'ILP', 'method', 'also', 'works', 'better', 'than', 'the', 'LSTM', 'and', 'LSTM', '+', 'methods', 'in', 'the', 'out', '-', 'of', '-', 'domain', 'setting', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['(', 'CD', ')', 'DT', 'NNP', 'NNP', 'NN', 'RB', 'VBZ', 'RBR', 'IN', 'DT', 'NNP', 'CC', 'NNP', 'NNP', 'NNS', 'IN', 'DT', 'RP', ':', 'IN', ':', 'NN', 'NN', '.']",26
sentence_compression,0,206,But the Traditional ILP method performs worse in the in - domain setting than both the LSTM and LSTM + methods and our methods .,"['But', 'the', 'Traditional', 'ILP', 'method', 'performs', 'worse', 'in', 'the', 'in', '-', 'domain', 'setting', 'than', 'both', 'the', 'LSTM', 'and', 'LSTM', '+', 'methods', 'and', 'our', 'methods', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['CC', 'DT', 'NNP', 'NNP', 'NN', 'NNS', 'RBR', 'IN', 'DT', 'IN', ':', 'NN', 'NN', 'IN', 'DT', 'DT', 'NNP', 'CC', 'NNP', 'NNP', 'NNS', 'CC', 'PRP$', 'NNS', '.']",25
sentence_compression,0,208,"Therefore , our method works reasonably well for both in - domain and out - ofdomain data .","['Therefore', ',', 'our', 'method', 'works', 'reasonably', 'well', 'for', 'both', 'in', '-', 'domain', 'and', 'out', '-', 'ofdomain', 'data', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP$', 'NN', 'VBZ', 'RB', 'RB', 'IN', 'DT', 'IN', ':', 'NN', 'CC', 'IN', ':', 'NN', 'NNS', '.']",18
sentence_compression,0,209,"We also notice that on Google News , adding the ILP layer decreased the sentence compression performance .","['We', 'also', 'notice', 'that', 'on', 'Google', 'News', ',', 'adding', 'the', 'ILP', 'layer', 'decreased', 'the', 'sentence', 'compression', 'performance', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'IN', 'IN', 'NNP', 'NNP', ',', 'VBG', 'DT', 'NNP', 'NN', 'VBD', 'DT', 'NN', 'NN', 'NN', '.']",18
sentence_compression,0,216,"We can see that in the in - domain setting , our method does not have any advantage over the LSTM + method .","['We', 'can', 'see', 'that', 'in', 'the', 'in', '-', 'domain', 'setting', ',', 'our', 'method', 'does', 'not', 'have', 'any', 'advantage', 'over', 'the', 'LSTM', '+', 'method', '.']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'DT', 'IN', 'DT', 'IN', ':', 'NN', 'NN', ',', 'PRP$', 'NN', 'VBZ', 'RB', 'VB', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'NN', '.']",24
sentence_compression,0,217,"But in the cross - domain setting , our method that uses ILP to impose syntax - based constraints clearly performs better than LSTM + when the amount of training data is relatively small .","['But', 'in', 'the', 'cross', '-', 'domain', 'setting', ',', 'our', 'method', 'that', 'uses', 'ILP', 'to', 'impose', 'syntax', '-', 'based', 'constraints', 'clearly', 'performs', 'better', 'than', 'LSTM', '+', 'when', 'the', 'amount', 'of', 'training', 'data', 'is', 'relatively', 'small', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['CC', 'IN', 'DT', 'NN', ':', 'NN', 'NN', ',', 'PRP$', 'NN', 'IN', 'VBZ', 'NNP', 'TO', 'VB', 'JJ', ':', 'VBN', 'NNS', 'RB', 'NNS', 'JJR', 'IN', 'NNP', 'NN', 'WRB', 'DT', 'NN', 'IN', 'NN', 'NNS', 'VBZ', 'RB', 'JJ', '.']",35
question_similarity,0,2,Tha3aroon at NSURL - 2019 Task 8 : Semantic Question Similarity in Arabic,"['Tha3aroon', 'at', 'NSURL', '-', '2019', 'Task', '8', ':', 'Semantic', 'Question', 'Similarity', 'in', 'Arabic']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NN', 'IN', 'NNP', ':', 'CD', 'NN', 'CD', ':', 'JJ', 'NNP', 'NNP', 'IN', 'NNP']",13
question_similarity,0,4,"In this paper , we describe our team 's effort on the semantic text question similarity task of NSURL 2019 .","['In', 'this', 'paper', ',', 'we', 'describe', 'our', 'team', ""'s"", 'effort', 'on', 'the', 'semantic', 'text', 'question', 'similarity', 'task', 'of', 'NSURL', '2019', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'PRP$', 'NN', 'POS', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', 'NN', 'IN', 'NNP', 'CD', '.']",21
question_similarity,0,10,Semantic Text Similarity ( STS ) problems are both real - life and challenging .,"['Semantic', 'Text', 'Similarity', '(', 'STS', ')', 'problems', 'are', 'both', 'real', '-', 'life', 'and', 'challenging', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNP', 'NNP', '(', 'NNP', ')', 'NNS', 'VBP', 'DT', 'JJ', ':', 'NN', 'CC', 'NN', '.']",15
question_similarity,0,11,"For example , in the paraphrase identification task , STS is used to predict if one sentence is a paraphrase of the other or not .","['For', 'example', ',', 'in', 'the', 'paraphrase', 'identification', 'task', ',', 'STS', 'is', 'used', 'to', 'predict', 'if', 'one', 'sentence', 'is', 'a', 'paraphrase', 'of', 'the', 'other', 'or', 'not', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ',', 'IN', 'DT', 'NN', 'NN', 'NN', ',', 'NNP', 'VBZ', 'VBN', 'TO', 'VB', 'IN', 'CD', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'JJ', 'CC', 'RB', '.']",26
question_similarity,0,14,A new task has been proposed by Mawdoo3 1 company with a new dataset provided by their data annotation team for Semantic Question Similarity ( SQS ) for the Arabic language .,"['A', 'new', 'task', 'has', 'been', 'proposed', 'by', 'Mawdoo3', '1', 'company', 'with', 'a', 'new', 'dataset', 'provided', 'by', 'their', 'data', 'annotation', 'team', 'for', 'Semantic', 'Question', 'Similarity', '(', 'SQS', ')', 'for', 'the', 'Arabic', 'language', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'VBN', 'VBN', 'IN', 'NNP', 'CD', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBN', 'IN', 'PRP$', 'NNS', 'NN', 'NN', 'IN', 'JJ', 'NNP', 'NNP', '(', 'NNP', ')', 'IN', 'DT', 'NNP', 'NN', '.']",32
question_similarity,0,15,"SQS is a variant of STS , which aims to compare a pair of questions and determine whether they have the same meaning or not .","['SQS', 'is', 'a', 'variant', 'of', 'STS', ',', 'which', 'aims', 'to', 'compare', 'a', 'pair', 'of', 'questions', 'and', 'determine', 'whether', 'they', 'have', 'the', 'same', 'meaning', 'or', 'not', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'IN', 'NNP', ',', 'WDT', 'VBZ', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNS', 'CC', 'VB', 'IN', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'CC', 'RB', '.']",26
question_similarity,0,19,We then build a neural network model with four components .,"['We', 'then', 'build', 'a', 'neural', 'network', 'model', 'with', 'four', 'components', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VB', 'DT', 'JJ', 'NN', 'NN', 'IN', 'CD', 'NNS', '.']",11
question_similarity,0,20,The model uses ELMo ( which stands for Embeddings from Language Models ) pre-trained contextual embeddings as an input and builds sequence representation vectors that are used to predict the relation between the question pairs .,"['The', 'model', 'uses', 'ELMo', '(', 'which', 'stands', 'for', 'Embeddings', 'from', 'Language', 'Models', ')', 'pre-trained', 'contextual', 'embeddings', 'as', 'an', 'input', 'and', 'builds', 'sequence', 'representation', 'vectors', 'that', 'are', 'used', 'to', 'predict', 'the', 'relation', 'between', 'the', 'question', 'pairs', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'NNP', '(', 'WDT', 'VBZ', 'IN', 'NNS', 'IN', 'NNP', 'NNP', ')', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CC', 'NNS', 'NN', 'NN', 'NNS', 'WDT', 'VBP', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'NNS', '.']",36
question_similarity,0,85,All experiments discussed in this work have been done on the Google Colab 7 environment using Tesla T4 GPU accelerator with the following hyperparameters :,"['All', 'experiments', 'discussed', 'in', 'this', 'work', 'have', 'been', 'done', 'on', 'the', 'Google', 'Colab', '7', 'environment', 'using', 'Tesla', 'T4', 'GPU', 'accelerator', 'with', 'the', 'following', 'hyperparameters', ':']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'VBP', 'VBN', 'VBN', 'IN', 'DT', 'NNP', 'NNP', 'CD', 'NN', 'VBG', 'NNP', 'NNP', 'NNP', 'NN', 'IN', 'DT', 'VBG', 'NNS', ':']",25
question_similarity,0,100,"The tables show that while GRU cells are the most efficient , the ON - LSTM cells ( with chunk size 8 ) are the most effective ( in terms of all considered measures ) .","['The', 'tables', 'show', 'that', 'while', 'GRU', 'cells', 'are', 'the', 'most', 'efficient', ',', 'the', 'ON', '-', 'LSTM', 'cells', '(', 'with', 'chunk', 'size', '8', ')', 'are', 'the', 'most', 'effective', '(', 'in', 'terms', 'of', 'all', 'considered', 'measures', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O']","['DT', 'NNS', 'VBP', 'IN', 'IN', 'NNP', 'NNS', 'VBP', 'DT', 'RBS', 'JJ', ',', 'DT', 'NNP', ':', 'NNP', 'NNS', '(', 'IN', 'NN', 'NN', 'CD', ')', 'VBP', 'DT', 'RBS', 'JJ', '(', 'IN', 'NNS', 'IN', 'DT', 'VBN', 'NNS', ')', '.']",36
question_similarity,0,101,Effect of Data Augmentation,"['Effect', 'of', 'Data', 'Augmentation']","['B-n', 'I-n', 'I-n', 'I-n']","['NN', 'IN', 'NNP', 'NNP']",4
question_similarity,0,106,The tables show that each augmentation step affects the model 's efficiency negatively .,"['The', 'tables', 'show', 'that', 'each', 'augmentation', 'step', 'affects', 'the', 'model', ""'s"", 'efficiency', 'negatively', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O']","['DT', 'NNS', 'VBP', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'DT', 'NN', 'POS', 'NN', 'RB', '.']",14
question_similarity,0,108,"On the other hand , not each increment step has a positive effect on the model 's effectiveness .","['On', 'the', 'other', 'hand', ',', 'not', 'each', 'increment', 'step', 'has', 'a', 'positive', 'effect', 'on', 'the', 'model', ""'s"", 'effectiveness', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'RB', 'DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'POS', 'NN', '.']",19
question_similarity,0,114,"For example , using pre-trained FastText embeddings as an input to our model yields worse F1score on both public and private leaderboards with 94.254 and 93.118 , respectively , compared with the ELMo contextual embeddings model .","['For', 'example', ',', 'using', 'pre-trained', 'FastText', 'embeddings', 'as', 'an', 'input', 'to', 'our', 'model', 'yields', 'worse', 'F1score', 'on', 'both', 'public', 'and', 'private', 'leaderboards', 'with', '94.254', 'and', '93.118', ',', 'respectively', ',', 'compared', 'with', 'the', 'ELMo', 'contextual', 'embeddings', 'model', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', ',', 'VBG', 'JJ', 'NNP', 'NNS', 'IN', 'DT', 'NN', 'TO', 'PRP$', 'NN', 'NNS', 'JJR', 'NN', 'IN', 'DT', 'JJ', 'CC', 'JJ', 'NNS', 'IN', 'CD', 'CC', 'CD', ',', 'RB', ',', 'VBN', 'IN', 'DT', 'NNP', 'JJ', 'NNS', 'NN', '.']",37
question_similarity,0,116,"However , the sequence weighted attention gives better results by about 1 point of the F1-score .","['However', ',', 'the', 'sequence', 'weighted', 'attention', 'gives', 'better', 'results', 'by', 'about', '1', 'point', 'of', 'the', 'F1-score', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['RB', ',', 'DT', 'NN', 'VBD', 'NN', 'VBZ', 'JJR', 'NNS', 'IN', 'IN', 'CD', 'NN', 'IN', 'DT', 'NNP', '.']",17
question_similarity,0,117,"Moreover , an attempt to overcome the weakness of the Arabic ELMo model is done by translating the data to English using Google Translate 8 and treating the problem as an English SQS problem instead , but the results are much worse with 88.868 and 87.504 F1 - scores on public and private leaderboards , respectively .","['Moreover', ',', 'an', 'attempt', 'to', 'overcome', 'the', 'weakness', 'of', 'the', 'Arabic', 'ELMo', 'model', 'is', 'done', 'by', 'translating', 'the', 'data', 'to', 'English', 'using', 'Google', 'Translate', '8', 'and', 'treating', 'the', 'problem', 'as', 'an', 'English', 'SQS', 'problem', 'instead', ',', 'but', 'the', 'results', 'are', 'much', 'worse', 'with', '88.868', 'and', '87.504', 'F1', '-', 'scores', 'on', 'public', 'and', 'private', 'leaderboards', ',', 'respectively', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['RB', ',', 'DT', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'NN', 'VBZ', 'VBN', 'IN', 'VBG', 'DT', 'NN', 'TO', 'VB', 'VBG', 'NNP', 'NNP', 'CD', 'CC', 'VBG', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'NN', 'RB', ',', 'CC', 'DT', 'NNS', 'VBP', 'JJ', 'JJR', 'IN', 'CD', 'CC', 'CD', 'NNP', ':', 'NNS', 'IN', 'JJ', 'CC', 'JJ', 'NNS', ',', 'RB', '.']",57
temporal_information_extraction,1,2,A Structured Learning Approach to Temporal Relation Extraction,"['A', 'Structured', 'Learning', 'Approach', 'to', 'Temporal', 'Relation', 'Extraction']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['DT', 'NNP', 'NNP', 'NNP', 'TO', 'NNP', 'NNP', 'NNP']",8
temporal_information_extraction,1,4,Identifying temporal relations between events is an essential step towards natural language understanding .,"['Identifying', 'temporal', 'relations', 'between', 'events', 'is', 'an', 'essential', 'step', 'towards', 'natural', 'language', 'understanding', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', 'NNS', 'IN', 'NNS', 'VBZ', 'DT', 'JJ', 'NN', 'NNS', 'JJ', 'NN', 'NN', '.']",14
temporal_information_extraction,1,13,"The fundamental tasks in temporal processing , as identified in the TE workshops , are 1 ) time expression ( the so - called "" timex "" ) extraction and normalization and 2 ) temporal relation ( also known as TLINKs ) extraction .","['The', 'fundamental', 'tasks', 'in', 'temporal', 'processing', ',', 'as', 'identified', 'in', 'the', 'TE', 'workshops', ',', 'are', '1', ')', 'time', 'expression', '(', 'the', 'so', '-', 'called', '""', 'timex', '""', ')', 'extraction', 'and', 'normalization', 'and', '2', ')', 'temporal', 'relation', '(', 'also', 'known', 'as', 'TLINKs', ')', 'extraction', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NNS', 'IN', 'JJ', 'NN', ',', 'IN', 'VBN', 'IN', 'DT', 'NNP', 'NNS', ',', 'VBP', 'CD', ')', 'NN', 'NN', '(', 'DT', 'RB', ':', 'VBN', 'NN', 'NN', 'NNP', ')', 'NN', 'CC', 'NN', 'CC', 'CD', ')', 'JJ', 'NN', '(', 'RB', 'VBN', 'IN', 'NNP', ')', 'NN', '.']",44
temporal_information_extraction,1,17,"In this paper , we propose a structured learning approach to temporal relation extraction , where local models are updated based on feedback from global inferences .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'structured', 'learning', 'approach', 'to', 'temporal', 'relation', 'extraction', ',', 'where', 'local', 'models', 'are', 'updated', 'based', 'on', 'feedback', 'from', 'global', 'inferences', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NN', 'TO', 'JJ', 'NN', 'NN', ',', 'WRB', 'JJ', 'NNS', 'VBP', 'VBN', 'VBN', 'IN', 'NN', 'IN', 'JJ', 'NNS', '.']",27
temporal_information_extraction,1,18,"The structured approach also gives rise to a semisupervised method , making it possible to take advantage of the readily available unlabeled data .","['The', 'structured', 'approach', 'also', 'gives', 'rise', 'to', 'a', 'semisupervised', 'method', ',', 'making', 'it', 'possible', 'to', 'take', 'advantage', 'of', 'the', 'readily', 'available', 'unlabeled', 'data', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'RB', 'VBZ', 'NN', 'TO', 'DT', 'JJ', 'NN', ',', 'VBG', 'PRP', 'JJ', 'TO', 'VB', 'NN', 'IN', 'DT', 'NN', 'JJ', 'JJ', 'NNS', '.']",24
temporal_information_extraction,1,199,The first is the regularized averaged perceptron ( AP ) implemented in the LBJava package and is a local method .,"['The', 'first', 'is', 'the', 'regularized', 'averaged', 'perceptron', '(', 'AP', ')', 'implemented', 'in', 'the', 'LBJava', 'package', 'and', 'is', 'a', 'local', 'method', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'VBZ', 'DT', 'JJ', 'VBD', 'NN', '(', 'NNP', ')', 'VBN', 'IN', 'DT', 'NNP', 'NN', 'CC', 'VBZ', 'DT', 'JJ', 'NN', '.']",21
temporal_information_extraction,1,200,"On top of the first baseline , we performed global inference in Eq .","['On', 'top', 'of', 'the', 'first', 'baseline', ',', 'we', 'performed', 'global', 'inference', 'in', 'Eq', '.']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O']","['IN', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'PRP', 'VBD', 'JJ', 'NN', 'IN', 'NNP', '.']",14
temporal_information_extraction,1,202,"Both of them used the same feature set ( i.e. , as designed in ) as in the proposed structured perceptron ( SP ) and CoDL for fair comparisons .","['Both', 'of', 'them', 'used', 'the', 'same', 'feature', 'set', '(', 'i.e.', ',', 'as', 'designed', 'in', ')', 'as', 'in', 'the', 'proposed', 'structured', 'perceptron', '(', 'SP', ')', 'and', 'CoDL', 'for', 'fair', 'comparisons', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O']","['DT', 'IN', 'PRP', 'VBD', 'DT', 'JJ', 'NN', 'VBN', '(', 'JJ', ',', 'IN', 'VBN', 'IN', ')', 'IN', 'IN', 'DT', 'VBN', 'VBN', 'NN', '(', 'NNP', ')', 'CC', 'NNP', 'IN', 'JJ', 'NNS', '.']",30
temporal_information_extraction,1,205,TE3 Task C - Relation Only,"['TE3', 'Task', 'C', '-', 'Relation', 'Only']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', ':', 'NN', 'RB']",6
temporal_information_extraction,1,214,"We can see that UT - Time is about 3 % better than AP - 1 in the absolute value of F 1 , which is expected since UTTime included more advanced features derived from syntactic parse trees .","['We', 'can', 'see', 'that', 'UT', '-', 'Time', 'is', 'about', '3', '%', 'better', 'than', 'AP', '-', '1', 'in', 'the', 'absolute', 'value', 'of', 'F', '1', ',', 'which', 'is', 'expected', 'since', 'UTTime', 'included', 'more', 'advanced', 'features', 'derived', 'from', 'syntactic', 'parse', 'trees', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'MD', 'VB', 'DT', 'NNP', ':', 'NN', 'VBZ', 'RB', 'CD', 'NN', 'JJR', 'IN', 'NNP', ':', 'CD', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'CD', ',', 'WDT', 'VBZ', 'VBN', 'IN', 'NNP', 'VBD', 'RBR', 'JJ', 'NNS', 'VBN', 'IN', 'JJ', 'NN', 'NNS', '.']",39
temporal_information_extraction,1,220,"On top of AP - 2 , a global inference step enforcing symmetry and transitivity constraints ( "" AP + ILP "" ) can further improve the F 1 score by 9.3 % , which is consistent with previous observations .","['On', 'top', 'of', 'AP', '-', '2', ',', 'a', 'global', 'inference', 'step', 'enforcing', 'symmetry', 'and', 'transitivity', 'constraints', '(', '""', 'AP', '+', 'ILP', '""', ')', 'can', 'further', 'improve', 'the', 'F', '1', 'score', 'by', '9.3', '%', ',', 'which', 'is', 'consistent', 'with', 'previous', 'observations', '.']","['B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', 'IN', 'NNP', ':', 'CD', ',', 'DT', 'JJ', 'NN', 'NN', 'VBG', 'NN', 'CC', 'NN', 'NNS', '(', 'JJ', 'NNP', 'NNP', 'NNP', 'NNP', ')', 'MD', 'RB', 'VB', 'DT', 'NNP', 'CD', 'NN', 'IN', 'CD', 'NN', ',', 'WDT', 'VBZ', 'JJ', 'IN', 'JJ', 'NNS', '.']",41
temporal_information_extraction,1,221,"SP + ILP further improved the performance in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score of 67.2 % .","['SP', '+', 'ILP', 'further', 'improved', 'the', 'performance', 'in', 'precision', ',', 'recall', ',', 'and', 'F', '1', 'significantly', '(', 'per', 'the', 'McNemar', ""'s"", 'test', 'with', 'p', '<', '0.0005', ')', ',', 'reaching', 'an', 'F', '1', 'score', 'of', '67.2', '%', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['NNP', 'NNP', 'NNP', 'RB', 'VBD', 'DT', 'NN', 'IN', 'NN', ',', 'NN', ',', 'CC', 'NNP', 'CD', 'RB', '(', 'IN', 'DT', 'NNP', 'POS', 'NN', 'IN', 'NN', '$', 'CD', ')', ',', 'VBG', 'DT', 'NN', 'CD', 'NN', 'IN', 'CD', 'NN', '.']",37
temporal_information_extraction,1,223,TE3 Task C,"['TE3', 'Task', 'C']","['B-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP']",3
temporal_information_extraction,1,232,"The improvement of SP + ILP ( line 4 ) over AP ( line 2 ) was small and AP + ILP ( line 3 ) was even worse than AP , which necessitates the use of a better approach towards vague TLINKs .","['The', 'improvement', 'of', 'SP', '+', 'ILP', '(', 'line', '4', ')', 'over', 'AP', '(', 'line', '2', ')', 'was', 'small', 'and', 'AP', '+', 'ILP', '(', 'line', '3', ')', 'was', 'even', 'worse', 'than', 'AP', ',', 'which', 'necessitates', 'the', 'use', 'of', 'a', 'better', 'approach', 'towards', 'vague', 'TLINKs', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'NNP', 'NNP', 'NNP', '(', 'NN', 'CD', ')', 'IN', 'NNP', '(', 'NN', 'CD', ')', 'VBD', 'JJ', 'CC', 'NNP', 'NNP', 'NNP', '(', 'NN', 'CD', ')', 'VBD', 'RB', 'JJR', 'IN', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'JJR', 'NN', 'NNS', 'JJ', 'NNP', '.']",44
temporal_information_extraction,1,233,"By applying the postfiltering method proposed in Sec. 4 , we were able to achieve better performances using SP + ILP ( line 5 ) , which shows the effectiveness of this strategy .","['By', 'applying', 'the', 'postfiltering', 'method', 'proposed', 'in', 'Sec.', '4', ',', 'we', 'were', 'able', 'to', 'achieve', 'better', 'performances', 'using', 'SP', '+', 'ILP', '(', 'line', '5', ')', ',', 'which', 'shows', 'the', 'effectiveness', 'of', 'this', 'strategy', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBG', 'DT', 'NN', 'NN', 'VBN', 'IN', 'NNP', 'CD', ',', 'PRP', 'VBD', 'JJ', 'TO', 'VB', 'JJR', 'NNS', 'VBG', 'NNP', 'NNP', 'NNP', '(', 'NN', 'CD', ')', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",34
temporal_information_extraction,1,239,Comparison with CAEVO,"['Comparison', 'with', 'CAEVO']","['B-n', 'I-n', 'I-n']","['NNP', 'IN', 'NNP']",3
temporal_information_extraction,1,249,"SP + ILP outperformed CAEVO and if additional unlabeled dataset TE3 - SV was used , CoDL + ILP achieved the best score with a relative improvement in F 1 score being 6.3 % .","['SP', '+', 'ILP', 'outperformed', 'CAEVO', 'and', 'if', 'additional', 'unlabeled', 'dataset', 'TE3', '-', 'SV', 'was', 'used', ',', 'CoDL', '+', 'ILP', 'achieved', 'the', 'best', 'score', 'with', 'a', 'relative', 'improvement', 'in', 'F', '1', 'score', 'being', '6.3', '%', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NNP', 'NNP', 'VBD', 'NNP', 'CC', 'IN', 'JJ', 'JJ', 'NN', 'NNP', ':', 'NN', 'VBD', 'VBN', ',', 'NNP', 'NNP', 'NNP', 'VBD', 'DT', 'JJS', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'CD', 'NN', 'VBG', 'CD', 'NN', '.']",35
temporal_information_extraction,0,4,"We present CATENA , a sieve - based system to perform temporal and causal relation extraction and classification from English texts , exploiting the interaction between the temporal and the causal model .","['We', 'present', 'CATENA', ',', 'a', 'sieve', '-', 'based', 'system', 'to', 'perform', 'temporal', 'and', 'causal', 'relation', 'extraction', 'and', 'classification', 'from', 'English', 'texts', ',', 'exploiting', 'the', 'interaction', 'between', 'the', 'temporal', 'and', 'the', 'causal', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'JJ', 'NNP', ',', 'DT', 'NN', ':', 'VBN', 'NN', 'TO', 'VB', 'JJ', 'CC', 'JJ', 'NN', 'NN', 'CC', 'NN', 'IN', 'NNP', 'NN', ',', 'VBG', 'DT', 'NN', 'IN', 'DT', 'JJ', 'CC', 'DT', 'NN', 'NN', '.']",33
temporal_information_extraction,0,19,"The CATENA system includes two main classification modules , one for temporal and the other for causal relations between events .","['The', 'CATENA', 'system', 'includes', 'two', 'main', 'classification', 'modules', ',', 'one', 'for', 'temporal', 'and', 'the', 'other', 'for', 'causal', 'relations', 'between', 'events', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O']","['DT', 'NNP', 'NN', 'VBZ', 'CD', 'JJ', 'NN', 'NNS', ',', 'CD', 'IN', 'JJ', 'CC', 'DT', 'JJ', 'IN', 'JJ', 'NNS', 'IN', 'NNS', '.']",21
temporal_information_extraction,0,20,"As shown in , they both take as input a document annotated with the so - called temporal entities according to TimeML guidelines , including the document creation time ( DCT ) , events and time expressions ( timexes ) .","['As', 'shown', 'in', ',', 'they', 'both', 'take', 'as', 'input', 'a', 'document', 'annotated', 'with', 'the', 'so', '-', 'called', 'temporal', 'entities', 'according', 'to', 'TimeML', 'guidelines', ',', 'including', 'the', 'document', 'creation', 'time', '(', 'DCT', ')', ',', 'events', 'and', 'time', 'expressions', '(', 'timexes', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'IN', ',', 'PRP', 'DT', 'VBP', 'IN', 'NN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'RB', ':', 'VBN', 'JJ', 'NNS', 'VBG', 'TO', 'NNP', 'NNS', ',', 'VBG', 'DT', 'NN', 'NN', 'NN', '(', 'NNP', ')', ',', 'NNS', 'CC', 'NN', 'NNS', '(', 'NNS', ')', '.']",41
temporal_information_extraction,0,21,"The output is the same document with temporal links ( TLINKs ) set between pairs of temporal entities , each assigned to one of the TimeML temporal relation types , such as or SIMULTANEOUS , which denotes the temporal ordering .","['The', 'output', 'is', 'the', 'same', 'document', 'with', 'temporal', 'links', '(', 'TLINKs', ')', 'set', 'between', 'pairs', 'of', 'temporal', 'entities', ',', 'each', 'assigned', 'to', 'one', 'of', 'the', 'TimeML', 'temporal', 'relation', 'types', ',', 'such', 'as', 'or', 'SIMULTANEOUS', ',', 'which', 'denotes', 'the', 'temporal', 'ordering', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', '(', 'NNP', ')', 'VBD', 'IN', 'NNS', 'IN', 'JJ', 'NNS', ',', 'DT', 'VBD', 'TO', 'CD', 'IN', 'DT', 'NNP', 'JJ', 'NN', 'NNS', ',', 'JJ', 'IN', 'CC', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', '.']",41
temporal_information_extraction,0,22,The document is also annotated with causal relations ( CLINKs ) between event pairs .,"['The', 'document', 'is', 'also', 'annotated', 'with', 'causal', 'relations', '(', 'CLINKs', ')', 'between', 'event', 'pairs', '.']","['O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'RB', 'VBN', 'IN', 'NN', 'NNS', '(', 'NNP', ')', 'IN', 'NN', 'NNS', '.']",15
temporal_information_extraction,0,23,"The modules for temporal and causal relation classification rely both on a sieve - based architecture , in which the remaining unlabelled pairs - after running a rule - based component and / or a transitive reasoner are fed into a supervised classifier .","['The', 'modules', 'for', 'temporal', 'and', 'causal', 'relation', 'classification', 'rely', 'both', 'on', 'a', 'sieve', '-', 'based', 'architecture', ',', 'in', 'which', 'the', 'remaining', 'unlabelled', 'pairs', '-', 'after', 'running', 'a', 'rule', '-', 'based', 'component', 'and', '/', 'or', 'a', 'transitive', 'reasoner', 'are', 'fed', 'into', 'a', 'supervised', 'classifier', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'JJ', 'CC', 'JJ', 'NN', 'NN', 'RB', 'DT', 'IN', 'DT', 'NN', ':', 'VBN', 'NN', ',', 'IN', 'WDT', 'DT', 'VBG', 'JJ', 'NNS', ':', 'IN', 'VBG', 'DT', 'NN', ':', 'VBN', 'NN', 'CC', 'NN', 'CC', 'DT', 'JJ', 'NN', 'VBP', 'VBN', 'IN', 'DT', 'VBN', 'NN', '.']",44
temporal_information_extraction,0,150,"The evaluation shows that CATENA is the best performing system in both tasks , even if in Task C best precision and best recall are yielded by and , respectively .","['The', 'evaluation', 'shows', 'that', 'CATENA', 'is', 'the', 'best', 'performing', 'system', 'in', 'both', 'tasks', ',', 'even', 'if', 'in', 'Task', 'C', 'best', 'precision', 'and', 'best', 'recall', 'are', 'yielded', 'by', 'and', ',', 'respectively', '.']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'VBZ', 'IN', 'NNP', 'VBZ', 'DT', 'JJS', 'NN', 'NN', 'IN', 'DT', 'NNS', ',', 'RB', 'IN', 'IN', 'NNP', 'NNP', 'JJS', 'NN', 'CC', 'JJS', 'NN', 'VBP', 'VBN', 'IN', 'CC', ',', 'RB', '.']",31
temporal_information_extraction,0,156,"If we consider the different entity pairs , CATENA performs best on timex - timex and event - timex relations , while CAEVO still achieves the best results on event - DCT and event - event pairs .","['If', 'we', 'consider', 'the', 'different', 'entity', 'pairs', ',', 'CATENA', 'performs', 'best', 'on', 'timex', '-', 'timex', 'and', 'event', '-', 'timex', 'relations', ',', 'while', 'CAEVO', 'still', 'achieves', 'the', 'best', 'results', 'on', 'event', '-', 'DCT', 'and', 'event', '-', 'event', 'pairs', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'NNS', ',', 'NNP', 'VBZ', 'JJS', 'IN', 'NN', ':', 'NN', 'CC', 'NN', ':', 'NN', 'NNS', ',', 'IN', 'NNP', 'RB', 'VBZ', 'DT', 'JJS', 'NNS', 'IN', 'NN', ':', 'NN', 'CC', 'NN', ':', 'NN', 'NNS', '.']",38
temporal_information_extraction,0,160,"As expected , running a transitive closure module after the temporal rule - based sieve ( RB + TR ) results in improving recall , but the over all performance is still lacking ( less than .30 F1-score ) .","['As', 'expected', ',', 'running', 'a', 'transitive', 'closure', 'module', 'after', 'the', 'temporal', 'rule', '-', 'based', 'sieve', '(', 'RB', '+', 'TR', ')', 'results', 'in', 'improving', 'recall', ',', 'but', 'the', 'over', 'all', 'performance', 'is', 'still', 'lacking', '(', 'less', 'than', '.30', 'F1-score', ')', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBN', ',', 'VBG', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'VBN', 'NN', '(', 'NNP', 'NNP', 'NNP', ')', 'NNS', 'IN', 'VBG', 'NN', ',', 'CC', 'DT', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'VBG', '(', 'JJR', 'IN', 'JJR', 'NN', ')', '.']",40
temporal_information_extraction,0,161,Combining rule - based and machine - learned sieves ( RB + ML ) yields a slight improvement compared with enabling only the machine - learned sieve in the system ( ML ) .,"['Combining', 'rule', '-', 'based', 'and', 'machine', '-', 'learned', 'sieves', '(', 'RB', '+', 'ML', ')', 'yields', 'a', 'slight', 'improvement', 'compared', 'with', 'enabling', 'only', 'the', 'machine', '-', 'learned', 'sieve', 'in', 'the', 'system', '(', 'ML', ')', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'NN', ':', 'VBN', 'CC', 'NN', ':', 'VBN', 'NNS', '(', 'NNP', 'NNP', 'NNP', ')', 'VBZ', 'DT', 'JJ', 'NN', 'VBN', 'IN', 'VBG', 'RB', 'DT', 'NN', ':', 'JJ', 'NN', 'IN', 'DT', 'NN', '(', 'NNP', ')', '.']",34
temporal_information_extraction,0,162,Introducing the temporal reasoner module between the two sieves ( RB + TR + ML ) proves to be even more beneficial .,"['Introducing', 'the', 'temporal', 'reasoner', 'module', 'between', 'the', 'two', 'sieves', '(', 'RB', '+', 'TR', '+', 'ML', ')', 'proves', 'to', 'be', 'even', 'more', 'beneficial', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['VBG', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'CD', 'NNS', '(', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', ')', 'VBZ', 'TO', 'VB', 'RB', 'RBR', 'JJ', '.']",23
phrase_grounding,0,2,Multi - level Multimodal Common Semantic Space for Image - Phrase Grounding,"['Multi', '-', 'level', 'Multimodal', 'Common', 'Semantic', 'Space', 'for', 'Image', '-', 'Phrase', 'Grounding']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NN', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', ':', 'NN', 'VBG']",12
phrase_grounding,0,4,We address the problem of phrase grounding by learning a multi - level common semantic space shared by the textual and visual modalities .,"['We', 'address', 'the', 'problem', 'of', 'phrase', 'grounding', 'by', 'learning', 'a', 'multi', '-', 'level', 'common', 'semantic', 'space', 'shared', 'by', 'the', 'textual', 'and', 'visual', 'modalities', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'NN', 'NN', 'IN', 'VBG', 'DT', 'JJ', ':', 'NN', 'JJ', 'JJ', 'NN', 'VBN', 'IN', 'DT', 'JJ', 'CC', 'JJ', 'NNS', '.']",24
phrase_grounding,0,24,"In this work , we propose to explicitly learn a non-linear mapping of the visual and textual modalities into a common space , and do so at different granularity for each domain .","['In', 'this', 'work', ',', 'we', 'propose', 'to', 'explicitly', 'learn', 'a', 'non-linear', 'mapping', 'of', 'the', 'visual', 'and', 'textual', 'modalities', 'into', 'a', 'common', 'space', ',', 'and', 'do', 'so', 'at', 'different', 'granularity', 'for', 'each', 'domain', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'TO', 'RB', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'CC', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', ',', 'CC', 'VBP', 'RB', 'IN', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",33
phrase_grounding,0,26,"This common space mapping is trained with weak supervision and exploited at test - time with a multi - level multimodal attention mechanism , where a natural formalism for computing attention heatmaps at each level , attended features and pertinence scoring , enables us to solve the phrase grounding task elegantly and effectively .","['This', 'common', 'space', 'mapping', 'is', 'trained', 'with', 'weak', 'supervision', 'and', 'exploited', 'at', 'test', '-', 'time', 'with', 'a', 'multi', '-', 'level', 'multimodal', 'attention', 'mechanism', ',', 'where', 'a', 'natural', 'formalism', 'for', 'computing', 'attention', 'heatmaps', 'at', 'each', 'level', ',', 'attended', 'features', 'and', 'pertinence', 'scoring', ',', 'enables', 'us', 'to', 'solve', 'the', 'phrase', 'grounding', 'task', 'elegantly', 'and', 'effectively', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', 'NN', 'CC', 'VBN', 'IN', 'JJS', ':', 'NN', 'IN', 'DT', 'JJ', ':', 'NN', 'JJ', 'NN', 'NN', ',', 'WRB', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'NN', 'NNS', 'IN', 'DT', 'NN', ',', 'VBD', 'NNS', 'CC', 'NN', 'NN', ',', 'VBZ', 'PRP', 'TO', 'VB', 'DT', 'NN', 'VBG', 'NN', 'RB', 'CC', 'RB', '.']",54
phrase_grounding,0,168,"We use a batch size of B = 32 , where for a batch of image - caption pairs each image ( caption ) is only related to one caption ( image ) .","['We', 'use', 'a', 'batch', 'size', 'of', 'B', '=', '32', ',', 'where', 'for', 'a', 'batch', 'of', 'image', '-', 'caption', 'pairs', 'each', 'image', '(', 'caption', ')', 'is', 'only', 'related', 'to', 'one', 'caption', '(', 'image', ')', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'NNP', 'NNP', 'CD', ',', 'WRB', 'IN', 'DT', 'NN', 'IN', 'NN', ':', 'NN', 'VBZ', 'DT', 'NN', '(', 'NN', ')', 'VBZ', 'RB', 'JJ', 'TO', 'CD', 'NN', '(', 'NN', ')', '.']",34
phrase_grounding,0,169,Image - caption pairs are sampled randomly with a uniform distribution .,"['Image', '-', 'caption', 'pairs', 'are', 'sampled', 'randomly', 'with', 'a', 'uniform', 'distribution', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', ':', 'NN', 'NNS', 'VBP', 'VBN', 'RB', 'IN', 'DT', 'JJ', 'NN', '.']",12
phrase_grounding,0,170,We train the network for 20 epochs with the Adam optimizer with lr = 0.001 where the learning rate is divided by 2 once at the 10 - th epoch and again at the 15 - th epoch .,"['We', 'train', 'the', 'network', 'for', '20', 'epochs', 'with', 'the', 'Adam', 'optimizer', 'with', 'lr', '=', '0.001', 'where', 'the', 'learning', 'rate', 'is', 'divided', 'by', '2', 'once', 'at', 'the', '10', '-', 'th', 'epoch', 'and', 'again', 'at', 'the', '15', '-', 'th', 'epoch', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'CD', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'IN', 'JJ', 'NN', 'CD', 'WRB', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', 'RB', 'IN', 'DT', 'CD', ':', 'NN', 'NN', 'CC', 'RB', 'IN', 'DT', 'CD', ':', 'NN', 'NN', '.']",39
phrase_grounding,0,171,We use D = 1024 for common space mapping dimension and ? = 0.25 for Leaky ReLU in the non-linear mappings .,"['We', 'use', 'D', '=', '1024', 'for', 'common', 'space', 'mapping', 'dimension', 'and', '?', '=', '0.25', 'for', 'Leaky', 'ReLU', 'in', 'the', 'non-linear', 'mappings', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', 'NN', 'CD', 'IN', 'JJ', 'NN', 'NN', 'NN', 'CC', '.', '$', 'CD', 'IN', 'NNP', 'NNP', 'IN', 'DT', 'JJ', 'NNS', '.']",22
phrase_grounding,0,172,We regularize weights of the mappings with l 2 regularization with reg value = 0.0005 .,"['We', 'regularize', 'weights', 'of', 'the', 'mappings', 'with', 'l', '2', 'regularization', 'with', 'reg', 'value', '=', '0.0005', '.']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'JJ', 'CD', 'NN', 'IN', 'JJ', 'NN', 'VBD', 'CD', '.']",16
phrase_grounding,0,173,"For VGG , we take outputs from { conv 4 1 , conv 4 3 , conv5 1 , conv5 3 } and map to semantic feature maps with dimension 18181024 , and for PNAS - Net we take outputs from { Cell 5 , Cell 7 , Cell 9 , Cell 11 } pointing game accuracy attention correctness Ours Ours Ours Ours Class","['For', 'VGG', ',', 'we', 'take', 'outputs', 'from', '{', 'conv', '4', '1', ',', 'conv', '4', '3', ',', 'conv5', '1', ',', 'conv5', '3', '}', 'and', 'map', 'to', 'semantic', 'feature', 'maps', 'with', 'dimension', '18181024', ',', 'and', 'for', 'PNAS', '-', 'Net', 'we', 'take', 'outputs', 'from', '{', 'Cell', '5', ',', 'Cell', '7', ',', 'Cell', '9', ',', 'Cell', '11', '}', 'pointing', 'game', 'accuracy', 'attention', 'correctness', 'Ours', 'Ours', 'Ours', 'Ours', 'Class']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ',', 'PRP', 'VBP', 'NNS', 'IN', '(', 'NN', 'CD', 'CD', ',', 'VBD', 'CD', 'CD', ',', 'NN', 'CD', ',', 'NN', 'CD', ')', 'CC', '$', 'TO', 'JJ', 'NN', 'NNS', 'IN', 'NN', 'CD', ',', 'CC', 'IN', 'NNP', ':', 'NN', 'PRP', 'VBP', 'NNS', 'IN', '(', 'NNP', 'CD', ',', 'NNP', 'CD', ',', 'NNP', 'CD', ',', 'NNP', 'CD', ')', 'VBG', 'NN', 'NN', 'NN', 'NN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']",64
phrase_grounding,0,176,Both visual and textual networks weights are fixed during training and only common space mapping weights are trainable .,"['Both', 'visual', 'and', 'textual', 'networks', 'weights', 'are', 'fixed', 'during', 'training', 'and', 'only', 'common', 'space', 'mapping', 'weights', 'are', 'trainable', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O']","['DT', 'JJ', 'CC', 'JJ', 'NNS', 'NNS', 'VBP', 'VBN', 'IN', 'NN', 'CC', 'RB', 'JJ', 'NN', 'NN', 'NNS', 'VBP', 'JJ', '.']",19
phrase_grounding,0,187,The results show that our method significantly outperforms all state - of - the - art methods in all conditions and all datasets .,"['The', 'results', 'show', 'that', 'our', 'method', 'significantly', 'outperforms', 'all', 'state', '-', 'of', '-', 'the', '-', 'art', 'methods', 'in', 'all', 'conditions', 'and', 'all', 'datasets', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'IN', 'PRP$', 'NN', 'RB', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'DT', 'NNS', 'CC', 'DT', 'NNS', '.']",24
phrase_grounding,0,188,"For fair comparison with To get a deeper understanding of our model , we first report in category - wise pointing game accuracy and attention correctness ( percentage of the heatmap falling into the ground truth bounding box ) and compare with the state - of - the - art method on Flickr30 k .","['For', 'fair', 'comparison', 'with', 'To', 'get', 'a', 'deeper', 'understanding', 'of', 'our', 'model', ',', 'we', 'first', 'report', 'in', 'category', '-', 'wise', 'pointing', 'game', 'accuracy', 'and', 'attention', 'correctness', '(', 'percentage', 'of', 'the', 'heatmap', 'falling', 'into', 'the', 'ground', 'truth', 'bounding', 'box', ')', 'and', 'compare', 'with', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'method', 'on', 'Flickr30', 'k', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'JJ', 'NN', 'IN', 'TO', 'VB', 'DT', 'JJR', 'NN', 'IN', 'PRP$', 'NN', ',', 'PRP', 'RB', 'VBP', 'IN', 'NN', ':', 'NN', 'VBG', 'NN', 'NN', 'CC', 'NN', 'NN', '(', 'NN', 'IN', 'DT', 'NN', 'VBG', 'IN', 'DT', 'NN', 'NN', 'VBG', 'NN', ')', 'CC', 'NN', 'IN', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'NNP', 'NN', '.']",55
phrase_grounding,0,189,We observe that our method obtains a higher performance on almost all categories even when VGG16 is used as the visual backbone .,"['We', 'observe', 'that', 'our', 'method', 'obtains', 'a', 'higher', 'performance', 'on', 'almost', 'all', 'categories', 'even', 'when', 'VGG16', 'is', 'used', 'as', 'the', 'visual', 'backbone', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'PRP$', 'NN', 'VBZ', 'DT', 'JJR', 'NN', 'IN', 'RB', 'DT', 'NNS', 'RB', 'WRB', 'NNP', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",23
phrase_grounding,0,190,The model based on PNASNet consistently outperforms the state - of - the - art on all categories on both metrics .,"['The', 'model', 'based', 'on', 'PNASNet', 'consistently', 'outperforms', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'on', 'all', 'categories', 'on', 'both', 'metrics', '.']","['O', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'VBN', 'IN', 'NNP', 'RB', 'VBZ', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNS', '.']",22
phrase_grounding,0,192,It shows that the 3rd level dominates the selection while the 4th level is also important for several categories such as scene and animals .,"['It', 'shows', 'that', 'the', '3rd', 'level', 'dominates', 'the', 'selection', 'while', 'the', '4th', 'level', 'is', 'also', 'important', 'for', 'several', 'categories', 'such', 'as', 'scene', 'and', 'animals', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'IN', 'DT', 'CD', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'CD', 'NN', 'VBZ', 'RB', 'JJ', 'IN', 'JJ', 'NNS', 'JJ', 'IN', 'NN', 'CC', 'NNS', '.']",25
phrase_grounding,0,193,The 1st level is exploited mostly for the animals and people categories .,"['The', '1st', 'level', 'is', 'exploited', 'mostly', 'for', 'the', 'animals', 'and', 'people', 'categories', '.']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'CD', 'NN', 'VBZ', 'VBN', 'RB', 'IN', 'DT', 'NNS', 'CC', 'NNS', 'NNS', '.']",13
phrase_grounding,0,194,"The full sentence selection relies mostly on the 3rd level as well , while for some sentences the 4th model has been selected .","['The', 'full', 'sentence', 'selection', 'relies', 'mostly', 'on', 'the', '3rd', 'level', 'as', 'well', ',', 'while', 'for', 'some', 'sentences', 'the', '4th', 'model', 'has', 'been', 'selected', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'RB', 'IN', 'DT', 'CD', 'NN', 'RB', 'RB', ',', 'IN', 'IN', 'DT', 'NNS', 'DT', 'CD', 'NN', 'VBZ', 'VBN', 'VBN', '.']",24
phrase_grounding,0,202,"The results in rows 1 , 2 show that using level - attention mechanism based on multi-level feature maps significantly improves the performance over single visual - textual feature comparison .","['The', 'results', 'in', 'rows', '1', ',', '2', 'show', 'that', 'using', 'level', '-', 'attention', 'mechanism', 'based', 'on', 'multi-level', 'feature', 'maps', 'significantly', 'improves', 'the', 'performance', 'over', 'single', 'visual', '-', 'textual', 'feature', 'comparison', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'JJ', 'CD', ',', 'CD', 'NN', 'IN', 'VBG', 'NN', ':', 'NN', 'NN', 'VBN', 'IN', 'JJ', 'NN', 'NNS', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'JJ', ':', 'JJ', 'NN', 'NN', '.']",31
phrase_grounding,0,204,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is really important , and replacing any mapping with a linear one significantly degrades the performance .","['By', 'comparing', 'rows', '2', ',', '4', ',', '5', ',', '7', ',', 'we', 'see', 'that', 'non-linear', 'mapping', 'in', 'our', 'model', 'is', 'really', 'important', ',', 'and', 'replacing', 'any', 'mapping', 'with', 'a', 'linear', 'one', 'significantly', 'degrades', 'the', 'performance', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['IN', 'VBG', 'NNS', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', ',', 'PRP', 'VBP', 'IN', 'JJ', 'NN', 'IN', 'PRP$', 'NN', 'VBZ', 'RB', 'JJ', ',', 'CC', 'VBG', 'DT', 'NN', 'IN', 'DT', 'JJ', 'CD', 'RB', 'VBZ', 'DT', 'NN', '.']",36
phrase_grounding,0,205,"We can also see that non-linear mapping seems more important on the visual side , but best results are obtained with both text and visual non-linear mappings .","['We', 'can', 'also', 'see', 'that', 'non-linear', 'mapping', 'seems', 'more', 'important', 'on', 'the', 'visual', 'side', ',', 'but', 'best', 'results', 'are', 'obtained', 'with', 'both', 'text', 'and', 'visual', 'non-linear', 'mappings', '.']","['O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'MD', 'RB', 'VB', 'IN', 'JJ', 'NN', 'VBZ', 'RBR', 'JJ', 'IN', 'DT', 'JJ', 'NN', ',', 'CC', 'JJS', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'NN', 'CC', 'JJ', 'JJ', 'NNS', '.']",28
phrase_grounding,0,208,"The results in rows 1 , 3 and 2 , 6 show the importance of using a strong contextualized text embedding as the performance drops significantly .","['The', 'results', 'in', 'rows', '1', ',', '3', 'and', '2', ',', '6', 'show', 'the', 'importance', 'of', 'using', 'a', 'strong', 'contextualized', 'text', 'embedding', 'as', 'the', 'performance', 'drops', 'significantly', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'JJ', 'CD', ',', 'CD', 'CC', 'CD', ',', 'CD', 'NN', 'DT', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'RB', '.']",27
phrase_grounding,0,209,"We also study the use of softmax on the heatmaps , comparing rows 2 , 8 , we see that applying softmax leads to a very negative effect on the performance .","['We', 'also', 'study', 'the', 'use', 'of', 'softmax', 'on', 'the', 'heatmaps', ',', 'comparing', 'rows', '2', ',', '8', ',', 'we', 'see', 'that', 'applying', 'softmax', 'leads', 'to', 'a', 'very', 'negative', 'effect', 'on', 'the', 'performance', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', 'IN', 'NN', 'IN', 'DT', 'NNS', ',', 'VBG', 'NNS', 'CD', ',', 'CD', ',', 'PRP', 'VBP', 'IN', 'VBG', 'JJ', 'NNS', 'TO', 'DT', 'RB', 'JJ', 'NN', 'IN', 'DT', 'NN', '.']",32
text_generation,1,2,Adversarial Ranking for Language Generation,"['Adversarial', 'Ranking', 'for', 'Language', 'Generation']","['O', 'O', 'O', 'B-n', 'I-n']","['JJ', 'NNP', 'IN', 'NNP', 'NNP']",5
text_generation,1,28,"In this paper , we propose a novel adversarial learning framework , RankGAN , for generating highquality language descriptions .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', 'adversarial', 'learning', 'framework', ',', 'RankGAN', ',', 'for', 'generating', 'highquality', 'language', 'descriptions', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'NN', ',', 'NNP', ',', 'IN', 'VBG', 'NN', 'NN', 'NNS', '.']",20
text_generation,1,29,RankGAN learns the model from the relative ranking information between the machine - written and the human - written sentences in an adversarial framework .,"['RankGAN', 'learns', 'the', 'model', 'from', 'the', 'relative', 'ranking', 'information', 'between', 'the', 'machine', '-', 'written', 'and', 'the', 'human', '-', 'written', 'sentences', 'in', 'an', 'adversarial', 'framework', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'DT', 'NN', ':', 'VBN', 'CC', 'DT', 'JJ', ':', 'VBN', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",25
text_generation,1,30,"In the proposed RankGAN , we relax the training of the discriminator to a learning - to - rank optimization problem .","['In', 'the', 'proposed', 'RankGAN', ',', 'we', 'relax', 'the', 'training', 'of', 'the', 'discriminator', 'to', 'a', 'learning', '-', 'to', '-', 'rank', 'optimization', 'problem', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'VBN', 'NNP', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NN', 'TO', 'DT', 'VBG', ':', 'TO', ':', 'NN', 'NN', 'NN', '.']",22
text_generation,1,31,"Specifically , the proposed new adversarial network consists of two neural network models , a generator and a ranker .","['Specifically', ',', 'the', 'proposed', 'new', 'adversarial', 'network', 'consists', 'of', 'two', 'neural', 'network', 'models', ',', 'a', 'generator', 'and', 'a', 'ranker', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'DT', 'VBN', 'JJ', 'JJ', 'NN', 'NNS', 'IN', 'CD', 'JJ', 'NN', 'NNS', ',', 'DT', 'NN', 'CC', 'DT', 'NN', '.']",20
text_generation,1,32,"As opposed to performing a binary classification task , we propose to train the ranker to rank the machine - written sentences lower than human - written sentences with respect to a reference sentence which is human-written .","['As', 'opposed', 'to', 'performing', 'a', 'binary', 'classification', 'task', ',', 'we', 'propose', 'to', 'train', 'the', 'ranker', 'to', 'rank', 'the', 'machine', '-', 'written', 'sentences', 'lower', 'than', 'human', '-', 'written', 'sentences', 'with', 'respect', 'to', 'a', 'reference', 'sentence', 'which', 'is', 'human-written', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['IN', 'VBN', 'TO', 'VBG', 'DT', 'JJ', 'NN', 'NN', ',', 'PRP', 'VBP', 'TO', 'VB', 'DT', 'NN', 'TO', 'VB', 'DT', 'NN', ':', 'VBN', 'NNS', 'JJR', 'IN', 'JJ', ':', 'VBN', 'NNS', 'IN', 'NN', 'TO', 'DT', 'NN', 'NN', 'WDT', 'VBZ', 'JJ', '.']",38
text_generation,1,33,"Accordingly , we train the generator to synthesize sentences which confuse the ranker so that machine - written sentences are ranked higher than human - written sentences in regard to the reference .","['Accordingly', ',', 'we', 'train', 'the', 'generator', 'to', 'synthesize', 'sentences', 'which', 'confuse', 'the', 'ranker', 'so', 'that', 'machine', '-', 'written', 'sentences', 'are', 'ranked', 'higher', 'than', 'human', '-', 'written', 'sentences', 'in', 'regard', 'to', 'the', 'reference', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'TO', 'VB', 'NNS', 'WDT', 'VBP', 'DT', 'NN', 'IN', 'DT', 'NN', ':', 'VBN', 'NNS', 'VBP', 'VBN', 'JJR', 'IN', 'JJ', ':', 'VBN', 'NNS', 'IN', 'NN', 'TO', 'DT', 'NN', '.']",33
text_generation,1,34,"During learning , we adopt the policy gradient technique to overcome the non-differentiable problem .","['During', 'learning', ',', 'we', 'adopt', 'the', 'policy', 'gradient', 'technique', 'to', 'overcome', 'the', 'non-differentiable', 'problem', '.']","['B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', '.']",15
text_generation,1,35,"Consequently , by viewing a set of data samples collectively and evaluating their quality through relative ranking , the discriminator is able to make better assessment of the quality of the samples , which in turn helps the generator to learn better .","['Consequently', ',', 'by', 'viewing', 'a', 'set', 'of', 'data', 'samples', 'collectively', 'and', 'evaluating', 'their', 'quality', 'through', 'relative', 'ranking', ',', 'the', 'discriminator', 'is', 'able', 'to', 'make', 'better', 'assessment', 'of', 'the', 'quality', 'of', 'the', 'samples', ',', 'which', 'in', 'turn', 'helps', 'the', 'generator', 'to', 'learn', 'better', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'O']","['RB', ',', 'IN', 'VBG', 'DT', 'NN', 'IN', 'NNS', 'NNS', 'RB', 'CC', 'VBG', 'PRP$', 'NN', 'IN', 'JJ', 'NN', ',', 'DT', 'NN', 'VBZ', 'JJ', 'TO', 'VB', 'JJR', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNS', ',', 'WDT', 'IN', 'NN', 'VBZ', 'DT', 'NN', 'TO', 'VB', 'JJR', '.']",43
text_generation,1,36,Our method is suitable for language learning in comparison to conventional GANs .,"['Our', 'method', 'is', 'suitable', 'for', 'language', 'learning', 'in', 'comparison', 'to', 'conventional', 'GANs', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'JJ', 'IN', 'NN', 'NN', 'IN', 'NN', 'TO', 'JJ', 'NNP', '.']",13
text_generation,1,168,Simulation on synthetic data,"['Simulation', 'on', 'synthetic', 'data']","['O', 'B-p', 'B-n', 'I-n']","['NN', 'IN', 'JJ', 'NNS']",4
text_generation,1,189,It can be seen that the proposed RankGAN performs more favourably against the compared methods .,"['It', 'can', 'be', 'seen', 'that', 'the', 'proposed', 'RankGAN', 'performs', 'more', 'favourably', 'against', 'the', 'compared', 'methods', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'VBN', 'IN', 'DT', 'VBN', 'NNP', 'VBZ', 'JJR', 'RB', 'IN', 'DT', 'VBN', 'NNS', '.']",16
text_generation,1,191,"While MLE , PG - BLEU and SeqGAN tend to converge after 200 training epochs , the proposed RankGAN consistently improves the language generator and achieves relatively lower NLL score .","['While', 'MLE', ',', 'PG', '-', 'BLEU', 'and', 'SeqGAN', 'tend', 'to', 'converge', 'after', '200', 'training', 'epochs', ',', 'the', 'proposed', 'RankGAN', 'consistently', 'improves', 'the', 'language', 'generator', 'and', 'achieves', 'relatively', 'lower', 'NLL', 'score', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-n', 'I-n', 'O']","['IN', 'NNP', ',', 'NNP', ':', 'NNP', 'CC', 'NNP', 'VBP', 'TO', 'VB', 'IN', 'CD', 'NN', 'NN', ',', 'DT', 'VBN', 'NNP', 'RB', 'VBZ', 'DT', 'NN', 'NN', 'CC', 'NNS', 'RB', 'JJR', 'NNP', 'NN', '.']",31
text_generation,1,193,It is worth noting that the proposed RankGAN achieves better performance than that of PG - BLEU .,"['It', 'is', 'worth', 'noting', 'that', 'the', 'proposed', 'RankGAN', 'achieves', 'better', 'performance', 'than', 'that', 'of', 'PG', '-', 'BLEU', '.']","['O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'JJ', 'VBG', 'IN', 'DT', 'VBN', 'NNP', 'VBZ', 'JJR', 'NN', 'IN', 'DT', 'IN', 'NNP', ':', 'NNP', '.']",18
text_generation,1,203,Results on Chinese poems composition,"['Results', 'on', 'Chinese', 'poems', 'composition']","['O', 'O', 'B-n', 'I-n', 'I-n']","['NNS', 'IN', 'JJ', 'NNS', 'NN']",5
text_generation,1,209,"Following the evaluation protocol in , we compute the BLEU - 2 score and estimate the similarity between the human - written poem and the machine - created one .","['Following', 'the', 'evaluation', 'protocol', 'in', ',', 'we', 'compute', 'the', 'BLEU', '-', '2', 'score', 'and', 'estimate', 'the', 'similarity', 'between', 'the', 'human', '-', 'written', 'poem', 'and', 'the', 'machine', '-', 'created', 'one', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'DT', 'NN', 'NN', 'IN', ',', 'PRP', 'VBP', 'DT', 'NNP', ':', 'CD', 'NN', 'CC', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', ':', 'VBN', 'NN', 'CC', 'DT', 'NN', ':', 'VBD', 'CD', '.']",30
text_generation,1,211,It can be seen that the proposed Rank GAN performs more favourably compared to the state - of - the - art methods in terms of BLEU - 2 score .,"['It', 'can', 'be', 'seen', 'that', 'the', 'proposed', 'Rank', 'GAN', 'performs', 'more', 'favourably', 'compared', 'to', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'methods', 'in', 'terms', 'of', 'BLEU', '-', '2', 'score', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'VBN', 'IN', 'DT', 'VBN', 'NNP', 'NNP', 'VBZ', 'JJR', 'RB', 'VBN', 'TO', 'DT', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNS', 'IN', 'NNS', 'IN', 'NNP', ':', 'CD', 'NN', '.']",31
text_generation,1,238,RankGAN outperforms the compared method in terms of the human evaluation score .,"['RankGAN', 'outperforms', 'the', 'compared', 'method', 'in', 'terms', 'of', 'the', 'human', 'evaluation', 'score', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'VBN', 'NN', 'IN', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",13
text_generation,1,240,Results on COCO image captions,"['Results', 'on', 'COCO', 'image', 'captions']","['O', 'O', 'B-n', 'I-n', 'I-n']","['NNS', 'IN', 'NNP', 'NN', 'NNS']",5
text_generation,1,249,RankGAN achieves better performance than the other methods in terms of different BLEU scores .,"['RankGAN', 'achieves', 'better', 'performance', 'than', 'the', 'other', 'methods', 'in', 'terms', 'of', 'different', 'BLEU', 'scores', '.']","['B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'JJR', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NNS', 'IN', 'JJ', 'NNP', 'NNS', '.']",15
text_generation,1,251,"These examples show that our model is able to generate fluent , novel sentences that are not existing in the training set .","['These', 'examples', 'show', 'that', 'our', 'model', 'is', 'able', 'to', 'generate', 'fluent', ',', 'novel', 'sentences', 'that', 'are', 'not', 'existing', 'in', 'the', 'training', 'set', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'IN', 'PRP$', 'NN', 'VBZ', 'JJ', 'TO', 'VB', 'NN', ',', 'JJ', 'NNS', 'WDT', 'VBP', 'RB', 'VBG', 'IN', 'DT', 'NN', 'NN', '.']",23
text_generation,1,257,"As can be seen , the human - written sentences get the highest score comparing to the language models .","['As', 'can', 'be', 'seen', ',', 'the', 'human', '-', 'written', 'sentences', 'get', 'the', 'highest', 'score', 'comparing', 'to', 'the', 'language', 'models', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'MD', 'VB', 'VBN', ',', 'DT', 'JJ', ':', 'VBN', 'NNS', 'VBP', 'DT', 'JJS', 'NN', 'VBG', 'TO', 'DT', 'NN', 'NNS', '.']",20
text_generation,1,258,"Among the GANs approaches , RankGAN receives better score than SeqGAN , which is consistent to the finding in the Chinese poem composition .","['Among', 'the', 'GANs', 'approaches', ',', 'RankGAN', 'receives', 'better', 'score', 'than', 'SeqGAN', ',', 'which', 'is', 'consistent', 'to', 'the', 'finding', 'in', 'the', 'Chinese', 'poem', 'composition', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NNP', 'NNS', ',', 'NNP', 'VBZ', 'JJR', 'NN', 'IN', 'NNP', ',', 'WDT', 'VBZ', 'JJ', 'TO', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",24
text_generation,1,260,Results on Shakespeare 's plays,"['Results', 'on', 'Shakespeare', ""'s"", 'plays']","['O', 'O', 'B-n', 'I-n', 'I-n']","['NNS', 'IN', 'NNP', 'POS', 'NNS']",5
text_generation,1,266,"As can be seen , the proposed method achieves consistently higher BLEU score than the other methods in terms of the different n-grams criteria .","['As', 'can', 'be', 'seen', ',', 'the', 'proposed', 'method', 'achieves', 'consistently', 'higher', 'BLEU', 'score', 'than', 'the', 'other', 'methods', 'in', 'terms', 'of', 'the', 'different', 'n-grams', 'criteria', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'MD', 'VB', 'VBN', ',', 'DT', 'VBN', 'NN', 'VBZ', 'RB', 'JJR', 'NNP', 'RBR', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NNS', 'IN', 'DT', 'JJ', 'JJ', 'NNS', '.']",25
text_generation,1,267,"The results indicate the proposed RankGAN is able to capture the transition pattern among the words , even if the training sentences are novel , delicate and complicated .","['The', 'results', 'indicate', 'the', 'proposed', 'RankGAN', 'is', 'able', 'to', 'capture', 'the', 'transition', 'pattern', 'among', 'the', 'words', ',', 'even', 'if', 'the', 'training', 'sentences', 'are', 'novel', ',', 'delicate', 'and', 'complicated', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'VBP', 'DT', 'VBN', 'NNP', 'VBZ', 'JJ', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'DT', 'NNS', ',', 'RB', 'IN', 'DT', 'NN', 'NNS', 'VBP', 'JJ', ',', 'JJ', 'CC', 'VBD', '.']",29
text_generation,5,2,Improved Variational Autoencoders for Text Modeling using Dilated Convolutions,"['Improved', 'Variational', 'Autoencoders', 'for', 'Text', 'Modeling', 'using', 'Dilated', 'Convolutions']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['VBN', 'JJ', 'NNS', 'IN', 'NNP', 'NNP', 'VBG', 'NNP', 'NNS']",9
text_generation,5,4,"Recent work on generative text modeling has found that variational autoencoders ( VAE ) with LSTM decoders perform worse than simpler LSTM language models ( Bowman et al. , 2015 ) .","['Recent', 'work', 'on', 'generative', 'text', 'modeling', 'has', 'found', 'that', 'variational', 'autoencoders', '(', 'VAE', ')', 'with', 'LSTM', 'decoders', 'perform', 'worse', 'than', 'simpler', 'LSTM', 'language', 'models', '(', 'Bowman', 'et', 'al.', ',', '2015', ')', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', 'NNS', '(', 'NNP', ')', 'IN', 'NNP', 'NNS', 'VBP', 'JJR', 'IN', 'JJR', 'NNP', 'NN', 'NNS', '(', 'NNP', 'VBZ', 'RB', ',', 'CD', ')', '.']",32
text_generation,5,33,"We propose the use of a dilated CNN as a decoder in VAE , inspired by the recent success of using CNNs for audio , image and language modeling ( van den .","['We', 'propose', 'the', 'use', 'of', 'a', 'dilated', 'CNN', 'as', 'a', 'decoder', 'in', 'VAE', ',', 'inspired', 'by', 'the', 'recent', 'success', 'of', 'using', 'CNNs', 'for', 'audio', ',', 'image', 'and', 'language', 'modeling', '(', 'van', 'den', '.']","['O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNP', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'NNP', 'IN', 'NN', ',', 'NN', 'CC', 'NN', 'NN', '(', 'JJ', 'NN', '.']",33
text_generation,5,34,"In contrast with prior work where extremely large CNNs are used , we exploit the dilated CNN for its flexibility in varying the amount of conditioning context .","['In', 'contrast', 'with', 'prior', 'work', 'where', 'extremely', 'large', 'CNNs', 'are', 'used', ',', 'we', 'exploit', 'the', 'dilated', 'CNN', 'for', 'its', 'flexibility', 'in', 'varying', 'the', 'amount', 'of', 'conditioning', 'context', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'NN', 'IN', 'JJ', 'NN', 'WRB', 'RB', 'JJ', 'NNP', 'VBP', 'VBN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNP', 'IN', 'PRP$', 'NN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'VBG', 'NN', '.']",28
text_generation,5,154,We use an LSTM as an encoder for VAE and explore LSTMs and CNNs as decoders .,"['We', 'use', 'an', 'LSTM', 'as', 'an', 'encoder', 'for', 'VAE', 'and', 'explore', 'LSTMs', 'and', 'CNNs', 'as', 'decoders', '.']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'IN', 'DT', 'NN', 'IN', 'NNP', 'CC', 'VB', 'NNP', 'CC', 'NNP', 'IN', 'NNS', '.']",17
text_generation,5,155,"For CNNs , we explore several different configurations .","['For', 'CNNs', ',', 'we', 'explore', 'several', 'different', 'configurations', '.']","['B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NNP', ',', 'PRP', 'VBP', 'JJ', 'JJ', 'NNS', '.']",9
text_generation,5,156,"We set the convolution filter size to be 3 and gradually increase the depth and dilation from [ 1 , 2 , 4 ] , ] to .","['We', 'set', 'the', 'convolution', 'filter', 'size', 'to', 'be', '3', 'and', 'gradually', 'increase', 'the', 'depth', 'and', 'dilation', 'from', '[', '1', ',', '2', ',', '4', ']', ',', ']', 'to', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NN', 'NN', 'NN', 'TO', 'VB', 'CD', 'CC', 'RB', 'VB', 'DT', 'NN', 'CC', 'NN', 'IN', 'JJ', 'CD', ',', 'CD', ',', 'CD', 'NN', ',', 'NN', 'TO', '.']",28
text_generation,5,165,We use Gumbel - softmax to sample y from q ( y|x ) .,"['We', 'use', 'Gumbel', '-', 'softmax', 'to', 'sample', 'y', 'from', 'q', '(', 'y|x', ')', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', ':', 'NN', 'TO', 'VB', 'NN', 'IN', 'NN', '(', 'NN', ')', '.']",14
text_generation,5,168,We use a vocabulary size of 20 k for both data sets and set the word embedding dimension to be 512 .,"['We', 'use', 'a', 'vocabulary', 'size', 'of', '20', 'k', 'for', 'both', 'data', 'sets', 'and', 'set', 'the', 'word', 'embedding', 'dimension', 'to', 'be', '512', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NN', 'IN', 'DT', 'NNS', 'NNS', 'CC', 'VB', 'DT', 'NN', 'VBG', 'NN', 'TO', 'VB', 'CD', '.']",22
text_generation,5,170,"The number of channels for convolutions in CNN decoders is 512 internally and 1024 externally , as shown in Section 2.3 .","['The', 'number', 'of', 'channels', 'for', 'convolutions', 'in', 'CNN', 'decoders', 'is', '512', 'internally', 'and', '1024', 'externally', ',', 'as', 'shown', 'in', 'Section', '2.3', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'NNS', 'IN', 'NNS', 'IN', 'NNP', 'NNS', 'VBZ', 'CD', 'RB', 'CC', 'CD', 'RB', ',', 'IN', 'VBN', 'IN', 'NN', 'CD', '.']",22
text_generation,5,173,"We use Adam to optimize all models and the learning rate is selected from [ 2e - 3 , 1 e - 3 , 7.5 e - 4 ] and ?","['We', 'use', 'Adam', 'to', 'optimize', 'all', 'models', 'and', 'the', 'learning', 'rate', 'is', 'selected', 'from', '[', '2e', '-', '3', ',', '1', 'e', '-', '3', ',', '7.5', 'e', '-', '4', ']', 'and', '?']","['O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBP', 'NNP', 'TO', 'VB', 'DT', 'NNS', 'CC', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', 'CD', ':', 'CD', ',', 'CD', 'SYM', ':', 'CD', ',', 'CD', 'SYM', ':', 'CD', 'NN', 'CC', '.']",31
text_generation,5,175,"Empirically , we find learning rate 1e - 3 and ?1 = 0.5 to perform the best .","['Empirically', ',', 'we', 'find', 'learning', 'rate', '1e', '-', '3', 'and', '?1', '=', '0.5', 'to', 'perform', 'the', 'best', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['RB', ',', 'PRP', 'VBP', 'VBG', 'NN', 'CD', ':', 'CD', 'CC', 'NNP', 'VBD', 'CD', 'TO', 'VB', 'DT', 'JJS', '.']",18
text_generation,5,176,"We select dropout ratio of LSTMs ( both encoder and decoder ) from [ 0.3 , 0.5 ] .","['We', 'select', 'dropout', 'ratio', 'of', 'LSTMs', '(', 'both', 'encoder', 'and', 'decoder', ')', 'from', '[', '0.3', ',', '0.5', ']', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBP', 'IN', 'NN', 'IN', 'NNP', '(', 'DT', 'NN', 'CC', 'NN', ')', 'IN', '$', 'CD', ',', 'CD', 'NN', '.']",19
text_generation,5,177,"Following , we also use drop word for the LSTM decoder , the drop word ratio is selected from [ 0 , 0.3 , 0.5 , 0.7 ] .","['Following', ',', 'we', 'also', 'use', 'drop', 'word', 'for', 'the', 'LSTM', 'decoder', ',', 'the', 'drop', 'word', 'ratio', 'is', 'selected', 'from', '[', '0', ',', '0.3', ',', '0.5', ',', '0.7', ']', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['VBG', ',', 'PRP', 'RB', 'VBP', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', ',', 'DT', 'NN', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', 'CD', ',', 'CD', ',', 'CD', ',', 'CD', 'NN', '.']",29
text_generation,5,178,"For the CNN decoder , we use a dropout ratio of 0.1 at each layer .","['For', 'the', 'CNN', 'decoder', ',', 'we', 'use', 'a', 'dropout', 'ratio', 'of', '0.1', 'at', 'each', 'layer', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NNP', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'IN', 'CD', 'IN', 'DT', 'NN', '.']",16
text_generation,5,180,We use batch size of 32 and all model are trained for 40 epochs .,"['We', 'use', 'batch', 'size', 'of', '32', 'and', 'all', 'model', 'are', 'trained', 'for', '40', 'epochs', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'CD', 'CC', 'DT', 'NN', 'VBP', 'VBN', 'IN', 'CD', 'NNS', '.']",15
text_generation,5,182,"Following , we use KL cost annealing strategy .","['Following', ',', 'we', 'use', 'KL', 'cost', 'annealing', 'strategy', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', ',', 'PRP', 'VBP', 'NNP', 'NN', 'VBG', 'NN', '.']",9
text_generation,5,183,We set the initial weight of KL cost term to be 0.01 and increase it linearly until a given iteration T .,"['We', 'set', 'the', 'initial', 'weight', 'of', 'KL', 'cost', 'term', 'to', 'be', '0.01', 'and', 'increase', 'it', 'linearly', 'until', 'a', 'given', 'iteration', 'T', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'NN', 'NN', 'TO', 'VB', 'CD', 'CC', 'VB', 'PRP', 'RB', 'IN', 'DT', 'VBN', 'NN', 'NNP', '.']",22
text_generation,5,186,The results for language modeling are shown in .,"['The', 'results', 'for', 'language', 'modeling', 'are', 'shown', 'in', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['DT', 'NNS', 'IN', 'NN', 'NN', 'VBP', 'VBN', 'IN', '.']",9
text_generation,5,191,"For SCNN , MCNN and LCNN , the VAE results improve over LM results from 345.3 to 337.8 , 338.3 to 336.2 , and 335.4 to 333.9 respectively .","['For', 'SCNN', ',', 'MCNN', 'and', 'LCNN', ',', 'the', 'VAE', 'results', 'improve', 'over', 'LM', 'results', 'from', '345.3', 'to', '337.8', ',', '338.3', 'to', '336.2', ',', 'and', '335.4', 'to', '333.9', 'respectively', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O']","['IN', 'NNP', ',', 'NNP', 'CC', 'NNP', ',', 'DT', 'NNP', 'NNS', 'VB', 'IN', 'NNP', 'NNS', 'IN', 'CD', 'TO', 'CD', ',', 'CD', 'TO', 'CD', ',', 'CC', 'CD', 'TO', 'CD', 'RB', '.']",29
text_generation,5,195,"When LCNN is used as the decoder , we obtain an optimal trade off between using contextual information and latent representation .","['When', 'LCNN', 'is', 'used', 'as', 'the', 'decoder', ',', 'we', 'obtain', 'an', 'optimal', 'trade', 'off', 'between', 'using', 'contextual', 'information', 'and', 'latent', 'representation', '.']","['B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['WRB', 'NNP', 'VBZ', 'VBN', 'IN', 'DT', 'NN', ',', 'PRP', 'VB', 'DT', 'JJ', 'NN', 'IN', 'IN', 'VBG', 'JJ', 'NN', 'CC', 'NN', 'NN', '.']",22
text_generation,5,196,"LCNN - VAE achieves a NLL of 333.9 , which improves over LSTM - LM with NLL of 334.9 .","['LCNN', '-', 'VAE', 'achieves', 'a', 'NLL', 'of', '333.9', ',', 'which', 'improves', 'over', 'LSTM', '-', 'LM', 'with', 'NLL', 'of', '334.9', '.']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']","['NNP', ':', 'NNP', 'VBZ', 'DT', 'NNP', 'IN', 'CD', ',', 'WDT', 'VBZ', 'IN', 'NNP', ':', 'NN', 'IN', 'NNP', 'IN', 'CD', '.']",20
text_generation,5,216,We can see that SCNN - VAE - Semi has the best classification accuracy of 65.5 .,"['We', 'can', 'see', 'that', 'SCNN', '-', 'VAE', '-', 'Semi', 'has', 'the', 'best', 'classification', 'accuracy', 'of', '65.5', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['PRP', 'MD', 'VB', 'DT', 'NNP', ':', 'NNP', ':', 'NN', 'VBZ', 'DT', 'JJS', 'NN', 'NN', 'IN', 'CD', '.']",17
text_generation,5,218,"On the other hand , LCNN - VAE - Semi has the best NLL result .","['On', 'the', 'other', 'hand', ',', 'LCNN', '-', 'VAE', '-', 'Semi', 'has', 'the', 'best', 'NLL', 'result', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'NNP', ':', 'NNP', ':', 'NN', 'VBZ', 'DT', 'JJS', 'JJ', 'NN', '.']",16
text_generation,4,2,Generating Text through Adversarial Training using Skip - Thought Vectors,"['Generating', 'Text', 'through', 'Adversarial', 'Training', 'using', 'Skip', '-', 'Thought', 'Vectors']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'NNP', 'IN', 'NNP', 'NNP', 'VBG', 'NNP', ':', 'VBD', 'NNS']",10
text_generation,4,7,Attempts have been made for utilizing GANs with word embeddings for text generation .,"['Attempts', 'have', 'been', 'made', 'for', 'utilizing', 'GANs', 'with', 'word', 'embeddings', 'for', 'text', 'generation', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['NNS', 'VBP', 'VBN', 'VBN', 'IN', 'JJ', 'NNP', 'IN', 'NN', 'NNS', 'IN', 'JJ', 'NN', '.']",14
text_generation,4,11,Numerous efforts have been made in the field of natural language text generation for tasks such as sentiment analysis and machine translation .,"['Numerous', 'efforts', 'have', 'been', 'made', 'in', 'the', 'field', 'of', 'natural', 'language', 'text', 'generation', 'for', 'tasks', 'such', 'as', 'sentiment', 'analysis', 'and', 'machine', 'translation', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', 'NNS', 'VBP', 'VBN', 'VBN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', 'NN', 'IN', 'NNS', 'JJ', 'IN', 'NN', 'NN', 'CC', 'NN', 'NN', '.']",23
text_generation,4,14,This work pro-Code available at : https://github.com/enigmaeth/skip-thought-gan poses an approach for text generation using Generative Adversarial Networks with Skip - Thought vectors .,"['This', 'work', 'pro-Code', 'available', 'at', ':', 'https://github.com/enigmaeth/skip-thought-gan', 'poses', 'an', 'approach', 'for', 'text', 'generation', 'using', 'Generative', 'Adversarial', 'Networks', 'with', 'Skip', '-', 'Thought', 'vectors', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', 'JJ', 'IN', ':', 'JJ', 'NNS', 'DT', 'NN', 'IN', 'JJ', 'NN', 'VBG', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', ':', 'NN', 'NNS', '.']",23
text_generation,4,71,The Skip - Thought encoder for the model encodes sentences with length less than 30 words using 2400 GRU units with word vector dimensionality of 620 to produce 4800 - dimensional combineskip vectors . .,"['The', 'Skip', '-', 'Thought', 'encoder', 'for', 'the', 'model', 'encodes', 'sentences', 'with', 'length', 'less', 'than', '30', 'words', 'using', '2400', 'GRU', 'units', 'with', 'word', 'vector', 'dimensionality', 'of', '620', 'to', 'produce', '4800', '-', 'dimensional', 'combineskip', 'vectors', '.', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['DT', 'NNP', ':', 'NNP', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'NNS', 'IN', 'NN', 'JJR', 'IN', 'CD', 'NNS', 'VBG', 'CD', 'NNP', 'NNS', 'IN', 'NN', 'NN', 'NN', 'IN', 'CD', 'TO', 'VB', 'CD', ':', 'JJ', 'NN', 'NNS', '.', '.']",35
text_generation,4,72,"The combine - skip vectors , with the first 2400 dimensions being uni-skip model and the last 2400 bi-skip model , are used as they have been found to be the best performing in the experiments","['The', 'combine', '-', 'skip', 'vectors', ',', 'with', 'the', 'first', '2400', 'dimensions', 'being', 'uni-skip', 'model', 'and', 'the', 'last', '2400', 'bi-skip', 'model', ',', 'are', 'used', 'as', 'they', 'have', 'been', 'found', 'to', 'be', 'the', 'best', 'performing', 'in', 'the', 'experiments']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['DT', 'NN', ':', 'NN', 'NNS', ',', 'IN', 'DT', 'JJ', 'CD', 'NNS', 'VBG', 'JJ', 'NN', 'CC', 'DT', 'JJ', 'CD', 'JJ', 'NN', ',', 'VBP', 'VBN', 'IN', 'PRP', 'VBP', 'VBN', 'VBN', 'TO', 'VB', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NNS']",36
text_generation,2,2,Long Text Generation via Adversarial Training with Leaked Information,"['Long', 'Text', 'Generation', 'via', 'Adversarial', 'Training', 'with', 'Leaked', 'Information']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'JJ', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NN']",9
text_generation,2,4,"Automatically generating coherent and semantically meaningful text has many applications in machine translation , dialogue systems , image captioning , etc .","['Automatically', 'generating', 'coherent', 'and', 'semantically', 'meaningful', 'text', 'has', 'many', 'applications', 'in', 'machine', 'translation', ',', 'dialogue', 'systems', ',', 'image', 'captioning', ',', 'etc', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', 'VBG', 'NN', 'CC', 'RB', 'JJ', 'NN', 'VBZ', 'JJ', 'NNS', 'IN', 'NN', 'NN', ',', 'NN', 'NNS', ',', 'NN', 'NN', ',', 'FW', '.']",22
text_generation,2,33,"In this paper , we propose a new algorithmic framework called Leak GAN to address both the non-informativeness and the sparsity issues .","['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'new', 'algorithmic', 'framework', 'called', 'Leak', 'GAN', 'to', 'address', 'both', 'the', 'non-informativeness', 'and', 'the', 'sparsity', 'issues', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'JJ', 'NN', 'VBN', 'NNP', 'NNP', 'TO', 'VB', 'DT', 'DT', 'JJ', 'CC', 'DT', 'NN', 'NNS', '.']",23
text_generation,2,34,LeakGAN is a new way of providing richer information from the discriminator to the generator by borrowing the recent advances in hierarchical reinforcement learning .,"['LeakGAN', 'is', 'a', 'new', 'way', 'of', 'providing', 'richer', 'information', 'from', 'the', 'discriminator', 'to', 'the', 'generator', 'by', 'borrowing', 'the', 'recent', 'advances', 'in', 'hierarchical', 'reinforcement', 'learning', '.']","['B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['NNP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'JJR', 'NN', 'IN', 'DT', 'NN', 'TO', 'DT', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NNS', 'IN', 'JJ', 'NN', 'NN', '.']",25
text_generation,2,35,"As illustrated in , we specifically introduce a hierarchical generator G , which consists of a high - level MANAGER module and a low - level WORKER module .","['As', 'illustrated', 'in', ',', 'we', 'specifically', 'introduce', 'a', 'hierarchical', 'generator', 'G', ',', 'which', 'consists', 'of', 'a', 'high', '-', 'level', 'MANAGER', 'module', 'and', 'a', 'low', '-', 'level', 'WORKER', 'module', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'IN', ',', 'PRP', 'RB', 'VBP', 'DT', 'JJ', 'NN', 'NNP', ',', 'WDT', 'VBZ', 'IN', 'DT', 'JJ', ':', 'NN', 'NNP', 'NN', 'CC', 'DT', 'JJ', ':', 'NN', 'NNP', 'NN', '.']",29
text_generation,2,36,The MANAGER is along shortterm memory network ( LSTM ) and serves as a mediator .,"['The', 'MANAGER', 'is', 'along', 'shortterm', 'memory', 'network', '(', 'LSTM', ')', 'and', 'serves', 'as', 'a', 'mediator', '.']","['O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['DT', 'NNP', 'VBZ', 'JJ', 'JJ', 'NN', 'NN', '(', 'NNP', ')', 'CC', 'VBZ', 'IN', 'DT', 'NN', '.']",16
text_generation,2,37,"In each step , it receives generator D 's high - level feature representation , e.g. , the feature map of the CNN , and uses it to form the guiding goal for the WORKER module in that timestep .","['In', 'each', 'step', ',', 'it', 'receives', 'generator', 'D', ""'s"", 'high', '-', 'level', 'feature', 'representation', ',', 'e.g.', ',', 'the', 'feature', 'map', 'of', 'the', 'CNN', ',', 'and', 'uses', 'it', 'to', 'form', 'the', 'guiding', 'goal', 'for', 'the', 'WORKER', 'module', 'in', 'that', 'timestep', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBZ', 'JJ', 'NNP', 'POS', 'JJ', ':', 'NN', 'NN', 'NN', ',', 'NN', ',', 'DT', 'NN', 'NN', 'IN', 'DT', 'NNP', ',', 'CC', 'VBZ', 'PRP', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'DT', 'NN', '.']",40
text_generation,2,40,"Next , given the goal embedding produced by the MAN - AGER , the WORKER first encodes current generated words with another LSTM , then combines the output of the LSTM and the goal embedding to take a final action at current state .","['Next', ',', 'given', 'the', 'goal', 'embedding', 'produced', 'by', 'the', 'MAN', '-', 'AGER', ',', 'the', 'WORKER', 'first', 'encodes', 'current', 'generated', 'words', 'with', 'another', 'LSTM', ',', 'then', 'combines', 'the', 'output', 'of', 'the', 'LSTM', 'and', 'the', 'goal', 'embedding', 'to', 'take', 'a', 'final', 'action', 'at', 'current', 'state', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['JJ', ',', 'VBN', 'DT', 'NN', 'VBG', 'VBN', 'IN', 'DT', 'NNP', ':', 'NN', ',', 'DT', 'NNP', 'RB', 'VBZ', 'JJ', 'VBD', 'NNS', 'IN', 'DT', 'NNP', ',', 'RB', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NNP', 'CC', 'DT', 'NN', 'VBG', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', '.']",44
text_generation,2,163,GAN Setting .,"['GAN', 'Setting', '.']","['B-n', 'I-n', 'O']","['NNP', 'NNP', '.']",3
text_generation,2,164,"For the discriminator , we choose the CNN architecture as the feature extractor and the binary classifier .","['For', 'the', 'discriminator', ',', 'we', 'choose', 'the', 'CNN', 'architecture', 'as', 'the', 'feature', 'extractor', 'and', 'the', 'binary', 'classifier', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NNP', 'NN', 'IN', 'DT', 'NN', 'NN', 'CC', 'DT', 'JJ', 'NN', '.']",18
text_generation,2,166,"For the synthetic data experiment , the CNN kernel size ranges from 1 to T .","['For', 'the', 'synthetic', 'data', 'experiment', ',', 'the', 'CNN', 'kernel', 'size', 'ranges', 'from', '1', 'to', 'T', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'NN', ',', 'DT', 'NNP', 'NN', 'NN', 'VBZ', 'IN', 'CD', 'TO', 'NNP', '.']",16
text_generation,2,167,The number of each kernel is between 100 and 200 .,"['The', 'number', 'of', 'each', 'kernel', 'is', 'between', '100', 'and', '200', '.']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'IN', 'CD', 'CC', 'CD', '.']",11
text_generation,2,168,"In this case , the feature of text is a 1,720 dimensional vector .","['In', 'this', 'case', ',', 'the', 'feature', 'of', 'text', 'is', 'a', '1,720', 'dimensional', 'vector', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'DT', 'NN', 'IN', 'NN', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', '.']",14
text_generation,2,169,Dropout with the keep rate 0.75 and L2 regularization are performed to avoid overfitting .,"['Dropout', 'with', 'the', 'keep', 'rate', '0.75', 'and', 'L2', 'regularization', 'are', 'performed', 'to', 'avoid', 'overfitting', '.']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['NN', 'IN', 'DT', 'NN', 'NN', 'CD', 'CC', 'NNP', 'NN', 'VBP', 'VBN', 'TO', 'VB', 'NN', '.']",15
text_generation,2,170,"For the generator , we adopt LSTM as the architectures of MANAGER and WORKER to capture the sequence context information .","['For', 'the', 'generator', ',', 'we', 'adopt', 'LSTM', 'as', 'the', 'architectures', 'of', 'MANAGER', 'and', 'WORKER', 'to', 'capture', 'the', 'sequence', 'context', 'information', '.']","['O', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'NNP', 'IN', 'DT', 'NNS', 'IN', 'NNP', 'CC', 'NNP', 'TO', 'VB', 'DT', 'NN', 'NN', 'NN', '.']",21
text_generation,2,171,The MANAGER produces the 16 - dimensional goal embedding feature vector wt using the feature map extracted by CNN .,"['The', 'MANAGER', 'produces', 'the', '16', '-', 'dimensional', 'goal', 'embedding', 'feature', 'vector', 'wt', 'using', 'the', 'feature', 'map', 'extracted', 'by', 'CNN', '.']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['DT', 'NNP', 'VBZ', 'DT', 'CD', ':', 'JJ', 'NN', 'VBG', 'NN', 'NN', 'NN', 'VBG', 'DT', 'NN', 'NN', 'VBN', 'IN', 'NNP', '.']",20
text_generation,2,172,The goal duration time c is a hyperparameter set as 4 after some preliminary experiments .,"['The', 'goal', 'duration', 'time', 'c', 'is', 'a', 'hyperparameter', 'set', 'as', '4', 'after', 'some', 'preliminary', 'experiments', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'NN', 'NN', 'VBZ', 'DT', 'NN', 'VBN', 'IN', 'CD', 'IN', 'DT', 'JJ', 'NNS', '.']",16
text_generation,2,178,Synthetic Data Experiments,"['Synthetic', 'Data', 'Experiments']","['B-n', 'I-n', 'I-n']","['JJ', 'NNP', 'NNS']",3
text_generation,2,182,"( i ) In the pre-training stage , LeakGAN has already shown observable performance superiority compared to other models , which indicates that the proposed hierarchical architecture itself brings improvement over the previous ones .","['(', 'i', ')', 'In', 'the', 'pre-training', 'stage', ',', 'LeakGAN', 'has', 'already', 'shown', 'observable', 'performance', 'superiority', 'compared', 'to', 'other', 'models', ',', 'which', 'indicates', 'that', 'the', 'proposed', 'hierarchical', 'architecture', 'itself', 'brings', 'improvement', 'over', 'the', 'previous', 'ones', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['(', 'NN', ')', 'IN', 'DT', 'JJ', 'NN', ',', 'NNP', 'VBZ', 'RB', 'VBN', 'JJ', 'NN', 'NN', 'VBN', 'TO', 'JJ', 'NNS', ',', 'WDT', 'VBZ', 'IN', 'DT', 'VBN', 'JJ', 'NN', 'PRP', 'VBZ', 'NN', 'IN', 'DT', 'JJ', 'NNS', '.']",35
text_generation,2,183,"( ii ) In the adversarial training stage , Leak GAN shows a better speed of convergence , and the local minimum it explores is significantly better than previous results .","['(', 'ii', ')', 'In', 'the', 'adversarial', 'training', 'stage', ',', 'Leak', 'GAN', 'shows', 'a', 'better', 'speed', 'of', 'convergence', ',', 'and', 'the', 'local', 'minimum', 'it', 'explores', 'is', 'significantly', 'better', 'than', 'previous', 'results', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['(', 'NN', ')', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'NNP', 'NNP', 'VBZ', 'DT', 'JJR', 'NN', 'IN', 'NN', ',', 'CC', 'DT', 'JJ', 'NN', 'PRP', 'VBZ', 'VBZ', 'RB', 'JJR', 'IN', 'JJ', 'NNS', '.']",31
text_generation,2,185,Long Text Generation : EMNLP2017 WMT News,"['Long', 'Text', 'Generation', ':', 'EMNLP2017', 'WMT', 'News']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['RB', 'JJ', 'NN', ':', 'NNP', 'NNP', 'NNP']",7
text_generation,2,196,"In all measured metrics , LeakGAN shows significant performance gain compared to baseline models .","['In', 'all', 'measured', 'metrics', ',', 'LeakGAN', 'shows', 'significant', 'performance', 'gain', 'compared', 'to', 'baseline', 'models', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['IN', 'DT', 'VBN', 'NNS', ',', 'NNP', 'VBZ', 'JJ', 'NN', 'NN', 'VBN', 'TO', 'VB', 'NNS', '.']",15
text_generation,2,198,Middle Text Generation : COCO Image Captions,"['Middle', 'Text', 'Generation', ':', 'COCO', 'Image', 'Captions']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'NNP']",7
text_generation,2,208,The results of the BLEU scores on the COCO dataset indicate that Leak GAN performs significantly better than baseline models in mid-length text generation task .,"['The', 'results', 'of', 'the', 'BLEU', 'scores', 'on', 'the', 'COCO', 'dataset', 'indicate', 'that', 'Leak', 'GAN', 'performs', 'significantly', 'better', 'than', 'baseline', 'models', 'in', 'mid-length', 'text', 'generation', 'task', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'DT', 'NNP', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'NN', 'WDT', 'NNP', 'NNP', 'VBZ', 'RB', 'JJR', 'IN', 'NN', 'NNS', 'IN', 'JJ', 'NN', 'NN', 'NN', '.']",26
text_generation,2,209,Short Text Generation : Chinese Poems,"['Short', 'Text', 'Generation', ':', 'Chinese', 'Poems']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'NNP', 'NNP', ':', 'JJ', 'NNS']",6
text_generation,2,214,The results on Chinese Poems indicate that LeakGAN successfully handles the short text generation tasks .,"['The', 'results', 'on', 'Chinese', 'Poems', 'indicate', 'that', 'LeakGAN', 'successfully', 'handles', 'the', 'short', 'text', 'generation', 'tasks', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'JJ', 'NNP', 'VBP', 'IN', 'NNP', 'RB', 'VBZ', 'DT', 'JJ', 'NN', 'NN', 'NNS', '.']",16
text_generation,2,223,Turing Test and Generated Samples,"['Turing', 'Test', 'and', 'Generated', 'Samples']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['VBG', 'NNP', 'CC', 'NNP', 'NNP']",5
text_generation,2,231,The performance on two datasets indicates that the generated sentences of Leak GAN are of higher global consistency and better readability than those of SeqGAN .,"['The', 'performance', 'on', 'two', 'datasets', 'indicates', 'that', 'the', 'generated', 'sentences', 'of', 'Leak', 'GAN', 'are', 'of', 'higher', 'global', 'consistency', 'and', 'better', 'readability', 'than', 'those', 'of', 'SeqGAN', '.']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['DT', 'NN', 'IN', 'CD', 'NNS', 'VBZ', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'NNP', 'NNP', 'VBP', 'IN', 'JJR', 'JJ', 'NN', 'CC', 'JJR', 'NN', 'IN', 'DT', 'IN', 'NNP', '.']",26
text_generation,3,2,An Auto - Encoder Matching Model for Learning Utterance - Level Semantic Dependency in Dialogue Generation,"['An', 'Auto', '-', 'Encoder', 'Matching', 'Model', 'for', 'Learning', 'Utterance', '-', 'Level', 'Semantic', 'Dependency', 'in', 'Dialogue', 'Generation']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['DT', 'NNP', ':', 'NN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', ':', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']",16
text_generation,3,12,"Automatic dialogue generation task is of great importance to many applications , ranging from open - domain chatbots to goal - oriented technical support agents .","['Automatic', 'dialogue', 'generation', 'task', 'is', 'of', 'great', 'importance', 'to', 'many', 'applications', ',', 'ranging', 'from', 'open', '-', 'domain', 'chatbots', 'to', 'goal', '-', 'oriented', 'technical', 'support', 'agents', '.']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'NN', 'NN', 'VBZ', 'IN', 'JJ', 'NN', 'TO', 'JJ', 'NNS', ',', 'VBG', 'IN', 'JJ', ':', 'NN', 'NNS', 'TO', 'NN', ':', 'JJ', 'JJ', 'NN', 'NNS', '.']",26
text_generation,3,20,"However , conversation generation is a much more complex and flexible task as there are less "" word - to - words "" relations between inputs and responses .","['However', ',', 'conversation', 'generation', 'is', 'a', 'much', 'more', 'complex', 'and', 'flexible', 'task', 'as', 'there', 'are', 'less', '""', 'word', '-', 'to', '-', 'words', '""', 'relations', 'between', 'inputs', 'and', 'responses', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'NN', 'NN', 'VBZ', 'DT', 'RB', 'RBR', 'JJ', 'CC', 'JJ', 'NN', 'IN', 'EX', 'VBP', 'JJR', 'JJ', 'NN', ':', 'TO', ':', 'NNS', 'VBP', 'NNS', 'IN', 'NNS', 'CC', 'NNS', '.']",29
text_generation,3,24,"To address this problem , we propose a novel Auto - Encoder Matching model to learn utterance - level dependency .","['To', 'address', 'this', 'problem', ',', 'we', 'propose', 'a', 'novel', 'Auto', '-', 'Encoder', 'Matching', 'model', 'to', 'learn', 'utterance', '-', 'level', 'dependency', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NNP', ':', 'NN', 'VBG', 'NN', 'TO', 'VB', 'JJ', ':', 'NN', 'NN', '.']",21
text_generation,3,25,"First , motivated by , we use two auto- encoders to learn the semantic representations of inputs and responses in an unsupervised style .","['First', ',', 'motivated', 'by', ',', 'we', 'use', 'two', 'auto-', 'encoders', 'to', 'learn', 'the', 'semantic', 'representations', 'of', 'inputs', 'and', 'responses', 'in', 'an', 'unsupervised', 'style', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'VBN', 'IN', ',', 'PRP', 'VBP', 'CD', 'JJ', 'NNS', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'IN', 'NNS', 'CC', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",24
text_generation,3,26,"Second , given the utterance - level representations , the mapping module is taught to learn the utterance - level dependency .","['Second', ',', 'given', 'the', 'utterance', '-', 'level', 'representations', ',', 'the', 'mapping', 'module', 'is', 'taught', 'to', 'learn', 'the', 'utterance', '-', 'level', 'dependency', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['JJ', ',', 'VBN', 'DT', 'NN', ':', 'NN', 'NNS', ',', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NN', ':', 'NN', 'NN', '.']",22
text_generation,3,86,"For dialogue generation , we set the maximum length to 15 words for each generated sentence .","['For', 'dialogue', 'generation', ',', 'we', 'set', 'the', 'maximum', 'length', 'to', '15', 'words', 'for', 'each', 'generated', 'sentence', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'TO', 'CD', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",17
text_generation,3,87,"Based on the performance on the validation set , we set the hidden size to 512 , embedding size to 64 and vocabulary size to 40 K for baseline models and the proposed model .","['Based', 'on', 'the', 'performance', 'on', 'the', 'validation', 'set', ',', 'we', 'set', 'the', 'hidden', 'size', 'to', '512', ',', 'embedding', 'size', 'to', '64', 'and', 'vocabulary', 'size', 'to', '40', 'K', 'for', 'baseline', 'models', 'and', 'the', 'proposed', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'TO', 'CD', ',', 'VBG', 'NN', 'TO', 'CD', 'CC', 'JJ', 'NN', 'TO', 'CD', 'NNP', 'IN', 'NN', 'NNS', 'CC', 'DT', 'VBN', 'NN', '.']",35
text_generation,3,88,"The parameters are updated by the Adam algorithm ( Kingma and Ba , 2014 ) and initialized by sampling from the uniform distribution ( [? 0.1 , 0.1 ] ) .","['The', 'parameters', 'are', 'updated', 'by', 'the', 'Adam', 'algorithm', '(', 'Kingma', 'and', 'Ba', ',', '2014', ')', 'and', 'initialized', 'by', 'sampling', 'from', 'the', 'uniform', 'distribution', '(', '[?', '0.1', ',', '0.1', ']', ')', '.']","['O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'VBP', 'VBN', 'IN', 'DT', 'NNP', 'NN', '(', 'NNP', 'CC', 'NNP', ',', 'CD', ')', 'CC', 'VBN', 'IN', 'VBG', 'IN', 'DT', 'JJ', 'NN', '(', 'JJ', 'CD', ',', 'CD', 'NN', ')', '.']",31
text_generation,3,89,The initial learning rate is 0.002 and the model is trained in minibatches with a batch size of 256 . ? 1 and ?,"['The', 'initial', 'learning', 'rate', 'is', '0.002', 'and', 'the', 'model', 'is', 'trained', 'in', 'minibatches', 'with', 'a', 'batch', 'size', 'of', '256', '.', '?', '1', 'and', '?']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'NN', 'VBZ', 'CD', 'CC', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'CD', '.', '.', 'CD', 'CC', '.']",24
text_generation,3,97,The proposed AEM model significantly outperforms the Seq2Seq model .,"['The', 'proposed', 'AEM', 'model', 'significantly', 'outperforms', 'the', 'Seq2Seq', 'model', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'VBN', 'NNP', 'NN', 'RB', 'VBZ', 'DT', 'NNP', 'NN', '.']",10
text_generation,3,98,It demonstrates the effectiveness of utterance - level dependency on improving the quality of generated text .,"['It', 'demonstrates', 'the', 'effectiveness', 'of', 'utterance', '-', 'level', 'dependency', 'on', 'improving', 'the', 'quality', 'of', 'generated', 'text', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'DT', 'NN', 'IN', 'JJ', ':', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'NN', '.']",17
text_generation,3,100,The improvement from the AEM model to the AEM + Attention model 2 is 0.68 BLEU - 4 point .,"['The', 'improvement', 'from', 'the', 'AEM', 'model', 'to', 'the', 'AEM', '+', 'Attention', 'model', '2', 'is', '0.68', 'BLEU', '-', '4', 'point', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'IN', 'DT', 'NNP', 'NN', 'TO', 'DT', 'NNP', 'NNP', 'NNP', 'NN', 'CD', 'VBZ', 'CD', 'NNP', ':', 'CD', 'NN', '.']",20
text_generation,3,104,We find that the AEM model achieves significant improvement on the diversity of generated text .,"['We', 'find', 'that', 'the', 'AEM', 'model', 'achieves', 'significant', 'improvement', 'on', 'the', 'diversity', 'of', 'generated', 'text', '.']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'IN', 'DT', 'NNP', 'NN', 'VBZ', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NN', '.']",16
text_generation,3,106,"Also , it should be noticed that the attention mechanism performs almost the same compared to the AEM model ( 31.2 K vs. 34.6 K in terms of Dist - 3 ) , which indicates that the utterance - level dependency and the word - level dependency are both indispensable for dialogue generation .","['Also', ',', 'it', 'should', 'be', 'noticed', 'that', 'the', 'attention', 'mechanism', 'performs', 'almost', 'the', 'same', 'compared', 'to', 'the', 'AEM', 'model', '(', '31.2', 'K', 'vs.', '34.6', 'K', 'in', 'terms', 'of', 'Dist', '-', '3', ')', ',', 'which', 'indicates', 'that', 'the', 'utterance', '-', 'level', 'dependency', 'and', 'the', 'word', '-', 'level', 'dependency', 'are', 'both', 'indispensable', 'for', 'dialogue', 'generation', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'MD', 'VB', 'VBN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'RB', 'DT', 'JJ', 'VBN', 'TO', 'DT', 'NNP', 'NN', '(', 'CD', 'NNP', 'FW', 'CD', 'NNP', 'IN', 'NNS', 'IN', 'NNP', ':', 'CD', ')', ',', 'WDT', 'VBZ', 'IN', 'DT', 'NN', ':', 'NN', 'NN', 'CC', 'DT', 'NN', ':', 'NN', 'NN', 'VBP', 'DT', 'JJ', 'IN', 'NN', 'NN', '.']",54
text_generation,3,107,"Therefore , by combining the two dependencies together , the AEM + Attention model achieves the best results .","['Therefore', ',', 'by', 'combining', 'the', 'two', 'dependencies', 'together', ',', 'the', 'AEM', '+', 'Attention', 'model', 'achieves', 'the', 'best', 'results', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'IN', 'VBG', 'DT', 'CD', 'NNS', 'RB', ',', 'DT', 'NNP', 'NNP', 'NNP', 'NN', 'VBZ', 'DT', 'JJS', 'NNS', '.']",19
text_generation,3,120,shows the results of human evaluation .,"['shows', 'the', 'results', 'of', 'human', 'evaluation', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['VBZ', 'DT', 'NNS', 'IN', 'JJ', 'NN', '.']",7
text_generation,3,121,The inter-annotator agreement is satisfactory considering the difficulty of human evaluation .,"['The', 'inter-annotator', 'agreement', 'is', 'satisfactory', 'considering', 'the', 'difficulty', 'of', 'human', 'evaluation', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'JJ', 'VBG', 'DT', 'NN', 'IN', 'JJ', 'NN', '.']",12
text_generation,3,122,"The Pearson 's correlation coefficient is 0.69 on coherence and 0.57 on fluency , with p < 0.0001 .","['The', 'Pearson', ""'s"", 'correlation', 'coefficient', 'is', '0.69', 'on', 'coherence', 'and', '0.57', 'on', 'fluency', ',', 'with', 'p', '<', '0.0001', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NNP', 'POS', 'NN', 'NN', 'VBZ', 'CD', 'IN', 'NN', 'CC', 'CD', 'IN', 'NN', ',', 'IN', 'JJ', '$', 'CD', '.']",19
text_generation,3,123,"First , it is clear that the AEM model outperforms the Seq2Seq model with a large margin , which proves the effectiveness of the AEM model on generating high quality responses .","['First', ',', 'it', 'is', 'clear', 'that', 'the', 'AEM', 'model', 'outperforms', 'the', 'Seq2Seq', 'model', 'with', 'a', 'large', 'margin', ',', 'which', 'proves', 'the', 'effectiveness', 'of', 'the', 'AEM', 'model', 'on', 'generating', 'high', 'quality', 'responses', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBZ', 'JJ', 'IN', 'DT', 'NNP', 'NN', 'VBZ', 'DT', 'NNP', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', 'IN', 'VBG', 'JJ', 'NN', 'NNS', '.']",32
text_generation,3,124,"Second , it is interesting to note that with the attention mechanism , the coherence is decreased slightly in the Seq2Seq model but increased significantly in the AEM model .","['Second', ',', 'it', 'is', 'interesting', 'to', 'note', 'that', 'with', 'the', 'attention', 'mechanism', ',', 'the', 'coherence', 'is', 'decreased', 'slightly', 'in', 'the', 'Seq2Seq', 'model', 'but', 'increased', 'significantly', 'in', 'the', 'AEM', 'model', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['JJ', ',', 'PRP', 'VBZ', 'VBG', 'TO', 'VB', 'IN', 'IN', 'DT', 'NN', 'NN', ',', 'DT', 'NN', 'VBZ', 'VBN', 'RB', 'IN', 'DT', 'NNP', 'NN', 'CC', 'VBD', 'RB', 'IN', 'DT', 'NNP', 'NN', '.']",30
text_generation,3,126,"Therefore , it is expected that the AEM + Attention model achieves the best G-score .","['Therefore', ',', 'it', 'is', 'expected', 'that', 'the', 'AEM', '+', 'Attention', 'model', 'achieves', 'the', 'best', 'G-score', '.']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBZ', 'VBN', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NN', 'VBZ', 'DT', 'JJS', 'NN', '.']",16
text_generation,0,4,"As a new way of training generative models , Generative Adversarial Net ( GAN ) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real - valued data .","['As', 'a', 'new', 'way', 'of', 'training', 'generative', 'models', ',', 'Generative', 'Adversarial', 'Net', '(', 'GAN', ')', 'that', 'uses', 'a', 'discriminative', 'model', 'to', 'guide', 'the', 'training', 'of', 'the', 'generative', 'model', 'has', 'enjoyed', 'considerable', 'success', 'in', 'generating', 'real', '-', 'valued', 'data', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'JJ', 'NNS', ',', 'NNP', 'NNP', 'NNP', '(', 'NNP', ')', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', 'JJ', 'NN', 'IN', 'VBG', 'JJ', ':', 'VBN', 'NNS', '.']",39
text_generation,0,5,"However , it has limitations when the goal is for generating sequences of discrete tokens .","['However', ',', 'it', 'has', 'limitations', 'when', 'the', 'goal', 'is', 'for', 'generating', 'sequences', 'of', 'discrete', 'tokens', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBZ', 'NNS', 'WRB', 'DT', 'NN', 'VBZ', 'IN', 'VBG', 'NNS', 'IN', 'JJ', 'NNS', '.']",16
text_generation,0,13,Generating sequential synthetic data that mimics the real one is an important problem in unsupervised learning .,"['Generating', 'sequential', 'synthetic', 'data', 'that', 'mimics', 'the', 'real', 'one', 'is', 'an', 'important', 'problem', 'in', 'unsupervised', 'learning', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'JJ', 'JJ', 'NNS', 'WDT', 'VBD', 'DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', '.']",17
text_generation,0,32,"In this paper , to address the above two issues , we follow ) and consider the sequence generation procedure as a sequential decision making process .","['In', 'this', 'paper', ',', 'to', 'address', 'the', 'above', 'two', 'issues', ',', 'we', 'follow', ')', 'and', 'consider', 'the', 'sequence', 'generation', 'procedure', 'as', 'a', 'sequential', 'decision', 'making', 'process', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', ',', 'TO', 'VB', 'DT', 'JJ', 'CD', 'NNS', ',', 'PRP', 'VBP', ')', 'CC', 'VB', 'DT', 'NN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBG', 'NN', '.']",27
text_generation,0,33,The generative model is treated as an agent of reinforcement learning ( RL ) ; the state is the generated tokens so far and the action is the next token to be generated .,"['The', 'generative', 'model', 'is', 'treated', 'as', 'an', 'agent', 'of', 'reinforcement', 'learning', '(', 'RL', ')', ';', 'the', 'state', 'is', 'the', 'generated', 'tokens', 'so', 'far', 'and', 'the', 'action', 'is', 'the', 'next', 'token', 'to', 'be', 'generated', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NN', '(', 'NNP', ')', ':', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NNS', 'RB', 'RB', 'CC', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'TO', 'VB', 'VBN', '.']",34
text_generation,0,34,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the evaluation to guide the learning of the generative model .","['Unlike', 'the', 'work', 'in', ')', 'that', 'requires', 'a', 'task', '-', 'specific', 'sequence', 'score', ',', 'such', 'as', 'BLEU', 'in', 'machine', 'translation', ',', 'to', 'give', 'the', 'reward', ',', 'we', 'employ', 'a', 'discriminator', 'to', 'evaluate', 'the', 'sequence', 'and', 'feedback', 'the', 'evaluation', 'to', 'guide', 'the', 'learning', 'of', 'the', 'generative', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'IN', ')', 'WDT', 'VBZ', 'DT', 'NN', ':', 'JJ', 'NN', 'NN', ',', 'JJ', 'IN', 'NNP', 'IN', 'NN', 'NN', ',', 'TO', 'VB', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'NN', 'TO', 'VB', 'DT', 'NN', 'CC', 'VB', 'DT', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",47
text_generation,0,35,"To solve the problem that the gradient can not pass back to the generative model when the output is discrete , we regard the generative model as a stochastic parametrized policy .","['To', 'solve', 'the', 'problem', 'that', 'the', 'gradient', 'can', 'not', 'pass', 'back', 'to', 'the', 'generative', 'model', 'when', 'the', 'output', 'is', 'discrete', ',', 'we', 'regard', 'the', 'generative', 'model', 'as', 'a', 'stochastic', 'parametrized', 'policy', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'MD', 'RB', 'VB', 'RB', 'TO', 'DT', 'JJ', 'NN', 'WRB', 'DT', 'NN', 'VBZ', 'JJ', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', '.']",32
text_generation,0,36,"In our policy gradient , we employ Monte Carlo ( MC ) search to approximate the state - action value .","['In', 'our', 'policy', 'gradient', ',', 'we', 'employ', 'Monte', 'Carlo', '(', 'MC', ')', 'search', 'to', 'approximate', 'the', 'state', '-', 'action', 'value', '.']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'PRP$', 'NN', 'NN', ',', 'PRP', 'VBP', 'NNP', 'NNP', '(', 'NNP', ')', 'NN', 'TO', 'VB', 'DT', 'NN', ':', 'NN', 'NN', '.']",21
text_generation,0,37,"We directly train the policy ( generative model ) via policy gradient , which naturally avoids the differentiation difficulty for discrete data in a conventional GAN .","['We', 'directly', 'train', 'the', 'policy', '(', 'generative', 'model', ')', 'via', 'policy', 'gradient', ',', 'which', 'naturally', 'avoids', 'the', 'differentiation', 'difficulty', 'for', 'discrete', 'data', 'in', 'a', 'conventional', 'GAN', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', '(', 'JJ', 'NN', ')', 'IN', 'NN', 'NN', ',', 'WDT', 'RB', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NNP', '.']",27
text_generation,0,185,"To setup the synthetic data experiments , we first initialize the parameters of an LSTM network following the normal distribution N ( 0 , 1 ) as the oracle describing the real data distribution G oracle ( x t |x 1 , . . . , x t?1 ) .","['To', 'setup', 'the', 'synthetic', 'data', 'experiments', ',', 'we', 'first', 'initialize', 'the', 'parameters', 'of', 'an', 'LSTM', 'network', 'following', 'the', 'normal', 'distribution', 'N', '(', '0', ',', '1', ')', 'as', 'the', 'oracle', 'describing', 'the', 'real', 'data', 'distribution', 'G', 'oracle', '(', 'x', 't', '|x', '1', ',', '.', '.', '.', ',', 'x', 't?1', ')', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['TO', 'VB', 'DT', 'JJ', 'NN', 'NNS', ',', 'PRP', 'RB', 'VB', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'VBG', 'DT', 'JJ', 'NN', 'NNP', '(', 'CD', ',', 'CD', ')', 'IN', 'DT', 'NN', 'VBG', 'DT', 'JJ', 'NNS', 'NN', 'NNP', 'NN', '(', 'JJ', 'NN', 'VBD', 'CD', ',', '.', '.', '.', ',', 'NNP', 'NN', ')', '.']",50
text_generation,0,187,"In SeqGAN algorithm , the training set for the discriminator is comprised by the generated examples with the label 0 and the instances from S with the label","['In', 'SeqGAN', 'algorithm', ',', 'the', 'training', 'set', 'for', 'the', 'discriminator', 'is', 'comprised', 'by', 'the', 'generated', 'examples', 'with', 'the', 'label', '0', 'and', 'the', 'instances', 'from', 'S', 'with', 'the', 'label']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n']","['IN', 'NNP', 'NN', ',', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'CD', 'CC', 'DT', 'NNS', 'IN', 'NNP', 'IN', 'DT', 'NN']",28
text_generation,0,189,"For different tasks , one should design specific structure for the convolutional layer and in our synthetic data experiments , the kernel size is from 1 to T and the number of each kernel size is between 100 to 200 3 .","['For', 'different', 'tasks', ',', 'one', 'should', 'design', 'specific', 'structure', 'for', 'the', 'convolutional', 'layer', 'and', 'in', 'our', 'synthetic', 'data', 'experiments', ',', 'the', 'kernel', 'size', 'is', 'from', '1', 'to', 'T', 'and', 'the', 'number', 'of', 'each', 'kernel', 'size', 'is', 'between', '100', 'to', '200', '3', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'JJ', 'NNS', ',', 'CD', 'MD', 'VB', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'CC', 'IN', 'PRP$', 'JJ', 'NN', 'NNS', ',', 'DT', 'NN', 'NN', 'VBZ', 'IN', 'CD', 'TO', 'NNP', 'CC', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'IN', 'CD', 'TO', 'CD', 'CD', '.']",42
text_generation,0,190,Dropout ) and L2 regularization are used to avoid over-fitting .,"['Dropout', ')', 'and', 'L2', 'regularization', 'are', 'used', 'to', 'avoid', 'over-fitting', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['NN', ')', 'CC', 'NNP', 'NN', 'VBP', 'VBN', 'TO', 'VB', 'NN', '.']",11
text_generation,0,192,The first model is a random token generation .,"['The', 'first', 'model', 'is', 'a', 'random', 'token', 'generation', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', '.']",9
text_generation,0,193,The second one is the MLE trained LSTM G ? .,"['The', 'second', 'one', 'is', 'the', 'MLE', 'trained', 'LSTM', 'G', '?', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['DT', 'JJ', 'NN', 'VBZ', 'DT', 'NNP', 'VBD', 'NNP', 'NNP', '.', '.']",11
text_generation,0,194,The third one is scheduled sampling .,"['The', 'third', 'one', 'is', 'scheduled', 'sampling', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['DT', 'JJ', 'CD', 'VBZ', 'VBN', 'VBG', '.']",7
text_generation,0,195,The fourth one is the Policy Gradient with BLEU ( PG - BLEU ) .,"['The', 'fourth', 'one', 'is', 'the', 'Policy', 'Gradient', 'with', 'BLEU', '(', 'PG', '-', 'BLEU', ')', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'JJ', 'CD', 'VBZ', 'DT', 'NNP', 'NNP', 'IN', 'NNP', '(', 'NNP', ':', 'NNP', ')', '.']",15
text_generation,0,196,"In the scheduled sampling , the training process gradually changes from a fully guided scheme feeding the true previous tokens into LSTM , towards a less guided scheme which mostly feeds the LSTM with its generated tokens .","['In', 'the', 'scheduled', 'sampling', ',', 'the', 'training', 'process', 'gradually', 'changes', 'from', 'a', 'fully', 'guided', 'scheme', 'feeding', 'the', 'true', 'previous', 'tokens', 'into', 'LSTM', ',', 'towards', 'a', 'less', 'guided', 'scheme', 'which', 'mostly', 'feeds', 'the', 'LSTM', 'with', 'its', 'generated', 'tokens', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', ',', 'DT', 'NN', 'NN', 'RB', 'VBZ', 'IN', 'DT', 'RB', 'JJ', 'NN', 'VBG', 'DT', 'JJ', 'JJ', 'NNS', 'IN', 'NNP', ',', 'VBZ', 'DT', 'RBR', 'JJ', 'NN', 'WDT', 'RB', 'VBZ', 'DT', 'NNP', 'IN', 'PRP$', 'JJ', 'NNS', '.']",38
text_generation,0,197,A curriculum rate ? is used to control the probability of replacing the true tokens with the generated ones .,"['A', 'curriculum', 'rate', '?', 'is', 'used', 'to', 'control', 'the', 'probability', 'of', 'replacing', 'the', 'true', 'tokens', 'with', 'the', 'generated', 'ones', '.']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NN', '.', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'VBG', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NNS', '.']",20
text_generation,0,202,"Since the evaluation metric is fundamentally instructive , we can see the impact of SeqGAN , which outperforms other baselines significantly .","['Since', 'the', 'evaluation', 'metric', 'is', 'fundamentally', 'instructive', ',', 'we', 'can', 'see', 'the', 'impact', 'of', 'SeqGAN', ',', 'which', 'outperforms', 'other', 'baselines', 'significantly', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'NN', 'JJ', 'VBZ', 'RB', 'JJ', ',', 'PRP', 'MD', 'VB', 'DT', 'NN', 'IN', 'NNP', ',', 'WDT', 'VBZ', 'JJ', 'NNS', 'RB', '.']",22
text_generation,0,203,"A significance T - test on the NLL oracle score distribution of the generated sequences from the compared models is also performed , which demonstrates the significant improvement of SeqGAN over all compared models .","['A', 'significance', 'T', '-', 'test', 'on', 'the', 'NLL', 'oracle', 'score', 'distribution', 'of', 'the', 'generated', 'sequences', 'from', 'the', 'compared', 'models', 'is', 'also', 'performed', ',', 'which', 'demonstrates', 'the', 'significant', 'improvement', 'of', 'SeqGAN', 'over', 'all', 'compared', 'models', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['DT', 'NN', 'NNP', ':', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'VBN', 'NNS', 'VBZ', 'RB', 'VBN', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'IN', 'DT', 'VBN', 'NNS', '.']",35
text_generation,0,205,"After about 150 training epochs , both the maximum likelihood estimation and the schedule sampling methods converge to a relatively high NLL oracle score , whereas SeqGAN can improve the limit of the generator with the same structure as the baselines significantly .","['After', 'about', '150', 'training', 'epochs', ',', 'both', 'the', 'maximum', 'likelihood', 'estimation', 'and', 'the', 'schedule', 'sampling', 'methods', 'converge', 'to', 'a', 'relatively', 'high', 'NLL', 'oracle', 'score', ',', 'whereas', 'SeqGAN', 'can', 'improve', 'the', 'limit', 'of', 'the', 'generator', 'with', 'the', 'same', 'structure', 'as', 'the', 'baselines', 'significantly', '.']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O']","['IN', 'RB', 'CD', 'NN', 'NN', ',', 'CC', 'DT', 'JJ', 'NN', 'NN', 'CC', 'DT', 'NN', 'VBG', 'NNS', 'NN', 'TO', 'DT', 'RB', 'JJ', 'NNP', 'NN', 'NN', ',', 'IN', 'NNP', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNS', 'RB', '.']",43
text_generation,0,206,This indicates the prospect of applying adversarial training strategies to discrete sequence generative models to breakthrough the limitations of MLE .,"['This', 'indicates', 'the', 'prospect', 'of', 'applying', 'adversarial', 'training', 'strategies', 'to', 'discrete', 'sequence', 'generative', 'models', 'to', 'breakthrough', 'the', 'limitations', 'of', 'MLE', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['DT', 'VBZ', 'DT', 'NN', 'IN', 'VBG', 'JJ', 'NN', 'NNS', 'TO', 'VB', 'NN', 'JJ', 'NNS', 'TO', 'IN', 'DT', 'NNS', 'IN', 'NNP', '.']",21
text_generation,0,207,"Additionally , SeqGAN outperforms PG - BLEU , which means the discriminative signal in GAN is more general and effective than a predefined score ( e.g. BLEU ) to guide the generative policy to capture the underlying distribution of the sequence data .","['Additionally', ',', 'SeqGAN', 'outperforms', 'PG', '-', 'BLEU', ',', 'which', 'means', 'the', 'discriminative', 'signal', 'in', 'GAN', 'is', 'more', 'general', 'and', 'effective', 'than', 'a', 'predefined', 'score', '(', 'e.g.', 'BLEU', ')', 'to', 'guide', 'the', 'generative', 'policy', 'to', 'capture', 'the', 'underlying', 'distribution', 'of', 'the', 'sequence', 'data', '.']","['O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'NNP', 'VBZ', 'NNP', ':', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'VBZ', 'RBR', 'JJ', 'CC', 'JJ', 'IN', 'DT', 'VBN', 'NN', '(', 'JJ', 'NNP', ')', 'TO', 'VB', 'DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'NNS', '.']",43
query_wellformedness,0,2,Identifying Well - formed Natural Language Questions,"['Identifying', 'Well', '-', 'formed', 'Natural', 'Language', 'Questions']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['VBG', 'NNP', ':', 'VBD', 'JJ', 'NNP', 'NNS']",7
query_wellformedness,0,19,"Thus , in this paper we present a model to predict whether a given query is a well - formed natural language question .","['Thus', ',', 'in', 'this', 'paper', 'we', 'present', 'a', 'model', 'to', 'predict', 'whether', 'a', 'given', 'query', 'is', 'a', 'well', '-', 'formed', 'natural', 'language', 'question', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'IN', 'DT', 'NN', 'PRP', 'VBD', 'DT', 'NN', 'TO', 'VB', 'IN', 'DT', 'VBN', 'NN', 'VBZ', 'DT', 'RB', ':', 'VBN', 'JJ', 'NN', 'NN', '.']",24
query_wellformedness,0,20,"We construct and publicly release a dataset of 25,100 queries annotated with the probability of being a well - formed natural language question ( 2.1 ) .","['We', 'construct', 'and', 'publicly', 'release', 'a', 'dataset', 'of', '25,100', 'queries', 'annotated', 'with', 'the', 'probability', 'of', 'being', 'a', 'well', '-', 'formed', 'natural', 'language', 'question', '(', '2.1', ')', '.']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'CC', 'RB', 'VB', 'DT', 'NN', 'IN', 'CD', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'IN', 'VBG', 'DT', 'RB', ':', 'VBN', 'JJ', 'NN', 'NN', '(', 'CD', ')', '.']",27
query_wellformedness,0,21,We then train a feed - forward neural network classifier that uses the lexical and syntactic features extracted from the query on this data ( 2.2 ) .,"['We', 'then', 'train', 'a', 'feed', '-', 'forward', 'neural', 'network', 'classifier', 'that', 'uses', 'the', 'lexical', 'and', 'syntactic', 'features', 'extracted', 'from', 'the', 'query', 'on', 'this', 'data', '(', '2.2', ')', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'NN', ':', 'RB', 'JJ', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'JJ', 'CC', 'JJ', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', '(', 'CD', ')', '.']",28
query_wellformedness,0,24,Our dataset ise available for download at http://goo.gl/language/ query-wellformedness .,"['Our', 'dataset', 'ise', 'available', 'for', 'download', 'at', 'http://goo.gl/language/', 'query-wellformedness', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'NN', 'JJ', 'IN', 'NN', 'IN', 'JJ', 'NN', '.']",10
query_wellformedness,0,58,"The best performance obtained is 70.7 % while using word - 1 , 2 - grams and POS - 1 , 2 , 3 - grams as features .","['The', 'best', 'performance', 'obtained', 'is', '70.7', '%', 'while', 'using', 'word', '-', '1', ',', '2', '-', 'grams', 'and', 'POS', '-', '1', ',', '2', ',', '3', '-', 'grams', 'as', 'features', '.']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['DT', 'JJS', 'NN', 'VBN', 'VBZ', 'CD', 'NN', 'IN', 'VBG', 'NN', ':', 'CD', ',', 'CD', ':', 'NNS', 'CC', 'NNP', ':', 'CD', ',', 'CD', ',', 'CD', ':', 'NN', 'IN', 'NNS', '.']",29
query_wellformedness,0,59,Using POS n-grams gave a strong boost of 5.2 points over word unigrams and bigrams .,"['Using', 'POS', 'n-grams', 'gave', 'a', 'strong', 'boost', 'of', '5.2', 'points', 'over', 'word', 'unigrams', 'and', 'bigrams', '.']","['B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['VBG', 'NNP', 'NNS', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'CD', 'NNS', 'IN', 'NN', 'NNS', 'CC', 'NNS', '.']",16
query_wellformedness,0,60,"Although character - 3 , 4 grams gave improvement over word unigrams and bigrams , the performance did not sustain when combined with POS tags .","['Although', 'character', '-', '3', ',', '4', 'grams', 'gave', 'improvement', 'over', 'word', 'unigrams', 'and', 'bigrams', ',', 'the', 'performance', 'did', 'not', 'sustain', 'when', 'combined', 'with', 'POS', 'tags', '.']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['IN', 'NN', ':', 'CD', ',', 'CD', 'NNS', 'VBD', 'NN', 'IN', 'NN', 'NNS', 'CC', 'NNS', ',', 'DT', 'NN', 'VBD', 'RB', 'VB', 'WRB', 'VBN', 'IN', 'NNP', 'NNS', '.']",26
query_wellformedness,0,71,The majority class baseline is 61.5 % which corresponds to all queries being classified non-wellformed .,"['The', 'majority', 'class', 'baseline', 'is', '61.5', '%', 'which', 'corresponds', 'to', 'all', 'queries', 'being', 'classified', 'non-wellformed', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'NN', 'NN', 'VBZ', 'CD', 'NN', 'WDT', 'VBZ', 'TO', 'DT', 'NNS', 'VBG', 'VBN', 'JJ', '.']",16
query_wellformedness,0,72,The question word baseline that classifies any query starting with a question word word n-grams char n-grams POS n -grams pwf ( q ) :,"['The', 'question', 'word', 'baseline', 'that', 'classifies', 'any', 'query', 'starting', 'with', 'a', 'question', 'word', 'word', 'n-grams', 'char', 'n-grams', 'POS', 'n', '-grams', 'pwf', '(', 'q', ')', ':']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'NN', 'NN', 'IN', 'VBZ', 'DT', 'NN', 'VBG', 'IN', 'DT', 'NN', 'NN', 'NN', 'JJ', 'NN', 'JJ', 'NNP', 'NN', 'NNP', 'NN', '(', 'NN', ')', ':']",25
text-to-speech_synthesis,1,2,"FastSpeech : Fast , Robust and Controllable Text to Speech","['FastSpeech', ':', 'Fast', ',', 'Robust', 'and', 'Controllable', 'Text', 'to', 'Speech']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['NN', ':', 'NNP', ',', 'NNP', 'CC', 'NNP', 'NNP', 'TO', 'VB']",10
text-to-speech_synthesis,1,4,Neural network based end - to - end text to speech ( TTS ) has significantly improved the quality of synthesized speech .,"['Neural', 'network', 'based', 'end', '-', 'to', '-', 'end', 'text', 'to', 'speech', '(', 'TTS', ')', 'has', 'significantly', 'improved', 'the', 'quality', 'of', 'synthesized', 'speech', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'VBN', 'NN', ':', 'TO', ':', 'NN', 'NN', 'TO', 'VB', '(', 'NNP', ')', 'VBZ', 'RB', 'VBN', 'DT', 'NN', 'IN', 'JJ', 'NN', '.']",23
text-to-speech_synthesis,1,15,Text to speech ( TTS ) has attracted a lot of attention in recent years due to the advance in deep learning .,"['Text', 'to', 'speech', '(', 'TTS', ')', 'has', 'attracted', 'a', 'lot', 'of', 'attention', 'in', 'recent', 'years', 'due', 'to', 'the', 'advance', 'in', 'deep', 'learning', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NN', 'TO', 'VB', '(', 'NNP', ')', 'VBZ', 'VBN', 'DT', 'NN', 'IN', 'NN', 'IN', 'JJ', 'NNS', 'JJ', 'TO', 'DT', 'NN', 'IN', 'JJ', 'NN', '.']",23
text-to-speech_synthesis,1,18,Neural network based TTS has outperformed conventional concatenative and statistical parametric approaches in terms of speech quality .,"['Neural', 'network', 'based', 'TTS', 'has', 'outperformed', 'conventional', 'concatenative', 'and', 'statistical', 'parametric', 'approaches', 'in', 'terms', 'of', 'speech', 'quality', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', 'NN', 'VBN', 'NNP', 'VBZ', 'VBN', 'JJ', 'JJ', 'CC', 'JJ', 'NN', 'NNS', 'IN', 'NNS', 'IN', 'NN', 'NN', '.']",18
text-to-speech_synthesis,1,28,"Considering the monotonous alignment between text and speech , to speedup mel- spectrogram generation , in this work , we propose a novel model , FastSpeech , which takes a text ( phoneme ) sequence as input and generates mel-spectrograms non-autoregressively .","['Considering', 'the', 'monotonous', 'alignment', 'between', 'text', 'and', 'speech', ',', 'to', 'speedup', 'mel-', 'spectrogram', 'generation', ',', 'in', 'this', 'work', ',', 'we', 'propose', 'a', 'novel', 'model', ',', 'FastSpeech', ',', 'which', 'takes', 'a', 'text', '(', 'phoneme', ')', 'sequence', 'as', 'input', 'and', 'generates', 'mel-spectrograms', 'non-autoregressively', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']","['VBG', 'DT', 'JJ', 'NN', 'IN', 'NN', 'CC', 'NN', ',', 'TO', 'VB', 'JJ', 'NN', 'NN', ',', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', 'NN', ',', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NN', '(', 'NN', ')', 'NN', 'IN', 'NN', 'CC', 'VBZ', 'JJ', 'RB', '.']",42
text-to-speech_synthesis,1,29,It adopts a feed - forward network based on the self - attention in Transformer and 1D convolution .,"['It', 'adopts', 'a', 'feed', '-', 'forward', 'network', 'based', 'on', 'the', 'self', '-', 'attention', 'in', 'Transformer', 'and', '1D', 'convolution', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBZ', 'DT', 'JJ', ':', 'NN', 'NN', 'VBN', 'IN', 'DT', 'NN', ':', 'NN', 'IN', 'NNP', 'CC', 'CD', 'NN', '.']",19
text-to-speech_synthesis,1,30,"Since a mel-spectrogram sequence is much longer than its corresponding phoneme sequence , in order to solve the problem of length mismatch between the two sequences , FastSpeech adopts a length regulator that up - samples the phoneme sequence according to the phoneme duration ( i.e. , the number of mel- spectrograms that each phoneme corresponds to ) to match the length of the mel-spectrogram sequence .","['Since', 'a', 'mel-spectrogram', 'sequence', 'is', 'much', 'longer', 'than', 'its', 'corresponding', 'phoneme', 'sequence', ',', 'in', 'order', 'to', 'solve', 'the', 'problem', 'of', 'length', 'mismatch', 'between', 'the', 'two', 'sequences', ',', 'FastSpeech', 'adopts', 'a', 'length', 'regulator', 'that', 'up', '-', 'samples', 'the', 'phoneme', 'sequence', 'according', 'to', 'the', 'phoneme', 'duration', '(', 'i.e.', ',', 'the', 'number', 'of', 'mel-', 'spectrograms', 'that', 'each', 'phoneme', 'corresponds', 'to', ')', 'to', 'match', 'the', 'length', 'of', 'the', 'mel-spectrogram', 'sequence', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'JJ', 'NN', 'VBZ', 'RB', 'JJR', 'IN', 'PRP$', 'JJ', 'NN', 'NN', ',', 'IN', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', 'NN', 'IN', 'DT', 'CD', 'NNS', ',', 'NNP', 'VBZ', 'DT', 'NN', 'NN', 'WDT', 'RP', ':', 'VBZ', 'DT', 'NN', 'NN', 'VBG', 'TO', 'DT', 'NN', 'NN', '(', 'FW', ',', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'VBZ', 'TO', ')', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",67
text-to-speech_synthesis,1,31,"The regulator is built on a phoneme duration predictor , which predicts the duration of each phoneme .","['The', 'regulator', 'is', 'built', 'on', 'a', 'phoneme', 'duration', 'predictor', ',', 'which', 'predicts', 'the', 'duration', 'of', 'each', 'phoneme', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', '.']",18
text-to-speech_synthesis,1,130,"We first train the autoregressive Transformer TTS model on 4 NVIDIA V100 GPUs , with batchsize of 16 sentences on each GPU .","['We', 'first', 'train', 'the', 'autoregressive', 'Transformer', 'TTS', 'model', 'on', '4', 'NVIDIA', 'V100', 'GPUs', ',', 'with', 'batchsize', 'of', '16', 'sentences', 'on', 'each', 'GPU', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBP', 'DT', 'JJ', 'NNP', 'NNP', 'NN', 'IN', 'CD', 'NNP', 'NNP', 'NNP', ',', 'IN', 'NN', 'IN', 'CD', 'NNS', 'IN', 'DT', 'NNP', '.']",23
text-to-speech_synthesis,1,131,"We use the Adam optimizer with ? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9 and follow the same learning rate schedule in .","['We', 'use', 'the', 'Adam', 'optimizer', 'with', '?', '1', '=', '0.9', ',', '?', '2', '=', '0.98', ',', '?', '=', '10', '?9', 'and', 'follow', 'the', 'same', 'learning', 'rate', 'schedule', 'in', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'NNP', 'NN', 'IN', '.', 'CD', 'NN', 'CD', ',', '.', 'CD', 'NN', 'CD', ',', '.', '$', 'CD', 'NN', 'CC', 'VB', 'DT', 'JJ', 'NN', 'NN', 'NN', 'IN', '.']",29
text-to-speech_synthesis,1,134,"In addition , we also leverage sequence - level knowledge distillation that has achieved good performance in non-autoregressive machine translation to transfer the knowledge from the teacher model to the student model .","['In', 'addition', ',', 'we', 'also', 'leverage', 'sequence', '-', 'level', 'knowledge', 'distillation', 'that', 'has', 'achieved', 'good', 'performance', 'in', 'non-autoregressive', 'machine', 'translation', 'to', 'transfer', 'the', 'knowledge', 'from', 'the', 'teacher', 'model', 'to', 'the', 'student', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'NN', ',', 'PRP', 'RB', 'VBP', 'JJ', ':', 'NN', 'NN', 'NN', 'WDT', 'VBZ', 'VBN', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'NN', 'TO', 'DT', 'NN', 'NN', '.']",33
text-to-speech_synthesis,1,139,"In the inference process , the output mel-spectrograms of our FastSpeech model are transformed into audio samples using the pretrained WaveGlow [ 20 ] 5 .","['In', 'the', 'inference', 'process', ',', 'the', 'output', 'mel-spectrograms', 'of', 'our', 'FastSpeech', 'model', 'are', 'transformed', 'into', 'audio', 'samples', 'using', 'the', 'pretrained', 'WaveGlow', '[', '20', ']', '5', '.']","['B-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'NN', 'NN', ',', 'DT', 'NN', 'NNS', 'IN', 'PRP$', 'NNP', 'NN', 'VBP', 'VBN', 'IN', 'JJ', 'NNS', 'VBG', 'DT', 'VBN', 'NNP', '$', 'CD', 'JJ', 'CD', '.']",26
text-to-speech_synthesis,1,142,Audio Quality,"['Audio', 'Quality']","['B-n', 'I-n']","['NNP', 'NN']",2
text-to-speech_synthesis,1,143,We conduct the MOS ( mean opinion score ) evaluation on the test set to measure the audio quality .,"['We', 'conduct', 'the', 'MOS', '(', 'mean', 'opinion', 'score', ')', 'evaluation', 'on', 'the', 'test', 'set', 'to', 'measure', 'the', 'audio', 'quality', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NNP', '(', 'JJ', 'NN', 'NN', ')', 'NN', 'IN', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', '.']",20
text-to-speech_synthesis,1,155,Robustness,['Robustness'],['B-n'],['NN'],1
text-to-speech_synthesis,1,159,"It can be seen that Transformer TTS is not robust to these hard cases and gets 34 % error rate , while FastSpeech can effectively eliminate word repeating and skipping to improve intelligibility .","['It', 'can', 'be', 'seen', 'that', 'Transformer', 'TTS', 'is', 'not', 'robust', 'to', 'these', 'hard', 'cases', 'and', 'gets', '34', '%', 'error', 'rate', ',', 'while', 'FastSpeech', 'can', 'effectively', 'eliminate', 'word', 'repeating', 'and', 'skipping', 'to', 'improve', 'intelligibility', '.']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['PRP', 'MD', 'VB', 'VBN', 'IN', 'NNP', 'NNP', 'VBZ', 'RB', 'JJ', 'TO', 'DT', 'JJ', 'NNS', 'CC', 'VBZ', 'CD', 'NN', 'NN', 'NN', ',', 'IN', 'NNP', 'MD', 'RB', 'VB', 'NN', 'NN', 'CC', 'NN', 'TO', 'VB', 'NN', '.']",34
text-to-speech_synthesis,1,166,Voice Speed,"['Voice', 'Speed']","['B-n', 'I-n']","['NNP', 'NNP']",2
text-to-speech_synthesis,1,169,"As demonstrated by the samples , FastSpeech can adjust the voice speed from 0.5x to 1.5 x smoothly , with stable and almost unchanged pitch .","['As', 'demonstrated', 'by', 'the', 'samples', ',', 'FastSpeech', 'can', 'adjust', 'the', 'voice', 'speed', 'from', '0.5x', 'to', '1.5', 'x', 'smoothly', ',', 'with', 'stable', 'and', 'almost', 'unchanged', 'pitch', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'IN', 'DT', 'NNS', ',', 'NNP', 'MD', 'VB', 'DT', 'NN', 'NN', 'IN', 'CD', 'TO', 'CD', 'NNS', 'RB', ',', 'IN', 'JJ', 'CC', 'RB', 'JJ', 'NN', '.']",26
text-to-speech_synthesis,1,189,1D Convolution in FFT Block,"['1D', 'Convolution', 'in', 'FFT', 'Block']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['CD', 'NN', 'IN', 'NNP', 'NNP']",5
text-to-speech_synthesis,1,190,"We propose to replace the original fully connected layer ( adopted in Transformer ) with 1D convolution in FFT block , as described in Section 3.1 .","['We', 'propose', 'to', 'replace', 'the', 'original', 'fully', 'connected', 'layer', '(', 'adopted', 'in', 'Transformer', ')', 'with', '1D', 'convolution', 'in', 'FFT', 'block', ',', 'as', 'described', 'in', 'Section', '3.1', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'TO', 'VB', 'DT', 'JJ', 'RB', 'VBN', 'NN', '(', 'VBN', 'IN', 'NNP', ')', 'IN', 'CD', 'NN', 'IN', 'NNP', 'NN', ',', 'IN', 'VBN', 'IN', 'NN', 'CD', '.']",27
text-to-speech_synthesis,1,192,"As shown in , replacing 1D convolution with fully connected layer results in - 0.113 CMOS , which demonstrates the effectiveness of 1D convolution .","['As', 'shown', 'in', ',', 'replacing', '1D', 'convolution', 'with', 'fully', 'connected', 'layer', 'results', 'in', '-', '0.113', 'CMOS', ',', 'which', 'demonstrates', 'the', 'effectiveness', 'of', '1D', 'convolution', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBN', 'IN', ',', 'VBG', 'CD', 'NN', 'IN', 'RB', 'VBN', 'NN', 'NNS', 'IN', ':', 'CD', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', '.']",25
text-to-speech_synthesis,1,193,Sequence - Level Knowledge Distillation,"['Sequence', '-', 'Level', 'Knowledge', 'Distillation']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NN', 'NNP', 'NNP']",5
text-to-speech_synthesis,1,196,"We find that removing sequence - level knowledge distillation results in - 0.325 CMOS , which demonstrates the effectiveness of sequence - level knowledge distillation .","['We', 'find', 'that', 'removing', 'sequence', '-', 'level', 'knowledge', 'distillation', 'results', 'in', '-', '0.325', 'CMOS', ',', 'which', 'demonstrates', 'the', 'effectiveness', 'of', 'sequence', '-', 'level', 'knowledge', 'distillation', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'IN', 'VBG', 'NN', ':', 'NN', 'NN', 'NN', 'NNS', 'IN', ':', 'CD', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'NN', ':', 'NN', 'NN', 'NN', '.']",26
text-to-speech_synthesis,2,2,Transfer Learning from Speaker Verification to Multispeaker Text - To - Speech Synthesis,"['Transfer', 'Learning', 'from', 'Speaker', 'Verification', 'to', 'Multispeaker', 'Text', '-', 'To', '-', 'Speech', 'Synthesis']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NN', 'VBG', 'IN', 'NNP', 'NNP', 'TO', 'NNP', 'NNP', ':', 'TO', ':', 'NN', 'NN']",13
text-to-speech_synthesis,2,4,"We describe a neural network - based system for text - to - speech ( TTS ) synthesis that is able to generate speech audio in the voice of different speakers , including those unseen during training .","['We', 'describe', 'a', 'neural', 'network', '-', 'based', 'system', 'for', 'text', '-', 'to', '-', 'speech', '(', 'TTS', ')', 'synthesis', 'that', 'is', 'able', 'to', 'generate', 'speech', 'audio', 'in', 'the', 'voice', 'of', 'different', 'speakers', ',', 'including', 'those', 'unseen', 'during', 'training', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'DT', 'JJ', 'NN', ':', 'VBN', 'NN', 'IN', 'JJ', ':', 'TO', ':', 'NN', '(', 'NNP', ')', 'NN', 'WDT', 'VBZ', 'JJ', 'TO', 'VB', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', ',', 'VBG', 'DT', 'JJ', 'IN', 'NN', '.']",38
text-to-speech_synthesis,2,10,The goal of this work is to build a TTS system which can generate natural speech for a variety of speakers in a data efficient manner .,"['The', 'goal', 'of', 'this', 'work', 'is', 'to', 'build', 'a', 'TTS', 'system', 'which', 'can', 'generate', 'natural', 'speech', 'for', 'a', 'variety', 'of', 'speakers', 'in', 'a', 'data', 'efficient', 'manner', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'TO', 'VB', 'DT', 'NNP', 'NN', 'WDT', 'MD', 'VB', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NN', 'NN', 'NN', '.']",27
text-to-speech_synthesis,2,18,Our approach is to decouple speaker modeling from speech synthesis by independently training a speaker - discriminative embedding network that captures the space of speaker characteristics and training a high quality TTS model on a smaller dataset conditioned on the representation learned by the first network .,"['Our', 'approach', 'is', 'to', 'decouple', 'speaker', 'modeling', 'from', 'speech', 'synthesis', 'by', 'independently', 'training', 'a', 'speaker', '-', 'discriminative', 'embedding', 'network', 'that', 'captures', 'the', 'space', 'of', 'speaker', 'characteristics', 'and', 'training', 'a', 'high', 'quality', 'TTS', 'model', 'on', 'a', 'smaller', 'dataset', 'conditioned', 'on', 'the', 'representation', 'learned', 'by', 'the', 'first', 'network', '.']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'TO', 'VB', 'NN', 'VBG', 'IN', 'JJ', 'NN', 'IN', 'RB', 'VBG', 'DT', 'NN', ':', 'NN', 'NN', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'NN', 'NNS', 'CC', 'VBG', 'DT', 'JJ', 'NN', 'NNP', 'NN', 'IN', 'DT', 'JJR', 'NN', 'VBN', 'IN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",47
text-to-speech_synthesis,2,20,We train the speaker embedding network on a speaker verification task to determine if two different utterances were spoken by the same speaker .,"['We', 'train', 'the', 'speaker', 'embedding', 'network', 'on', 'a', 'speaker', 'verification', 'task', 'to', 'determine', 'if', 'two', 'different', 'utterances', 'were', 'spoken', 'by', 'the', 'same', 'speaker', '.']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'VBG', 'NN', 'IN', 'DT', 'NN', 'NN', 'NN', 'TO', 'VB', 'IN', 'CD', 'JJ', 'NNS', 'VBD', 'VBN', 'IN', 'DT', 'JJ', 'NN', '.']",24
text-to-speech_synthesis,2,21,"In contrast to the subsequent TTS model , this network is trained on untranscribed speech containing reverberation and background noise from a large number of speakers .","['In', 'contrast', 'to', 'the', 'subsequent', 'TTS', 'model', ',', 'this', 'network', 'is', 'trained', 'on', 'untranscribed', 'speech', 'containing', 'reverberation', 'and', 'background', 'noise', 'from', 'a', 'large', 'number', 'of', 'speakers', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'NN', 'TO', 'DT', 'JJ', 'NNP', 'NN', ',', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'JJ', 'NN', 'VBG', 'NN', 'CC', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNS', '.']",27
text-to-speech_synthesis,2,115,Speech naturalness,"['Speech', 'naturalness']","['B-n', 'I-n']","['NNP', 'NN']",2
text-to-speech_synthesis,2,123,"The proposed model achieved about 4.0 MOS in all datasets , with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech model when evaluated on seen speakers .","['The', 'proposed', 'model', 'achieved', 'about', '4.0', 'MOS', 'in', 'all', 'datasets', ',', 'with', 'the', 'VCTK', 'model', 'obtaining', 'a', 'MOS', 'about', '0.2', 'points', 'higher', 'than', 'the', 'LibriSpeech', 'model', 'when', 'evaluated', 'on', 'seen', 'speakers', '.']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['DT', 'VBN', 'NN', 'VBN', 'IN', 'CD', 'NNP', 'IN', 'DT', 'NNS', ',', 'IN', 'DT', 'NNP', 'NN', 'VBG', 'DT', 'NNP', 'IN', 'CD', 'NNS', 'JJR', 'IN', 'DT', 'NNP', 'NN', 'WRB', 'VBN', 'IN', 'VBN', 'NNS', '.']",32
text-to-speech_synthesis,2,125,"Most importantly , the audio generated by our model for unseen speakers is deemed to be at least as natural as that generated for seen speakers .","['Most', 'importantly', ',', 'the', 'audio', 'generated', 'by', 'our', 'model', 'for', 'unseen', 'speakers', 'is', 'deemed', 'to', 'be', 'at', 'least', 'as', 'natural', 'as', 'that', 'generated', 'for', 'seen', 'speakers', '.']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['JJS', 'RB', ',', 'DT', 'NN', 'VBN', 'IN', 'PRP$', 'NN', 'IN', 'JJ', 'NNS', 'VBZ', 'VBN', 'TO', 'VB', 'IN', 'JJS', 'RB', 'JJ', 'IN', 'DT', 'VBD', 'IN', 'VBN', 'NNS', '.']",27
text-to-speech_synthesis,2,126,"Surprisingly , the MOS on unseen speakers is higher than that of seen speakers , by as much as 0.2 points on LibriSpeech .","['Surprisingly', ',', 'the', 'MOS', 'on', 'unseen', 'speakers', 'is', 'higher', 'than', 'that', 'of', 'seen', 'speakers', ',', 'by', 'as', 'much', 'as', '0.2', 'points', 'on', 'LibriSpeech', '.']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['RB', ',', 'DT', 'NNP', 'IN', 'JJ', 'NNS', 'VBZ', 'JJR', 'IN', 'DT', 'IN', 'VBN', 'NNS', ',', 'IN', 'RB', 'JJ', 'IN', 'CD', 'NNS', 'IN', 'NNP', '.']",24
text-to-speech_synthesis,2,131,Speaker similarity,"['Speaker', 'similarity']","['B-n', 'I-n']","['NNP', 'NN']",2
text-to-speech_synthesis,2,135,"The scores for the VCTK model tend to be higher than those for LibriSpeech , reflecting the cleaner nature of the dataset .","['The', 'scores', 'for', 'the', 'VCTK', 'model', 'tend', 'to', 'be', 'higher', 'than', 'those', 'for', 'LibriSpeech', ',', 'reflecting', 'the', 'cleaner', 'nature', 'of', 'the', 'dataset', '.']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'NNS', 'IN', 'DT', 'NNP', 'NN', 'NN', 'TO', 'VB', 'JJR', 'IN', 'DT', 'IN', 'NNP', ',', 'VBG', 'DT', 'JJR', 'NN', 'IN', 'DT', 'NN', '.']",23
text-to-speech_synthesis,2,137,"For seen speakers on VCTK , the proposed model performs about as well as the baseline which uses an embedding lookup table for speaker conditioning .","['For', 'seen', 'speakers', 'on', 'VCTK', ',', 'the', 'proposed', 'model', 'performs', 'about', 'as', 'well', 'as', 'the', 'baseline', 'which', 'uses', 'an', 'embedding', 'lookup', 'table', 'for', 'speaker', 'conditioning', '.']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'NNS', 'IN', 'NNP', ',', 'DT', 'VBN', 'NN', 'VBZ', 'IN', 'RB', 'RB', 'IN', 'DT', 'NN', 'WDT', 'VBZ', 'DT', 'VBG', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",26
text-to-speech_synthesis,2,152,Speaker verification,"['Speaker', 'verification']","['B-n', 'I-n']","['NNP', 'NN']",2
text-to-speech_synthesis,2,160,"As shown in , as long as the synthesizer was trained on a sufficiently large set of speakers , i.e. on LibriSpeech , the synthesized speech is typically most similar to the ground truth voices .","['As', 'shown', 'in', ',', 'as', 'long', 'as', 'the', 'synthesizer', 'was', 'trained', 'on', 'a', 'sufficiently', 'large', 'set', 'of', 'speakers', ',', 'i.e.', 'on', 'LibriSpeech', ',', 'the', 'synthesized', 'speech', 'is', 'typically', 'most', 'similar', 'to', 'the', 'ground', 'truth', 'voices', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['IN', 'VBN', 'IN', ',', 'RB', 'RB', 'IN', 'DT', 'NN', 'VBD', 'VBN', 'IN', 'DT', 'RB', 'JJ', 'NN', 'IN', 'NNS', ',', 'FW', 'IN', 'NNP', ',', 'DT', 'JJ', 'NN', 'VBZ', 'RB', 'RBS', 'JJ', 'TO', 'DT', 'NN', 'NN', 'NNS', '.']",36
text-to-speech_synthesis,2,164,"On this 20 voice discrimination task we obtain an EER of 2.86 % , demonstrating that , while the synthetic speech tends to be close to the target speaker ( cosine similarity > 0.6 , and as in ) , it is nearly always even closer to other synthetic utterances for the same speaker ( similarity > 0.7 ) .","['On', 'this', '20', 'voice', 'discrimination', 'task', 'we', 'obtain', 'an', 'EER', 'of', '2.86', '%', ',', 'demonstrating', 'that', ',', 'while', 'the', 'synthetic', 'speech', 'tends', 'to', 'be', 'close', 'to', 'the', 'target', 'speaker', '(', 'cosine', 'similarity', '>', '0.6', ',', 'and', 'as', 'in', ')', ',', 'it', 'is', 'nearly', 'always', 'even', 'closer', 'to', 'other', 'synthetic', 'utterances', 'for', 'the', 'same', 'speaker', '(', 'similarity', '>', '0.7', ')', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'DT', 'CD', 'NN', 'NN', 'NN', 'PRP', 'VB', 'DT', 'NNP', 'IN', 'CD', 'NN', ',', 'VBG', 'IN', ',', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'TO', 'VB', 'RB', 'TO', 'DT', 'NN', 'NN', '(', 'JJ', 'NN', 'NNP', 'CD', ',', 'CC', 'IN', 'IN', ')', ',', 'PRP', 'VBZ', 'RB', 'RB', 'RB', 'RBR', 'TO', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', '(', 'NN', 'RB', 'CD', ')', '.']",60
text-to-speech_synthesis,2,166,Speaker embedding space,"['Speaker', 'embedding', 'space']","['B-n', 'I-n', 'I-n']","['NNP', 'VBG', 'NN']",3
text-to-speech_synthesis,2,169,The PCA visualization ( left ) shows that synthesized utterances tend to lie very close to real speech from the same speaker in the embedding space .,"['The', 'PCA', 'visualization', '(', 'left', ')', 'shows', 'that', 'synthesized', 'utterances', 'tend', 'to', 'lie', 'very', 'close', 'to', 'real', 'speech', 'from', 'the', 'same', 'speaker', 'in', 'the', 'embedding', 'space', '.']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['DT', 'NNP', 'NN', '(', 'VBN', ')', 'VBZ', 'IN', 'JJ', 'NNS', 'VBP', 'TO', 'VB', 'RB', 'RB', 'TO', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']",27
text-to-speech_synthesis,2,170,"However , synthetic utterances are still easily distinguishable from the real human speech as demonstrated by the t - SNE visualization ( right ) where utterances from each synthetic speaker form a distinct cluster adjacent to a cluster of real utterances from the corresponding speaker .","['However', ',', 'synthetic', 'utterances', 'are', 'still', 'easily', 'distinguishable', 'from', 'the', 'real', 'human', 'speech', 'as', 'demonstrated', 'by', 'the', 't', '-', 'SNE', 'visualization', '(', 'right', ')', 'where', 'utterances', 'from', 'each', 'synthetic', 'speaker', 'form', 'a', 'distinct', 'cluster', 'adjacent', 'to', 'a', 'cluster', 'of', 'real', 'utterances', 'from', 'the', 'corresponding', 'speaker', '.']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['RB', ',', 'JJ', 'NNS', 'VBP', 'RB', 'RB', 'JJ', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'VBN', 'IN', 'DT', 'NN', ':', 'NNP', 'NN', '(', 'NN', ')', 'WRB', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', 'DT', 'JJ', 'NN', 'NN', 'TO', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'NN', '.']",46
text-to-speech_synthesis,2,173,Number of speaker encoder training speakers,"['Number', 'of', 'speaker', 'encoder', 'training', 'speakers']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', 'IN', 'NN', 'NN', 'NN', 'NNS']",6
text-to-speech_synthesis,2,185,"As the number of training speakers increases , both naturalness and similarity improve significantly .","['As', 'the', 'number', 'of', 'training', 'speakers', 'increases', ',', 'both', 'naturalness', 'and', 'similarity', 'improve', 'significantly', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O']","['IN', 'DT', 'NN', 'IN', 'VBG', 'NNS', 'NNS', ',', 'CC', 'RB', 'CC', 'NN', 'VBP', 'RB', '.']",15
text-to-speech_synthesis,2,190,Fictitious speakers,"['Fictitious', 'speakers']","['B-n', 'I-n']","['JJ', 'NNS']",2
text-to-speech_synthesis,2,191,Bypassing the speaker encoder network and conditioning the synthesizer on random points in the speaker embedding space results in speech from fictitious speakers which are not present in the train or test sets of either the synthesizer or the speaker encoder .,"['Bypassing', 'the', 'speaker', 'encoder', 'network', 'and', 'conditioning', 'the', 'synthesizer', 'on', 'random', 'points', 'in', 'the', 'speaker', 'embedding', 'space', 'results', 'in', 'speech', 'from', 'fictitious', 'speakers', 'which', 'are', 'not', 'present', 'in', 'the', 'train', 'or', 'test', 'sets', 'of', 'either', 'the', 'synthesizer', 'or', 'the', 'speaker', 'encoder', '.']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBG', 'DT', 'NN', 'NN', 'NN', 'CC', 'VBG', 'DT', 'NN', 'IN', 'NN', 'NNS', 'IN', 'DT', 'NN', 'VBG', 'NN', 'NNS', 'IN', 'NN', 'IN', 'JJ', 'NNS', 'WDT', 'VBP', 'RB', 'JJ', 'IN', 'DT', 'NN', 'CC', 'NN', 'NNS', 'IN', 'CC', 'DT', 'NN', 'CC', 'DT', 'NN', 'NN', '.']",42
text-to-speech_synthesis,0,2,Token - Level Ensemble Distillation for Grapheme - to - Phoneme Conversion,"['Token', '-', 'Level', 'Ensemble', 'Distillation', 'for', 'Grapheme', '-', 'to', '-', 'Phoneme', 'Conversion']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['NNP', ':', 'NN', 'JJ', 'NNP', 'IN', 'NNP', ':', 'TO', ':', 'NN', 'NN']",12
text-to-speech_synthesis,0,4,Grapheme - to - phoneme ( G2P ) conversion is an important task in automatic speech recognition and text - to - speech systems .,"['Grapheme', '-', 'to', '-', 'phoneme', '(', 'G2P', ')', 'conversion', 'is', 'an', 'important', 'task', 'in', 'automatic', 'speech', 'recognition', 'and', 'text', '-', 'to', '-', 'speech', 'systems', '.']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['NNP', ':', 'TO', ':', 'NN', '(', 'NNP', ')', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'NN', 'CC', 'JJ', ':', 'TO', ':', 'NN', 'NNS', '.']",25
text-to-speech_synthesis,0,22,"Inspired by the knowledge distillation in computer vision and natural language processing , in this work , we propose the token - level ensemble distillation for G2P conversion , to address the practical problems mentioned above .","['Inspired', 'by', 'the', 'knowledge', 'distillation', 'in', 'computer', 'vision', 'and', 'natural', 'language', 'processing', ',', 'in', 'this', 'work', ',', 'we', 'propose', 'the', 'token', '-', 'level', 'ensemble', 'distillation', 'for', 'G2P', 'conversion', ',', 'to', 'address', 'the', 'practical', 'problems', 'mentioned', 'above', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['VBN', 'IN', 'DT', 'NN', 'NN', 'IN', 'NN', 'NN', 'CC', 'JJ', 'NN', 'NN', ',', 'IN', 'DT', 'NN', ',', 'PRP', 'VBP', 'DT', 'JJ', ':', 'NN', 'JJ', 'NN', 'IN', 'NNP', 'NN', ',', 'TO', 'VB', 'DT', 'JJ', 'NNS', 'VBN', 'IN', '.']",37
text-to-speech_synthesis,0,23,"First , we use knowledge distillation to leverage the large amount of unlabeled words .","['First', ',', 'we', 'use', 'knowledge', 'distillation', 'to', 'leverage', 'the', 'large', 'amount', 'of', 'unlabeled', 'words', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'JJ', 'NN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NNS', '.']",15
text-to-speech_synthesis,0,24,"Specifically , we train a teacher model to generate the phoneme sequence as well as its probability distribution given unlabeled grapheme sequence , and regard the unlabeled grapheme sequence and the generated phoneme sequence as pseudo labeled data , and add them into the original training data .","['Specifically', ',', 'we', 'train', 'a', 'teacher', 'model', 'to', 'generate', 'the', 'phoneme', 'sequence', 'as', 'well', 'as', 'its', 'probability', 'distribution', 'given', 'unlabeled', 'grapheme', 'sequence', ',', 'and', 'regard', 'the', 'unlabeled', 'grapheme', 'sequence', 'and', 'the', 'generated', 'phoneme', 'sequence', 'as', 'pseudo', 'labeled', 'data', ',', 'and', 'add', 'them', 'into', 'the', 'original', 'training', 'data', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'RB', 'RB', 'IN', 'PRP$', 'NN', 'NN', 'VBN', 'JJ', 'NN', 'NN', ',', 'CC', 'VB', 'DT', 'JJ', 'NN', 'NN', 'CC', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NN', 'VBN', 'NNS', ',', 'CC', 'VB', 'PRP', 'IN', 'DT', 'JJ', 'NN', 'NNS', '.']",48
text-to-speech_synthesis,0,25,"Second , we train a variety of models ( CNN , RNN and Transformer ) for ensemble to get higher accuracy , and transfer the knowledge of the ensemble models to a light - weight model that is suitable for online deployment , again by knowledge distillation .","['Second', ',', 'we', 'train', 'a', 'variety', 'of', 'models', '(', 'CNN', ',', 'RNN', 'and', 'Transformer', ')', 'for', 'ensemble', 'to', 'get', 'higher', 'accuracy', ',', 'and', 'transfer', 'the', 'knowledge', 'of', 'the', 'ensemble', 'models', 'to', 'a', 'light', '-', 'weight', 'model', 'that', 'is', 'suitable', 'for', 'online', 'deployment', ',', 'again', 'by', 'knowledge', 'distillation', '.']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['JJ', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'NNS', '(', 'NNP', ',', 'NNP', 'CC', 'NNP', ')', 'IN', 'NN', 'TO', 'VB', 'JJR', 'NN', ',', 'CC', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NNS', 'TO', 'DT', 'JJ', ':', 'NN', 'NN', 'WDT', 'VBZ', 'JJ', 'IN', 'JJ', 'NN', ',', 'RB', 'IN', 'NN', 'NN', '.']",48
text-to-speech_synthesis,0,26,"Besides , we adopt Transformer instead of RNN or CNN as the basic encoder - decoder model structure , since it demonstrates advantages in a variety of sequence to sequence tasks , such as neural machine translation , text summarization , automatic speech recognition .","['Besides', ',', 'we', 'adopt', 'Transformer', 'instead', 'of', 'RNN', 'or', 'CNN', 'as', 'the', 'basic', 'encoder', '-', 'decoder', 'model', 'structure', ',', 'since', 'it', 'demonstrates', 'advantages', 'in', 'a', 'variety', 'of', 'sequence', 'to', 'sequence', 'tasks', ',', 'such', 'as', 'neural', 'machine', 'translation', ',', 'text', 'summarization', ',', 'automatic', 'speech', 'recognition', '.']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', ',', 'PRP', 'VBP', 'NNP', 'RB', 'IN', 'NNP', 'CC', 'NNP', 'IN', 'DT', 'JJ', 'NN', ':', 'NN', 'NN', 'NN', ',', 'IN', 'PRP', 'VBZ', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NN', 'TO', 'VB', 'NNS', ',', 'JJ', 'IN', 'JJ', 'NN', 'NN', ',', 'JJ', 'NN', ',', 'JJ', 'NN', 'NN', '.']",45
text-to-speech_synthesis,0,110,Ensemble Model,"['Ensemble', 'Model']","['B-n', 'I-n']","['JJ', 'NN']",2
text-to-speech_synthesis,0,112,"We use 4 Transformer models , 3 CNN models and 3 Bi - LSTM models with different hyperparameters for ensemble , which give the best performance on the validation set .","['We', 'use', '4', 'Transformer', 'models', ',', '3', 'CNN', 'models', 'and', '3', 'Bi', '-', 'LSTM', 'models', 'with', 'different', 'hyperparameters', 'for', 'ensemble', ',', 'which', 'give', 'the', 'best', 'performance', 'on', 'the', 'validation', 'set', '.']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'CD', 'NNP', 'NNS', ',', 'CD', 'NNP', 'NNS', 'CC', 'CD', 'NNP', ':', 'NNP', 'NNS', 'IN', 'JJ', 'NNS', 'IN', 'NN', ',', 'WDT', 'VBP', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NN', 'NN', '.']",31
text-to-speech_synthesis,0,113,The 4 Transformer models share the same hidden size ( 256 ) but vary in the number of the encoder - decoder layers .,"['The', '4', 'Transformer', 'models', 'share', 'the', 'same', 'hidden', 'size', '(', '256', ')', 'but', 'vary', 'in', 'the', 'number', 'of', 'the', 'encoder', '-', 'decoder', 'layers', '.']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['DT', 'CD', 'NNP', 'NNS', 'NN', 'DT', 'JJ', 'JJ', 'NN', '(', 'CD', ')', 'CC', 'JJ', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', ':', 'NN', 'NNS', '.']",24
text-to-speech_synthesis,0,114,"For the 3 CNN models , they share the same hidden size ( 256 ) but vary in the number of encoder - decoder layers ( 10 - 10 , 10 - 10 , 8 - 8 ) and convolutional kernel widths ( 3 , 2 , 2 ) respectively .","['For', 'the', '3', 'CNN', 'models', ',', 'they', 'share', 'the', 'same', 'hidden', 'size', '(', '256', ')', 'but', 'vary', 'in', 'the', 'number', 'of', 'encoder', '-', 'decoder', 'layers', '(', '10', '-', '10', ',', '10', '-', '10', ',', '8', '-', '8', ')', 'and', 'convolutional', 'kernel', 'widths', '(', '3', ',', '2', ',', '2', ')', 'respectively', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['IN', 'DT', 'CD', 'NNP', 'NNS', ',', 'PRP', 'NN', 'DT', 'JJ', 'JJ', 'NN', '(', 'CD', ')', 'CC', 'JJ', 'IN', 'DT', 'NN', 'IN', 'FW', ':', 'NN', 'NNS', '(', 'CD', ':', 'CD', ',', 'CD', ':', 'CD', ',', 'CD', ':', 'CD', ')', 'CC', 'JJ', 'NNS', 'NNS', '(', 'CD', ',', 'CD', ',', 'CD', ')', 'RB', '.']",51
text-to-speech_synthesis,0,115,"For the 3 Bi - LSTM models , they share the same number of encoder - decoder layers ( 1 - 1 ) , but with different hidden sizes ( 256 , 384 and 512 ) .","['For', 'the', '3', 'Bi', '-', 'LSTM', 'models', ',', 'they', 'share', 'the', 'same', 'number', 'of', 'encoder', '-', 'decoder', 'layers', '(', '1', '-', '1', ')', ',', 'but', 'with', 'different', 'hidden', 'sizes', '(', '256', ',', '384', 'and', '512', ')', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['IN', 'DT', 'CD', 'NNP', ':', 'NNP', 'NNS', ',', 'PRP', 'NN', 'DT', 'JJ', 'NN', 'IN', 'FW', ':', 'NN', 'NNS', '(', 'CD', ':', 'CD', ')', ',', 'CC', 'IN', 'JJ', 'NN', 'NNS', '(', 'CD', ',', 'CD', 'CC', 'CD', ')', '.']",37
text-to-speech_synthesis,0,116,Student Model,"['Student', 'Model']","['B-n', 'I-n']","['NN', 'NN']",2
text-to-speech_synthesis,0,117,We choose Transformer as the student model and use the default configurations ( 256 hidden size and 6 - 6 layers of encoder - decoder ) unless otherwise stated .,"['We', 'choose', 'Transformer', 'as', 'the', 'student', 'model', 'and', 'use', 'the', 'default', 'configurations', '(', '256', 'hidden', 'size', 'and', '6', '-', '6', 'layers', 'of', 'encoder', '-', 'decoder', ')', 'unless', 'otherwise', 'stated', '.']","['O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'NNP', 'IN', 'DT', 'NN', 'NN', 'CC', 'VB', 'DT', 'NN', 'NNS', '(', 'CD', 'VBN', 'NN', 'CC', 'CD', ':', 'CD', 'NNS', 'IN', 'NN', ':', 'NN', ')', 'IN', 'RB', 'VBN', '.']",30
text-to-speech_synthesis,0,120,We implement experiments with the fairseq - py 4 library in Py-Torch .,"['We', 'implement', 'experiments', 'with', 'the', 'fairseq', '-', 'py', '4', 'library', 'in', 'Py-Torch', '.']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNS', 'IN', 'DT', 'NN', ':', 'NN', 'CD', 'NN', 'IN', 'NNP', '.']",13
text-to-speech_synthesis,0,121,We use Adam optimizer for all models and follow the learning rate schedule in .,"['We', 'use', 'Adam', 'optimizer', 'for', 'all', 'models', 'and', 'follow', 'the', 'learning', 'rate', 'schedule', 'in', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'DT', 'NNS', 'CC', 'VB', 'DT', 'NN', 'NN', 'NN', 'IN', '.']",15
text-to-speech_synthesis,0,122,"The dropout is 0.3 for Bi - LSTM and CNN models , while the residual dropout , attention dropout and ReLU dropout for Transformer models is 0.2 , 0.4 , 0.4 respectively .","['The', 'dropout', 'is', '0.3', 'for', 'Bi', '-', 'LSTM', 'and', 'CNN', 'models', ',', 'while', 'the', 'residual', 'dropout', ',', 'attention', 'dropout', 'and', 'ReLU', 'dropout', 'for', 'Transformer', 'models', 'is', '0.2', ',', '0.4', ',', '0.4', 'respectively', '.']","['O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['DT', 'NN', 'VBZ', 'CD', 'IN', 'NNP', ':', 'NNP', 'CC', 'NNP', 'NNS', ',', 'IN', 'DT', 'JJ', 'NN', ',', 'NN', 'NN', 'CC', 'NNP', 'NN', 'IN', 'NNP', 'NNS', 'VBZ', 'CD', ',', 'CD', ',', 'CD', 'RB', '.']",33
text-to-speech_synthesis,0,124,We train each model on 8 NVIDIA M40 GPUs .,"['We', 'train', 'each', 'model', 'on', '8', 'NVIDIA', 'M40', 'GPUs', '.']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'DT', 'NN', 'IN', 'CD', 'NNP', 'NNP', 'NNP', '.']",10
text-to-speech_synthesis,0,125,Each GPU contains roughly 4000 tokens in one mini-batch .,"['Each', 'GPU', 'contains', 'roughly', '4000', 'tokens', 'in', 'one', 'mini-batch', '.']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['DT', 'NNP', 'VBZ', 'RB', 'CD', 'NNS', 'IN', 'CD', 'NN', '.']",10
text-to-speech_synthesis,0,126,We use beam search during inference and set beam size to 10 .,"['We', 'use', 'beam', 'search', 'during', 'inference', 'and', 'set', 'beam', 'size', 'to', '10', '.']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'JJ', 'NN', 'IN', 'NN', 'CC', 'VB', 'NN', 'NN', 'TO', 'CD', '.']",13
text-to-speech_synthesis,0,127,We use WER ( word error rate ) and PER ( phoneme error rate ) to measure the accuracy of G2P conversion .,"['We', 'use', 'WER', '(', 'word', 'error', 'rate', ')', 'and', 'PER', '(', 'phoneme', 'error', 'rate', ')', 'to', 'measure', 'the', 'accuracy', 'of', 'G2P', 'conversion', '.']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['PRP', 'VBP', 'NNP', '(', 'NN', 'NN', 'NN', ')', 'CC', 'NNP', '(', 'JJ', 'NN', 'NN', ')', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNP', 'NN', '.']",23
text-to-speech_synthesis,0,132,"We first compare our method with previous works on CMUDict 0.7 b dataset , as shown in .","['We', 'first', 'compare', 'our', 'method', 'with', 'previous', 'works', 'on', 'CMUDict', '0.7', 'b', 'dataset', ',', 'as', 'shown', 'in', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VB', 'PRP$', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NNP', 'CD', 'NN', 'NN', ',', 'IN', 'VBN', 'IN', '.']",18
text-to-speech_synthesis,0,136,"It can be seen that our method on 6 - layer encoder and 6 - layer decoder Transformer achieves the new state - of - the - art result of 19.88 % WER , outperforming NSGD by 4.22 % WER .","['It', 'can', 'be', 'seen', 'that', 'our', 'method', 'on', '6', '-', 'layer', 'encoder', 'and', '6', '-', 'layer', 'decoder', 'Transformer', 'achieves', 'the', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'result', 'of', '19.88', '%', 'WER', ',', 'outperforming', 'NSGD', 'by', '4.22', '%', 'WER', '.']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'VBN', 'IN', 'PRP$', 'NN', 'IN', 'CD', ':', 'NN', 'NN', 'CC', 'CD', ':', 'NN', 'NN', 'NNP', 'VBZ', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NN', 'IN', 'CD', 'NN', 'NNP', ',', 'VBG', 'NNP', 'IN', 'CD', 'NN', 'NNP', '.']",41
text-to-speech_synthesis,0,145,"We first study the effect of distilling from unlabeled source words , as shown in .","['We', 'first', 'study', 'the', 'effect', 'of', 'distilling', 'from', 'unlabeled', 'source', 'words', ',', 'as', 'shown', 'in', '.']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'RB', 'VBD', 'DT', 'NN', 'IN', 'VBG', 'IN', 'JJ', 'NN', 'NNS', ',', 'IN', 'VBN', 'IN', '.']",16
text-to-speech_synthesis,0,146,"It can be seen that unlabeled source words can boost the accuracy by nearly 1 % WER , demonstrating the effectiveness by introducing abundant unlabeled data into knowledge distillation .","['It', 'can', 'be', 'seen', 'that', 'unlabeled', 'source', 'words', 'can', 'boost', 'the', 'accuracy', 'by', 'nearly', '1', '%', 'WER', ',', 'demonstrating', 'the', 'effectiveness', 'by', 'introducing', 'abundant', 'unlabeled', 'data', 'into', 'knowledge', 'distillation', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP', 'MD', 'VB', 'VBN', 'DT', 'VBD', 'NN', 'NNS', 'MD', 'VB', 'DT', 'NN', 'IN', 'RB', 'CD', 'NN', 'NNP', ',', 'VBG', 'DT', 'NN', 'IN', 'VBG', 'JJ', 'JJ', 'NNS', 'IN', 'NN', 'NN', '.']",30
text-to-speech_synthesis,0,152,"Furthermore , we study the effect of ensemble teacher model in knowledge distillation .","['Furthermore', ',', 'we', 'study', 'the', 'effect', 'of', 'ensemble', 'teacher', 'model', 'in', 'knowledge', 'distillation', '.']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['RB', ',', 'PRP', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",14
text-to-speech_synthesis,0,153,"As shown in , the ensemble teacher model can boost the accuracy by more than 1 % WER , compared with the single teacher model ( a Transformer model with 6 - layer encoder and 6 - layer decoder ) , which demonstrates the strong ensemble teacher model is essential to guarantee the performance of student model in knowledge distillation .","['As', 'shown', 'in', ',', 'the', 'ensemble', 'teacher', 'model', 'can', 'boost', 'the', 'accuracy', 'by', 'more', 'than', '1', '%', 'WER', ',', 'compared', 'with', 'the', 'single', 'teacher', 'model', '(', 'a', 'Transformer', 'model', 'with', '6', '-', 'layer', 'encoder', 'and', '6', '-', 'layer', 'decoder', ')', ',', 'which', 'demonstrates', 'the', 'strong', 'ensemble', 'teacher', 'model', 'is', 'essential', 'to', 'guarantee', 'the', 'performance', 'of', 'student', 'model', 'in', 'knowledge', 'distillation', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['IN', 'VBN', 'IN', ',', 'DT', 'JJ', 'NN', 'NN', 'MD', 'VB', 'DT', 'NN', 'IN', 'JJR', 'IN', 'CD', 'NN', 'NNP', ',', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'NN', '(', 'DT', 'NNP', 'NN', 'IN', 'CD', ':', 'NN', 'NN', 'CC', 'CD', ':', 'NN', 'NN', ')', ',', 'WDT', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'VBZ', 'JJ', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', 'NN', 'IN', 'NN', 'NN', '.']",61
text-to-speech_synthesis,0,154,"At last , we compare Transformer with RNN and CNN based models , without using knowledge distillation and unlabeled data , as shown in .","['At', 'last', ',', 'we', 'compare', 'Transformer', 'with', 'RNN', 'and', 'CNN', 'based', 'models', ',', 'without', 'using', 'knowledge', 'distillation', 'and', 'unlabeled', 'data', ',', 'as', 'shown', 'in', '.']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['IN', 'JJ', ',', 'PRP', 'VBP', 'JJ', 'IN', 'NNP', 'CC', 'NNP', 'VBN', 'NNS', ',', 'IN', 'VBG', 'NN', 'NN', 'CC', 'JJ', 'NNS', ',', 'IN', 'VBN', 'IN', '.']",25
text-to-speech_synthesis,0,155,"We can see that Transformer model outperforms the RNN and CNN based models used in previous works , demonstrating the advantage of Transformer model .","['We', 'can', 'see', 'that', 'Transformer', 'model', 'outperforms', 'the', 'RNN', 'and', 'CNN', 'based', 'models', 'used', 'in', 'previous', 'works', ',', 'demonstrating', 'the', 'advantage', 'of', 'Transformer', 'model', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['PRP', 'MD', 'VB', 'DT', 'NNP', 'NN', 'VBZ', 'DT', 'NNP', 'CC', 'NNP', 'VBN', 'NNS', 'VBN', 'IN', 'JJ', 'NNS', ',', 'VBG', 'DT', 'NN', 'IN', 'NNP', 'NN', '.']",25
text-to-speech_synthesis,0,157,"We compare our method with the previous state - of - the - art CNN with NSGD ( which is reproduced by ourself ) on our internal dataset , as shown in .","['We', 'compare', 'our', 'method', 'with', 'the', 'previous', 'state', '-', 'of', '-', 'the', '-', 'art', 'CNN', 'with', 'NSGD', '(', 'which', 'is', 'reproduced', 'by', 'ourself', ')', 'on', 'our', 'internal', 'dataset', ',', 'as', 'shown', 'in', '.']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['PRP', 'VBP', 'PRP$', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'IN', ':', 'DT', ':', 'NN', 'NNP', 'IN', 'NNP', '(', 'WDT', 'VBZ', 'VBN', 'IN', 'PRP', ')', 'IN', 'PRP$', 'JJ', 'NN', ',', 'IN', 'VBN', 'IN', '.']",33
text-to-speech_synthesis,0,158,"Our method outperforms CNN with NSGD by 3.52 % WER , which demonstrates the effectiveness of our method for G2P conversion .","['Our', 'method', 'outperforms', 'CNN', 'with', 'NSGD', 'by', '3.52', '%', 'WER', ',', 'which', 'demonstrates', 'the', 'effectiveness', 'of', 'our', 'method', 'for', 'G2P', 'conversion', '.']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['PRP$', 'NN', 'VBZ', 'NNP', 'IN', 'NNP', 'IN', 'CD', 'NN', 'NNP', ',', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'IN', 'NNP', 'NN', '.']",22
