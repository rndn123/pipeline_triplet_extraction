topic,paper_ID,main_heading,sub_heading,pos1,pos2,pos3,text,prev_text,next_text,ofs1,ofs2,ofs3,labels,info_units
constituency_parsing,0,title,title,2,2,2,Recurrent Neural Network Grammars, , ,0.0089686098654708,1.0,1.0,research-problem,approach
constituency_parsing,0,introduction,introduction,12,5,5,"In this paper , we introduce recurrent neural network grammars ( RNNGs ; 2 ) , a new generative probabilistic model of sentences that explicitly models nested , hierarchical relationships among words and phrases .","Despite these impressive results , sequential models area priori inappropriate models of natural language , since relationships among words are largely organized in terms of latent nested structures rather than sequential surface order .","RNNGs operate via a recursive syntactic process reminiscent of probabilistic context - free grammar generation , but decisions are parameterized using RNNs that condition on the entire syntactic derivation history , greatly relaxing context - free independence assumptions .",0.0538116591928251,0.2380952380952381,0.2380952380952381,model,experimental-setup
constituency_parsing,0,introduction,introduction,13,6,6,"RNNGs operate via a recursive syntactic process reminiscent of probabilistic context - free grammar generation , but decisions are parameterized using RNNs that condition on the entire syntactic derivation history , greatly relaxing context - free independence assumptions .","In this paper , we introduce recurrent neural network grammars ( RNNGs ; 2 ) , a new generative probabilistic model of sentences that explicitly models nested , hierarchical relationships among words and phrases .",The foundation of this work is a top - down variant of transition - based parsing ( 3 ) .,0.0582959641255605,0.2857142857142857,0.2857142857142857,model,ablation-analysis
constituency_parsing,0,introduction,introduction,15,8,8,"We give two variants of the algorithm , one for parsing ( given an observed sentence , transform it into a tree ) , and one for generation .",The foundation of this work is a top - down variant of transition - based parsing ( 3 ) .,"While several transition - based neural models of syntactic generation exist , these have relied on structure building operations based on parsing actions in shift - reduce and leftcorner parsers which operate in a largely bottomup fashion .",0.0672645739910313,0.3809523809523809,0.3809523809523809,model,approach
constituency_parsing,0,introduction,introduction,24,17,17,"The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences , and this is used to solve a second practical challenge with RNNGs : approximating the marginal likelihood and MAP tree of a sentence under the generative model .","time deterministic parser ( provided an upper bound on the number of actions taken between processing subsequent terminal symbols is imposed ) ; however , our algorithm generates arbitrary tree structures directly , without the binarization required by shift - reduce parsers .",We present a simple importance sampling algorithm which uses samples from the discriminative parser to solve inference problems in the generative model ( 5 ) .,0.1076233183856502,0.8095238095238095,0.8095238095238095,model,ablation-analysis
constituency_parsing,0,introduction,introduction,25,18,18,We present a simple importance sampling algorithm which uses samples from the discriminative parser to solve inference problems in the generative model ( 5 ) .,"The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences , and this is used to solve a second practical challenge with RNNGs : approximating the marginal likelihood and MAP tree of a sentence under the generative model .",Experiments show that RNNGs are effective for both language modeling and parsing ( 6 ) .,0.1121076233183856,0.8571428571428571,0.8571428571428571,model,experimental-setup
constituency_parsing,0,experiments,experiments,160,4,4,"For the discriminative model , we used hidden dimensions of 128 and 2 - layer LSTMs ( larger numbers of dimensions reduced validation set performance ) .",Model and training parameters .,"For the generative model , we used 256 dimensions and 2 layer LSTMs .",0.7174887892376681,0.2352941176470588,0.2352941176470588,hyperparameters,approach
constituency_parsing,0,experiments,experiments,161,5,5,"For the generative model , we used 256 dimensions and 2 layer LSTMs .","For the discriminative model , we used hidden dimensions of 128 and 2 - layer LSTMs ( larger numbers of dimensions reduced validation set performance ) .","For both models , we tuned the dropout rate to maximize validation set likelihood , obtaining optimal rates of 0.2 ( discriminative ) and 0.3 ( generative ) .",0.7219730941704036,0.2941176470588235,0.2941176470588235,hyperparameters,approach
constituency_parsing,0,experiments,experiments,162,6,6,"For both models , we tuned the dropout rate to maximize validation set likelihood , obtaining optimal rates of 0.2 ( discriminative ) and 0.3 ( generative ) .","For the generative model , we used 256 dimensions and 2 layer LSTMs .","For the sequential LSTM baseline for the language model , we also found an optimal dropout rate of 0.3 .",0.726457399103139,0.3529411764705882,0.3529411764705882,hyperparameters,approach
constituency_parsing,0,experiments,experiments,163,7,7,"For the sequential LSTM baseline for the language model , we also found an optimal dropout rate of 0.3 .","For both models , we tuned the dropout rate to maximize validation set likelihood , obtaining optimal rates of 0.2 ( discriminative ) and 0.3 ( generative ) .",For training we used stochastic gradient descent with a learning rate of 0.1 .,0.7309417040358744,0.4117647058823529,0.4117647058823529,hyperparameters,baselines
constituency_parsing,0,experiments,experiments,164,8,8,For training we used stochastic gradient descent with a learning rate of 0.1 .,"For the sequential LSTM baseline for the language model , we also found an optimal dropout rate of 0.3 .",All parameters were initialized according to recommendations given by .,0.7354260089686099,0.4705882352941176,0.4705882352941176,hyperparameters,approach
constituency_parsing,1,title,title,2,2,2,Cloze - driven Pretraining of Self - attention Networks, , ,0.0098039215686274,1.0,1.0,research-problem,experimental-setup
constituency_parsing,1,abstract,abstract,4,2,2,We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems ., ,"Our model solves a cloze - style word reconstruction task , where each word is ablated and must be predicted given the rest of the text .",0.0196078431372549,0.4,0.4,research-problem,baselines
constituency_parsing,1,introduction,introduction,9,2,2,Language model pretraining has recently been shown to provide significant performance gains for a range of challenging language understanding problems ., ,"However , existing work has either used unidirectional ( left - to - right ) language models ( LMs ) or bi-directional ( both left - to - right and right - to - left ) LMs ( BiLMs ) where each direction is trained with an independent loss function .",0.0441176470588235,0.1111111111111111,0.1111111111111111,research-problem,experimental-setup
constituency_parsing,1,introduction,introduction,11,4,4,"In this paper , we show that even larger performance gains are possible by jointly pretraining both directions of a large language - model - inspired self - attention cloze model .","However , existing work has either used unidirectional ( left - to - right ) language models ( LMs ) or bi-directional ( both left - to - right and right - to - left ) LMs ( BiLMs ) where each direction is trained with an independent loss function .",Our bi-directional transformer architecture predicts every token in the training data ( ) .,0.0539215686274509,0.2222222222222222,0.2222222222222222,approach,experimental-setup
constituency_parsing,1,introduction,introduction,12,5,5,Our bi-directional transformer architecture predicts every token in the training data ( ) .,"In this paper , we show that even larger performance gains are possible by jointly pretraining both directions of a large language - model - inspired self - attention cloze model .",We achieve this by introducing a cloze - style training objective where the model must predict the center word given left - to - right and right - to - left context representations .,0.0588235294117647,0.2777777777777778,0.2777777777777778,approach,model
constituency_parsing,1,introduction,introduction,13,6,6,We achieve this by introducing a cloze - style training objective where the model must predict the center word given left - to - right and right - to - left context representations .,Our bi-directional transformer architecture predicts every token in the training data ( ) .,Our model separately computes both forward and backward states with * Equal contribution . :,0.0637254901960784,0.3333333333333333,0.3333333333333333,approach,approach
constituency_parsing,1,introduction,introduction,14,7,7,Our model separately computes both forward and backward states with * Equal contribution . :,We achieve this by introducing a cloze - style training objective where the model must predict the center word given left - to - right and right - to - left context representations .,Illustration of the model .,0.0686274509803921,0.3888888888888889,0.3888888888888889,approach,model
constituency_parsing,1,experimental setup,experimental setup,110,1,1,Experimental setup, , ,0.5392156862745098,0.04,1.0,experimental-setup,baselines
constituency_parsing,1,experimental setup,pretraining hyper parameters,128,19,6,"CNN models use an adaptive softmax in the output : the headband contains the 60K most frequent types with dimensionality 1024 , followed by a 160 K band with dimensionality 256 . with a momentum of 0.99 and we renormalize gradients if their norm exceeds 0.1 .","The CNN models have unconstrained input vocabulary , and an output vocabulary limited to 1 M most common types for the large model , and 700 K most common types for the base model .",The learning rate is linearly warmed up from 10 ? 7 to 1 for 16 K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 .,0.6274509803921569,0.76,0.5,experimental-setup,experimental-setup
constituency_parsing,1,experimental setup,pretraining hyper parameters,129,20,7,The learning rate is linearly warmed up from 10 ? 7 to 1 for 16 K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 .,"CNN models use an adaptive softmax in the output : the headband contains the 60K most frequent types with dimensionality 1024 , followed by a 160 K band with dimensionality 256 . with a momentum of 0.99 and we renormalize gradients if their norm exceeds 0.1 .",We run experiments on DGX - 1 machines with 8 NVIDIA V100 GPUs and machines are interconnected by Infiniband .,0.6323529411764706,0.8,0.5833333333333334,experimental-setup,experimental-setup
constituency_parsing,1,experimental setup,pretraining hyper parameters,130,21,8,We run experiments on DGX - 1 machines with 8 NVIDIA V100 GPUs and machines are interconnected by Infiniband .,The learning rate is linearly warmed up from 10 ? 7 to 1 for 16 K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 .,We also use the NCCL2 library and the torch .,0.6372549019607843,0.84,0.6666666666666666,experimental-setup,experimental-setup
constituency_parsing,1,experimental setup,pretraining hyper parameters,131,22,9,We also use the NCCL2 library and the torch .,We run experiments on DGX - 1 machines with 8 NVIDIA V100 GPUs and machines are interconnected by Infiniband .,distributed package for inter - GPU communication .,0.6421568627450981,0.88,0.75,experimental-setup,experimental-setup
constituency_parsing,1,results,results,135,1,1,Results, , ,0.6617647058823529,0.0158730158730158,1.0,results,baselines
constituency_parsing,1,results,glue,136,2,1,GLUE, , ,0.6666666666666666,0.0317460317460317,0.0434782608695652,results,baselines
constituency_parsing,1,results,glue,151,17,16,"All our models outperform the uni-directional transformer ( OpenAI GPT ) of , however , our model is about 50 % larger than their model .","The BPE model has more parameters than the CNN model but does not perform better in aggregate , however , it is faster to train .",We also show results for the concurrently introduced STILTs and BERT .,0.7401960784313726,0.2698412698412698,0.6956521739130435,results,model
constituency_parsing,1,results,glue,153,19,18,"Our CNN base model performs as well as STILTs in aggregate , however , on some tasks involving sentence - pairs , STILTs performs much better ( MRPC , RTE ) ; there is a similar trend for BERT .",We also show results for the concurrently introduced STILTs and BERT .,STILTs adds another fine - tuning step on another downstream task which is similar to the final task .,0.75,0.3015873015873016,0.7826086956521741,results,baselines
constituency_parsing,1,results,structured prediction,163,29,5,Named Entity Recognition,"We evaluate two ways of stacking : ( 1 ) ELMo-style , where the pretrained models are not fine - tuned but are linearly combined at different depths , and ( 2 ) with fine - tuning , where we set different learning rates for the task - specific layers but otherwise update all of the parameters during the task - specific training .","We evaluated span - level F1 performance on the CoNLL 2003 Named Entity Recognition ( NER ) task , where spans of text must be segmented and labeled as Person , Organization , Location , or Miscellaneous .",0.7990196078431373,0.4603174603174603,0.3333333333333333,results,baselines
constituency_parsing,1,results,structured prediction,167,33,9,"shows the results , with comparison to previous published ELMo BASE results the art , but fine tuning gives the biggest gain .","We did grid search on the pairs of learning rate , and found that projection - biLSTM - CRF with 1E - 03 and pretrained language model with 1E - 05 gave us the best result .",Constituency Parsing,0.8186274509803921,0.5238095238095238,0.6,results,approach
constituency_parsing,1,results,structured prediction,168,34,10,Constituency Parsing,"shows the results , with comparison to previous published ELMo BASE results the art , but fine tuning gives the biggest gain .",We also report parseval F1 for Penn Treebank constituency parsing .,0.8235294117647058,0.5396825396825397,0.6666666666666666,results,baselines
constituency_parsing,1,results,objective functions for pretraining,180,46,7,This results in the bilm loss dominating the triplet loss and we found that scaling the bilm term by a factor of 0.15 results in better performance .,"The latter is much harder than the cloze loss since less context is available and therefore gradients for the bilm loss are much larger : the cloze model achieves perplexity of about 4 while as for the bilm it is 27 - 30 , depending on the direction .",shows that the cloze loss performs significantly better than the bilm loss and that combining the two loss types does not improve over the cloze loss by itself .,0.8823529411764706,0.7301587301587301,0.7777777777777778,results,baselines
constituency_parsing,1,results,objective functions for pretraining,181,47,8,shows that the cloze loss performs significantly better than the bilm loss and that combining the two loss types does not improve over the cloze loss by itself .,This results in the bilm loss dominating the triplet loss and we found that scaling the bilm term by a factor of 0.15 results in better performance .,We conjecture that in - dividual left and right context prediction tasks are too different from center word prediction and that their learning signals are not complementary enough .,0.8872549019607843,0.7460317460317459,0.8888888888888888,results,baselines
constituency_parsing,2,title,title,2,2,2,An Empirical Study of Building a Strong Baseline for Constituency Parsing, , ,0.0148148148148148,1.0,1.0,research-problem,approach
constituency_parsing,2,introduction,introduction,13,8,8,Our aim is to update the Seq2seq approach proposed in as a stronger baseline of constituency parsing .,"After the first proposal of an Seq2seq constituency parser , many task - independent techniques have been developed , mainly in the NLG research area .",Our motivation is basically identical to that described in .,0.0962962962962963,0.6153846153846154,0.6153846153846154,approach,experimental-setup
constituency_parsing,2,experiments,experiments,111,20,20,( 1 ) Smaller mini-batch size M and gradient clipping G provided the better performance .,The following observations appear informative for building strong baseline systems :,"Such settings lead to slower and longer training , but higher performance .",0.8222222222222222,0.5,0.6060606060606061,results,approach
constituency_parsing,2,experiments,experiments,113,22,22,"( 2 ) Larger layer size , hidden state dimension , and beam size have little impact on the performance ; our setting , L = 2 , H = 200 , and B = 5 looks adequate in terms of speed / performance trade - off .","Such settings lead to slower and longer training , but higher performance .",Input unit selection :,0.837037037037037,0.55,0.6666666666666666,results,approach
constituency_parsing,2,experiments,experiments,115,24,24,"As often demonstrated in the NMT literature , using subword split as input token unit instead of standard tokenized word unit has potential to improve the performance .",Input unit selection :,( e ) shows the results of utilizing subword splits .,0.8518518518518519,0.6,0.7272727272727273,results,approach
constituency_parsing,2,experiments,experiments,119,28,28,"Thus , using subword information as features is one promising approach for leveraging subword information into constituency parsing .","It seems that the numbers of XX - tags in output and tokens in input should keep consistent for better performance since Seq2seq models look to somehow learn such relationship , and used it during the decoding .",lists the reported constituency parsing scores on PTB that were recently published in the literature .,0.8814814814814815,0.7,0.8484848484848485,results,approach
constituency_parsing,2,experiments,comparison to current top systems,126,35,2,Our Seq2seq approach successfully achieved the competitive level as the current top - notch methods : RNNG and its variants ., ,"Note here that , as described in , RNNG uses Berkeley parser 's mapping rules for effectively handling singleton words in the training corpus .",0.9333333333333332,0.875,0.2857142857142857,results,baselines
constituency_parsing,3,abstract,abstract,4,2,2,Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades ., ,"As a result , the most accurate parsers are domain specific , complex , and inefficient .",0.019047619047619,0.2857142857142857,0.2857142857142857,research-problem,experimental-setup
constituency_parsing,3,introduction,introduction,21,12,12,"We found this model to work poorly when we trained it on standard human - annotated parsing datasets ( 1M tokens ) , so we constructed an artificial dataset by labelling a large corpus with the BerkeleyParser .",Our early experiments focused on the sequence - to - sequence model of Sutskever et al ..,"To our surprise , the sequence - to - sequence model matched the BerkeleyParser that produced the annotation , having achieved an F 1 score of 90.5 on the test set ( section 23 of the WSJ ) .",0.1,0.6,0.6,approach,model
constituency_parsing,3,introduction,introduction,25,16,16,"We trained a sequence - to - sequence model with attention on the small human - annotated parsing dataset and were able to achieve an F 1 score of 88.3 on section 23 of the WSJ without the use of an ensemble and 90.5 with an ensemble , which matches the performance of the BerkeleyParser ( 90.4 ) when trained on the same data .",might be more data efficient and we found that it is indeed the case .,"Finally , we constructed a second artificial dataset consisting of only high - confidence parse trees , as measured by the agreement of two parsers .",0.119047619047619,0.8,0.8,approach,baselines
constituency_parsing,3,introduction,introduction,26,17,17,"Finally , we constructed a second artificial dataset consisting of only high - confidence parse trees , as measured by the agreement of two parsers .","We trained a sequence - to - sequence model with attention on the small human - annotated parsing dataset and were able to achieve an F 1 score of 88.3 on section 23 of the WSJ without the use of an ensemble and 90.5 with an ensemble , which matches the performance of the BerkeleyParser ( 90.4 ) when trained on the same data .",We trained a sequence - to - sequence model with attention on this data and achieved an F 1 score of 92.5 on section 23 of the WSJ - a new state - of - the - art .,0.1238095238095238,0.85,0.85,approach,model
constituency_parsing,3,introduction,introduction,27,18,18,We trained a sequence - to - sequence model with attention on this data and achieved an F 1 score of 92.5 on section 23 of the WSJ - a new state - of - the - art .,"Finally , we constructed a second artificial dataset consisting of only high - confidence parse trees , as measured by the agreement of two parsers .","This result did not require an ensemble , and as a result , the parser is also very fast .",0.1285714285714285,0.9,0.9,approach,baselines
constituency_parsing,3,lstm a parsing model,parameters and initialization,67,38,3,"In our experiments we used a model with 3 LSTM layers and 256 units in each layer , which we call LSTM + A .",Sizes .,Our input vocabulary size was 90 K and we output 128 symbols .,0.3190476190476189,0.6333333333333333,0.12,hyperparameters,baselines
constituency_parsing,3,lstm a parsing model,parameters and initialization,70,41,6,"Training on a small dataset we additionally used 2 dropout layers , one between LSTM 1 and LSTM 2 , and one between LSTM 2 and LSTM 3 .",Dropout .,We call this model LSTM + A+D .,0.3333333333333333,0.6833333333333333,0.24,hyperparameters,baselines
constituency_parsing,3,lstm a parsing model,parameters and initialization,82,53,18,The embedding layer for our 90K vocabulary can be initialized randomly or using pre-trained word - vector embeddings .,Pre-training word vectors .,We pre-trained skip - gram embeddings of size 512 using word2vec [ 6 ] on a 10B - word corpus .,0.3904761904761905,0.8833333333333333,0.72,hyperparameters,experimental-setup
constituency_parsing,3,lstm a parsing model,parameters and initialization,83,54,19,We pre-trained skip - gram embeddings of size 512 using word2vec [ 6 ] on a 10B - word corpus .,The embedding layer for our 90K vocabulary can be initialized randomly or using pre-trained word - vector embeddings .,"These embeddings were used to initialize our network but not fixed , they were later modified during training .",0.3952380952380952,0.9,0.76,hyperparameters,baselines
constituency_parsing,3,experiments,evaluation,118,29,10,But a single attention model gets to 88.3 and an ensemble of 5 LSTM + A+D models achieves 90.5 matching a single - model BerkeleyParser on WSJ 23 .,"It can be seen that , when training on WSJ only , a baseline LSTM does not achieve any reasonable score , even with dropout and early stopping .","When trained on the large high - confidence corpus , a single LSTM + A model achieves 92.5 and so outperforms not only the best single model , but also the best ensemble result reported previously .",0.5619047619047619,0.3766233766233766,0.1724137931034483,results,baselines
constituency_parsing,3,experiments,evaluation,119,30,11,"When trained on the large high - confidence corpus , a single LSTM + A model achieves 92.5 and so outperforms not only the best single model , but also the best ensemble result reported previously .",But a single attention model gets to 88.3 and an ensemble of 5 LSTM + A+D models achieves 90.5 matching a single - model BerkeleyParser on WSJ 23 .,An ensemble of 5 LSTM+ A models further improves this score to 92.8 .,0.5666666666666667,0.3896103896103896,0.1896551724137931,results,baselines
constituency_parsing,3,experiments,evaluation,120,31,12,An ensemble of 5 LSTM+ A models further improves this score to 92.8 .,"When trained on the large high - confidence corpus , a single LSTM + A model achieves 92.5 and so outperforms not only the best single model , but also the best ensemble result reported previously .",Generating well - formed trees .,0.5714285714285714,0.4025974025974026,0.2068965517241379,results,baselines
constituency_parsing,3,experiments,evaluation,122,33,14,"The LSTM + A model trained on WSJ dataset only produced malformed trees for 25 of the 1700 sentences in our development set ( 1.5 % of all cases ) , and the model trained on full high - confidence dataset did this for 14 sentences ( 0.8 % ) .",Generating well - formed trees .,"In these few cases where LSTM + A outputs a malformed tree , we simply add brackets to either the beginning or the end of the tree in order to make it balanced .",0.580952380952381,0.4285714285714285,0.2413793103448276,results,baselines
constituency_parsing,3,experiments,evaluation,130,41,22,"The difference between the F 1 score on sentences of length upto 30 and that upto 70 is 1.3 for the BerkeleyParser , 1.7 for the baseline LSTM , and 0.7 for LSTM + A .","The results , presented in , are surprising .","So already the baseline LSTM has similar performance to the BerkeleyParser , it degrades with length only slightly .",0.6190476190476191,0.5324675324675324,0.3793103448275862,results,baselines
constituency_parsing,3,experiments,evaluation,157,68,49,LSTM + A trained on the high - confidence corpus ( which only includes text from news ) achieved an F 1 score of 95.7 on QTB and 84.6 on WEB .,WEB the first half of each domain from the English Web Treebank ( 8310 sentences ) .,Our score on WEB is higher both than the best score reported in ( 83.5 ) and the best score we achieved with an in - house reimplementation of Berkeley Parser trained on human - annotated data ( 84.4 ) .,0.7476190476190476,0.8831168831168831,0.8448275862068966,results,baselines
constituency_parsing,4,abstract,abstract,4,2,2,Recent work has proposed several generative neural models for constituency parsing that achieve state - of - the - art results ., ,"Since direct search in these generative models is difficult , they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward .",0.0277777777777777,0.3333333333333333,0.3333333333333333,research-problem,baselines
constituency_parsing,4,introduction,introduction,10,2,2,Recent work on neural constituency parsing has found multiple cases where generative scoring models for which inference is complex outperform base models for which inference is simpler ., ,Let,0.0694444444444444,0.0555555555555555,0.0555555555555555,research-problem,ablation-analysis
constituency_parsing,4,introduction,introduction,35,27,27,"In this paper , we present experiments to isolate the degree to which each gain occurs for each of two state - of - the - art generative neural parsing models : the Recurrent Neural Network Grammar generative parser ( RG ) of , and the LSTM language modeling generative parser ( LM ) of .","Of course , many real hybrids will exhibit both reranking and model combination gains .","In particular , we present and use a beam - based search procedure with an augmented state space that can search directly in the generative models , allowing us to explore A ?",0.2430555555555555,0.75,0.75,approach,ablation-analysis
constituency_parsing,4,introduction,introduction,36,28,28,"In particular , we present and use a beam - based search procedure with an augmented state space that can search directly in the generative models , allowing us to explore A ?","In this paper , we present experiments to isolate the degree to which each gain occurs for each of two state - of - the - art generative neural parsing models : the Recurrent Neural Network Grammar generative parser ( RG ) of , and the LSTM language modeling generative parser ( LM ) of .",A for these generative parsers A independent of any base parsers .,0.25,0.7777777777777778,0.7777777777777778,approach,model
constituency_parsing,4,introduction,introduction,43,35,35,"We find that this is indeed the case : simply taking a weighted average of the scores of both models when selecting a parse from the base parser 's candidate list improves over using only the score of the generative model , in many cases substantially ( Section 3.2 ) .","Here we consider our second question : if crossscoring gains are at least partly due to implicit model combination , can we gain even more by combining the models explicitly ?","Using this technique , in combination with ensembling , we obtain new state - of - the - art results on the Penn Treebank : 94.25 F1 when training only on gold parse trees and 94.66 F1 when using external silver data .",0.2986111111111111,0.9722222222222222,0.9722222222222222,approach,model
constituency_parsing,4,experiments,augmenting the candidate set,90,14,1,Augmenting the candidate set, , ,0.625,0.2333333333333333,0.0588235294117647,results,approach
constituency_parsing,4,experiments,augmenting the candidate set,98,22,9,RG decreases performance from 93.45 F1 to 92.78 F1 on the development set .,RG ?,This difference is statistically significant at the p < 0.05 level under a paired bootstrap test .,0.6805555555555556,0.3666666666666665,0.5294117647058824,results,baselines
constituency_parsing,4,experiments,score combination,107,31,1,Score combination, , ,0.7430555555555556,0.5166666666666667,0.0833333333333333,results,approach
constituency_parsing,4,experiments,score combination,111,35,5,"We find that combining the scores of both models improves on using the score of either model alone , regardless of the source of candidates .",These results are given in columns RD + RG and RD + LM in .,These improvements are statistically significant in all cases .,0.7708333333333334,0.5833333333333334,0.4166666666666667,results,baselines
constituency_parsing,4,experiments,strengthening model combination,119,43,1,Strengthening model combination, , ,0.8263888888888888,0.7166666666666667,0.0555555555555555,results,baselines
constituency_parsing,4,experiments,strengthening model combination,123,47,5,"2 Combining candidates and scores from all three models ( row 9 ) , we obtain 93.94 F1 . :","The same trends we observed on the development data , on which the interpolation parameters were tuned , hold here : score combination improves results for all models ( row 3 vs. row 2 ; row 6 vs. row 5 ) , with candidate augmentation from the generative models giving a further increase ( rows 4 and 7 ) .","Test F1 scores on section 23 of the PTB , by treebank training data conditions : either using only the training sections of the PTB , or using additional silver data ( + S ) .",0.8541666666666666,0.7833333333333333,0.2777777777777778,results,baselines
constituency_parsing,4,experiments,strengthening model combination,131,55,13,"Ensembling Finally , we compare to another commonly used model combination method : ensembling multiple instances of the same model type trained from different random initializations .","As in the PTB training data setting , using all models for candidates and score combinations is best , achieving 94.66 F1 ( row 9 ) .","We train ensembles of 8 copies each of RD and RG in both the PTB and silver data settings , combining scores from models within an ensemble by averaging the models ' distributions for each action ( in beam search as well as rescoring ) .",0.9097222222222222,0.9166666666666666,0.7222222222222222,results,approach
constituency_parsing,4,experiments,strengthening model combination,134,58,16,"Performance when using only the ensembled RD models ( row 10 ) is lower than rescoring a single RD model with score combinations of single models , either RD + RG ( row 3 ) or RD + LM ( row 6 ) .","These results are shown in the bottom section , Ensembling , of .","In the PTB setting , ensembling with score combination achieves the best over all result of 94.25 ( row 13 ) .",0.9305555555555556,0.9666666666666668,0.8888888888888888,results,baselines
constituency_parsing,4,experiments,strengthening model combination,135,59,17,"In the PTB setting , ensembling with score combination achieves the best over all result of 94.25 ( row 13 ) .","Performance when using only the ensembled RD models ( row 10 ) is lower than rescoring a single RD model with score combinations of single models , either RD + RG ( row 3 ) or RD + LM ( row 6 ) .","In the silver training data setting , while this does improve on the analogous unensembled result ( row 8 ) , it is not better than the combination of single models when candidates from the generative models are also included ( row 9 ) .",0.9375,0.9833333333333332,0.9444444444444444,results,baselines
constituency_parsing,5,title,title,2,2,2,In- Order Transition - based Constituent Parsing, , ,0.0093896713615023,1.0,1.0,research-problem,approach
constituency_parsing,5,abstract,abstract,4,2,2,Both bottom - up and top - down strategies have been used for neural transition - based constituent parsing ., ,"The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree , where bottom - up strategies and top - down strategies take post-order and pre-order traversal over trees , respectively .",0.0187793427230046,0.2857142857142857,0.2857142857142857,research-problem,experimental-setup
constituency_parsing,5,introduction,introduction,32,23,23,"In this paper , we propose a novel transition system for constituent parsing , mitigating issues of both bottom - up and top - down systems by finding a compromise between bottom - up constituent information and top - down lookahead information .",transition - based parsing .,The process of the proposed constituent parsing can be regarded as in - order traversal over a tree .,0.1502347417840375,0.3538461538461539,0.5,model,ablation-analysis
constituency_parsing,5,introduction,introduction,49,40,40,We release our code at https://github.com/LeonCrashCode/InOrderParser .,"To our knowledge , we are the first to systematically compare top - down and bottom - up constituent parsing under the same neural framework .",Transition - based constituent parsing,0.2300469483568075,0.6153846153846154,0.8695652173913043,code,baselines
constituency_parsing,5,experiments,settings,131,15,4,Reranking experiments,"For both English and Chinese experiments , we use the same hyper - parameters as the work of without further optimization , as shown in .",Following the same reranking setting of shows the development results of the three parsing systems .,0.6150234741784038,0.4838709677419355,0.2857142857142857,results,approach
constituency_parsing,5,experiments,settings,133,17,6,The bottom - up system performs slightly better than the top - down system .,Following the same reranking setting of shows the development results of the three parsing systems .,The inorder system outperforms both the bottom - up and the top - down system .,0.6244131455399061,0.5483870967741935,0.4285714285714285,results,approach
constituency_parsing,5,experiments,settings,134,18,7,The inorder system outperforms both the bottom - up and the top - down system .,The bottom - up system performs slightly better than the top - down system .,shows the parsing results on the English test dataset .,0.6291079812206573,0.5806451612903226,0.5,results,approach
constituency_parsing,5,experiments,settings,136,20,9,"We find that the bottom - up parser and the top - down parser have similar results under the greedy setting , and the in - order parser outperforms both of them .",shows the parsing results on the English test dataset .,"Also , with supervised reranking , the in - order parser achieves the best results .",0.6384976525821596,0.6451612903225806,0.6428571428571429,results,baselines
constituency_parsing,5,experiments,settings,138,22,11,English constituent results,"Also , with supervised reranking , the in - order parser achieves the best results .","We compare our models with previous work , as shown in Table 4 .",0.647887323943662,0.7096774193548387,0.7857142857142857,results,approach
constituency_parsing,5,experiments,settings,140,24,13,"With the fully - supervise setting 5 , the inorder parser outperforms the state - of - the - art discrete parser , the state - of - the - art neural parsers","We compare our models with previous work , as shown in Table 4 .","Here , we only consider the work of a single model .",0.6572769953051644,0.7741935483870968,0.9285714285714286,results,experimental-setup
constituency_parsing,5,experiments,results,145,29,3,Chinese dependency results,Model,"As shown in , by converting the results to dependencies 6 , our final model achieves the best results among transitionbased parsing , and obtains comparable results to the state - of - the - art graph - based models . 85.5 84.0 87.7 86.2",0.6807511737089202,0.935483870967742,0.6,results,approach
constituency_parsing,5,experiments,results,146,30,4,"As shown in , by converting the results to dependencies 6 , our final model achieves the best results among transitionbased parsing , and obtains comparable results to the state - of - the - art graph - based models . 85.5 84.0 87.7 86.2",Chinese dependency results,UAS LAS,0.6854460093896714,0.967741935483871,0.8,results,baselines
constituency_parsing,6,title,parsing as language modeling,2,2,1,Parsing as Language Modeling, , ,0.0222222222222222,1.0,1.0,research-problem,approach
constituency_parsing,6,abstract,abstract,4,2,2,"We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing - 93.8 F 1 on section 23 , using 2 - 21 as training , 24 as development , plus tri-training .", ,"When trees are converted to Stanford dependencies , UAS and LAS are 95.9 % and 94.1 % .",0.0444444444444444,0.6666666666666666,0.6666666666666666,research-problem,baselines
constituency_parsing,6,introduction,introduction,7,2,2,"Recent work on deep learning syntactic parsing models has achieved notably good results , e.g. , with 92.4 F 1 on Penn Treebank constituency parsing and with 92.8 F 1 .", ,"In this paper we borrow from the approaches of both of these works and present a neural - net parse reranker that achieves very good results , 93.8 F 1 , with a comparatively simple architecture .",0.0777777777777777,0.1111111111111111,0.25,research-problem,baselines
constituency_parsing,6,introduction,introduction,8,3,3,"In this paper we borrow from the approaches of both of these works and present a neural - net parse reranker that achieves very good results , 93.8 F 1 , with a comparatively simple architecture .","Recent work on deep learning syntactic parsing models has achieved notably good results , e.g. , with 92.4 F 1 on Penn Treebank constituency parsing and with 92.8 F 1 .",In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem .,0.0888888888888888,0.1666666666666666,0.375,model,approach
constituency_parsing,6,model,hyper parameters,51,7,2,"The model has three LSTM layers with 1,500 units and gets trained with truncated backpropagation through time with mini-batch size 20 and step size 50 .", ,We initialize starting states with previous minibatch 's last hidden states .,0.5666666666666667,0.5833333333333334,0.2857142857142857,hyperparameters,baselines
constituency_parsing,6,model,hyper parameters,52,8,3,We initialize starting states with previous minibatch 's last hidden states .,"The model has three LSTM layers with 1,500 units and gets trained with truncated backpropagation through time with mini-batch size 20 and step size 50 .","The forget gate bias is initialized to be one and the rest of model parameters are sampled from U ( ? 0.05 , 0.05 ) .",0.5777777777777777,0.6666666666666666,0.4285714285714285,hyperparameters,approach
constituency_parsing,6,model,hyper parameters,53,9,4,"The forget gate bias is initialized to be one and the rest of model parameters are sampled from U ( ? 0.05 , 0.05 ) .",We initialize starting states with previous minibatch 's last hidden states .,Dropout is applied to non-recurrent connections and gradients are clipped when their norm is bigger than 20 .,0.5888888888888889,0.75,0.5714285714285714,hyperparameters,experimental-setup
constituency_parsing,6,model,hyper parameters,54,10,5,Dropout is applied to non-recurrent connections and gradients are clipped when their norm is bigger than 20 .,"The forget gate bias is initialized to be one and the rest of model parameters are sampled from U ( ? 0.05 , 0.05 ) .",The learning rate is 0.25 0.85 max where is an epoch number .,0.6,0.8333333333333334,0.7142857142857143,hyperparameters,experimental-setup
constituency_parsing,6,model,hyper parameters,55,11,6,The learning rate is 0.25 0.85 max where is an epoch number .,Dropout is applied to non-recurrent connections and gradients are clipped when their norm is bigger than 20 .,"For simplicity , we use vanilla softmax over an entire vocabulary as opposed to hierarchical softmax or noise contrastive estimation ( Gutmann and Hyvrinen , 2012 ) .",0.6111111111111112,0.9166666666666666,0.8571428571428571,hyperparameters,approach
constituency_parsing,6,model,hyper parameters,56,12,7,"For simplicity , we use vanilla softmax over an entire vocabulary as opposed to hierarchical softmax or noise contrastive estimation ( Gutmann and Hyvrinen , 2012 ) .",The learning rate is 0.25 0.85 max where is an epoch number ., ,0.6222222222222222,1.0,1.0,hyperparameters,baselines
constituency_parsing,6,results,results,72,1,1,Results, , ,0.8,0.125,1.0,results,baselines
constituency_parsing,6,results,improved semi supervision,77,6,5,"A single LSTM - LM ( GS ) together with Charniak ( GS ) reaches 93.6 and an ensemble of eight LSTM - LMs ( GS ) with Charniak ( GS ) achieves a new state of the art , 93.8 F 1 .","As shown in , both LSTM - LM ( G ) and LSTM - LM ( GS ) are affected by the quality of Y ( x ) .","When trees are converted to Stanford dependencies , 5 UAS and LAS are 95.9 % and 94.1 % , 6 more than 1 % higher than those of the state of the art dependency parser .",0.8555555555555555,0.75,0.7142857142857143,results,baselines
constituency_parsing,6,results,improved semi supervision,78,7,6,"When trees are converted to Stanford dependencies , 5 UAS and LAS are 95.9 % and 94.1 % , 6 more than 1 % higher than those of the state of the art dependency parser .","A single LSTM - LM ( GS ) together with Charniak ( GS ) reaches 93.6 and an ensemble of eight LSTM - LMs ( GS ) with Charniak ( GS ) achieves a new state of the art , 93.8 F 1 .",Why an indirect method ( converting trees to dependencies ) is more accurate than a direct one ( dependency parsing ) remains unanswered .,0.8666666666666667,0.875,0.8571428571428571,results,baselines
constituency_parsing,7,title,title,2,2,2,What Do Recurrent Neural Network Grammars Learn About Syntax ?, , ,0.008695652173913,1.0,1.0,research-problem,approach
constituency_parsing,7,abstract,abstract,4,2,2,Recurrent neural network grammars ( RNNG ) area recently proposed probabilistic generative modeling family for natural language ., ,They show state - of the - art language modeling and parsing performance .,0.017391304347826,0.2857142857142857,0.2857142857142857,research-problem,approach
constituency_parsing,7,introduction,introduction,12,3,3,"We focus on RNNGs as generative probabilistic models over trees , as summarized in 2 .","In this paper , we focus on a recently proposed class of probability distributions , recurrent neural network grammars ( RNNGs ; ) , designed to model syntactic derivations of sentences .",Fitting a probabilistic model to data has often been understood as away to test or confirm some aspect of a theory .,0.0521739130434782,0.15,0.15,research-problem,experimental-setup
constituency_parsing,7,introduction,introduction,21,12,12,This paper manipulates the inductive bias of RNNGs to test linguistic hypotheses .,"If they are mini-scientists , the discoveries they make should be of particular interest as propositions about syntax ( at least for the particular genre and dialect of the data ) .",We begin with an ablation study to discover the importance of the composition function in 3 .,0.0913043478260869,0.6,0.6,approach,ablation-analysis
constituency_parsing,7,introduction,introduction,22,13,13,We begin with an ablation study to discover the importance of the composition function in 3 .,This paper manipulates the inductive bias of RNNGs to test linguistic hypotheses .,"Based on the findings , we augment the RNNG composition function with a novel gated attention mechanism ( leading to the GA - RNNG ) to incorporate more interpretability into the model in 4 .",0.0956521739130434,0.65,0.65,approach,model
constituency_parsing,7,introduction,introduction,23,14,14,"Based on the findings , we augment the RNNG composition function with a novel gated attention mechanism ( leading to the GA - RNNG ) to incorporate more interpretability into the model in 4 .",We begin with an ablation study to discover the importance of the composition function in 3 .,"Using the GA - RNNG , we proceed by investigating the role that individual heads play in phrasal representation ( 5 ) and the role that nonterminal category labels play ( 6 ) .",0.1,0.7,0.7,approach,model
constituency_parsing,7,introduction,introduction,24,15,15,"Using the GA - RNNG , we proceed by investigating the role that individual heads play in phrasal representation ( 5 ) and the role that nonterminal category labels play ( 6 ) .","Based on the findings , we augment the RNNG composition function with a novel gated attention mechanism ( leading to the GA - RNNG ) to incorporate more interpretability into the model in 4 .","Our key findings are that lexical heads play an important role in representing most phrase types ( although compositions of multiple salient heads are not infrequent , especially 1 RNNGs have less inductive bias relative to traditional unlexicalized probabilistic context - free grammars , but more than models that parse by transducing word sequences to linearized parse trees represented as strings .",0.1043478260869565,0.75,0.75,approach,approach
constituency_parsing,7,composition is key,ablated rnngs,88,31,24,"The RNNG with only a stack is the strongest of the ablations , and it even outperforms the "" full "" RNNG with all three data structures .",Discussion .,Ablating the stack gives the worst among the new results .,0.3826086956521739,0.8157894736842105,0.7741935483870968,ablation-analysis,model
constituency_parsing,7,composition is key,ablated rnngs,89,32,25,Ablating the stack gives the worst among the new results .,"The RNNG with only a stack is the strongest of the ablations , and it even outperforms the "" full "" RNNG with all three data structures .",This strongly supports the importance of the composition function : a proper REDUCE operation that transforms a constituent 's parts and nonterminal label into a single explicit ( vector ) representation is helpful to performance .,0.3869565217391304,0.8421052631578947,0.8064516129032258,ablation-analysis,model
constituency_parsing,7,composition is key,ablated rnngs,93,36,29,"A similar performance degradation is seen in language modeling : the stack - only RNNG achieves the best performance , and ablating the stack is most harmful .","Since the stack maintains syntactically "" recent "" information near its top , we conjecture that the learner is overfitting to spurious predictors in the buffer and action history that explain the training data but do not generalize well .","Indeed , modeling syntax without explicit composition ( the stackablated RNNG ) provides little benefit over a sequential LSTM language model .",0.4043478260869565,0.9473684210526316,0.935483870967742,ablation-analysis,model
constituency_parsing,7,composition is key,ablated rnngs,94,37,30,"Indeed , modeling syntax without explicit composition ( the stackablated RNNG ) provides little benefit over a sequential LSTM language model .","A similar performance degradation is seen in language modeling : the stack - only RNNG achieves the best performance , and ablating the stack is most harmful .",We remark that the stack - only results are the best published PTB results for both phrasestructure and dependency parsing among supervised models .,0.408695652173913,0.9736842105263158,0.967741935483871,ablation-analysis,model
constituency_parsing,7,composition is key,ablated rnngs,95,38,31,We remark that the stack - only results are the best published PTB results for both phrasestructure and dependency parsing among supervised models .,"Indeed , modeling syntax without explicit composition ( the stackablated RNNG ) provides little benefit over a sequential LSTM language model .", ,0.4130434782608696,1.0,1.0,ablation-analysis,model
constituency_parsing,7,gated attention rnng,gated attention rnng,96,1,1,Gated Attention RNNG, , ,0.4173913043478261,0.0256410256410256,0.25,results,baselines
constituency_parsing,7,gated attention rnng,gated attention composition,134,39,25,"It is clear that the model outperforms the baseline RNNG with all three structures present and achieves competitive performance with the strongest , stack - only , RNNG variant .",We include this model 's performance in Tables 2 - 4 ( last row in all tables ) ., ,0.5826086956521739,1.0,1.0,results,baselines
constituency_parsing,7,headedness in phrases,headedness in phrases,135,1,1,Headedness in Phrases, , ,0.5869565217391305,0.0208333333333333,0.5,results,approach
constituency_parsing,7,headedness in phrases,comparison to existing head rules,173,39,8,The model has a higher overlap with the conversion using Collins head rules ( 49.8 UAS ) rather than the Stanford head rules ( 40.4 UAS ) .,Results .,"We attribute this large gap to the fact that the Stanford head rules incorporate more semantic considerations , while the RNNG is a purely syntactic model .",0.7521739130434782,0.8125,0.4705882352941176,results,model
constituency_parsing,7,headedness in phrases,comparison to existing head rules,175,41,10,"In general , the attention - based tree output has a high error rate ( ? 90 % ) when the dependent is a verb , since the constituent with the highest attention weight in a verb phrase is often the noun phrase instead of the verb , as discussed above .","We attribute this large gap to the fact that the Stanford head rules incorporate more semantic considerations , while the RNNG is a purely syntactic model .","The conversion accuracy is better for nouns ( ? 50 % error ) , and much better for determiners ( 30 % ) and particles ( 6 % ) with respect to the Collins head rules .",0.7608695652173914,0.8541666666666666,0.5882352941176471,results,model
constituency_parsing,7,headedness in phrases,comparison to existing head rules,176,42,11,"The conversion accuracy is better for nouns ( ? 50 % error ) , and much better for determiners ( 30 % ) and particles ( 6 % ) with respect to the Collins head rules .","In general , the attention - based tree output has a high error rate ( ? 90 % ) when the dependent is a verb , since the constituent with the highest attention weight in a verb phrase is often the noun phrase instead of the verb , as discussed above .",Discussion .,0.7652173913043478,0.875,0.6470588235294118,results,approach
constituency_parsing,7,the role of nonterminal labels,the role of nonterminal labels,183,1,1,The Role of Nonterminal Labels, , ,0.7956521739130434,0.0344827586206896,0.0344827586206896,results,experimental-setup
constituency_parsing,7,the role of nonterminal labels,the role of nonterminal labels,192,10,10,"On test data ( with the usual split ) , the GA - RNNG achieves 94.2 % , while the U - GA - RNNG achieves 93.5 % .","Using the same hyperparameters and the PTB dataset , we first consider unlabeled F 1 parsing accuracy .",This result suggests that nonterminal category labels add a relatively small amount of information compared to purely endocentric representations .,0.8347826086956521,0.3448275862068966,0.3448275862068966,results,baselines
constituency_parsing,8,title,title,2,2,2,Constituency Parsing with a Self - Attentive Encoder, , ,0.0098522167487684,1.0,1.0,research-problem,approach
constituency_parsing,8,introduction,introduction,16,7,7,"In this paper , we introduce a parser that combines an encoder built using this kind of self - attentive architecture with a decoder customized for parsing ( ) .","However , RNNs are not the only architecture capable of summarizing large global contexts : recent work by presented a new state - of - the - art approach to machine translation with an architecture that entirely eliminates recurrent connections and relies instead on a repeated neural attention mechanism .","In Section 2 of this paper , we describe the architecture and present our finding that self - attention can outperform an LSTM - based approach .",0.0788177339901477,0.3181818181818182,0.3181818181818182,model,experimental-setup
constituency_parsing,8,introduction,introduction,24,15,15,"We also present a version of our model that uses a character LSTM , which performs better than other lexical representationseven if word embeddings are removed from the model .","In Section 5.1 , we demonstrate that a simple scheme based on concatenating character embeddings of word prefixes / suffixes can outperform using part - of - speech tags from an external system .","In Section 5.2 , we explore an alternative approach for lexical representations that makes use of pre-training on a large unsupervised corpus .",0.1182266009852216,0.6818181818181818,0.6818181818181818,model,approach
constituency_parsing,8,results,results,163,1,1,6 Results, , ,0.8029556650246306,0.05,1.0,results,baselines
constituency_parsing,8,results,english wsj,164,2,1,English ( WSJ ), , ,0.8078817733990148,0.1,0.1428571428571428,results,model
constituency_parsing,8,results,english wsj,168,6,5,"The test score of 93.55 F1 for our CharLSTM parser exceeds the previous best numbers for single - system parsers trained on the Penn Treebank ( without the use of any external data , such as pre-trained word embeddings ) .",The results of evaluating our model on the test set are shown in .,"When our parser is augmented with ELMo word representations , it achieves a new state - of - the - art score of 95.13 F1 on the WSJ test set .",0.8275862068965517,0.3,0.7142857142857143,results,baselines
constituency_parsing,8,results,english wsj,169,7,6,"When our parser is augmented with ELMo word representations , it achieves a new state - of - the - art score of 95.13 F1 on the WSJ test set .","The test score of 93.55 F1 for our CharLSTM parser exceeds the previous best numbers for single - system parsers trained on the Penn Treebank ( without the use of any external data , such as pre-trained word embeddings ) .",Our WSJ - only parser took 18 hours to train using a single Tesla K80 GPU and can parse the,0.8325123152709359,0.35,0.8571428571428571,results,baselines
constituency_parsing,8,results,multilingual spmrl,171,9,1,Multilingual ( SPMRL ), , ,0.8423645320197044,0.45,0.0833333333333333,results,model
constituency_parsing,8,results,multilingual spmrl,178,16,8,"Development set results show that the addition of word embeddings to a model that uses a character LSTM has a mixed effect : it improves performance for some languages , but hurts for others .",Results are shown in .,"For each language , we selected the trained model that performed better on the development set and evaluated it on the test set .",0.8768472906403941,0.8,0.6666666666666666,results,baselines
constituency_parsing,8,results,multilingual spmrl,180,18,10,"On 8 of the 9 languages , our test set result exceeds the previous best - published numbers from any system we are aware of .","For each language , we selected the trained model that performed better on the development set and evaluated it on the test set .","The exception is Swedish , where the model of continues to be state - of - the - art despite a number of approaches proposed in the intervening years that have achieved better performance on other languages .",0.8866995073891626,0.9,0.8333333333333334,results,approach
coreference_resolution,0,title,title,2,2,2,Improving Coreference Resolution by Learning Entity - Level Distributed Representations, , ,0.0078125,1.0,1.0,research-problem,approach
coreference_resolution,0,introduction,introduction,16,8,8,"In this work , we instead train a deep neural network to build distributed representations of pairs of coreference clusters .",Previous work has incorporated entity - level information through features that capture hard constraints like having gender or number agreement between clusters .,"This captures entity - level information with a large number of learned , continuous features instead of a small number of hand - crafted categorical ones .",0.0625,0.3809523809523809,0.3809523809523809,model,experimental-setup
coreference_resolution,0,introduction,introduction,17,9,9,"This captures entity - level information with a large number of learned , continuous features instead of a small number of hand - crafted categorical ones .","In this work , we instead train a deep neural network to build distributed representations of pairs of coreference clusters .","Using the cluster - pair representations , our network learns when combining two coreference clusters is desirable .",0.06640625,0.4285714285714285,0.4285714285714285,model,approach
coreference_resolution,0,introduction,introduction,18,10,10,"Using the cluster - pair representations , our network learns when combining two coreference clusters is desirable .","This captures entity - level information with a large number of learned , continuous features instead of a small number of hand - crafted categorical ones .","At test time it builds up coreference clusters incrementally , starting with each mention in its own cluster and then merging a pair of clusters each step .",0.0703125,0.4761904761904762,0.4761904761904762,model,approach
coreference_resolution,0,introduction,introduction,19,11,11,"At test time it builds up coreference clusters incrementally , starting with each mention in its own cluster and then merging a pair of clusters each step .","Using the cluster - pair representations , our network learns when combining two coreference clusters is desirable .",It makes these decisions with a novel easy - first cluster - ranking procedure that combines the strengths of cluster - ranking ( Rahman and and easy - first coreference algorithms .,0.07421875,0.5238095238095238,0.5238095238095238,model,model
coreference_resolution,0,introduction,introduction,20,12,12,It makes these decisions with a novel easy - first cluster - ranking procedure that combines the strengths of cluster - ranking ( Rahman and and easy - first coreference algorithms .,"At test time it builds up coreference clusters incrementally , starting with each mention in its own cluster and then merging a pair of clusters each step .",Training incremental coreference systems is challenging because the coreference decisions facing a model depend on previous decisions it has already made .,0.078125,0.5714285714285714,0.5714285714285714,model,model
coreference_resolution,0,introduction,introduction,22,14,14,We address this by using a learning - to - search algorithm inspired by SEARN to train our neural network .,Training incremental coreference systems is challenging because the coreference decisions facing a model depend on previous decisions it has already made .,This approach allows the model to learn which action ( a cluster merge ) available from the current state ( a partially completed coreference clustering ) will eventually lead to a high - scoring coreference partition .,0.0859375,0.6666666666666666,0.6666666666666666,model,approach
coreference_resolution,0,introduction,introduction,23,15,15,This approach allows the model to learn which action ( a cluster merge ) available from the current state ( a partially completed coreference clustering ) will eventually lead to a high - scoring coreference partition .,We address this by using a learning - to - search algorithm inspired by SEARN to train our neural network .,"Our system uses little manual feature engineering , which means it is easily extended to multiple languages .",0.08984375,0.7142857142857143,0.7142857142857143,model,approach
coreference_resolution,0,building representations,final system performance,229,134,3,Our mention - ranking model surpasses all previous systems .,In we compare the results of our system with state - of - the - art approaches for English and Chinese .,"We attribute its improvement over the neural mention ranker from to our model using a deeper neural network , pretrained word embeddings , and more sophisticated pretraining .",0.89453125,0.950354609929078,0.3,results,approach
coreference_resolution,0,building representations,final system performance,231,136,5,"The cluster - ranking model improves results further across both languages and all evaluation metrics , demonstrating the utility of incorporating entity - level information .","We attribute its improvement over the neural mention ranker from to our model using a deeper neural network , pretrained word embeddings , and more sophisticated pretraining .",The improvement is largest in CEAF ?,0.90234375,0.9645390070921984,0.5,results,baselines
coreference_resolution,1,title,title,2,2,2,End - to - end Deep Reinforcement Learning Based Coreference Resolution, , ,0.0136054421768707,1.0,1.0,research-problem,approach
coreference_resolution,1,introduction,introduction,23,13,13,"In this paper , we propose a goal - directed endto - end deep reinforcement learning framework to resolve coreference as shown in .","Therefore , the next key research question is how to integrate and directly optimize coreference evaluation metrics in an end - to - end manner .","Specifically , we leverage the neural architecture in as our policy network , which includes learning span representation , scoring potential entity mentions , and generating a probability distribution over all possible coreference linking actions from the current mention to its antecedents .",0.1564625850340136,0.6842105263157895,0.6842105263157895,model,experimental-setup
coreference_resolution,1,introduction,introduction,24,14,14,"Specifically , we leverage the neural architecture in as our policy network , which includes learning span representation , scoring potential entity mentions , and generating a probability distribution over all possible coreference linking actions from the current mention to its antecedents .","In this paper , we propose a goal - directed endto - end deep reinforcement learning framework to resolve coreference as shown in .","Once a sequence of linking actions are made , our reward function is used to measure how good the generated coreference clusters are , which is directly related to coreference evaluation metrics .",0.1632653061224489,0.7368421052631579,0.7368421052631579,model,model
coreference_resolution,1,introduction,introduction,25,15,15,"Once a sequence of linking actions are made , our reward function is used to measure how good the generated coreference clusters are , which is directly related to coreference evaluation metrics .","Specifically , we leverage the neural architecture in as our policy network , which includes learning span representation , scoring potential entity mentions , and generating a probability distribution over all possible coreference linking actions from the current mention to its antecedents .","Besides , we introduce an entropy regularization term to encourage exploration and prevent the policy from prematurely converging to a bad local optimum .",0.1700680272108843,0.7894736842105263,0.7894736842105263,model,approach
coreference_resolution,1,introduction,introduction,26,16,16,"Besides , we introduce an entropy regularization term to encourage exploration and prevent the policy from prematurely converging to a bad local optimum .","Once a sequence of linking actions are made , our reward function is used to measure how good the generated coreference clusters are , which is directly related to coreference evaluation metrics .","Finally , we update the regularized policy network parameters based on the rewards associated with sequences of sampled actions , which are computed on the whole input document .",0.1768707482993197,0.8421052631578947,0.8421052631578947,model,ablation-analysis
coreference_resolution,1,introduction,introduction,27,17,17,"Finally , we update the regularized policy network parameters based on the rewards associated with sequences of sampled actions , which are computed on the whole input document .","Besides , we introduce an entropy regularization term to encourage exploration and prevent the policy from prematurely converging to a bad local optimum .",We evaluate our end - to - end reinforced coreference resolution model on the English OntoNotes v5.0 benchmark .,0.1836734693877551,0.8947368421052632,0.8947368421052632,model,model
coreference_resolution,1,experiments,experiments,101,1,1,Experiments, , ,0.6870748299319728,0.024390243902439,0.125,hyperparameters,experimental-setup
coreference_resolution,1,experiments,experiments,104,4,4,"First , we pretrain our model using Eq. ( 4 ) for around 200 K steps and use the learned parameters for initialization .",We reuse the hyperparameters and evaluation metrics from with a few exceptions .,"Besides , we set the number of sampled trajectories N s = 100 , tune the regularization parameter ? expr in { 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 } and set it to 10 ? 4 based on the development set .",0.7074829931972789,0.0975609756097561,0.5,hyperparameters,approach
coreference_resolution,1,experiments,experiments,105,5,5,"Besides , we set the number of sampled trajectories N s = 100 , tune the regularization parameter ? expr in { 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 } and set it to 10 ? 4 based on the development set .","First , we pretrain our model using Eq. ( 4 ) for around 200 K steps and use the learned parameters for initialization .","We use three standard metrics : MUC ( Grishman and Sundheim , 1995 ) , B 3 and CEAF ?",0.7142857142857143,0.1219512195121951,0.625,hyperparameters,approach
coreference_resolution,1,experiments,results,109,9,1,Results, , ,0.7414965986394558,0.2195121951219512,0.037037037037037,results,baselines
coreference_resolution,1,experiments,results,115,15,7,"Built on top of the model in but excluding ELMo , our base reinforced model improves the average F 1 score around 2 points ( statistical significant t- test with p < 0.05 ) compared with .","Note that 's method contains 3 tasks : named entity recognition , relation inference and coreference resolution and we dis able the relation inference task and train the other two tasks .","Besides , it is even comparable with the end - to - end multi-task coreference model that has ELMo support , which demonstrates the power of reinforcement learning combined with the state - of - the - art end - to - end model in .",0.782312925170068,0.3658536585365853,0.2592592592592592,results,model
coreference_resolution,1,experiments,results,117,17,9,"Regarding our model , using entropy regularization to encourage exploration can improve the result by 1 point .","Besides , it is even comparable with the end - to - end multi-task coreference model that has ELMo support , which demonstrates the power of reinforcement learning combined with the state - of - the - art end - to - end model in .","Moreover , introducing the context - dependent ELMo embedding to our base model can further boosts the performance , which is consistent with the results in .",0.7959183673469388,0.4146341463414634,0.3333333333333333,results,approach
coreference_resolution,1,experiments,results,118,18,10,"Moreover , introducing the context - dependent ELMo embedding to our base model can further boosts the performance , which is consistent with the results in .","Regarding our model , using entropy regularization to encourage exploration can improve the result by 1 point .","We also notice that our full model 's improvement is mainly from higher precision scores and reasonably good recall scores , which indicates that our reinforced model combined with more active exploration produces better coreference scores to reduce false positive coreference links .",0.8027210884353742,0.4390243902439024,0.3703703703703704,results,baselines
coreference_resolution,1,experiments,results,120,20,12,"Overall , our full model achieves the state - of the - art performance of 73.8 % F1 - score when using ELMo and entropy regularization ( compared to models marked with * in , and our approach simultaneously obtains the best F1 -score of 70.5 % when using fixed word embedding only .","We also notice that our full model 's improvement is mainly from higher precision scores and reasonably good recall scores , which indicates that our reinforced model combined with more active exploration produces better coreference scores to reduce false positive coreference links .",Model,0.8163265306122449,0.4878048780487805,0.4444444444444444,results,baselines
coreference_resolution,2,abstract,abstract,4,2,2,Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning ., ,In this paper we instead apply reinforcement learning to directly optimize a neural mention - ranking model for coreference evaluation metrics .,0.0266666666666666,0.3333333333333333,0.3333333333333333,research-problem,model
coreference_resolution,2,introduction,introduction,15,7,7,"To address this , we explore using two variants of reinforcement learning to directly optimize a coreference system for coreference evaluation metrics .","This complicates training , especially across different languages and datasets where systems may work best with different settings of the hyperparameters .","In particular , we modify the max-margin coreference objective proposed by by incorporating the reward associated with each coreference decision into the loss 's slack rescaling .",0.1,0.3888888888888889,0.3888888888888889,approach,approach
coreference_resolution,2,introduction,introduction,16,8,8,"In particular , we modify the max-margin coreference objective proposed by by incorporating the reward associated with each coreference decision into the loss 's slack rescaling .","To address this , we explore using two variants of reinforcement learning to directly optimize a coreference system for coreference evaluation metrics .",We also test the REINFORCE policy gradient algorithm .,0.1066666666666666,0.4444444444444444,0.4444444444444444,approach,ablation-analysis
coreference_resolution,2,introduction,introduction,17,9,9,We also test the REINFORCE policy gradient algorithm .,"In particular , we modify the max-margin coreference objective proposed by by incorporating the reward associated with each coreference decision into the loss 's slack rescaling .",Our model is a neural mention - ranking model .,0.1133333333333333,0.5,0.5,approach,approach
coreference_resolution,2,introduction,introduction,18,10,10,Our model is a neural mention - ranking model .,We also test the REINFORCE policy gradient algorithm .,Mention - ranking models score pairs of mentions for their likelihood of coreference rather than comparing partial coreference clusters .,0.12,0.5555555555555556,0.5555555555555556,approach,approach
coreference_resolution,2,experiments and results,results,108,7,1,Results, , ,0.72,0.175,0.125,results,baselines
coreference_resolution,2,experiments and results,results,110,9,3,"We find that REINFORCE does slightly better than the heuristic loss , but reward rescaling performs significantly better than both on both languages .","We compare the heuristic loss , REINFORCE , and reward rescaling approaches on both datasets .",We attribute the modest improvement of REIN - FORCE to it being poorly suited for a ranking task .,0.7333333333333333,0.225,0.375,results,approach
coreference_resolution,2,experiments and results,results,115,14,8,"The reward - rescaled max - margin loss combines the best of both worlds , resulting in superior performance .","We believe the benefit of REINFORCE being guided by coreference evaluation metrics is offset by this dis advantage , which does not occur in the max-margin approaches .", ,0.7666666666666667,0.35,1.0,results,baselines
coreference_resolution,3,title,title,2,2,2,Higher - order Coreference Resolution with Coarse - to - fine Inference, , ,0.0149253731343283,1.0,1.0,research-problem,approach
coreference_resolution,3,introduction,introduction,15,7,7,We introduce an approximation of higher - order inference that uses the span - ranking architecture from in an iterative manner .,"The plurality of [ you ] is underspecified , making it locally compatible with both [ I ] and [ all of you ] , while the full cluster would have mixed plurality , resulting in global inconsistency .","At each iteration , the antecedent distribution is used as an attention mechanism to optionally update existing span representations , enabling later corefer - Speaker 1 : U m and think that is what 's - Go ahead Linda .",0.1119402985074626,0.4117647058823529,0.4117647058823529,model,ablation-analysis
coreference_resolution,3,introduction,introduction,16,8,8,"At each iteration , the antecedent distribution is used as an attention mechanism to optionally update existing span representations , enabling later corefer - Speaker 1 : U m and think that is what 's - Go ahead Linda .",We introduce an approximation of higher - order inference that uses the span - ranking architecture from in an iterative manner .,Speaker 2 : Welland uh thanks goes to and to the media to help us ...,0.1194029850746268,0.4705882352941176,0.4705882352941176,model,model
coreference_resolution,3,introduction,introduction,19,11,11,"To alleviate computational challenges from this higher - order inference , we also propose a coarseto - fine approach that is learned with a single endto - end objective .",So our hat is off to [ all of you ] as well .,We introduce a less accurate but more efficient coarse factor in the pairwise scoring function .,0.1417910447761194,0.6470588235294118,0.6470588235294118,model,ablation-analysis
coreference_resolution,3,introduction,introduction,20,12,12,We introduce a less accurate but more efficient coarse factor in the pairwise scoring function .,"To alleviate computational challenges from this higher - order inference , we also propose a coarseto - fine approach that is learned with a single endto - end objective .",This additional factor enables an extra pruning step during inference that reduces the number of antecedents considered by the more accurate but inefficient fine factor .,0.1492537313432835,0.7058823529411765,0.7058823529411765,model,experimental-setup
coreference_resolution,3,introduction,introduction,21,13,13,This additional factor enables an extra pruning step during inference that reduces the number of antecedents considered by the more accurate but inefficient fine factor .,We introduce a less accurate but more efficient coarse factor in the pairwise scoring function .,"Intuitively , the model cheaply computes a rough sketch of likely antecedents before applying a more expensive scoring function .",0.1567164179104477,0.7647058823529411,0.7647058823529411,model,model
coreference_resolution,3,introduction,introduction,22,14,14,"Intuitively , the model cheaply computes a rough sketch of likely antecedents before applying a more expensive scoring function .",This additional factor enables an extra pruning step during inference that reduces the number of antecedents considered by the more accurate but inefficient fine factor .,Our experiments show that both of the above contributions improve the performance of coreference resolution on the English OntoNotes benchmark .,0.1641791044776119,0.8235294117647058,0.8235294117647058,model,model
coreference_resolution,3,results,results,112,1,1,Results, , ,0.8358208955223879,0.0714285714285714,0.0714285714285714,results,baselines
coreference_resolution,3,results,results,118,7,7,"The baseline relative to our contributions is the span - ranking model from augmented with both ELMo and hyperparameter tuning , which achieves 72.3 F1 .",We include performance of systems proposed in the past 3 years for reference .,"Our full approach achieves 73.0 F1 , setting a new state of the art for coreference resolution .",0.8805970149253731,0.5,0.5,results,baselines
coreference_resolution,3,results,results,119,8,8,"Our full approach achieves 73.0 F1 , setting a new state of the art for coreference resolution .","The baseline relative to our contributions is the span - ranking model from augmented with both ELMo and hyperparameter tuning , which achieves 72.3 F1 .","Compared to the heuristic pruning with up to 250 antecedents , our coarse - to - fine model only computes the expensive scores s a ( i , j ) for 50 antecedents .",0.8880597014925373,0.5714285714285714,0.5714285714285714,results,baselines
coreference_resolution,3,results,results,121,10,10,"Despite using far less computation , it outperforms the baseline because the coarse scores s c ( i , j ) can be computed for all antecedents , enabling the model to potentially predict a coreference link between any two spans in the document .","Compared to the heuristic pruning with up to 250 antecedents , our coarse - to - fine model only computes the expensive scores s a ( i , j ) for 50 antecedents .","As a result , we observe a much higher recall when adopting the coarse - to - fine approach .",0.9029850746268656,0.7142857142857143,0.7142857142857143,results,baselines
coreference_resolution,3,results,results,122,11,11,"As a result , we observe a much higher recall when adopting the coarse - to - fine approach .","Despite using far less computation , it outperforms the baseline because the coarse scores s c ( i , j ) can be computed for all antecedents , enabling the model to potentially predict a coreference link between any two spans in the document .",We also observe further improvement by including the second - order inference ( Section 3 ) .,0.9104477611940298,0.7857142857142857,0.7857142857142857,results,approach
coreference_resolution,3,results,results,123,12,12,We also observe further improvement by including the second - order inference ( Section 3 ) .,"As a result , we observe a much higher recall when adopting the coarse - to - fine approach .","The improvement is largely driven by the over all increase in precision , which is expected since the higher - order inference mainly serves to rule out inconsistent clusters .",0.917910447761194,0.8571428571428571,0.8571428571428571,results,approach
coreference_resolution,4,title,title,2,2,2,A Mention - Ranking Model for Abstract Anaphora Resolution, , ,0.0083333333333333,1.0,1.0,research-problem,approach
coreference_resolution,4,introduction,introduction,15,2,2,"Current research in anaphora ( or coreference ) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real Leo Born , Juri Opitz and Anette Frank contributed equally to this work .", ,"world , which is arguably the most frequently occurring type .",0.0625,0.0714285714285714,0.0714285714285714,research-problem,experimental-setup
coreference_resolution,4,introduction,introduction,28,15,15,"Our model is inspired by the mention - ranking model for coreference resolution and combines it with a Siamese Net , for learning similarity between sentences .","This enables us to use neural methods which have shown great success in related tasks : coreference resolution ( Clark and Manning , 2016 a ) , textual entailment , learning textual similarity , and discourse relation sense classification .","Given an anaphoric sentence ( AntecS in ( 1 ) ) and a candidate antecedent ( any constituent in a given context , e.g. being obsoleted by microprocessor - based machines in ( 1 ) ) , the LSTM - Siamese Net learns representations for the candidate and the anaphoric sentence in a shared space .",0.1166666666666666,0.5357142857142857,0.5357142857142857,model,approach
coreference_resolution,4,introduction,introduction,29,16,16,"Given an anaphoric sentence ( AntecS in ( 1 ) ) and a candidate antecedent ( any constituent in a given context , e.g. being obsoleted by microprocessor - based machines in ( 1 ) ) , the LSTM - Siamese Net learns representations for the candidate and the anaphoric sentence in a shared space .","Our model is inspired by the mention - ranking model for coreference resolution and combines it with a Siamese Net , for learning similarity between sentences .",These representations are combined into a joint representation used to calculate a score that characterizes the relation between them .,0.1208333333333333,0.5714285714285714,0.5714285714285714,model,ablation-analysis
coreference_resolution,4,introduction,introduction,30,17,17,These representations are combined into a joint representation used to calculate a score that characterizes the relation between them .,"Given an anaphoric sentence ( AntecS in ( 1 ) ) and a candidate antecedent ( any constituent in a given context , e.g. being obsoleted by microprocessor - based machines in ( 1 ) ) , the LSTM - Siamese Net learns representations for the candidate and the anaphoric sentence in a shared space .",The learned score is used to select the highest - scoring antecedent candidate for the given anaphoric sentence and hence its anaphor .,0.125,0.6071428571428571,0.6071428571428571,model,approach
coreference_resolution,4,introduction,introduction,31,18,18,The learned score is used to select the highest - scoring antecedent candidate for the given anaphoric sentence and hence its anaphor .,These representations are combined into a joint representation used to calculate a score that characterizes the relation between them .,We consider one anaphor at a time and provide the embedding of the context of the anaphor and the embedding of the head of the anaphoric phrase to the input to characterize each individual anaphorsimilar to the encoding proposed by for individuating multiply occurring predicates in SRL .,0.1291666666666666,0.6428571428571429,0.6428571428571429,model,model
coreference_resolution,4,introduction,introduction,32,19,19,We consider one anaphor at a time and provide the embedding of the context of the anaphor and the embedding of the head of the anaphoric phrase to the input to characterize each individual anaphorsimilar to the encoding proposed by for individuating multiply occurring predicates in SRL .,The learned score is used to select the highest - scoring antecedent candidate for the given anaphoric sentence and hence its anaphor .,With deeper inspection we show that the model learns a relation between the anaphor in the anaphoric sentence and its antecedent .,0.1333333333333333,0.6785714285714286,0.6785714285714286,model,model
coreference_resolution,4,introduction,introduction,33,20,20,With deeper inspection we show that the model learns a relation between the anaphor in the anaphoric sentence and its antecedent .,We consider one anaphor at a time and provide the embedding of the context of the anaphor and the embedding of the head of the anaphoric phrase to the input to characterize each individual anaphorsimilar to the encoding proposed by for individuating multiply occurring predicates in SRL .,displays our architecture .,0.1375,0.7142857142857143,0.7142857142857143,model,approach
coreference_resolution,4,introduction,introduction,36,23,23,It produces large amounts of instances and is easily adaptable to other languages .,"In contrast to other work , our method for generating training data is not confined to specific types of anaphora such as shell nouns or anaphoric connectives .","This enables us to build a robust , knowledge - lean model for abstract anaphora resolution that easily extends to multiple languages .",0.15,0.8214285714285714,0.8214285714285714,model,approach
coreference_resolution,4,introduction,introduction,41,28,28,Our Tensor Flow 2 implementation of the model and scripts for data extraction are available at : https://github.com/amarasovic / neural-abstract-anaphora.,To our knowledge this provides the first state - of - the - art benchmark on this data subset ., ,0.1708333333333333,1.0,1.0,code,baselines
coreference_resolution,4,training data construction,baselines and evaluation metrics,156,57,1,Baselines and evaluation metrics, , ,0.65,0.4453125,0.2,baselines,baselines
coreference_resolution,4,training data construction,baselines and evaluation metrics,158,59,3,"Additionally , we report the preceding sentence baseline ( PS BL ) that chooses the previous sentence for the antecedent and TAGbaseline ( TAG BL ) that randomly chooses a candidate with the constituent tag label in {S , VP , ROOT , SBAR } .","Following KZH13 , we report success@n ( s@n ) , which measures whether the antecedent , or a candidate that differs in one word 14 , is in the first n ranked candidates , for n ? { 1 , 2 , 3 , 4 }.",For TAG BL we report the average of 10 runs with 10 fixed seeds .,0.6583333333333333,0.4609375,0.6,baselines,baselines
coreference_resolution,4,training data construction,training details for our models,172,73,12,"Glo Ve word embeddings pre-trained on the Gigaword and Wikipedia , and did not fine - tune them .","To construct word vectors w i as defined in Section 3 , we used 100 - dim .","Vocabulary was built from the words in the training data with frequency in { 3 , U ( 1 , 10 ) } , and OOV words were replaced with an UNK token .",0.7166666666666667,0.5703125,0.3157894736842105,experimental-setup,experimental-setup
coreference_resolution,4,training data construction,training details for our models,173,74,13,"Vocabulary was built from the words in the training data with frequency in { 3 , U ( 1 , 10 ) } , and OOV words were replaced with an UNK token .","Glo Ve word embeddings pre-trained on the Gigaword and Wikipedia , and did not fine - tune them .","Embeddings for tags are initialized with values drawn from the uniform distribution U ? 1 ? d+t , 1 ? d+t , where t is the number of tags 16 and d ? { 50 , qlog - U ( 30 , 100 ) } the size of the tag embeddings .",0.7208333333333333,0.578125,0.3421052631578947,experimental-setup,experimental-setup
coreference_resolution,4,training data construction,training details for our models,177,78,17,"The size of the LSTMs hidden states was set to { 100 , qlog - U ( 30 , 150 ) } .",Weights initialization .,"We initialized the weight matrices of the LSTMs with random orthogonal matrices , all other weight matrices with the initialization proposed in .",0.7375,0.609375,0.4473684210526316,experimental-setup,experimental-setup
coreference_resolution,4,training data construction,training details for our models,178,79,18,"We initialized the weight matrices of the LSTMs with random orthogonal matrices , all other weight matrices with the initialization proposed in .","The size of the LSTMs hidden states was set to { 100 , qlog - U ( 30 , 150 ) } .",The first feed - forward layer size is set to a value in Optimization .,0.7416666666666667,0.6171875,0.4736842105263158,experimental-setup,experimental-setup
coreference_resolution,4,training data construction,training details for our models,179,80,19,The first feed - forward layer size is set to a value in Optimization .,"We initialized the weight matrices of the LSTMs with random orthogonal matrices , all other weight matrices with the initialization proposed in .","We trained our model in minibatches using Adam ( Kingma and Ba , 2015 ) with the learning rate of 10 ? 4 and maximal batch size 64 .",0.7458333333333333,0.625,0.5,experimental-setup,experimental-setup
coreference_resolution,4,training data construction,training details for our models,180,81,20,"We trained our model in minibatches using Adam ( Kingma and Ba , 2015 ) with the learning rate of 10 ? 4 and maximal batch size 64 .",The first feed - forward layer size is set to a value in Optimization .,"We clip gradients by global norm , with a clipping value in { 1.0 , U ( 1 , 100 ) } .",0.75,0.6328125,0.5263157894736842,experimental-setup,experimental-setup
coreference_resolution,4,training data construction,training details for our models,181,82,21,"We clip gradients by global norm , with a clipping value in { 1.0 , U ( 1 , 100 ) } .","We trained our model in minibatches using Adam ( Kingma and Ba , 2015 ) with the learning rate of 10 ? 4 and maximal batch size 64 .",We train for 10 epochs and choose the model that performs best on the devset .,0.7541666666666667,0.640625,0.5526315789473685,experimental-setup,experimental-setup
coreference_resolution,4,training data construction,training details for our models,182,83,22,We train for 10 epochs and choose the model that performs best on the devset .,"We clip gradients by global norm , with a clipping value in { 1.0 , U ( 1 , 100 ) } .",Regularization .,0.7583333333333333,0.6484375,0.5789473684210527,experimental-setup,experimental-setup
coreference_resolution,4,training data construction,training details for our models,184,85,24,"We used the l 2 - regularization with ? ? { 10 ?5 , log - U (10 ?7 , 10 ?2 ) }.",Regularization .,"Dropout with a keep probability k p ? { 0.8 , U( 0.5 , 1.0 ) } was applied to the outputs of the LSTMs , both feed - forward layers and optionally to the input with k p ? U (0.8 , 1.0 ) .",0.7666666666666667,0.6640625,0.631578947368421,experimental-setup,experimental-setup
coreference_resolution,4,training data construction,training details for our models,185,86,25,"Dropout with a keep probability k p ? { 0.8 , U( 0.5 , 1.0 ) } was applied to the outputs of the LSTMs , both feed - forward layers and optionally to the input with k p ? U (0.8 , 1.0 ) .","We used the l 2 - regularization with ? ? { 10 ?5 , log - U (10 ?7 , 10 ?2 ) }.",6 Results and analysis 6.1 Results on shell noun resolution dataset provides the results of the mentionranking model ( MR - LSTM ) on the ASN corpus using default HPs .,0.7708333333333334,0.671875,0.6578947368421053,experimental-setup,experimental-setup
coreference_resolution,4,training data construction,training details for our models,188,89,28,"In terms of s@1 score , MR - LSTM outperforms both KZH13 's results and TAG BL without even necessitating HP tuning .",Column 2 states which model produced the results : KZH13 refers to the best reported results in and TAG BL is the baseline described in Section 5.2 .,"For the outlier reason we tuned HPs ( on ARRAU - AA ) for different variants of the architecture : the full architecture , without embedding of the context of the anaphor ( ctx ) , of the anaphor ( aa ) , of both constituent tag em - bedding and shortcut ( tag , cut ) , dropping only the shortcut ( cut ) , using only word embeddings as input ( ctx , aa , tag , cut ) , without the first ( ffl1 ) and second ( ffl2 ) layer .",0.7833333333333333,0.6953125,0.7368421052631579,results,baselines
coreference_resolution,4,training data construction,training details for our models,190,91,30,"From we observe : ( 1 ) with HPs tuned on ARRAU - AA , we obtain results well beyond KZH13 , ( 2 ) all ablated model variants perform worse than the full model , ( 3 ) a large performance drop when omitting syntactic information ( tag , cut ) suggests that the model makes good use of it .","For the outlier reason we tuned HPs ( on ARRAU - AA ) for different variants of the architecture : the full architecture , without embedding of the context of the anaphor ( ctx ) , of the anaphor ( aa ) , of both constituent tag em - bedding and shortcut ( tag , cut ) , dropping only the shortcut ( cut ) , using only word embeddings as input ( ctx , aa , tag , cut ) , without the first ( ffl1 ) and second ( ffl2 ) layer .","However , this could also be due to a bias in the tag distribution , given that all candidates stem from the single sentence that contains antecedents .",0.7916666666666666,0.7109375,0.7894736842105263,results,baselines
coreference_resolution,4,training data construction,training details for our models,195,96,35,"Performance of 68.10 s@1 score indicates that the model is able to learn without syntactic guidance , contrary to the 19.68 s@1 score before tuning .","For example , without tuning the model with and without syntactic information achieves 71.27 and 19.68 ( not shown in table ) s@1 score , respectively , and with tuning : 87.78 and 68.10 .","shows the performance of different variants of the MR - LSTM with HPs tuned on the ASN corpus ( always better than the default HPs ) , when evaluated on 3 different subparts of the ARRAU - AA : all 600 abstract anaphors , 397 nominal and 203 pronominal ones .",0.8125,0.75,0.9210526315789472,results,baselines
coreference_resolution,4,training data construction,results on the arrau corpus,199,100,1,Results on the ARRAU corpus, , ,0.8291666666666667,0.78125,0.0625,results,experimental-setup
coreference_resolution,4,training data construction,results on the arrau corpus,200,101,2,"The MR - LSTM is more successful in resolving nominal than pronominal anaphors , although the training data provides only pronominal ones .", ,"This indicates that resolving pronominal abstract anaphora is harder compared to nominal abstract anaphora , such as shell nouns .",0.8333333333333334,0.7890625,0.125,results,baselines
coreference_resolution,4,training data construction,results on the arrau corpus,202,103,4,"Moreover , for shell noun resolution in KZH13 's dataset , the MR - LSTM achieved s@1 scores in the range 76.09-93.14 , while the best variant of the model achieves 51.89 s@1 score for nominal anaphors in ARRAU - AA .","This indicates that resolving pronominal abstract anaphora is harder compared to nominal abstract anaphora , such as shell nouns .","Although lower performance is expected , since we do not have specific training data for individual nominals in ARRAU - AA , we suspect that the reason for better performance for shell noun resolution in KZH13 is due to a larger number of positive candidates in ASN ( cf. , rows : antecedents / negatives ) .",0.8416666666666667,0.8046875,0.25,results,baselines
coreference_resolution,4,training data construction,results on the arrau corpus,209,110,11,"This is what we can observe from row 2 vs. row 6 in Table 5 : the MR - LSTM without context embedding ( ctx ) achieves a comparable s@ 2 score with the variant that omits syntactic information , but better s@3 - 4 scores .","Thus , the model is notable to point to exactly one antecedent , resulting in a lower s@1 score , but does well in picking a few good candidates , which yields good s@2 - 4 scores .","Further , median occurrence of tags not in {S , VP , ROOT , SBAR } among top - 4 ranked candidates is 0 for the full architecture , and 1 when syntactic information is omitted .",0.8708333333333333,0.859375,0.6875,results,baselines
coreference_resolution,5,title,title,2,2,2,Learning Global Features for Coreference Resolution, , ,0.0085106382978723,1.0,1.0,research-problem,approach
coreference_resolution,5,abstract,abstract,4,2,2,There is compelling evidence that coreference prediction would benefit from modeling global information about entity - clusters ., ,"Yet , state - of - the - art performance can be achieved with systems treating each mention prediction independently , which we attribute to the inherent difficulty of crafting informative clusterlevel features .",0.0170212765957446,0.2857142857142857,0.2857142857142857,research-problem,baselines
coreference_resolution,5,introduction,introduction,12,3,3,"In this work , we posit that global context is indeed necessary for further improvements in coreference resolution , but argue that informative cluster , rather than mention , level features are very difficult to devise , limiting their effectiveness .","While structured , non-local coreference models would seem to hold promise for avoiding many common coreference errors ( as discussed further in Section 3 ) , the results of employing such models in practice are decidedly mixed , and state - of - the - art results can be obtained using a completely local , mention - ranking system .","Accordingly , we instead propose to learn representations of mention clusters by embedding them sequentially using a recurrent neural network ( shown in Section 4 ) .",0.051063829787234,0.25,0.25,model,experimental-setup
coreference_resolution,5,introduction,introduction,13,4,4,"Accordingly , we instead propose to learn representations of mention clusters by embedding them sequentially using a recurrent neural network ( shown in Section 4 ) .","In this work , we posit that global context is indeed necessary for further improvements in coreference resolution , but argue that informative cluster , rather than mention , level features are very difficult to devise , limiting their effectiveness .","Our model has no manually defined cluster features , but instead learns a global representation from the individual mentions present in each cluster .",0.0553191489361702,0.3333333333333333,0.3333333333333333,model,ablation-analysis
coreference_resolution,5,introduction,introduction,14,5,5,"Our model has no manually defined cluster features , but instead learns a global representation from the individual mentions present in each cluster .","Accordingly , we instead propose to learn representations of mention clusters by embedding them sequentially using a recurrent neural network ( shown in Section 4 ) .",We incorporate these representations into a mention - ranking style coreference system .,0.0595744680851063,0.4166666666666667,0.4166666666666667,model,experimental-setup
coreference_resolution,5,introduction,introduction,15,6,6,We incorporate these representations into a mention - ranking style coreference system .,"Our model has no manually defined cluster features , but instead learns a global representation from the individual mentions present in each cluster .","The entire model , including the recurrent neural network and the mention - ranking sub-system , is trained end - to - end on the coreference task .",0.0638297872340425,0.5,0.5,model,approach
coreference_resolution,5,introduction,introduction,16,7,7,"The entire model , including the recurrent neural network and the mention - ranking sub-system , is trained end - to - end on the coreference task .",We incorporate these representations into a mention - ranking style coreference system .,"We train the model as a local classifier with fixed context ( that is , as a history - based model ) .",0.0680851063829787,0.5833333333333334,0.5833333333333334,model,experimental-setup
coreference_resolution,5,introduction,introduction,17,8,8,"We train the model as a local classifier with fixed context ( that is , as a history - based model ) .","The entire model , including the recurrent neural network and the mention - ranking sub-system , is trained end - to - end on the coreference task .","As such , unlike several recent approaches , which may require complicated inference during training , we are able to train our model in much the same way as a vanilla mentionranking model .",0.0723404255319148,0.6666666666666666,0.6666666666666666,model,approach
coreference_resolution,5,experiments,methods,181,21,20,"For training , we use document - size minibatches , which allows for efficient pre-computation of RNN states , and we minimize the loss described in Section 5 with AdaGrad ( after clipping LSTM gradients to lie ( elementwise ) in ( ?10 , 10 ) ) .","These modifications result in there being approximately 14 K distinct features in ? a and approximately 28 K distinct features in ? p , which is far fewer features than has been typical in past work .","We find that the initial learning rate chosen for AdaGrad has a significant impact on results , and we choose learning rates for each layer out of { 0.1 , 0.02 , 0.01 , 0.002 , 0.001 } .",0.7702127659574468,0.3230769230769231,0.6896551724137931,experimental-setup,experiments
coreference_resolution,5,experiments,methods,182,22,21,"We find that the initial learning rate chosen for AdaGrad has a significant impact on results , and we choose learning rates for each layer out of { 0.1 , 0.02 , 0.01 , 0.002 , 0.001 } .","For training , we use document - size minibatches , which allows for efficient pre-computation of RNN states , and we minimize the loss described in Section 5 with AdaGrad ( after clipping LSTM gradients to lie ( elementwise ) in ( ?10 , 10 ) ) .","In experiments , we set ha ( x n ) , h c ( x n ) , and h ( m ) to be ? R 200 , and hp ( x n , y) ? R 700 .",0.774468085106383,0.3384615384615385,0.7241379310344828,experimental-setup,approach
coreference_resolution,5,experiments,methods,183,23,22,"In experiments , we set ha ( x n ) , h c ( x n ) , and h ( m ) to be ? R 200 , and hp ( x n , y) ? R 700 .","We find that the initial learning rate chosen for AdaGrad has a significant impact on results , and we choose learning rates for each layer out of { 0.1 , 0.02 , 0.01 , 0.002 , 0.001 } .","We use a single - layer LSTM ( without "" peep - hole "" connections ) , as implemented in the element - rnn library .",0.7787234042553192,0.3538461538461539,0.7586206896551724,experimental-setup,approach
coreference_resolution,5,experiments,methods,184,24,23,"We use a single - layer LSTM ( without "" peep - hole "" connections ) , as implemented in the element - rnn library .","In experiments , we set ha ( x n ) , h c ( x n ) , and h ( m ) to be ? R 200 , and hp ( x n , y) ? R 700 .","For regularization , we apply Dropout with a rate of 0.4 before applying the linear weights u , and we also apply Dropout with a rate of 0.3 to the LSTM states before forming the dot -product scores .",0.7829787234042553,0.3692307692307693,0.7931034482758621,experimental-setup,baselines
coreference_resolution,5,experiments,methods,185,25,24,"For regularization , we apply Dropout with a rate of 0.4 before applying the linear weights u , and we also apply Dropout with a rate of 0.3 to the LSTM states before forming the dot -product scores .","We use a single - layer LSTM ( without "" peep - hole "" connections ) , as implemented in the element - rnn library .","Following we use the costweights ? = 0.5 , 1.2 , 1 in defining ? , and we use their pre-training scheme as well .",0.7872340425531915,0.3846153846153846,0.8275862068965517,experimental-setup,baselines
coreference_resolution,5,experiments,methods,189,29,28,Code for our system is available at https : //github.com/swiseman/nn_coref .,Scoring uses the official CoNLL 2012 script .,"The system makes use of a GPU for training , and trains in about two hours .",0.8042553191489362,0.4461538461538462,0.9655172413793104,code,baselines
coreference_resolution,5,experiments,methods,190,30,29,"The system makes use of a GPU for training , and trains in about two hours .",Code for our system is available at https : //github.com/swiseman/nn_coref ., ,0.8085106382978723,0.4615384615384616,1.0,experimental-setup,approach
coreference_resolution,5,experiments,results,191,31,1,Results, , ,0.8127659574468085,0.476923076923077,0.0588235294117647,results,baselines
coreference_resolution,5,experiments,results,193,33,3,"We see a statistically significant improvement of over 0.8 Co NLL points over the previous state of the art , and the highest F 1 scores to date on all three CoNLL metrics .","In we present our main results on the CoNLL English test set , and compare with other recent stateof - the - art systems .",We now consider in more detail the impact of global features and RNNs on performance .,0.8212765957446808,0.5076923076923077,0.1764705882352941,results,baselines
coreference_resolution,5,experiments,results,203,43,13,"In we see that the RNN improves performance over all , with the most dramatic improve - ments on non-anaphoric pronouns , though errors are also decreased significantly for non-anaphoric nominal and proper mentions that follow at least one mention with the same head .","1 . Finally , for baselines we consider the mention - ranking system ( MR ) of using our updated feature - set , as well as a non-local baseline with oracle history ( Avg , OH ) , which averages the representations h c ( x j ) for all x j ? X ( m ) , rather than feed them through an RNN ; errors are still backpropagated through the h c representations during learning .","While WL errors also decrease for both these mention - categories under the RNN model , FN errors increase .",0.8638297872340426,0.6615384615384615,0.7647058823529411,results,model
coreference_resolution,5,experiments,results,205,45,15,"Importantly , the RNN performance is significantly better than that of the Avg baseline , which barely improves over mention - ranking , even with oracle history .","While WL errors also decrease for both these mention - categories under the RNN model , FN errors increase .",This suggests that modeling the sequence of mentions in a cluster is advantageous .,0.8723404255319149,0.6923076923076923,0.8823529411764706,results,baselines
coreference_resolution,6,title,title,2,2,2,Learning Word Representations with Cross - Sentence Dependency for End - to - End Co -reference Resolution, , ,0.0157480314960629,1.0,1.0,research-problem,approach
coreference_resolution,6,abstract,abstract,4,2,2,"In this work , we present a word embedding model that learns cross - sentence dependency for improving end - to - end co-reference resolution ( E2E - CR ) .", ,"While the traditional E2E - CR model generates word representations by running long short - term memory ( LSTM ) recurrent neural networks on each sentence of an input article or conversation separately , we propose linear sentence linking and attentional sentence linking models to learn crosssentence dependency .",0.0314960629921259,0.3333333333333333,0.3333333333333333,research-problem,baselines
coreference_resolution,6,abstract,abstract,5,3,3,"While the traditional E2E - CR model generates word representations by running long short - term memory ( LSTM ) recurrent neural networks on each sentence of an input article or conversation separately , we propose linear sentence linking and attentional sentence linking models to learn crosssentence dependency .","In this work , we present a word embedding model that learns cross - sentence dependency for improving end - to - end co-reference resolution ( E2E - CR ) .",Both sentence linking strategies enable the LSTMs to make use of valuable information from context sentences while calculating the representation of the current input word .,0.0393700787401574,0.5,0.5,research-problem,ablation-analysis
coreference_resolution,6,introduction,introduction,10,2,2,Co-reference resolution requires models to cluster mentions that refer to the same physical entities ., ,The models based on neural networks typically require different levels of semantic representations of input sentences .,0.0787401574803149,0.1052631578947368,0.1052631578947368,research-problem,model
coreference_resolution,6,introduction,introduction,24,16,16,"To solve the problem that traditional LSTM encoders , which treat the input sentences as a batch , lack an ability to capture cross - sentence dependency , and to avoid the time complexity and difficulties of training the model concatenating all input sentences , we propose a cross - sentence encoder for end - to - end co-reference ( E2E - CR ) .",Regarding the entire input corpus as one sequence instead of a batch also significantly increases the time complexity of the model .,"Borrowing the idea of an external memory module from , an external memory block containing syntactic and semantic information from context sentences is added to the standard LSTM model .",0.1889763779527559,0.8421052631578947,0.8421052631578947,model,ablation-analysis
coreference_resolution,6,introduction,introduction,25,17,17,"Borrowing the idea of an external memory module from , an external memory block containing syntactic and semantic information from context sentences is added to the standard LSTM model .","To solve the problem that traditional LSTM encoders , which treat the input sentences as a batch , lack an ability to capture cross - sentence dependency , and to avoid the time complexity and difficulties of training the model concatenating all input sentences , we propose a cross - sentence encoder for end - to - end co-reference ( E2E - CR ) .","With this context memory block , the proposed model is able to encode input sentences as a batch , and also calculate the representations of input words by taking both target sentences and context sentences into consideration .",0.1968503937007874,0.8947368421052632,0.8947368421052632,model,ablation-analysis
coreference_resolution,6,introduction,introduction,26,18,18,"With this context memory block , the proposed model is able to encode input sentences as a batch , and also calculate the representations of input words by taking both target sentences and context sentences into consideration .","Borrowing the idea of an external memory module from , an external memory block containing syntactic and semantic information from context sentences is added to the standard LSTM model .",Experiments showed that this approach improved the performance of co-reference resolution models .,0.2047244094488189,0.9473684210526316,0.9473684210526316,model,approach
coreference_resolution,6,experiments,model and hyperparameter setup,87,7,2,"In practice , the LSTM modules applied in our model have 200 output units .", ,"In ASL , we calculate cross - sentence dependency using a multilayer perceptron with one hidden layer consisting of 150 hidden units .",0.6850393700787402,0.1627906976744186,0.2857142857142857,hyperparameters,baselines
coreference_resolution,6,experiments,model and hyperparameter setup,88,8,3,"In ASL , we calculate cross - sentence dependency using a multilayer perceptron with one hidden layer consisting of 150 hidden units .","In practice , the LSTM modules applied in our model have 200 output units .",The initial learning rate is set as 0.001 and decays 0.001 % every 100 steps .,0.6929133858267716,0.1860465116279069,0.4285714285714285,hyperparameters,ablation-analysis
coreference_resolution,6,experiments,model and hyperparameter setup,89,9,4,The initial learning rate is set as 0.001 and decays 0.001 % every 100 steps .,"In ASL , we calculate cross - sentence dependency using a multilayer perceptron with one hidden layer consisting of 150 hidden units .","The model is optimized with the Adam algorithm ( Kingma and Ba , 2014 ) .",0.7007874015748031,0.2093023255813953,0.5714285714285714,hyperparameters,experiments
coreference_resolution,6,experiments,model and hyperparameter setup,90,10,5,"The model is optimized with the Adam algorithm ( Kingma and Ba , 2014 ) .",The initial learning rate is set as 0.001 and decays 0.001 % every 100 steps .,We randomly select up to 40 continuous sentences for training if the input is too long .,0.7086614173228346,0.2325581395348837,0.7142857142857143,hyperparameters,approach
coreference_resolution,6,experiments,model and hyperparameter setup,91,11,6,We randomly select up to 40 continuous sentences for training if the input is too long .,"The model is optimized with the Adam algorithm ( Kingma and Ba , 2014 ) .","In co-reference prediction , we select 250 candidate antecedents as our baseline model .",0.7165354330708661,0.2558139534883721,0.8571428571428571,hyperparameters,approach
coreference_resolution,6,experiments,experiment results and discussion,93,13,1,Experiment Results and Discussion, , ,0.7322834645669292,0.3023255813953488,0.032258064516129,results,baselines
coreference_resolution,6,experiments,experiment results and discussion,97,17,5,"Comparing with the baseline model that achieved 67.2 % F1 score , the ASL model improved the performance by 0.6 % and achieved 67.8 % average F1 .","We mainly focus on the average F 1 score of MUC , B 3 , and CEAF metrics .",Experiments : Experimental results of previous models and cross - sentence dependency learning models on the CoNLL - 2012 shared task .,0.7637795275590551,0.3953488372093023,0.1612903225806451,results,baselines
coreference_resolution,6,experiments,experiment results and discussion,108,28,16,"show that the models that consider cross - sentence dependency significantly outperform the baseline model , which encodes each sentence from the input document separately .",- These include some Afghan - Arab volunteers .,"Experiments also indicated that the ASL model has better performance than the LSL model , since it summarizes extracts context information with an attention mechanism instead of simply viewing sentence - level embeddings .",0.8503937007874016,0.6511627906976745,0.5161290322580645,results,baselines
coreference_resolution,6,experiments,experiment results and discussion,109,29,17,"Experiments also indicated that the ASL model has better performance than the LSL model , since it summarizes extracts context information with an attention mechanism instead of simply viewing sentence - level embeddings .","show that the models that consider cross - sentence dependency significantly outperform the baseline model , which encodes each sentence from the input document separately .",This gives the model a better ability to model cross - sentence dependency .,0.8582677165354331,0.6744186046511628,0.5483870967741935,results,baselines
coreference_resolution,7,title,title,2,2,2,Coreference Resolution with Entity Equalization, , ,0.0155038759689922,1.0,1.0,research-problem,approach
coreference_resolution,7,introduction,introduction,18,9,9,"Here we propose an approach that provides an entity - level representation in a simple and intuitive manner , and also facilitates end - to - end optimization .","Specifically , tackled this problem by iteratively averaging the antecedents of each mention to create mention representations thatare more "" global "" ( i.e. , reflect information about the entity to which the mention refers ) .","Our "" Entity Equalization "" approach posits that each entity should be represented via the sum of its corresponding mention representations .",0.1395348837209302,0.4090909090909091,0.4090909090909091,model,experimental-setup
coreference_resolution,7,introduction,introduction,19,10,10,"Our "" Entity Equalization "" approach posits that each entity should be represented via the sum of its corresponding mention representations .","Here we propose an approach that provides an entity - level representation in a simple and intuitive manner , and also facilitates end - to - end optimization .","It is not immediately obvious how to perform this equalization , which relies on the entity - to- mention mapping , but we provide a natural smoothed representation of this mapping , and demonstrate how to use it for equalization .",0.1472868217054263,0.4545454545454545,0.4545454545454545,model,ablation-analysis
coreference_resolution,7,introduction,introduction,22,13,13,"Similar to recent coreference models , our approach uses contextual embeddings as input mention representations .","Now that each mention contains information about all its corresponding entities , we can use a standard pairwise scoring model , and this model will be able to use global entity - level information .","While previous approaches employed the ELMo model , we propose to use BERT embeddings , motivated by the impressive empirical performance of BERT on other tasks .",0.1705426356589147,0.5909090909090909,0.5909090909090909,model,approach
coreference_resolution,7,introduction,introduction,23,14,14,"While previous approaches employed the ELMo model , we propose to use BERT embeddings , motivated by the impressive empirical performance of BERT on other tasks .","Similar to recent coreference models , our approach uses contextual embeddings as input mention representations .",It is challenging to apply BERT to the coreference resolution setting because BERT is limited to a fixed sequence length which is shorter than most coreference resolution documents .,0.1782945736434108,0.6363636363636364,0.6363636363636364,model,ablation-analysis
coreference_resolution,7,introduction,introduction,25,16,16,We show that this can be done by using BERT in a fully convolutional manner .,It is challenging to apply BERT to the coreference resolution setting because BERT is limited to a fixed sequence length which is shorter than most coreference resolution documents .,"Our work is the first to use BERT for the task of coreference resolution , and we demonstrate that this results in significant improvement over current state - of - the - art .",0.1937984496124031,0.7272727272727273,0.7272727272727273,model,approach
coreference_resolution,7,results,results,115,1,1,Results, , ,0.8914728682170543,0.0666666666666666,0.0666666666666666,results,baselines
coreference_resolution,7,results,results,119,5,5,"Our baseline is the span - ranking model from with ELMo input features and second - order span representations , which achieves 73.0 % Avg.",Results on the test set are shown in .,F1 . Replacing the ELMo features with BERT features achieves 76. 25 % average F1 .,0.9224806201550388,0.3333333333333333,0.3333333333333333,results,baselines
coreference_resolution,7,results,results,120,6,6,F1 . Replacing the ELMo features with BERT features achieves 76. 25 % average F1 .,"Our baseline is the span - ranking model from with ELMo input features and second - order span representations , which achieves 73.0 % Avg.","Removing the second - order span - representations while using BERT features achieves 76.37 % F1 , achieving higher recall and lower precision on all evaluation metrics , while somewhat surprisingly being better over all .",0.9302325581395348,0.4,0.4,results,baselines
coreference_resolution,7,results,results,121,7,7,"Removing the second - order span - representations while using BERT features achieves 76.37 % F1 , achieving higher recall and lower precision on all evaluation metrics , while somewhat surprisingly being better over all .",F1 . Replacing the ELMo features with BERT features achieves 76. 25 % average F1 .,"Replacing secondorder span representations with Entity Equalization achieves 76. 64 % average F1 , while also consistently achieving the highest F 1 score on all three evaluation metrics .",0.9379844961240308,0.4666666666666667,0.4666666666666667,results,baselines
coreference_resolution,7,results,results,122,8,8,"Replacing secondorder span representations with Entity Equalization achieves 76. 64 % average F1 , while also consistently achieving the highest F 1 score on all three evaluation metrics .","Removing the second - order span - representations while using BERT features achieves 76.37 % F1 , achieving higher recall and lower precision on all evaluation metrics , while somewhat surprisingly being better over all .","Our results set a new state of the art for coreference resolution , improving the previous state of the art by 3.6 % average F1 .",0.9457364341085271,0.5333333333333333,0.5333333333333333,results,baselines
coreference_resolution,7,results,results,123,9,9,"Our results set a new state of the art for coreference resolution , improving the previous state of the art by 3.6 % average F1 .","Replacing secondorder span representations with Entity Equalization achieves 76. 64 % average F1 , while also consistently achieving the highest F 1 score on all three evaluation metrics .",Conclusion,0.9534883720930232,0.6,0.6,results,approach
coreference_resolution,8,title,title,2,2,2,End - to - end Neural Coreference Resolution, , ,0.0080645161290322,1.0,1.0,research-problem,approach
coreference_resolution,8,abstract,abstract,4,2,2,We introduce the first end - to - end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or handengineered mention detector ., ,The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each .,0.0161290322580645,0.3333333333333333,0.3333333333333333,research-problem,baselines
coreference_resolution,8,introduction,introduction,10,2,2,We present the first state - of - the - art neural coreference resolution model that is learned end - toend given only gold mention clusters ., ,"All recent coreference models , including neural approaches that achieved impressive performance gains , rely on syntactic parsers , both for headword features and as the input to carefully handengineered mention proposal algorithms .",0.0403225806451612,0.1176470588235294,0.1176470588235294,model,ablation-analysis
coreference_resolution,8,introduction,introduction,12,4,4,"We demonstrate for the first time that these resources are not required , and in fact performance can be improved significantly without them , by training an end - to - end neural model that jointly learns which spans are entity mentions and how to best cluster them .","All recent coreference models , including neural approaches that achieved impressive performance gains , rely on syntactic parsers , both for headword features and as the input to carefully handengineered mention proposal algorithms .",Our model reasons over the space of all spans up to a maximum length and directly optimizes the marginal likelihood of antecedent spans from gold coreference clusters .,0.0483870967741935,0.2352941176470588,0.2352941176470588,model,baselines
coreference_resolution,8,introduction,introduction,13,5,5,Our model reasons over the space of all spans up to a maximum length and directly optimizes the marginal likelihood of antecedent spans from gold coreference clusters .,"We demonstrate for the first time that these resources are not required , and in fact performance can be improved significantly without them , by training an end - to - end neural model that jointly learns which spans are entity mentions and how to best cluster them .","It includes a span - ranking model that decides , for each span , which of the previous spans ( if any ) is a good antecedent .",0.0524193548387096,0.2941176470588235,0.2941176470588235,model,ablation-analysis
coreference_resolution,8,introduction,introduction,14,6,6,"It includes a span - ranking model that decides , for each span , which of the previous spans ( if any ) is a good antecedent .",Our model reasons over the space of all spans up to a maximum length and directly optimizes the marginal likelihood of antecedent spans from gold coreference clusters .,"At the core of our model are vector embeddings representing spans of text in the document , which combine context - dependent boundary representations with a head - finding attention mechanism over the span .",0.0564516129032258,0.3529411764705882,0.3529411764705882,model,model
coreference_resolution,8,introduction,introduction,15,7,7,"At the core of our model are vector embeddings representing spans of text in the document , which combine context - dependent boundary representations with a head - finding attention mechanism over the span .","It includes a span - ranking model that decides , for each span , which of the previous spans ( if any ) is a good antecedent .","The attention component is inspired by parser - derived head - word matching features from previous systems , but is less susceptible to cascading errors .",0.0604838709677419,0.4117647058823529,0.4117647058823529,model,ablation-analysis
coreference_resolution,8,introduction,introduction,16,8,8,"The attention component is inspired by parser - derived head - word matching features from previous systems , but is less susceptible to cascading errors .","At the core of our model are vector embeddings representing spans of text in the document , which combine context - dependent boundary representations with a head - finding attention mechanism over the span .","In our analyses , we show empirically that these learned attention weights correlate strongly with traditional headedness definitions .",0.064516129032258,0.4705882352941176,0.4705882352941176,model,experimental-setup
coreference_resolution,8,experiments,hyperparameters,116,5,1,Hyperparameters, , ,0.4677419354838709,0.25,0.0625,experimental-setup,baselines
coreference_resolution,8,experiments,hyperparameters,118,7,3,"The word embeddings area fixed concatenation of 300 - dimensional GloVe embeddings and 50 - dimensional embeddings from , both normalized to be unit vectors .",Word representations,Outof - vocabulary words are represented by a vector of zeros .,0.4758064516129033,0.35,0.1875,experimental-setup,experimental-setup
coreference_resolution,8,experiments,hyperparameters,119,8,4,Outof - vocabulary words are represented by a vector of zeros .,"The word embeddings area fixed concatenation of 300 - dimensional GloVe embeddings and 50 - dimensional embeddings from , both normalized to be unit vectors .","In the character CNN , characters are represented as learned 8 - dimensional embeddings .",0.4798387096774194,0.4,0.25,experimental-setup,experimental-setup
coreference_resolution,8,experiments,hyperparameters,120,9,5,"In the character CNN , characters are represented as learned 8 - dimensional embeddings .",Outof - vocabulary words are represented by a vector of zeros .,"The convolutions have window sizes of 3 , 4 , and 5 characters , each consisting of 50 filters .",0.4838709677419355,0.45,0.3125,experimental-setup,approach
coreference_resolution,8,experiments,hyperparameters,121,10,6,"The convolutions have window sizes of 3 , 4 , and 5 characters , each consisting of 50 filters .","In the character CNN , characters are represented as learned 8 - dimensional embeddings .",Hidden dimensions,0.4879032258064516,0.5,0.375,experimental-setup,baselines
coreference_resolution,8,experiments,hyperparameters,123,12,8,The hidden states in the LSTMs have 200 dimensions .,Hidden dimensions,Each feedforward neural network consists of two hidden layers with 150 dimensions and rectified linear units .,0.4959677419354839,0.6,0.5,experimental-setup,approach
coreference_resolution,8,experiments,hyperparameters,124,13,9,Each feedforward neural network consists of two hidden layers with 150 dimensions and rectified linear units .,The hidden states in the LSTMs have 200 dimensions .,Feature encoding,0.5,0.65,0.5625,experimental-setup,approach
coreference_resolution,8,learning,learning,133,2,2,We use ADAM for learning with a minibatch size of 1 ., ,The LSTM weights are initialized with random orthonormal matrices as described in .,0.5362903225806451,0.1333333333333333,0.2,experimental-setup,experiments
coreference_resolution,8,learning,learning,134,3,3,The LSTM weights are initialized with random orthonormal matrices as described in .,We use ADAM for learning with a minibatch size of 1 .,We apply 0.5 dropout to the word embeddings and character CNN outputs .,0.5403225806451613,0.2,0.3,experimental-setup,baselines
coreference_resolution,8,learning,learning,135,4,4,We apply 0.5 dropout to the word embeddings and character CNN outputs .,The LSTM weights are initialized with random orthonormal matrices as described in .,We apply 0.2 dropout to all hidden layers and feature embeddings .,0.5443548387096774,0.2666666666666666,0.4,experimental-setup,baselines
coreference_resolution,8,learning,learning,136,5,5,We apply 0.2 dropout to all hidden layers and feature embeddings .,We apply 0.5 dropout to the word embeddings and character CNN outputs .,Dropout masks are shared across timesteps to preserve long - distance information as described in .,0.5483870967741935,0.3333333333333333,0.5,experimental-setup,experiments
coreference_resolution,8,learning,learning,137,6,6,Dropout masks are shared across timesteps to preserve long - distance information as described in .,We apply 0.2 dropout to all hidden layers and feature embeddings .,The learning rate is decayed by 0.1 % every 100 steps .,0.5524193548387096,0.4,0.6,experimental-setup,model
coreference_resolution,8,learning,learning,138,7,7,The learning rate is decayed by 0.1 % every 100 steps .,Dropout masks are shared across timesteps to preserve long - distance information as described in .,"The model is trained for up to 150 epochs , with early stopping based on the development set .",0.5564516129032258,0.4666666666666667,0.7,experimental-setup,experimental-setup
coreference_resolution,8,learning,learning,139,8,8,"The model is trained for up to 150 epochs , with early stopping based on the development set .",The learning rate is decayed by 0.1 % every 100 steps .,All code is implemented in Tensor - Flow and is publicly available .,0.5604838709677419,0.5333333333333333,0.8,experimental-setup,model
coreference_resolution,8,learning,learning,140,9,9,All code is implemented in Tensor - Flow and is publicly available .,"The model is trained for up to 150 epochs , with early stopping based on the development set .",3,0.5645161290322581,0.6,0.9,experimental-setup,experiments
coreference_resolution,8,results,results,147,1,1,Results, , ,0.592741935483871,0.027027027027027,0.25,results,baselines
coreference_resolution,8,results,coreference results,151,5,1,Coreference Results, , ,0.6088709677419355,0.1351351351351351,0.125,results,baselines
coreference_resolution,8,results,coreference results,153,7,3,We outperform previous systems in all metrics .,Table 1 compares our model to several previous systems that have driven substantial improvements over the past several years on the OntoNotes benchmark .,"In particular , our single model improves the state - of - the - art average F1 by 1.5 , and our 5 - model ensemble improves it by 3.1 .",0.6169354838709677,0.1891891891891892,0.375,results,baselines
coreference_resolution,8,results,coreference results,154,8,4,"In particular , our single model improves the state - of - the - art average F1 by 1.5 , and our 5 - model ensemble improves it by 3.1 .",We outperform previous systems in all metrics .,"The most significant gains come from improvements in recall , which is likely due to our end - toend setup .",0.6209677419354839,0.2162162162162162,0.5,results,baselines
coreference_resolution,8,results,coreference results,155,9,5,"The most significant gains come from improvements in recall , which is likely due to our end - toend setup .","In particular , our single model improves the state - of - the - art average F1 by 1.5 , and our 5 - model ensemble improves it by 3.1 .","During training , pipelined systems typically discard any mentions that the mention detector misses , which for consists of more than 9 % of the labeled mentions in the training data .",0.625,0.2432432432432433,0.625,results,approach
coreference_resolution,8,results,ablations,162,16,4,"The distance between spans and the width of spans are crucial signals for coreference resolution , consistent with previous findings from other coreference models .",Features,They contribute 3.8 F1 to the final result .,0.6532258064516129,0.4324324324324325,0.2105263157894736,ablation-analysis,approach
coreference_resolution,8,results,ablations,163,17,5,They contribute 3.8 F1 to the final result .,"The distance between spans and the width of spans are crucial signals for coreference resolution , consistent with previous findings from other coreference models .",Word representations,0.657258064516129,0.4594594594594595,0.2631578947368421,ablation-analysis,model
coreference_resolution,8,results,ablations,169,23,11,"Since coreference decisions often involve rare named entities , we see a contribution of 0.9 F1 from character - level modeling .",The character CNN provides morphological information and away to backoff for out - ofvocabulary words .,Metadata Speaker and genre indicators many not be available in downstream applications .,0.6814516129032258,0.6216216216216216,0.5789473684210527,ablation-analysis,approach
coreference_resolution,8,results,ablations,173,27,15,Ablations also show a 1.3 F1 degradation in performance without the attention mechanism for finding task - specific heads .,Head - finding attention,"As we will see in Section 9.4 , the attention mechanism should not be viewed as simply an approximation of syntactic heads .",0.6975806451612904,0.7297297297297297,0.7894736842105263,ablation-analysis,baselines
coreference_resolution,8,results,comparing span pruning strategies,181,35,4,"As shown in , keeping mention candidates detected by the rule - based system over predicted parse trees ( Raghunathan et al. , 2010 ) degrades performance by 1 F1 .","In these experiments , we use the alternate spans for both training and evaluation .","We also provide oracle experiment results , where we keep exactly the mentions thatare present in gold coreference clusters .",0.7298387096774194,0.945945945945946,0.6666666666666666,ablation-analysis,baselines
coreference_resolution,8,results,comparing span pruning strategies,183,37,6,"With oracle mentions , we see an improvement of 17.5 F1 , suggesting an enormous room for improvement if our model can produce better mention scores and anaphoricity decisions .","We also provide oracle experiment results , where we keep exactly the mentions thatare present in gold coreference clusters .", ,0.7379032258064516,1.0,1.0,ablation-analysis,baselines
coreference_resolution,9,title,title,2,2,2,BERT for Coreference Resolution : Baselines and Analysis, , ,0.0169491525423728,1.0,1.0,research-problem,approach
coreference_resolution,9,introduction,introduction,11,4,4,"We fine - tune BERT to coreference resolution , achieving strong improvements on the GAP and benchmarks .","Apart from better bidirectional reasoning , one of BERT 's major improvements over previous methods is passage - level training , 2 which allows it to better model longer sequences .",We present two ways of extending the c 2f - coref model in .,0.0932203389830508,0.2105263157894736,0.2105263157894736,approach,approach
coreference_resolution,9,introduction,introduction,12,5,5,We present two ways of extending the c 2f - coref model in .,"We fine - tune BERT to coreference resolution , achieving strong improvements on the GAP and benchmarks .",The independent variant uses non-overlapping segments each of which acts as an independent instance for BERT .,0.1016949152542373,0.2631578947368421,0.2631578947368421,approach,approach
coreference_resolution,9,introduction,introduction,16,9,9,1 https://github.com/mandarjoshi90/coref,BERT - large improves over ELMo - based c 2f - coref 3.9 % on OntoNotes and 11.5 % on GAP ( both absolute ) .,"2 Each BERT training example consists of around 512 word pieces , while ELMo is trained on single sentences .",0.135593220338983,0.4736842105263158,0.4736842105263158,code,baselines
coreference_resolution,9,experiments,experiments,63,5,5,We extend the original Tensorflow implementations of c 2f - coref 3 and BERT .,Implementation and Hyperparameters,"We fine tune all models on the OntoNotes English data for 20 epochs using a dropout of 0.3 , and learning rates of 1 10 ?5 and 2 10 ? 4 with linear decay for the BERT parameters and the task parameters respectively .",0.5338983050847458,0.1515151515151515,0.4545454545454545,experimental-setup,approach
coreference_resolution,9,experiments,experiments,64,6,6,"We fine tune all models on the OntoNotes English data for 20 epochs using a dropout of 0.3 , and learning rates of 1 10 ?5 and 2 10 ? 4 with linear decay for the BERT parameters and the task parameters respectively .",We extend the original Tensorflow implementations of c 2f - coref 3 and BERT .,We found that this made a sizable impact of 2 - 3 % over using the same learning rate for all parameters .,0.5423728813559322,0.1818181818181818,0.5454545454545454,experimental-setup,approach
coreference_resolution,9,experiments,experiments,66,8,8,"We trained separate models with max segment len of 128 , 256 , 384 , and 512 ; the models trained on 128 and 384 word pieces performed the best for BERT - base and BERT - large respectively .",We found that this made a sizable impact of 2 - 3 % over using the same learning rate for all parameters .,"As span representations are memory intensive , we truncate documents randomly to eleven segments for BERT - base and",0.559322033898305,0.2424242424242424,0.7272727272727273,experimental-setup,approach
coreference_resolution,9,experiments,paragraph level gap,70,12,1,Paragraph Level : GAP, , ,0.5932203389830508,0.3636363636363637,0.125,results,approach
coreference_resolution,9,experiments,paragraph level gap,76,18,7,Table 2 shows that BERT improves c 2 f - coref by 9 % and 11.5 % for the base and large models respectively .,The predicted clusters were scored against GAP examples according to the official evaluation script .,These results are inline with large gains reported for a variety of semantic tasks by BERTbased models .,0.6440677966101694,0.5454545454545454,0.875,results,baselines
coreference_resolution,9,experiments,document level ontonotes,78,20,1,Document Level : OntoNotes, , ,0.6610169491525424,0.6060606060606061,0.0714285714285714,results,approach
coreference_resolution,9,experiments,document level ontonotes,83,25,6,shows that BERT - base offers an improvement of 0.9 % over the ELMo - based c2 fcoref model .,4 on the test set according to the official CoNLL - 2012 evaluation scripts .,"Given how gains on coreference resolution have been hard to come by as evidenced by the table , this is still a considerable improvement .",0.7033898305084746,0.7575757575757576,0.4285714285714285,results,baselines
coreference_resolution,9,experiments,document level ontonotes,87,29,10,"BERT - large , however , improves c 2 f - coref by the much larger margin of 3.9 % .",This is in sharp contrast to how even the base variant of BERT has very substantially improved the state of the art in other tasks .,We also observe that the overlap variant offers no improvement over independent .,0.7372881355932204,0.8787878787878788,0.7142857142857143,results,baselines
coreference_resolution,9,experiments,document level ontonotes,88,30,11,We also observe that the overlap variant offers no improvement over independent .,"BERT - large , however , improves c 2 f - coref by the much larger margin of 3.9 % .","Concurrent with our work , , who use higher - order entity - level representations over "" frozen "" BERT features , also report large gains over c 2 f - coref .",0.7457627118644068,0.9090909090909092,0.7857142857142857,results,approach
data-to-text_generation,0,title,title,2,2,2,A Hierarchical Model for Data - to - Text Generation, , ,0.0072992700729927,1.0,1.0,research-problem,experimental-setup
data-to-text_generation,0,abstract,abstract,4,2,2,"Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as "" data - to - text "" .", ,"These structures generally regroup multiple elements , as well as their attributes .",0.0145985401459854,0.0285714285714285,0.0285714285714285,research-problem,experimental-setup
data-to-text_generation,0,abstract,abstract,36,34,34,"To address these shortcomings , we propose a new structured - data encoder assuming that structures should be hierarchically captured .","This way of encoding unordered sequences ( i.e. collections of entities ) implicitly assumes an arbitrary order within the collection which , as demonstrated by Vinyals et al. , significantly impacts the learning performance .","Our contribution focuses on the encoding of the data - structure , thus the decoder is chosen to be a classical module as used in .",0.1313868613138686,0.4857142857142857,0.4857142857142857,model,baselines
data-to-text_generation,0,abstract,abstract,37,35,35,"Our contribution focuses on the encoding of the data - structure , thus the decoder is chosen to be a classical module as used in .","To address these shortcomings , we propose a new structured - data encoder assuming that structures should be hierarchically captured .",Our contribution is threefold :,0.1350364963503649,0.5,0.5,model,approach
data-to-text_generation,0,abstract,abstract,39,37,37,"- We model the general structure of the data using a two - level architecture , first encoding all entities on the basis of their elements , then encoding the data structure on the basis of its entities ; - We introduce the Transformer encoder in data - to - text models to ensure robust encoding of each element / entities in comparison to all others , no matter their initial positioning ; - We integrate a hierarchical attention mechanism to compute the hierarchical context fed into the decoder .",Our contribution is threefold :,We report experiments on the RotoWire benchmark which contains around 5 K statistical tables of NBA basketball games paired with humanwritten descriptions .,0.1423357664233576,0.5285714285714286,0.5285714285714286,model,baselines
data-to-text_generation,0,experimental setup,baselines,188,38,1,Baselines, , ,0.6861313868613139,0.5846153846153846,0.0625,baselines,baselines
data-to-text_generation,0,experimental setup,baselines,191,41,4,Wiseman is a standard encoder - decoder system with copy mechanism .,"For each of them , we report the results of the best performing models presented in each paper .","Li is a standard encoder - decoder with a delayed copy mechanism : text is first generated with placeholders , which are replaced by salient records extracted from the table by a pointer network .",0.6970802919708029,0.6307692307692307,0.25,baselines,model
data-to-text_generation,0,experimental setup,baselines,192,42,5,"Li is a standard encoder - decoder with a delayed copy mechanism : text is first generated with placeholders , which are replaced by salient records extracted from the table by a pointer network .",Wiseman is a standard encoder - decoder system with copy mechanism .,"Puduppully - plan acts in two steps : a first standard encoder - decoder generates a plan , i.e. a list of salient records from the table ; a second standard encoder - decoder generates text from this plan .",0.7007299270072993,0.6461538461538462,0.3125,baselines,model
data-to-text_generation,0,experimental setup,baselines,193,43,6,"Puduppully - plan acts in two steps : a first standard encoder - decoder generates a plan , i.e. a list of salient records from the table ; a second standard encoder - decoder generates text from this plan .","Li is a standard encoder - decoder with a delayed copy mechanism : text is first generated with placeholders , which are replaced by salient records extracted from the table by a pointer network .",Puduppully - updt .,0.7043795620437956,0.6615384615384615,0.375,baselines,model
data-to-text_generation,0,experimental setup,baselines,194,44,7,Puduppully - updt .,"Puduppully - plan acts in two steps : a first standard encoder - decoder generates a plan , i.e. a list of salient records from the table ; a second standard encoder - decoder generates text from this plan .","It consists in a standard encoder - decoder , with an added module aimed at updating record representations during the generation process .",0.708029197080292,0.676923076923077,0.4375,baselines,baselines
data-to-text_generation,0,experimental setup,baselines,195,45,8,"It consists in a standard encoder - decoder , with an added module aimed at updating record representations during the generation process .",Puduppully - updt .,"At each decoding step , a gated recurrent network computes which records should be updated and what should be their new representation .",0.7116788321167883,0.6923076923076923,0.5,baselines,model
data-to-text_generation,0,experimental setup,baselines,196,46,9,"At each decoding step , a gated recurrent network computes which records should be updated and what should be their new representation .","It consists in a standard encoder - decoder , with an added module aimed at updating record representations during the generation process .",Model scenarios,0.7153284671532847,0.7076923076923077,0.5625,baselines,model
data-to-text_generation,0,experimental setup,implementation details,208,58,5,The size of the record value embeddings and hidden layers of the Transformer encoders are both set to 300 .,"To fit with the small number of record keys in our dataset , their embedding size is fixed to 20 .",We use dropout at rate 0.5 .,0.7591240875912408,0.8923076923076924,0.4166666666666667,experimental-setup,approach
data-to-text_generation,0,experimental setup,implementation details,209,59,6,We use dropout at rate 0.5 .,The size of the record value embeddings and hidden layers of the Transformer encoders are both set to 300 .,The models are trained with a batch size of 64 .,0.7627737226277372,0.9076923076923076,0.5,experimental-setup,approach
data-to-text_generation,0,experimental setup,implementation details,210,60,7,The models are trained with a batch size of 64 .,We use dropout at rate 0.5 .,"We follow the training procedure in and train the model for a fixed number of 25 K updates , and average the weights of the last 5 checkpoints ( at every 1 K updates ) to ensure more stability across runs .",0.7664233576642335,0.9230769230769232,0.5833333333333334,experimental-setup,approach
data-to-text_generation,0,experimental setup,implementation details,211,61,8,"We follow the training procedure in and train the model for a fixed number of 25 K updates , and average the weights of the last 5 checkpoints ( at every 1 K updates ) to ensure more stability across runs .",The models are trained with a batch size of 64 .,"All models were trained with the Adam optimizer ; the initial learning rate is 0.001 , and is reduced by half every 10 K steps .",0.7700729927007299,0.9384615384615383,0.6666666666666666,experimental-setup,approach
data-to-text_generation,0,experimental setup,implementation details,212,62,9,"All models were trained with the Adam optimizer ; the initial learning rate is 0.001 , and is reduced by half every 10 K steps .","We follow the training procedure in and train the model for a fixed number of 25 K updates , and average the weights of the last 5 checkpoints ( at every 1 K updates ) to ensure more stability across runs .",We used beam search with beam size of 5 during inference .,0.7737226277372263,0.953846153846154,0.75,experimental-setup,baselines
data-to-text_generation,0,experimental setup,implementation details,213,63,10,We used beam search with beam size of 5 during inference .,"All models were trained with the Adam optimizer ; the initial learning rate is 0.001 , and is reduced by half every 10 K steps .",All the models are implemented in Open NMT - py .,0.7773722627737226,0.9692307692307692,0.8333333333333334,experimental-setup,approach
data-to-text_generation,0,experimental setup,implementation details,214,64,11,All the models are implemented in Open NMT - py .,We used beam search with beam size of 5 during inference .,All code is available at https://github.com/KaijuML/data-to-text-hierarchical,0.7810218978102191,0.9846153846153848,0.9166666666666666,experimental-setup,baselines
data-to-text_generation,0,experimental setup,implementation details,215,65,12,All code is available at https://github.com/KaijuML/data-to-text-hierarchical,All the models are implemented in Open NMT - py ., ,0.7846715328467153,1.0,1.0,code,experimental-setup
data-to-text_generation,0,results,results,226,11,11,"As shown in , we can see the lower results obtained by the Flat scenario compared to the other scenarios ( e.g. BLEU 16.7 vs. 17.5 for resp .","To evaluate the impact of our model components , we first compare scenarios Flat , Hierarchical - k , and Hierarchical - kv .","Flat and Hierarchical -k ) , suggesting the effectiveness of encoding the data - structure using a hierarchy .",0.8248175182481752,0.2244897959183673,0.2244897959183673,ablation-analysis,model
data-to-text_generation,0,results,results,229,14,14,"Second , the comparison between scenario Hierarchical - kv and Hierarchical -k shows that omitting entirely the influence of the record values in the attention mechanism is more effective : this last variant performs slightly better in all metrics excepted CS - R% , reinforcing our intuition that focusing on the structure modeling is an important part of data encoding as well as confirming the intuition explained in Section 3.3 : once an entity is selected , facts about this entity are relevant based on their key , not value which might add noise .","This is expected , as losing explicit delimitation between entities makes it harder a ) for the encoder to encode semantics of the objects contained in the table and b ) for the attention mechanism to extract salient entities / records .","To illustrate this intuition , we depict in attention scores ( recall ?",0.8357664233576643,0.2857142857142857,0.2857142857142857,ablation-analysis,model
data-to-text_generation,0,results,results,244,29,29,"Our hierarchical models achieve significantly better scores on all metrics when compared to the flat architecture Wiseman , reinforcing the crucial role of structure in data semantics and saliency .","Specifically , regarding all baselines , we can outline the following statements .",The analysis of RG metrics shows that Wiseman seems to be the more naturalistic in terms of number of factual mentions ( RG# ) since it is the closest scenario to the gold value ( 16.83 vs. 17.31 for resp .,0.8905109489051095,0.5918367346938775,0.5918367346938775,ablation-analysis,baselines
data-to-text_generation,0,results,results,251,36,36,Results shows that our Flat scenario obtains a significant higher BLEU score ( 16.7 vs. 14.5 ) and generates fluent descriptions with accurate mentions ( RG - P % ) thatare also included in the gold descriptions ( CS - R% ) .,"The only difference stands on the encoder mechanism : bi - LSTM vs. Transformer , for Wiseman and Flat respectively .",This suggests that introducing the Transformer architecture is promising way to implicitly account for data structure .,0.916058394160584,0.7346938775510204,0.7346938775510204,ablation-analysis,baselines
data-to-text_generation,0,results,results,253,38,38,"Our hierarchical models outperform the two - step decoders of Li and Puduppully - plan on both BLEU and all qualitative metrics , showing that capturing structure in the encoding process is more effective that predicting a structure in the decoder ( i.e. , planning or templating ) .",This suggests that introducing the Transformer architecture is promising way to implicitly account for data structure .,"While our models sensibly outperform in precision at factual mentions , the baseline Puduppully - plan reaches 34.28 mentions on average , showing that incorporating modules dedicated to entity extraction leads to over- focusing on entities ; contrasting with our models that learn to generate more balanced descriptions .",0.9233576642335768,0.7755102040816326,0.7755102040816326,ablation-analysis,baselines
data-to-text_generation,1,title,title,2,2,2,A Deep Ensemble Model with Slot Alignment for Sequence - to - Sequence Natural Language Generation, , ,0.0088495575221238,1.0,1.0,research-problem,experimental-setup
data-to-text_generation,1,abstract,abstract,4,2,2,Natural language generation lies at the core of generative dialogue systems and conversational agents ., ,"We describe an ensemble neural language generator , and present several novel methods for data representation and augmentation that yield improved results in our model .",0.0176991150442477,0.4,0.4,research-problem,baselines
data-to-text_generation,1,introduction,introduction,28,21,21,"Here we present a neural ensemble natural language generator , which we train and test on three large unaligned datasets in the restaurant , television , and laptop domains .","These NLG models , however , typically require greater amount of data for training due to the lack of semantic alignment , and they still have problems producing syntactically and semantically correct output , as well as being limited in naturalness .","We explore novel ways to represent the MR inputs , including novel methods for delexicalizing slots and their values , automatic slot alignment , as well as the use of a semantic reranker .",0.1238938053097345,0.84,0.84,model,ablation-analysis
data-to-text_generation,1,introduction,introduction,29,22,22,"We explore novel ways to represent the MR inputs , including novel methods for delexicalizing slots and their values , automatic slot alignment , as well as the use of a semantic reranker .","Here we present a neural ensemble natural language generator , which we train and test on three large unaligned datasets in the restaurant , television , and laptop domains .",We use automatic evaluation metrics to show that these methods appreciably improve the performance of our model .,0.1283185840707964,0.88,0.88,model,experimental-setup
data-to-text_generation,1,evaluation,experimental setup,158,6,2,We built our ensemble model using the seq2seq framework for TensorFlow ., ,"Our individual LSTM models use a bidirectional LSTM encoder with 512 cells per layer , and the CNN models use a pooling encoder as in .",0.6991150442477876,0.09375,0.25,experimental-setup,baselines
data-to-text_generation,1,evaluation,experimental setup,159,7,3,"Our individual LSTM models use a bidirectional LSTM encoder with 512 cells per layer , and the CNN models use a pooling encoder as in .",We built our ensemble model using the seq2seq framework for TensorFlow .,The decoder in all models was a 4 - layer RNN decoder with 512 LSTM cells per layer and with attention .,0.7035398230088495,0.109375,0.375,experimental-setup,baselines
data-to-text_generation,1,evaluation,experimental setup,160,8,4,The decoder in all models was a 4 - layer RNN decoder with 512 LSTM cells per layer and with attention .,"Our individual LSTM models use a bidirectional LSTM encoder with 512 cells per layer , and the CNN models use a pooling encoder as in .",The hyperparameters were determined empirically .,0.7079646017699115,0.125,0.5,experimental-setup,approach
data-to-text_generation,1,evaluation,experiments on the e2e dataset,165,13,1,Experiments on the E2E Dataset, , ,0.7300884955752213,0.203125,0.0222222222222222,experiments,experimental-setup
data-to-text_generation,1,evaluation,experiments on the e2e dataset,170,18,6,Automatic Metric Evaluation,The final model selection was done based on a human evaluation of the models ' outputs on the test set .,"In the first experiment , we assess what effect the augmenting of the training set via utterance splitting has on the performance of different models .",0.7522123893805309,0.28125,0.1333333333333333,experiments,approach
data-to-text_generation,1,evaluation,experiments on the e2e dataset,172,20,8,The results in show that both the LSTM and the CNN models clearly benefit from additional pseudo - samples in the training set .,"In the first experiment , we assess what effect the augmenting of the training set via utterance splitting has on the performance of different models .",This can likely be attributed to the model having access to more granular information about which parts of the utterance correspond to which slots in the MR .,0.7610619469026548,0.3125,0.1777777777777777,experiments,experimental-setup
data-to-text_generation,1,evaluation,experiments on the e2e dataset,180,28,16,"On the official E2E test set , our ensemble model performs comparably to the baseline model , TGen , in terms of automatic metrics ) .","Analyzing the outputs , we also observed that the CNN model surpassed the two LSTM models in the ability to realize the "" fast food "" and "" pub "" values reliably , both of which were hardly present in the validation set but very frequent in the test set .",Human Evaluation,0.7964601769911505,0.4375,0.3555555555555556,experiments,baselines
data-to-text_generation,1,evaluation,experiments on tv and laptop datasets,210,58,1,Experiments on TV and Laptop Datasets, , ,0.9292035398230089,0.90625,0.1428571428571428,experiments,approach
data-to-text_generation,1,evaluation,experiments on tv and laptop datasets,212,60,3,"As shows , our ensemble model performs competitively with the baseline on the TV dataset , and it outperforms it on the Laptop dataset by a wide margin .","In order to provide a better frame of reference for the performance of our proposed model , we utilize the RNNLG benchmark toolkit 5 to evaluate our system on two additional , widely used datasets in NLG , and compare our results with those of a state - of - the - art model , SCLSTM .",We believe the higher error rate of our model can be explained by the significantly less aggressive slot delexicalization than the one used in SCLSTM .,0.9380530973451328,0.9375,0.4285714285714285,experiments,baselines
data-to-text_generation,2,title,title,2,2,2,Deep Graph Convolutional Encoders for Structured Data to Text Generation, , ,0.0104712041884816,1.0,1.0,research-problem,experimental-setup
data-to-text_generation,2,abstract,abstract,4,2,2,Most previous work on neural text generation from graph - structured data relies on standard sequence - to - sequence methods ., ,These approaches linearise the input graph to be fed to a recurrent neural network .,0.0209424083769633,0.3333333333333333,0.3333333333333333,research-problem,ablation-analysis
data-to-text_generation,2,introduction,introduction,22,14,14,Most previous work casts the graph structured data to text generation task as a sequenceto - sequence problem .,illustrates a source dependency graph and the corresponding target text .,They rely on recurrent data encoders with memory and gating mechanisms ( LSTM ; ) .,0.1151832460732984,0.5185185185185185,0.5185185185185185,research-problem,ablation-analysis
data-to-text_generation,2,results,results,131,1,1,Results, , ,0.6858638743455497,0.0277777777777777,1.0,results,baselines
data-to-text_generation,2,results,webnlg task,132,2,1,WebNLG task, , ,0.6910994764397905,0.0555555555555555,0.0454545454545454,results,baselines
data-to-text_generation,2,results,webnlg task,134,4,3,"In this setting , the model with GCN encoder outperforms a strong baseline that employs the LSTM encoder , with .009 BLEU points .",In we report results on the WebNLG test data .,The GCN model is also more stable than the baseline with a standard deviation of .004 vs . 010 .,0.7015706806282722,0.1111111111111111,0.1363636363636363,results,baselines
data-to-text_generation,2,results,webnlg task,135,5,4,The GCN model is also more stable than the baseline with a standard deviation of .004 vs . 010 .,"In this setting , the model with GCN encoder outperforms a strong baseline that employs the LSTM encoder , with .009 BLEU points .",We also compared the GCN EC model with the neural models submitted to the WebNLG shared task .,0.7068062827225131,0.1388888888888889,0.1818181818181818,results,baselines
data-to-text_generation,2,results,webnlg task,137,7,6,The GCN EC model outperforms PKUWRITER that uses an ensemble of 7 models and a further reinforcement learning step by .047 BLEU points ; and MELBOURNE by .014 BLEU points .,We also compared the GCN EC model with the neural models submitted to the WebNLG shared task .,GCN EC is behind ADAPT which relies on sub-word encoding .,0.7172774869109948,0.1944444444444444,0.2727272727272727,results,baselines
data-to-text_generation,2,results,webnlg task,139,9,8,SR11 Deep task,GCN EC is behind ADAPT which relies on sub-word encoding .,"In this more challenging task , the GCN encoder is able to better capture the William Anders was a crew member of OPERATOR ' s Apollo 8 alongside backup pilot Buzz Aldrin and backup pilot Buzz Aldrin .",0.7277486910994765,0.25,0.3636363636363637,results,baselines
data-to-text_generation,2,results,webnlg task,150,20,19,"We also compare the neural models with upper bound results on the same dataset by the pipeline model of The STUMBA - D and TBDIL model obtains respectively .794 and . 805 BLUE , outperforming the GCN - based model .",When we add linguistic features to the GCN encoding we get . 666 BLEU points .,"It is worth noting that these models rely on separate modules for syntax prediction , tree linearis ation and morphology generation .",0.7853403141361257,0.5555555555555556,0.8636363636363636,results,baselines
data-to-text_generation,2,results,ablation study,163,33,3,The first thing we notice is the importance of skip connections between GCN layers .,In ( BLEU ) we report an ablation study on the impact of the number of layers and the type of skip connections on the WebNLG dataset .,Residual and dense connections lead to similar results .,0.8534031413612565,0.9166666666666666,0.5,ablation-analysis,experimental-setup
data-to-text_generation,2,results,ablation study,164,34,4,Residual and dense connections lead to similar results .,The first thing we notice is the importance of skip connections between GCN layers .,"Dense connections ( Table 4 ( SIZE ) ) produce models bigger , but slightly less accurate , than residual connections .",0.8586387434554974,0.9444444444444444,0.6666666666666666,ablation-analysis,approach
data-to-text_generation,2,results,ablation study,165,35,5,"Dense connections ( Table 4 ( SIZE ) ) produce models bigger , but slightly less accurate , than residual connections .",Residual and dense connections lead to similar results .,The best GCN model has slightly more parameters than the baseline model ( 4.9 M vs. 4.3M ) .,0.8638743455497382,0.9722222222222222,0.8333333333333334,ablation-analysis,approach
data-to-text_generation,3,title,title,2,2,2,Pragmatically Informative Text Generation, , ,0.0158730158730158,1.0,1.0,research-problem,approach
data-to-text_generation,3,introduction,introduction,10,2,2,Computational approaches to pragmatics cast language generation and interpretation as gametheoretic or Bayesian inference procedures ., ,"While such approaches are capable of modeling a variety of pragmatic phenomena , their main application in natural language processing has been to improve the informativeness of generated text in grounded language learning problems .",0.0793650793650793,0.0769230769230769,0.0769230769230769,research-problem,ablation-analysis
data-to-text_generation,3,introduction,introduction,13,5,5,"Our work builds on a line of learned Rational Speech Acts ( RSA ) models , in which generated strings are selected to optimize the behav - Human - written A cheap coffee shop in riverside with a 5 out of 5 customer rating is Fitzbillies .","In this paper , we show that pragmatic reasoning can be similarly used to improve performance in more traditional language generation tasks like generation from structured meaning representations ) and summarization .",Fitzbillies is family friendly and serves English food .,0.1031746031746031,0.1923076923076923,0.1923076923076923,approach,model
data-to-text_generation,3,introduction,introduction,26,18,18,"The canonical presentation of the RSA framework ( Frank and Goodman , 2012 ) is grounded in reference resolution : models of speakers attempt to describe referents in the presence of distractors , and models of listeners attempt to resolve descriptors to referents .",ior of an embedded listener model .,"Recent work has extended these models to more complex groundings , including images and trajectories .",0.2063492063492063,0.6923076923076923,0.6923076923076923,approach,ablation-analysis
data-to-text_generation,3,experiments,abstractive summarization,89,5,1,Abstractive Summarization, , ,0.7063492063492064,0.3846153846153846,0.1111111111111111,results,approach
data-to-text_generation,3,experiments,abstractive summarization,94,10,6,"The pragmatic methods obtain improvements of 0.2-0.5 in ROUGE scores and 0.2-1.8 METEOR over the base S 0 model , with the distractor - based approach SD 1 outperforming the reconstructorbased approach S R 1 .","We also report two extractive baselines : Lead - 3 , which uses the first three sentences of the document as the summary , and Inputs , the concatenation of the extracted sentences used as inputs to our models ( i.e. , i ( 1 ) , . . . , i ( P ) ) .","SD 1 is strong across all metrics , obtaining results competitive to the best previous abstractive systems . ( b ) Coverage ratios by attribute type ( columns ) for the base model S0 , and for the pragmatic system SD 1 when constructing the distractor by masking the specified attribute ( rows ) .",0.7460317460317459,0.7692307692307693,0.6666666666666666,results,baselines
data-to-text_generation,3,experiments,abstractive summarization,95,11,7,"SD 1 is strong across all metrics , obtaining results competitive to the best previous abstractive systems . ( b ) Coverage ratios by attribute type ( columns ) for the base model S0 , and for the pragmatic system SD 1 when constructing the distractor by masking the specified attribute ( rows ) .","The pragmatic methods obtain improvements of 0.2-0.5 in ROUGE scores and 0.2-1.8 METEOR over the base S 0 model , with the distractor - based approach SD 1 outperforming the reconstructorbased approach S R 1 .",Cell colors are the degree the coverage ratio increases ( green ) or decreases ( red ) relative to S0 . :,0.7539682539682541,0.8461538461538461,0.7777777777777778,results,experiments
data-to-text_generation,4,title,title,2,2,2,Data - to - Text Generation with Content Selection and Planning, , ,0.0066445182724252,1.0,1.0,research-problem,experimental-setup
data-to-text_generation,4,introduction,introduction,20,12,12,Our model learns a content plan from the input and conditions on the content plan in order to generate the output document ( see for an illustration ) .,"In this paper , we address these shortcomings by explicitly modeling content selection and planning within a neural data - to - text architecture .","An explicit content planning mechanism has at least three advantages for multi-sentence document generation : it represents a high - level organization of the document structure allowing the decoder to concentrate on the easier tasks of sentence planning and surface realization ; it makes the process of data - to - document generation more interpretable by generating an intermediate representation ; and reduces redundancy in the output , since it is less likely for the content plan to contain the same information in multiple places .",0.0664451827242525,0.4615384615384616,0.4615384615384616,model,experimental-setup
data-to-text_generation,4,introduction,introduction,22,14,14,"We train our model end - to - end using neural networks and evaluate its performance on ROTOWIRE , a recently released dataset which contains statistics of NBA basketball games paired with human - written summaries ( see ) .","An explicit content planning mechanism has at least three advantages for multi-sentence document generation : it represents a high - level organization of the document structure allowing the decoder to concentrate on the easier tasks of sentence planning and surface realization ; it makes the process of data - to - document generation more interpretable by generating an intermediate representation ; and reduces redundancy in the output , since it is less likely for the content plan to contain the same information in multiple places .",Automatic and human evaluation shows that modeling content selection and planning improves generation considerably over competitive baselines .,0.0730897009966777,0.5384615384615384,0.5384615384615384,model,ablation-analysis
data-to-text_generation,4,problem formulation,problem formulation,158,104,104,"We used one - layer pointer networks during content planning , and two - layer LSTMs during text generation .",We did not tune the dimensions of word embeddings and LSTM hidden layers ; we used the same value of 600 reported in .,Input feeding was employed for the text decoder .,0.5249169435215947,0.5502645502645502,0.5502645502645502,experimental-setup,approach
data-to-text_generation,4,problem formulation,problem formulation,159,105,105,Input feeding was employed for the text decoder .,"We used one - layer pointer networks during content planning , and two - layer LSTMs during text generation .",We applied dropout ) at a rate of 0.3 .,0.5282392026578073,0.5555555555555556,0.5555555555555556,experimental-setup,approach
data-to-text_generation,4,problem formulation,problem formulation,160,106,106,We applied dropout ) at a rate of 0.3 .,Input feeding was employed for the text decoder .,"Models were trained for 25 epochs with the Adagrad optimizer ; the initial learning rate was 0.15 , learning rate decay was selected from { 0.5 , 0.97 } , and batch size was 5 .",0.53156146179402,0.5608465608465608,0.5608465608465608,experimental-setup,approach
data-to-text_generation,4,problem formulation,problem formulation,161,107,107,"Models were trained for 25 epochs with the Adagrad optimizer ; the initial learning rate was 0.15 , learning rate decay was selected from { 0.5 , 0.97 } , and batch size was 5 .",We applied dropout ) at a rate of 0.3 .,"For text decoding , we made use of BPTT ) and set the truncation size to 100 .",0.5348837209302325,0.5661375661375662,0.5661375661375662,experimental-setup,approach
data-to-text_generation,4,problem formulation,problem formulation,162,108,108,"For text decoding , we made use of BPTT ) and set the truncation size to 100 .","Models were trained for 25 epochs with the Adagrad optimizer ; the initial learning rate was 0.15 , learning rate decay was selected from { 0.5 , 0.97 } , and batch size was 5 .",We set the beam size to 5 during inference .,0.5382059800664452,0.5714285714285714,0.5714285714285714,experimental-setup,baselines
data-to-text_generation,4,problem formulation,problem formulation,163,109,109,We set the beam size to 5 during inference .,"For text decoding , we made use of BPTT ) and set the truncation size to 100 .",All models are implemented in Open NMT - py .,0.5415282392026578,0.5767195767195767,0.5767195767195767,experimental-setup,approach
data-to-text_generation,4,problem formulation,problem formulation,164,110,110,All models are implemented in Open NMT - py .,We set the beam size to 5 during inference .,Results,0.5448504983388704,0.582010582010582,0.582010582010582,experimental-setup,baselines
data-to-text_generation,4,problem formulation,problem formulation,165,111,111,Results,All models are implemented in Open NMT - py .,Automatic Evaluation,0.5481727574750831,0.5873015873015873,0.5873015873015873,results,baselines
data-to-text_generation,4,problem formulation,problem formulation,179,125,125,"As can be seen , NCP improves upon vanilla encoderdecoder models ( ED + JC , ED + CC ) , irrespective of the copy mechanism being employed .","In addition , we report the performance of a template - based generator which creates a document consisting of eight template sentences : an introductory sentence ( who won / lost ) , six player - specific sentences ( based on the six highest - scoring players in the game ) , and a conclusion sentence .","In fact , NCP achieves comparable scores with either joint or conditional copy mechanism which indicates that it is the content planner which brings performance improvements .",0.5946843853820598,0.6613756613756614,0.6613756613756614,results,baselines
data-to-text_generation,4,problem formulation,problem formulation,180,126,126,"In fact , NCP achieves comparable scores with either joint or conditional copy mechanism which indicates that it is the content planner which brings performance improvements .","As can be seen , NCP improves upon vanilla encoderdecoder models ( ED + JC , ED + CC ) , irrespective of the copy mechanism being employed .","Overall , NCP + CC achieves best content selection and content ordering scores in terms of BLEU .",0.5980066445182725,0.6666666666666666,0.6666666666666666,results,baselines
data-to-text_generation,4,problem formulation,problem formulation,181,127,127,"Overall , NCP + CC achieves best content selection and content ordering scores in terms of BLEU .","In fact , NCP achieves comparable scores with either joint or conditional copy mechanism which indicates that it is the content planner which brings performance improvements .","Compared to the best reported system in Wiseman et al. , we achieve an absolute improvement of approximately 12 % in terms of relation generation ; content selection precision also improves by 5 % and recall by 15 % , content ordering increases by 3 % , and BLEU by 1.5 points .",0.6013289036544851,0.671957671957672,0.671957671957672,results,baselines
data-to-text_generation,4,problem formulation,problem formulation,182,128,128,"Compared to the best reported system in Wiseman et al. , we achieve an absolute improvement of approximately 12 % in terms of relation generation ; content selection precision also improves by 5 % and recall by 15 % , content ordering increases by 3 % , and BLEU by 1.5 points .","Overall , NCP + CC achieves best content selection and content ordering scores in terms of BLEU .",The results of the oracle system ( NCP + OR ) show that content selection and ordering do indeed correlate with the quality of the content plan and that any improvements in our planning component would result in better output .,0.6046511627906976,0.6772486772486772,0.6772486772486772,results,baselines
data-to-text_generation,4,problem formulation,problem formulation,184,130,130,"As far as the template - based system is concerned , we observe that it obtains low BLEU and CS precision but scores high on CS recall and RG metrics .",The results of the oracle system ( NCP + OR ) show that content selection and ordering do indeed correlate with the quality of the content plan and that any improvements in our planning component would result in better output .,"This is not surprising as the template system is provided with domain knowledge which our model does not have , and thus represents an upper-bound on content selection and relation generation .",0.6112956810631229,0.6878306878306878,0.6878306878306878,results,approach
data-to-text_generation,5,title,title,2,2,2,Step - by - Step : Separating Planning from Realization in Neural Data - to - Text Generation, ,*,0.006269592476489,0.6666666666666666,0.6666666666666666,research-problem,experimental-setup
data-to-text_generation,5,abstract,abstract,5,2,2,"Data - to - text generation can be conceptually divided into two parts : ordering and structuring the information ( planning ) , and generating fluent language describing the information ( realization ) .", ,Modern neural generation systems conflate these two steps into a single end - to - end differentiable system .,0.0156739811912225,0.1666666666666666,0.1666666666666666,research-problem,model
data-to-text_generation,5,introduction,introduction,49,34,34,"Proposal we propose an explicit , symbolic , text planning stage , whose output is fed into a neural generation system .","We speculate that this is due to demanding too much of the network : while the neural system excels at capturing the language details required for fluent realization , they are less well equipped to deal with the higher levels text structuring in a consistent and verifiable manner .",The text planner determines the information structure and expresses it unambiguously - in our case as a sequence of ordered trees .,0.1536050156739812,0.8095238095238095,0.8095238095238095,model,model
data-to-text_generation,5,introduction,introduction,50,35,35,The text planner determines the information structure and expresses it unambiguously - in our case as a sequence of ordered trees .,"Proposal we propose an explicit , symbolic , text planning stage , whose output is fed into a neural generation system .",This stage is performed symbolically and is guaranteed to remain faithful and complete with regards to the input facts .,0.1567398119122257,0.8333333333333334,0.8333333333333334,model,model
data-to-text_generation,5,introduction,introduction,52,37,37,"Once the plan is determined , 2 a neural generation system is used to transform it into fluent , natural language text .",This stage is performed symbolically and is guaranteed to remain faithful and complete with regards to the input facts .,"By being able to follow the plan structure closely , the network is alleviated from the need to determine higher - level structural decisions and can track what was already covered more easily .",0.1630094043887147,0.8809523809523809,0.8809523809523809,model,model
data-to-text_generation,5,introduction,introduction,57,42,42,We release our code and the corpus extended with matching plans in https://github.com/AmitMY/ chimera .,The method also allows explicit control of the output structure and the generation of diverse outputs ( some diversity examples are available in the Appendix ) ., ,0.1786833855799373,1.0,1.0,code,baselines
data-to-text_generation,5,experimental setup,experimental setup,208,13,13,"We compare to the best submissions in the WebNLG challenge : Melbourne , an end - to - end system that scored best on all categories in the automatic evaluation , and UPF - FORGe , a classic grammar - based NLG system that scored best in the human evaluation .",Compared Systems,"Additionally , we developed an end - to - end neural baseline which outperforms the WebNLG neural systems .",0.6520376175548589,0.7647058823529411,0.7647058823529411,baselines,approach
data-to-text_generation,5,experimental setup,experimental setup,209,14,14,"Additionally , we developed an end - to - end neural baseline which outperforms the WebNLG neural systems .","We compare to the best submissions in the WebNLG challenge : Melbourne , an end - to - end system that scored best on all categories in the automatic evaluation , and UPF - FORGe , a classic grammar - based NLG system that scored best in the human evaluation .","It uses a set encoder , an LSTM decoder with attention , a copy - attention mechanism and a neural checklist model , as well as applying entity dropout .",0.6551724137931034,0.8235294117647058,0.8235294117647058,baselines,approach
data-to-text_generation,5,experimental setup,experimental setup,210,15,15,"It uses a set encoder , an LSTM decoder with attention , a copy - attention mechanism and a neural checklist model , as well as applying entity dropout .","Additionally , we developed an end - to - end neural baseline which outperforms the WebNLG neural systems .",The entity - dropout and checklist component are the key differentiators from previous systems .,0.6583072100313481,0.8823529411764706,0.8823529411764706,baselines,experimental-setup
data-to-text_generation,5,experiments and results,experiments and results,213,1,1,6 Experiments and Results, , ,0.6677115987460815,0.02,0.02,results,approach
data-to-text_generation,5,experiments and results,experiments and results,217,5,5,Both the StrongNeural and BestPlan systems outperform all the WebNLG participating systems on all automatic metrics,"In the original challenge , the best performing system in automatic metric was based on end - toend NMT ( Melbourne ) .",Manual Evaluation,0.6802507836990596,0.1,0.1,results,approach
data-to-text_generation,6,title,title,2,2,2,Copy Mechanism and Tailored Training for Character - based Data - to - text Generation, , ,0.0111731843575419,1.0,1.0,research-problem,experimental-setup
data-to-text_generation,6,abstract,abstract,4,2,2,"In the last few years , many different methods have been focusing on using deep recurrent neural networks for natural language generation .", ,"The most widely used sequence - to - sequence neural methods are word - based : as such , they need a pre-processing step called delexicalization ( conversely , relexicalization ) to deal with uncommon or unknown words .",0.0223463687150838,0.2222222222222222,0.2222222222222222,research-problem,experimental-setup
data-to-text_generation,6,abstract,abstract,8,6,6,"Moreover , since characters constitute the common "" building blocks "" of every text , it also allows a more general approach to text generation , enabling the possibility to exploit transfer learning for training .","In this work , we present an end - to - end sequence - to - sequence model with attention mechanism which reads and generates at a character level , no longer requiring delexicalization , tokenization , nor even lowercasing .","These skills are obtained thanks to two major features : ( i ) the possibility to alternate between the standard generation mechanism and a copy one , which allows to directly copy input facts to produce outputs , and ( ii ) the use of an original training pipeline that further improves the quality of the generated texts .",0.0446927374301676,0.6666666666666666,0.6666666666666666,research-problem,baselines
data-to-text_generation,6,introduction,introduction,23,12,12,"In order to give an original contribution to the field , in this paper we present a character - level sequence - to - sequence model with attention mechanism that results in a completely neural end - to - end architecture .","A significantly different approach consists in employing characters instead of words , for input slot - value pairs tokenization as well as for the generation of the final utterances , as done for instance in .","In contrast to traditional word - based ones , it does not require delexicalization , tokenization nor lowercasing ; besides , according to our experiments it never hallucinates words , nor duplicates them .",0.1284916201117318,0.631578947368421,0.631578947368421,model,baselines
data-to-text_generation,6,introduction,introduction,24,13,13,"In contrast to traditional word - based ones , it does not require delexicalization , tokenization nor lowercasing ; besides , according to our experiments it never hallucinates words , nor duplicates them .","In order to give an original contribution to the field , in this paper we present a character - level sequence - to - sequence model with attention mechanism that results in a completely neural end - to - end architecture .","As we will see , such an approach achieves rather interesting performance results and produces a vocabulary - free model that is inherently more general , as it does not depend on a specific domain 's set of terms , but rather on a general alphabet .",0.1340782122905028,0.6842105263157895,0.6842105263157895,model,baselines
data-to-text_generation,6,introduction,introduction,25,14,14,"As we will see , such an approach achieves rather interesting performance results and produces a vocabulary - free model that is inherently more general , as it does not depend on a specific domain 's set of terms , but rather on a general alphabet .","In contrast to traditional word - based ones , it does not require delexicalization , tokenization nor lowercasing ; besides , according to our experiments it never hallucinates words , nor duplicates them .","Because of this , it opens up the possibility , not viable when using words , to adapt already trained networks to deal with different datasets .",0.1396648044692737,0.7368421052631579,0.7368421052631579,model,approach
data-to-text_generation,6,introduction,introduction,27,16,16,"More specifically , our model shows two important features , with respect to the state - of - art architecture proposed by : ( i ) a character - wise copy mechanism , consisting in a soft switch between generation and copy mode , that disengages the model to learn rare and unhelpful self - correspondences , and ( ii ) a peculiar training procedure , which improves the internal representation capabilities , enhancing recall ; it consists in the exchange of encoder and decoder RNNs , ( GRUs As a further original contribution , we also introduce a new dataset , described in section 3.1 , whose particular structure allows to better highlight improvements in copying / recalling abilities with respect to character - based state - of - art approaches .","Because of this , it opens up the possibility , not viable when using words , to adapt already trained networks to deal with different datasets .","In section 2 , after resuming the main ideas on encoder - decoder methods with attention , we detail our model : section 2.2 is devoted to explaining the copy mechanism while in section 2.3 our peculiar training procedure is presented .",0.1508379888268156,0.8421052631578947,0.8421052631578947,model,model
data-to-text_generation,6,experiments,implementation details,105,22,2,"We developed our system using the PyTorch framework 2 , release 0.4.1 3 .", ,"The training has been carried out as described in subsection 2.3 : this training procedure needs the two GRUs to have the same dimensions , in terms of input size , hidden size , number of layers and presence of a bias term .",0.5865921787709497,0.2588235294117647,0.1428571428571428,experimental-setup,approach
data-to-text_generation,6,experiments,implementation details,108,25,5,"We minimize the negative log - likelihood loss using teacher forcing and Adam , the latter being an optimizer that computes individual adaptive learning rates .","Moreover , they both have to be bidirectional , even if the decoder ignores the backward part of its current GRU .","As a consequence of the length of the input sequences , a characterbased model is often subject to the exploding gradient problem , that we solved via the well - known technique of gradient norm clipping .",0.6033519553072626,0.2941176470588235,0.3571428571428571,experimental-setup,baselines
data-to-text_generation,6,experiments,results and discussion,118,35,1,Results and Discussion, , ,0.659217877094972,0.4117647058823529,0.0196078431372549,results,baselines
data-to-text_generation,6,experiments,results and discussion,137,54,20,"A first interesting result is that our model EDA_CS always obtains higher metric values with respect to TGen on the Hotel and Restaurant datasets , and three out of five higher metrics values on the E2E dataset .",We named this approach EDA_CS TL .,"However , in the case of E2E + , TGen achieves three out of five higher metrics values .",0.7653631284916201,0.6352941176470588,0.392156862745098,results,baselines
data-to-text_generation,6,experiments,results and discussion,138,55,21,"However , in the case of E2E + , TGen achieves three out of five higher metrics values .","A first interesting result is that our model EDA_CS always obtains higher metric values with respect to TGen on the Hotel and Restaurant datasets , and three out of five higher metrics values on the E2E dataset .","These results suggest that EDA_CS and TGen are comparable , at least from the point of view of automatic metrics ' evaluation .",0.770949720670391,0.6470588235294118,0.4117647058823529,results,approach
data-to-text_generation,6,experiments,results and discussion,140,57,23,"A more surprising result is that the approach EDA_CS TL allows to obtain better performance with respect to training EDA_CS in the standard way on the Hotel and Restaurant datasets ( for the majority of metrics ) ; on E2E , EDA_CS TL outperforms EDA_CS only in one case ( i.e. meteor metric ) .","These results suggest that EDA_CS and TGen are comparable , at least from the point of view of automatic metrics ' evaluation .","Moreover , EDA_CS TL shows a bleu increment of at least 14 % with respect to TGen 's score when compared to both Hotel and Restaurant datasets .",0.7821229050279329,0.6705882352941176,0.4509803921568628,results,baselines
data-to-text_generation,6,experiments,results and discussion,141,58,24,"Moreover , EDA_CS TL shows a bleu increment of at least 14 % with respect to TGen 's score when compared to both Hotel and Restaurant datasets .","A more surprising result is that the approach EDA_CS TL allows to obtain better performance with respect to training EDA_CS in the standard way on the Hotel and Restaurant datasets ( for the majority of metrics ) ; on E2E , EDA_CS TL outperforms EDA_CS only in one case ( i.e. meteor metric ) .","Finally , the baseline model , EDA , is largely outperformed by all other examined methods .",0.7877094972067039,0.6823529411764706,0.4705882352941176,results,baselines
data-to-text_generation,6,experiments,results and discussion,142,59,25,"Finally , the baseline model , EDA , is largely outperformed by all other examined methods .","Moreover , EDA_CS TL shows a bleu increment of at least 14 % with respect to TGen 's score when compared to both Hotel and Restaurant datasets .","Therefore , we can claim that our model exploits its transfer learning capabilities effectively , showing very good performances in a context like data - to - text generation in which the portability of features learned from different datasets , in the extent of our knowledge , has not yet been explored .",0.7932960893854749,0.6941176470588235,0.4901960784313725,results,approach
dependency_parsing,0,title,title,2,2,2,An improved neural network model for joint POS tagging and dependency parsing, , ,0.0117647058823529,1.0,1.0,research-problem,experimental-setup
dependency_parsing,0,abstract,abstract,4,2,2,We propose a novel neural network model for joint part - of - speech ( POS ) tagging and dependency parsing ., ,"Our model extends the well - known BIST graph - based dependency parser ( Kiperwasser and Goldberg , 2016 ) by incorporating a BiLSTM - based tagging component to produce automatically predicted POS tags for the parser .",0.0235294117647058,0.2857142857142857,0.2857142857142857,research-problem,experimental-setup
dependency_parsing,0,abstract,abstract,9,7,7,Our code is available together with all pretrained models at : https://github.com/datquocnguyen/jPTDP .,"In addition , with our model , we also obtain state - of - the - art downstream task scores for biomedical event extraction and opinion analysis applications .", ,0.0529411764705882,1.0,1.0,code,baselines
dependency_parsing,0,introduction,introduction,11,2,2,"Dependency parsing - a key research topic in natural language processing ( NLP ) in the last decade ) - has also been demonstrated to be extremely useful in many applications such as relation extraction , semantic parsing and machine translation ) .", ,"In general , dependency parsing models can be categorized as graph - based and transition - based .",0.0647058823529411,0.1428571428571428,0.1428571428571428,research-problem,experimental-setup
dependency_parsing,0,introduction,introduction,18,9,9,"In this paper , we present a novel neural network - based model for jointly learning POS tagging and dependency paring .","Alternatively , joint POS tagging and dependency parsing has also attracted a lot of attention in NLP community as it could help improve both tagging and parsing results over independent modeling .",Our joint model extends the well - known BIST graph - based dependency parser with an additional lower - level BiLSTM - based tagging component .,0.1058823529411764,0.6428571428571429,0.6428571428571429,model,experimental-setup
dependency_parsing,0,introduction,introduction,19,10,10,Our joint model extends the well - known BIST graph - based dependency parser with an additional lower - level BiLSTM - based tagging component .,"In this paper , we present a novel neural network - based model for jointly learning POS tagging and dependency paring .","In particular , this tagging component generates predicted POS tags for the parser component .",0.1117647058823529,0.7142857142857143,0.7142857142857143,model,experimental-setup
dependency_parsing,0,our joint model,implementation details,75,52,4,Our jPTDP v 2.0 is implemented using DYNET v2.0 with a fixed random seed .,jPTDP .,1,0.4411764705882353,0.7878787878787878,0.2222222222222222,experimental-setup,model
dependency_parsing,0,our joint model,implementation details,77,54,6,"Word embeddings are initialized either randomly or by pre-trained word vectors , while character and POS tag embeddings are randomly initialized .",1,"For learning character - level word embeddings , we use one - layer BiLSTM seq , and set the size of LSTM hidden states to be equal to the vector size of character embeddings .",0.4529411764705882,0.8181818181818182,0.3333333333333333,experimental-setup,experimental-setup
dependency_parsing,0,our joint model,implementation details,79,56,8,We apply dropout with a 67 % keep probability to the inputs of BiLSTMs and MLPs .,"For learning character - level word embeddings , we use one - layer BiLSTM seq , and set the size of LSTM hidden states to be equal to the vector size of character embeddings .","Following and , we also apply word dropout to learn an embedding for unknown words : we replace each word token w appearing # ( w ) times in the training set with a special "" unk "" symbol with probability punk ( w ) = 0.25 0.25 + # ( w ) .",0.4647058823529412,0.8484848484848485,0.4444444444444444,experimental-setup,baselines
dependency_parsing,0,our joint model,implementation details,80,57,9,"Following and , we also apply word dropout to learn an embedding for unknown words : we replace each word token w appearing # ( w ) times in the training set with a special "" unk "" symbol with probability punk ( w ) = 0.25 0.25 + # ( w ) .",We apply dropout with a 67 % keep probability to the inputs of BiLSTMs and MLPs .,This procedure only involves the word embedding part in the input word vector representation .,0.4705882352941176,0.8636363636363636,0.5,experimental-setup,experimental-setup
dependency_parsing,0,our joint model,implementation details,82,59,11,"We optimize the objective loss using Adam ( Kingma and Ba , 2014 ) with an initial learning rate at 0.001 and no mini-batches .",This procedure only involves the word embedding part in the input word vector representation .,"For training , we run for 30 epochs , and restart the Adam optimizer and anneal its initial learning rate at a proportion of 0.5 every 10 epochs .",0.4823529411764706,0.8939393939393939,0.6111111111111112,experimental-setup,baselines
dependency_parsing,0,our joint model,implementation details,86,63,15,"For all experiments presented in this paper , we use 100 - dimensional word embeddings , 50 - dimensional character embeddings and 100 dimensional POS tag embeddings .","We choose the model with the highest mixed accuracy on the development set , which is then applied to the test set for the evaluation phase .",We also fix the number of hidden nodes in MLPs at 100 .,0.5058823529411764,0.9545454545454546,0.8333333333333334,experimental-setup,approach
dependency_parsing,0,our joint model,implementation details,87,64,16,We also fix the number of hidden nodes in MLPs at 100 .,"For all experiments presented in this paper , we use 100 - dimensional word embeddings , 50 - dimensional character embeddings and 100 dimensional POS tag embeddings .","Due to limited computational resource , for experiments presented in Section 3 , we perform a minimal grid search of hyper - parameters to select the number of BiLSTM pos and BiLSTM dep layers from { 1 , 2 } and the size of LSTM hidden states in each layer from { 128 , 256 } .",0.5117647058823529,0.9696969696969696,0.8888888888888888,experimental-setup,baselines
dependency_parsing,0,experiments on english penn treebank,experiments on english penn treebank,99,10,10,"Clearly , our model produces very competitive parsing results .",The last 6 rows present scores for joint models .,"In particular , our model obtains a UAS score at 94.51 % and a LAS score at 92.87 % which are about 1.4 % and 1.9 % absolute higher than UAS and LAS scores of the BIST graph - based model , respectively .",0.5823529411764706,0.136986301369863,0.136986301369863,results,approach
dependency_parsing,0,experiments on english penn treebank,experiments on english penn treebank,100,11,11,"In particular , our model obtains a UAS score at 94.51 % and a LAS score at 92.87 % which are about 1.4 % and 1.9 % absolute higher than UAS and LAS scores of the BIST graph - based model , respectively .","Clearly , our model produces very competitive parsing results .","Our model also does better than the previous transition - based joint models in , and , while obtaining similar UAS and LAS scores to the joint model JMT proposed by .",0.5882352941176471,0.1506849315068493,0.1506849315068493,results,baselines
dependency_parsing,0,experiments on english penn treebank,experiments on english penn treebank,102,13,13,We achieve 0.9 % lower parsing scores than the state - of - the - art dependency parser of .,"Our model also does better than the previous transition - based joint models in , and , while obtaining similar UAS and LAS scores to the joint model JMT proposed by .","While also a BiLSTM - and graph - based model , it uses a more sophisticated attention mechanism "" biaffine "" for better decoding dependency arcs and relation types .",0.6,0.1780821917808219,0.1780821917808219,results,baselines
dependency_parsing,0,experiments on english penn treebank,experiments on english penn treebank,106,17,17,"We also obtain a state - of - the - art POS tagging accuracy at 97.97 % on the test Section 23 , which is about 0.4 + % higher than those by , and .","Other differences are that they use a higher dimensional representation than ours , but rely on predicted POS tags .",Other previous joint models did not mention their specific POS tagging accuracies .,0.6235294117647059,0.2328767123287671,0.2328767123287671,results,baselines
dependency_parsing,1,title,title,2,2,2,Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations, , ,0.0063897763578274,1.0,1.0,research-problem,experimental-setup
dependency_parsing,1,introduction,introduction,24,16,16,"Our proposal ( Section 3 ) is centered around BiRNNs , and more specifically BiLSTMs , which are strong and trainable sequence models ( see Section 2.3 ) .","In this work , we suggest an approach which is much simpler in terms of both feature engineering and architecture engineering .","The BiLSTM excels at representing elements in a sequence ( i.e. , words ) together with their contexts , capturing the element and an "" infinite "" window around it .",0.0766773162939297,0.5925925925925926,0.5925925925925926,approach,experimental-setup
dependency_parsing,1,introduction,introduction,26,18,18,"We represent each word by its BiLSTM encoding , and use a concatenation of a minimal set of such BiLSTM encodings as our feature function , which is then passed to a non-linear scoring function ( multi - layer perceptron ) .","The BiLSTM excels at representing elements in a sequence ( i.e. , words ) together with their contexts , capturing the element and an "" infinite "" window around it .","Crucially , the BiLSTM is trained with the rest of the parser in order to learn a good feature representation for the parsing problem .",0.0830670926517571,0.6666666666666666,0.6666666666666666,approach,ablation-analysis
dependency_parsing,1,introduction,introduction,27,19,19,"Crucially , the BiLSTM is trained with the rest of the parser in order to learn a good feature representation for the parsing problem .","We represent each word by its BiLSTM encoding , and use a concatenation of a minimal set of such BiLSTM encodings as our feature function , which is then passed to a non-linear scoring function ( multi - layer perceptron ) .","If we set aside the inherent complexity of the BiLSTM itself and treat it as a black box , our proposal results in a pleasingly simple feature extractor .",0.0862619808306709,0.7037037037037037,0.7037037037037037,approach,ablation-analysis
dependency_parsing,1,introduction,introduction,29,21,21,"We demonstrate the effectiveness of the approach by using the BiLSTM feature extractor in two parsing architectures , transition - based ( Section 4 ) as well as a graph - based ( Section 5 ) .","If we set aside the inherent complexity of the BiLSTM itself and treat it as a black box , our proposal results in a pleasingly simple feature extractor .","In the graphbased parser , we jointly train a structured - prediction model on top of a BiLSTM , propagating errors from the structured objective all the way back to the BiLSTM feature - encoder .",0.0926517571884984,0.7777777777777778,0.7777777777777778,approach,experimental-setup
dependency_parsing,1,introduction,introduction,30,22,22,"In the graphbased parser , we jointly train a structured - prediction model on top of a BiLSTM , propagating errors from the structured objective all the way back to the BiLSTM feature - encoder .","We demonstrate the effectiveness of the approach by using the BiLSTM feature extractor in two parsing architectures , transition - based ( Section 4 ) as well as a graph - based ( Section 5 ) .","To the best of our knowledge , we are the first to perform such end - to - end training of a structured prediction model and a recurrent feature extractor for non-sequential outputs .",0.0958466453674121,0.8148148148148148,0.8148148148148148,approach,ablation-analysis
dependency_parsing,1,experiments and results,experiments and results,281,11,11,"The parsers are implemented in python , using the PyCNN toolkit 11 for neural network training .",10 Implementation Details,The code is available at the github repository https://github.com/elikip / bist -parser .,0.8977635782747604,0.2682926829268293,0.2682926829268293,experimental-setup,approach
dependency_parsing,1,experiments and results,experiments and results,282,12,12,The code is available at the github repository https://github.com/elikip / bist -parser .,"The parsers are implemented in python , using the PyCNN toolkit 11 for neural network training .","We use the LSTM variant implemented in PyCNN , and optimize using the Adam optimizer .",0.9009584664536742,0.2926829268292683,0.2926829268292683,code,experimental-setup
dependency_parsing,1,experiments and results,experiments and results,283,13,13,"We use the LSTM variant implemented in PyCNN , and optimize using the Adam optimizer .",The code is available at the github repository https://github.com/elikip / bist -parser .,"Unless otherwise noted , we use the default values provided by PyCNN ( e.g. for random initialization , learning rates etc ) .",0.9041533546325878,0.3170731707317073,0.3170731707317073,experimental-setup,baselines
dependency_parsing,1,experiments and results,experiments and results,299,29,29,"It is clear that our parsers are very competitive , despite using very simple parsing architectures and minimal feature extractors .","The hyper- parameters of the final networks used for all the reported experiments are detailed in Main Results lists the test - set accuracies of our best parsing models , compared to other state - of the - art parsers from the literature .","When not using external embeddings , the first - order graph - based parser with 2 features outperforms all other systems thatare not using external resources , including the third - order TurboParser .",0.9552715654952076,0.7073170731707317,0.7073170731707317,results,approach
dependency_parsing,1,experiments and results,experiments and results,300,30,30,"When not using external embeddings , the first - order graph - based parser with 2 features outperforms all other systems thatare not using external resources , including the third - order TurboParser .","It is clear that our parsers are very competitive , despite using very simple parsing architectures and minimal feature extractors .","The greedy transition based parser with 4 features also matches or outperforms most other parsers , including the beam - based transition parser with heavily engineered features of and the Stack - LSTM parser of , as well as the same parser when trained using a dynamic oracle .",0.9584664536741214,0.7317073170731707,0.7317073170731707,results,approach
dependency_parsing,1,experiments and results,experiments and results,301,31,31,"The greedy transition based parser with 4 features also matches or outperforms most other parsers , including the beam - based transition parser with heavily engineered features of and the Stack - LSTM parser of , as well as the same parser when trained using a dynamic oracle .","When not using external embeddings , the first - order graph - based parser with 2 features outperforms all other systems thatare not using external resources , including the third - order TurboParser .",Moving from the simple ( 4 features ) to the extended ( 11 features ) feature set leads to some gains in accuracy for both English and Chinese .,0.9616613418530352,0.7560975609756098,0.7560975609756098,results,baselines
dependency_parsing,1,experiments and results,experiments and results,302,32,32,Moving from the simple ( 4 features ) to the extended ( 11 features ) feature set leads to some gains in accuracy for both English and Chinese .,"The greedy transition based parser with 4 features also matches or outperforms most other parsers , including the beam - based transition parser with heavily engineered features of and the Stack - LSTM parser of , as well as the same parser when trained using a dynamic oracle .","Interestingly , when adding external word embeddings the accuracy of the graph - based parser degrades .",0.9648562300319488,0.7804878048780488,0.7804878048780488,results,approach
dependency_parsing,1,experiments and results,experiments and results,303,33,33,"Interestingly , when adding external word embeddings the accuracy of the graph - based parser degrades .",Moving from the simple ( 4 features ) to the extended ( 11 features ) feature set leads to some gains in accuracy for both English and Chinese .,"We are not sure why this happens , and leave the exploration of effective semi-supervised parsing with the graph - based model for future work .",0.9680511182108626,0.8048780487804879,0.8048780487804879,results,approach
dependency_parsing,2,introduction,introduction,14,5,5,"We give a probabilistic interpretation to the ensemble parser ( with a minor modification ) , viewing it as an instance of minimum Bayes risk inference .","In 3 , we apply this idea to build a firstorder graph - based ( FOG ) ensemble parser ) that seeks consensus among 20 randomly - initialized stack LSTM parsers , achieving nearly the best - reported performance on the standard Penn Treebank Stanford dependencies task ( 94.51 UAS , 92.70 LAS ) .",We propose that dis agreements among the ensemble 's members maybe taken as a signal that an attachment decision is difficult or ambiguous .,0.0648148148148148,0.3333333333333333,0.3333333333333333,approach,ablation-analysis
dependency_parsing,2,introduction,introduction,18,9,9,"We address this issue in 5 by distilling the ensemble into a single FOG parser with discriminative training by defining a new cost function , inspired by the notion of "" soft targets "" .","N times as much computation , plus the runtime of finding consensus .","The essential idea is to derive the cost of each possible attachment from the ensemble 's division of votes , and use this cost in discriminative learning .",0.0833333333333333,0.6,0.6,approach,ablation-analysis
dependency_parsing,2,introduction,introduction,19,10,10,"The essential idea is to derive the cost of each possible attachment from the ensemble 's division of votes , and use this cost in discriminative learning .","We address this issue in 5 by distilling the ensemble into a single FOG parser with discriminative training by defining a new cost function , inspired by the notion of "" soft targets "" .","The application of distilliation to structured prediction is , to our knowledge , new , as is the idea of empirically estimating cost functions .",0.0879629629629629,0.6666666666666666,0.6666666666666666,approach,ablation-analysis
dependency_parsing,2,introduction,introduction,22,13,13,"It represents a new state of the art for graphbased dependency parsing for English , Chinese , and German .","The distilled model performs almost as well as the ensemble consensus and much better than ( i ) a strong LSTM FOG parser trained using the conventional Hamming cost function , ( ii ) recently published strong , and ( iii ) many higher - order graph - based parsers .",The code to reproduce our results is publicly available .,0.1018518518518518,0.8666666666666667,0.8666666666666667,research-problem,experimental-setup
dependency_parsing,2,experiments,experiments,176,25,25,"First , consider the neural FOG parser trained with Hamming cost ( C H in the second - to - last row ) .",All scores are shown in .,"This is a very strong benchmark , outperforming many higherorder graph - based and neural network models on all three datasets .",0.8148148148148148,0.5813953488372093,0.5813953488372093,results,baselines
dependency_parsing,2,experiments,experiments,177,26,26,"This is a very strong benchmark , outperforming many higherorder graph - based and neural network models on all three datasets .","First , consider the neural FOG parser trained with Hamming cost ( C H in the second - to - last row ) .","Nonetheless , training the same model with distillation cost gives consistent improvements for all languages .",0.8194444444444444,0.6046511627906976,0.6046511627906976,results,approach
dependency_parsing,2,experiments,experiments,178,27,27,"Nonetheless , training the same model with distillation cost gives consistent improvements for all languages .","This is a very strong benchmark , outperforming many higherorder graph - based and neural network models on all three datasets .","For English , we see that this model comes close to the slower ensemble it was trained to simulate .",0.8240740740740741,0.627906976744186,0.627906976744186,results,approach
dependency_parsing,2,experiments,experiments,179,28,28,"For English , we see that this model comes close to the slower ensemble it was trained to simulate .","Nonetheless , training the same model with distillation cost gives consistent improvements for all languages .","For Chinese , it achieves the best published scores , for German the best published UAS scores , and just after Bohnet and Nivre ( 2012 ) for LAS .",0.8287037037037037,0.6511627906976745,0.6511627906976745,results,approach
dependency_parsing,2,experiments,experiments,180,29,29,"For Chinese , it achieves the best published scores , for German the best published UAS scores , and just after Bohnet and Nivre ( 2012 ) for LAS .","For English , we see that this model comes close to the slower ensemble it was trained to simulate .",Effects of Pre-trained Word Embedding .,0.8333333333333334,0.6744186046511628,0.6744186046511628,results,baselines
dependency_parsing,3,title,title,2,2,2,From POS tagging to dependency parsing for biomedical event extraction, , ,0.0076923076923076,1.0,1.0,research-problem,approach
dependency_parsing,3,background,background,12,9,9,We make the retrained models available at https://github.com/datquocnguyen/BioPosDep.,Availability of data and material :, ,0.0461538461538461,1.0,1.0,code,baselines
dependency_parsing,3,background,background,31,19,19,"In this paper , we therefore investigate current stateof - the - art ( SOTA ) approaches to dependency parsing as applied to biomedical texts .","Event extraction few years , thanks to renewed attention to the problem and exploration of neural methods , it is important to revisit whether the commonly used tools remain the best choices for syntactic analysis of biomedical texts .","We also present detailed results on the precursor task of POS tagging , since parsing depends heavily on POS tags .",0.1192307692307692,0.8636363636363636,0.8636363636363636,approach,experimental-setup
dependency_parsing,3,background,background,33,21,21,"Finally , we study the impact of parser choice on biomedical event extraction , following the structure of the extrinsic parser evaluation shared task ( EPE 2017 ) for biomedical event extraction .","We also present detailed results on the precursor task of POS tagging , since parsing depends heavily on POS tags .",We find that differences in over all intrinsic parser performance do not consistently explain differences in information extraction performance .,0.1269230769230769,0.9545454545454546,0.9545454545454546,approach,approach
dependency_parsing,3,experimental methodology,implementation details,97,63,7,"For the three BiLSTM - CRF - based models , Stanford - NNdep , jPTDP and Stanford - Biaffine which utilizes pre-trained word embeddings , we employ 200 dimensional pre-trained word vectors from .",LAS is the proportion of words which are correctly assigned both dependency arc and label while UAS is the proportion of words for which the dependency arc is assigned correctly .,These pre-trained vectors were obtained by training the Word2Vec skip - gram model on a PubMed abstract corpus of 3 billion word tokens .,0.3730769230769231,0.5294117647058824,0.2592592592592592,hyperparameters,baselines
dependency_parsing,3,experimental methodology,implementation details,99,65,9,"For the traditional feature - based models MarMoT , NLP4J - POS and NLP4J - dep , we use their original pure Java implementations with default hyperparameter settings .",These pre-trained vectors were obtained by training the Word2Vec skip - gram model on a PubMed abstract corpus of 3 billion word tokens .,"For the BiLSTM - CRF - based models , we use default hyper - parameters provided in with the following exceptions : for training , we use Nadam and run for 50 epochs .",0.3807692307692308,0.5462184873949579,0.3333333333333333,hyperparameters,approach
dependency_parsing,3,experimental methodology,implementation details,100,66,10,"For the BiLSTM - CRF - based models , we use default hyper - parameters provided in with the following exceptions : for training , we use Nadam and run for 50 epochs .","For the traditional feature - based models MarMoT , NLP4J - POS and NLP4J - dep , we use their original pure Java implementations with default hyperparameter settings .","We perform a grid search of hyperparameters to select the number of BiLSTM layers from { 1 , 2 } and the number of LSTM units in each layer from { 100 , 150 , 200 , 250 , 300 } .",0.3846153846153846,0.5546218487394958,0.3703703703703704,hyperparameters,baselines
dependency_parsing,3,experimental methodology,implementation details,103,69,13,"For Stanford - NNdep , we select the word CutOff from { 1 , 2 } and the size of the hidden layer from { 100 , 150 , 200 , 250 , 300 , 350 , 400 } and fix other hyperparameters with their default values .",Early stopping is applied when no performance improvement on the development set is obtained after 10 contiguous epochs .,"For jPTDP , we use 50 - dimensional character embeddings and fix the initial learning rate at 0.0005 .",0.3961538461538461,0.5798319327731093,0.4814814814814815,hyperparameters,experimental-setup
dependency_parsing,3,experimental methodology,implementation details,104,70,14,"For jPTDP , we use 50 - dimensional character embeddings and fix the initial learning rate at 0.0005 .","For Stanford - NNdep , we select the word CutOff from { 1 , 2 } and the size of the hidden layer from { 100 , 150 , 200 , 250 , 300 , 350 , 400 } and fix other hyperparameters with their default values .","We also fix the number of BiLSTM layers at 2 and select the number of LSTM units in each layer from { 100 , 150 , 200 , 250 , 300 } .",0.4,0.5882352941176471,0.5185185185185185,hyperparameters,approach
dependency_parsing,3,experimental methodology,implementation details,105,71,15,"We also fix the number of BiLSTM layers at 2 and select the number of LSTM units in each layer from { 100 , 150 , 200 , 250 , 300 } .","For jPTDP , we use 50 - dimensional character embeddings and fix the initial learning rate at 0.0005 .",Other hyper - parameters are set at their default values .,0.4038461538461538,0.5966386554621849,0.5555555555555556,hyperparameters,experimental-setup
dependency_parsing,3,experimental methodology,pos tagging results,118,84,1,POS tagging results, , ,0.4538461538461538,0.7058823529411765,0.0454545454545454,results,approach
dependency_parsing,3,experimental methodology,pos tagging results,120,86,3,"BiLSTM - CRF and Mar - MoT obtain the lowest scores on GENIA and CRAFT , respectively .","In general , we find that the six retrained models produce competitive results .",jPTDP obtains a similar score to Mar - MoT on GENIA and similar score to BiLSTM - CRF on CRAFT .,0.4615384615384616,0.7226890756302521,0.1363636363636363,results,baselines
dependency_parsing,3,experimental methodology,pos tagging results,121,87,4,jPTDP obtains a similar score to Mar - MoT on GENIA and similar score to BiLSTM - CRF on CRAFT .,"BiLSTM - CRF and Mar - MoT obtain the lowest scores on GENIA and CRAFT , respectively .","In particular , MarMoT obtains accuracy results at 98.61 % and 97.07 % on GENIA and CRAFT , which are about 0.2 % and 0.4 % absolute lower than NLP4J - POS , respectively .",0.4653846153846154,0.7310924369747899,0.1818181818181818,results,baselines
dependency_parsing,3,experimental methodology,pos tagging results,122,88,5,"In particular , MarMoT obtains accuracy results at 98.61 % and 97.07 % on GENIA and CRAFT , which are about 0.2 % and 0.4 % absolute lower than NLP4J - POS , respectively .",jPTDP obtains a similar score to Mar - MoT on GENIA and similar score to BiLSTM - CRF on CRAFT .,"NLP4J - POS uses additional features based on Brown clusters and pre-trained word vectors learned from a large external corpus , providing useful extra information .",0.4692307692307692,0.7394957983193278,0.2272727272727273,results,baselines
dependency_parsing,3,experimental methodology,pos tagging results,124,90,7,BiLSTM - CRF obtains accuracies of 98.44 % on GE - NIA and 97.25 % on CRAFT .,"NLP4J - POS uses additional features based on Brown clusters and pre-trained word vectors learned from a large external corpus , providing useful extra information .","Using character - level word embeddings helps to produce about 0.5 % and Trained on the PTB sections 0 - 18 , the accuracies for the GENIA tagger , Stanford tagger , MarMoT , NLP4J - POS , BiLSTM- CRF and BiLSTM - CRF + CNN - char on the benchmark test set of PTB sections 22 - 24 were reported at 97.05 % , 97.23 % , 97.28 % , 97.64 % , 97.45 % and 97.55 % , respectively .",0.476923076923077,0.7563025210084033,0.3181818181818182,results,baselines
dependency_parsing,3,experimental methodology,pos tagging results,130,96,13,"Note that for PTB , CNN - based character - level word embeddings only provided a 0.1 % improvement to BiLSTM - CRF .","0.3 % absolute improvements to BiLSTM - CRF on GE - NIA and CRAFT , respectively , resulting in the highest accuracies on both experimental corpora .",The larger improvements on GENIA and CRAFT show that character - level word embeddings are specifically useful to capture rare or unseen words in biomedical text data .,0.5,0.8067226890756303,0.5909090909090909,results,baselines
dependency_parsing,3,experimental methodology,pos tagging results,135,101,18,"On both GENIA and CRAFT , BiLSTM - CRF with character - level word embeddings obtains the highest accuracy scores .",It is typically difficult for character - level word embeddings to capture those unseen abbreviated words .,These are just 0.1 % absolute higher than the accuracies of NLP4J - POS .,0.5192307692307693,0.8487394957983193,0.8181818181818182,results,baselines
dependency_parsing,3,experimental methodology,overall dependency parsing results,140,106,1,Overall dependency parsing results, , ,0.5384615384615384,0.8907563025210085,0.0714285714285714,results,approach
dependency_parsing,3,experimental methodology,overall dependency parsing results,148,114,9,"On GENIA , among pre-trained models , BLLIP obtains highest results .",The remaining rows show results of our retrained dependency parsing models .,"This model , unlike the other pretrained models , was trained using GENIA , so this result is unsurprising .",0.5692307692307692,0.9579831932773109,0.6428571428571429,results,baselines
dependency_parsing,3,experimental methodology,overall dependency parsing results,150,116,11,The pre-trained Stanford - Biaffine ( v1 ) model produces lower scores than the pre-trained Stanford - NNdep model on GENIA .,"This model , unlike the other pretrained models , was trained using GENIA , so this result is unsurprising .",It is also unsurprising because the pre-trained Stanford - Biaffine utilizes pre-trained word vectors which were learned from newswire corpora .,0.5769230769230769,0.9747899159663864,0.7857142857142857,results,baselines
dependency_parsing,3,experimental methodology,overall dependency parsing results,152,118,13,Note that the pre-trained NNdep and Biaffine models result in no significant performance differences irrespective of the source of POS tags ( i.e. the pre-trained Stanford tagger at 98.37 % vs. the retrained NLP4J - POS model at 98.80 % ) .,It is also unsurprising because the pre-trained Stanford - Biaffine utilizes pre-trained word vectors which were learned from newswire corpora .,"Regarding the retrained parsing models , on both GENIA and CRAFT , Stanford - Biaffine achieves the",0.5846153846153846,0.9915966386554622,0.9285714285714286,results,baselines
dependency_parsing,4,title,title,2,2,2,Stack - Pointer Networks for Dependency Parsing, , ,0.0085106382978723,1.0,1.0,research-problem,approach
dependency_parsing,4,introduction,introduction,25,16,16,"In this paper , we propose a novel neural network architecture for dependency parsing , stackpointer networks ( STACKPTR ) .","Nevertheless , these models , while accurate , are usually slow ( e.g. decoding is O ( n 3 ) time complexity for first - order models and higher polynomials for higherorder models ) .","STACKPTR is a transition - based architecture , with the corresponding asymptotic efficiency , but still maintains a global view of the sentence that proves essential for achieving competitive accuracy .",0.1063829787234042,0.6666666666666666,0.6666666666666666,model,experimental-setup
dependency_parsing,4,introduction,introduction,26,17,17,"STACKPTR is a transition - based architecture , with the corresponding asymptotic efficiency , but still maintains a global view of the sentence that proves essential for achieving competitive accuracy .","In this paper , we propose a novel neural network architecture for dependency parsing , stackpointer networks ( STACKPTR ) .","Our STACKPTR parser has a pointer network as its backbone , and is equipped with an internal stack to maintain the order of head words in tree structures .",0.1106382978723404,0.7083333333333334,0.7083333333333334,model,model
dependency_parsing,4,introduction,introduction,27,18,18,"Our STACKPTR parser has a pointer network as its backbone , and is equipped with an internal stack to maintain the order of head words in tree structures .","STACKPTR is a transition - based architecture , with the corresponding asymptotic efficiency , but still maintains a global view of the sentence that proves essential for achieving competitive accuracy .","The STACKPTR parser performs parsing in an incremental , topdown , depth - first fashion ; at each step , it generates an arc by assigning a child for the headword at the top of the internal stack .",0.1148936170212766,0.75,0.75,model,experimental-setup
dependency_parsing,4,introduction,introduction,28,19,19,"The STACKPTR parser performs parsing in an incremental , topdown , depth - first fashion ; at each step , it generates an arc by assigning a child for the headword at the top of the internal stack .","Our STACKPTR parser has a pointer network as its backbone , and is equipped with an internal stack to maintain the order of head words in tree structures .","This architecture makes it possible to capture information from the whole sentence and all the previously derived subtrees , while maintaining a number of parsing steps linear in the sentence length .",0.1191489361702127,0.7916666666666666,0.7916666666666666,model,model
dependency_parsing,4,experiments,setup,158,14,13,"For fair comparison of the parsing performance , we re-implemented the graph - based Deep Biaffine ( BIAF ) parser , which achieved state - of - the - art results on a wide range of languages .",Baseline,"Our re-implementation adds character - level information using the same LSTM - CNN encoder as our model ( 3.2 ) to the original BIAF model , which boosts its performance on all languages .",0.6723404255319149,0.1772151898734177,0.9285714285714286,baselines,approach
dependency_parsing,4,experiments,main results,160,16,1,Main Results, , ,0.6808510638297872,0.2025316455696202,0.0555555555555555,results,baselines
dependency_parsing,4,experiments,main results,165,21,6,"On UAS and LAS , the Full variation of STACKPTR with decoding beam size 10 outperforms BIAF on Chinese , and obtains competitive performance on English and German .","illustrates the performance ( five metrics ) of different variations of our STACKPTR parser together with the results of baseline BIAF re-implemented by us , on the test sets of the three languages .","An interesting observation is that the Full model achieves the best accuracy on English and Chinese , while performs slightly worse than + sib on German .",0.7021276595744681,0.2658227848101265,0.3333333333333333,results,baselines
dependency_parsing,4,experiments,main results,166,22,7,"An interesting observation is that the Full model achieves the best accuracy on English and Chinese , while performs slightly worse than + sib on German .","On UAS and LAS , the Full variation of STACKPTR with decoding beam size 10 outperforms BIAF on Chinese , and obtains competitive performance on English and German .",This shows that the importance of higher - order information varies in languages .,0.7063829787234043,0.2784810126582278,0.3888888888888889,results,baselines
dependency_parsing,4,experiments,main results,168,24,9,"On LCM and UCM , STACKPTR significantly outperforms BIAF on all languages , showing the superiority of our parser on complete sentence parsing .",This shows that the importance of higher - order information varies in languages .,The results of our parser on RA are slightly worse than BIAF .,0.7148936170212766,0.3037974683544304,0.5,results,baselines
dependency_parsing,4,experiments,main results,169,25,10,The results of our parser on RA are slightly worse than BIAF .,"On LCM and UCM , STACKPTR significantly outperforms BIAF on all languages , showing the superiority of our parser on complete sentence parsing .","More details of results are provided in Appendix C. illustrates the UAS and LAS of the four versions of our model ( with decoding beam size 10 ) on the three treebanks , together with previous top - performing systems for comparison .",0.7191489361702128,0.3164556962025317,0.5555555555555556,results,approach
dependency_parsing,4,experiments,main results,175,31,16,"re-implementation of BIAF obtains better performance than the original one in , demonstrating the effectiveness of the character - level information .","For STACKPTR and our re-implementation of BiAF , we report the average over 5 runs .","Our model achieves state - of - the - art performance on both UAS and LAS on Chinese , and best UAS on English .",0.7446808510638298,0.3924050632911392,0.8888888888888888,results,baselines
dependency_parsing,4,experiments,main results,176,32,17,"Our model achieves state - of - the - art performance on both UAS and LAS on Chinese , and best UAS on English .","re-implementation of BIAF obtains better performance than the original one in , demonstrating the effectiveness of the character - level information .","On German , the performance is competitive with BIAF , and significantly better than other models .",0.7489361702127659,0.4050632911392405,0.9444444444444444,results,approach
dependency_parsing,4,experiments,main results,177,33,18,"On German , the performance is competitive with BIAF , and significantly better than other models .","Our model achieves state - of - the - art performance on both UAS and LAS on Chinese , and best UAS on English .", ,0.7531914893617021,0.4177215189873418,1.0,results,approach
dependency_parsing,5,title,title,2,2,2,Structured Training for Neural Network Transition - Based Parsing, , ,0.0070175438596491,1.0,1.0,research-problem,approach
dependency_parsing,5,abstract,abstract,4,2,2,We present structured perceptron training for neural network transition - based dependency parsing ., ,We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences .,0.0140350877192982,0.3333333333333333,0.3333333333333333,research-problem,experimental-setup
dependency_parsing,5,introduction,introduction,11,3,3,"Lately , dependency parsing has emerged as a popular approach to this problem due to the availability of dependency treebanks in many languages and the efficiency of dependency parsers .",Syntactic analysis is a central problem in language understanding that has received a tremendous amount of attention .,Transition - based parsers have been shown to provide a good balance between efficiency and accuracy .,0.0385964912280701,0.0638297872340425,0.0638297872340425,research-problem,ablation-analysis
dependency_parsing,5,introduction,introduction,13,5,5,"In transition - based parsing , sentences are processed in a linear left to right pass ; at each position , the parser needs to choose from a set of possible actions defined by the transition strategy .",Transition - based parsers have been shown to provide a good balance between efficiency and accuracy .,"In greedy models , a classifier is used to independently decide which transition to take based on local features of the current parse configuration .",0.0456140350877193,0.1063829787234042,0.1063829787234042,research-problem,ablation-analysis
dependency_parsing,5,introduction,introduction,24,16,16,"In this work , we combine the representational power of neural networks with the superior search enabled by structured training and inference , making our parser one of the most accurate dependency parsers to date .","However , although their model outperforms its greedy hand - engineered counterparts , it is not competitive with state - of - the - art dependency parsers thatare trained for structured search .","Training and testing on the Penn Treebank , our transition - based parser achieves 93.99 % unlabeled ( UAS ) / 92.05 % labeled ( LAS ) attachment accuracy , outperforming the 93.22 % UAS / 91.02 % LAS of and 93.27 UAS / 91.19 LAS of .",0.0842105263157894,0.3404255319148936,0.3404255319148936,approach,experimental-setup
dependency_parsing,5,introduction,introduction,29,21,21,"As in prior work , we train the neural network to model the probability of individual parse actions .",These modifications ( Section 2 ) increase the performance of the greedy model by as much as 1 % .,"However , we do not use these probabilities directly for prediction .",0.1017543859649122,0.4468085106382978,0.4468085106382978,approach,experimental-setup
dependency_parsing,5,introduction,introduction,31,23,23,"Instead , we use the activations from all layers of the neural network as the representation in a structured perceptron model that is trained with beam search and early updates ( Section 3 ) .","However , we do not use these probabilities directly for prediction .","On the Penn Treebank , this structured learning approach significantly improves parsing accuracy by 0.8 % .",0.1087719298245614,0.4893617021276596,0.4893617021276596,approach,model
dependency_parsing,5,introduction,introduction,35,27,27,"To this end , we generate large quantities of high - confidence parse trees by parsing unlabeled data with two different parsers and selecting only the sentences for which the two parsers produced the same trees ( Section 3.3 ) .","Neural networks are known to perform very well in the presence of large amounts of training data ; however , obtaining more expert - annotated parse trees is very expensive .","This approach is known as "" tri-training "" and we show that it benefits our neural network parser significantly more than other approaches .",0.1228070175438596,0.5744680851063829,0.5744680851063829,approach,experimental-setup
dependency_parsing,5,introduction,introduction,36,28,28,"This approach is known as "" tri-training "" and we show that it benefits our neural network parser significantly more than other approaches .","To this end , we generate large quantities of high - confidence parse trees by parsing unlabeled data with two different parsers and selecting only the sentences for which the two parsers produced the same trees ( Section 3.3 ) .","By adding 10 million automatically parsed tokens to the training data , we improve the accuracy of our parsers by almost ? 1.0 % on web domain data .",0.1263157894736842,0.5957446808510638,0.5957446808510638,approach,experimental-setup
dependency_parsing,5,experiments,model initialization hyperparameters,200,35,7,We used the publicly available word2vec 2 tool to learn CBOW embeddings following the sample configuration provided with the tool .,"For the word embedding matrix E word , we initialized the parameters using pretrained word embeddings .","For words not appearing in the unsupervised data and the special "" NULL "" etc. tokens , we used random initialization .",0.7017543859649122,0.603448275862069,0.28,hyperparameters,approach
dependency_parsing,5,experiments,results,219,54,1,Results, , ,0.7684210526315789,0.9310344827586208,0.2,results,baselines
dependency_parsing,5,experiments,results,222,57,4,"The highest of these is , with a reported accuracy of 94.22 % UAS .",These parsers use the dependency conversion and the accuracies are therefore not directly comparable .,"Even though the UAS is not directly comparable , it is typically similar , and this suggests that our model is competitive with some of the highest reported accuries for dependencies on WSJ .",0.7789473684210526,0.9827586206896552,0.8,results,approach
dependency_parsing,5,experiments,results,223,58,5,"Even though the UAS is not directly comparable , it is typically similar , and this suggests that our model is competitive with some of the highest reported accuries for dependencies on WSJ .","The highest of these is , with a reported accuracy of 94.22 % UAS .", ,0.7824561403508772,1.0,1.0,results,approach
dependency_parsing,6,title,title,2,2,2,DEEP BIAFFINE ATTENTION FOR NEURAL DEPENDENCY PARSING, , ,0.0175438596491228,1.0,1.0,research-problem,experimental-setup
dependency_parsing,6,introduction,introduction,13,5,5,"We modify the neural graphbased approach first proposed by in a few ways to achieve competitive performance : we build a network that 's larger but uses more regularization ; we replace the traditional MLP - based attention mechanism and affine label classifier with biaffine ones ; and rather than using the top recurrent states of the LSTM in the biaffine transformations , we first put them through MLP operations that reduce their dimensionality .",The current state - of - the - art transition - based neural dependency parser substantially outperforms many much simpler neural graph - based parsers .,"Furthermore , we compare models trained with different architectures and hyperparameters to motivate our approach empirically .",0.1140350877192982,0.7142857142857143,0.7142857142857143,approach,ablation-analysis
dependency_parsing,6,experiments results,hyperparameter choices,96,33,27,"We choose to optimize with Adam , which ( among other things ) keeps a moving average of the L 2 norm of the gradient for each parameter throughout training and divides the gradient for each parameter by this moving average , ensuring that the magnitude of the gradients will on average be close to one .",OPTIMIZER,"However , we find that the value for ?",0.8421052631578947,0.7173913043478259,0.8709677419354839,hyperparameters,baselines
dependency_parsing,6,experiments results,results,102,39,2,"Our model gets nearly the same UAS performance on PTB - SD 3.3.0 as the current SOTA model from in spite of its substantially simpler architecture , and gets SOTA UAS performance on CTB 5.1 7 as well as SOTA performance on all CoNLL 09 languages .", ,"It is worth noting that the CoNLL 09 datasets contain many non-projective dependencies , which are difficult or impossible for transition - based - but not graph - based - parsers to predict .",0.8947368421052632,0.8478260869565217,0.2222222222222222,results,baselines
dependency_parsing,7,abstract,abstract,5,3,3,"This form of training , which accounts for model predictions at training time , improves parsing accuracies .","We adapt the greedy stack LSTM dependency parser of Dyer et al. ( 2015 ) to support a training - with - exploration procedure using dynamic oracles ( Goldberg and Nivre , 2013 ) instead of assuming an error - free action history .",We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser .,0.0485436893203883,0.75,0.75,research-problem,baselines
dependency_parsing,7,introduction,introduction,8,2,2,"Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures ; this formalization is known as transitionbased parsing , and is often coupled with a greedy search procedure .", ,"The literature on transition - based parsing is vast , but all works share in common a classification component that takes into account features of the current parser state 1 and predicts the next action to take conditioned on the state .",0.0776699029126213,0.1428571428571428,0.1428571428571428,research-problem,ablation-analysis
dependency_parsing,7,introduction,introduction,9,3,3,"The literature on transition - based parsing is vast , but all works share in common a classification component that takes into account features of the current parser state 1 and predicts the next action to take conditioned on the state .","Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures ; this formalization is known as transitionbased parsing , and is often coupled with a greedy search procedure .",The state is of unbounded size .,0.087378640776699,0.2142857142857142,0.2142857142857142,research-problem,ablation-analysis
dependency_parsing,7,introduction,introduction,17,11,11,"In this work , we adapt the training criterion so as to explore parser states drawn not only from the training data , but also from the model as it is being learned .","Although this setup obtains very good performance , the training and testing conditions are mismatched in the following way : at training time the historical context of an action is always derived from the gold standard ( i.e. , perfectly correct past actions ) , but at test time , it will be a model prediction .","To do so , we use the method of to dynamically chose an optimal ( relative to the final attachment accuracy ) action given an imperfect history .",0.1650485436893204,0.7857142857142857,0.7857142857142857,approach,experimental-setup
dependency_parsing,7,introduction,introduction,18,12,12,"To do so , we use the method of to dynamically chose an optimal ( relative to the final attachment accuracy ) action given an imperfect history .","In this work , we adapt the training criterion so as to explore parser states drawn not only from the training data , but also from the model as it is being learned .","By interpolating between algorithm states sampled from the model and those sampled from the training data , more robust predictions at test time can be made .",0.174757281553398,0.8571428571428571,0.8571428571428571,approach,model
dependency_parsing,7,introduction,introduction,19,13,13,"By interpolating between algorithm states sampled from the model and those sampled from the training data , more robust predictions at test time can be made .","To do so , we use the method of to dynamically chose an optimal ( relative to the final attachment accuracy ) action given an imperfect history .",We show that the technique can be used to improve the strong parser of Dyer et al .,0.1844660194174757,0.9285714285714286,0.9285714285714286,approach,model
dependency_parsing,7,experiments,experiments,76,1,1,Experiments, , ,0.7378640776699029,0.0625,0.0625,results,experimental-setup
dependency_parsing,7,experiments,experiments,78,3,3,The score achieved by the dynamic oracle for English is 93.56 UAS .,Following the same settings of Chen and Manning ( 2014 ) and we report results 4 in the English PTB and Chinese CTB - 5 .,This is remarkable given that the parser uses a completely greedy search procedure .,0.7572815533980582,0.1875,0.1875,results,approach
dependency_parsing,7,experiments,experiments,80,5,5,"Moreover , the Chinese score establishes the state - of - the - art , using the same settings as Chen and Manning ( 2014 ) .",This is remarkable given that the parser uses a completely greedy search procedure .,"and Andor et al. , respectively .",0.7766990291262136,0.3125,0.3125,results,approach
dependency_parsing,8,title,title,2,2,2,Globally Normalized Transition - Based Neural Networks, , ,0.0084388185654008,1.0,1.0,research-problem,approach
dependency_parsing,8,abstract,abstract,4,2,2,"We introduce a globally normalized transition - based neural network model that achieves state - of - the - art part - ofspeech tagging , dependency parsing and sentence compression results .", ,"Our model is a simple feed - forward neural network that operates on a task - specific transition system , yet achieves comparable or better accuracies than recurrent models .",0.0168776371308016,0.5,0.5,research-problem,baselines
dependency_parsing,8,introduction,introduction,11,5,5,"In this work we demonstrate that simple feed - forward networks without any recurrence can achieve comparable or better accuracies than LSTMs , as long as they are globally normalized .",One might speculate that it is the recurrent nature of these models that enables these results .,"Our model , described in detail in Section 2 , uses a transition system and feature embeddings as introduced by * On leave from Columbia University ..",0.0464135021097046,0.2380952380952381,0.2380952380952381,model,baselines
dependency_parsing,8,introduction,introduction,12,6,6,"Our model , described in detail in Section 2 , uses a transition system and feature embeddings as introduced by * On leave from Columbia University ..","In this work we demonstrate that simple feed - forward networks without any recurrence can achieve comparable or better accuracies than LSTMs , as long as they are globally normalized .","We do not use any recurrence , but perform beam search for maintaining multiple hypotheses and introduce global normalization with a conditional random field ( CRF ) objective to overcome the label bias problem that locally normalized models suffer from .",0.050632911392405,0.2857142857142857,0.2857142857142857,model,approach
dependency_parsing,8,introduction,introduction,13,7,7,"We do not use any recurrence , but perform beam search for maintaining multiple hypotheses and introduce global normalization with a conditional random field ( CRF ) objective to overcome the label bias problem that locally normalized models suffer from .","Our model , described in detail in Section 2 , uses a transition system and feature embeddings as introduced by * On leave from Columbia University ..","Since we use beam inference , we approximate the partition function by summing over the elements in the beam , and use early updates .",0.0548523206751054,0.3333333333333333,0.3333333333333333,model,ablation-analysis
dependency_parsing,8,introduction,introduction,14,8,8,"Since we use beam inference , we approximate the partition function by summing over the elements in the beam , and use early updates .","We do not use any recurrence , but perform beam search for maintaining multiple hypotheses and introduce global normalization with a conditional random field ( CRF ) objective to overcome the label bias problem that locally normalized models suffer from .",We compute gradients based on this approximate global normalization and perform full backpropagation training of all neural network parameters based on the CRF loss .,0.0590717299578059,0.3809523809523809,0.3809523809523809,model,model
dependency_parsing,8,introduction,introduction,15,9,9,We compute gradients based on this approximate global normalization and perform full backpropagation training of all neural network parameters based on the CRF loss .,"Since we use beam inference , we approximate the partition function by summing over the elements in the beam , and use early updates .",In Section 3 we revisit the label bias problem and the implication that globally normalized models are strictly more expressive than locally normalized models .,0.0632911392405063,0.4285714285714285,0.4285714285714285,model,approach
dependency_parsing,8,experiments,experiments,149,1,1,Experiments, , ,0.6286919831223629,0.0208333333333333,0.1111111111111111,experiments,experimental-setup
dependency_parsing,8,experiments,part of speech tagging,158,10,1,Part of Speech Tagging, , ,0.6666666666666666,0.2083333333333333,0.032258064516129,experiments,approach
dependency_parsing,8,experiments,part of speech tagging,169,21,12,Results .,We use a single hidden layer of size 400 .,shows our sentence compression results .,0.7130801687763713,0.4375,0.3870967741935484,experiments,baselines
dependency_parsing,8,experiments,part of speech tagging,171,23,14,Our globally normalized model again significantly outperforms the local model .,shows our sentence compression results .,Beam search with a locally normalized model suffers from severe label bias issues that we discuss on a concrete example in Section 5 .,0.7215189873417721,0.4791666666666667,0.4516129032258064,experiments,baselines
dependency_parsing,8,experiments,part of speech tagging,172,24,15,Beam search with a locally normalized model suffers from severe label bias issues that we discuss on a concrete example in Section 5 .,Our globally normalized model again significantly outperforms the local model .,"We also compare to the sentence compression system from , a 3 - layer stacked LSTM which uses dependency label information .",0.7257383966244726,0.5,0.4838709677419355,experiments,ablation-analysis
dependency_parsing,8,experiments,part of speech tagging,186,38,29,"Using beam search with a locally normalized model does not help , but with global normalization it leads to a 7 % reduction in relative error , empirically demonstrating the effect of label bias .",Our local model already compares favorably against these methods on average .,"The set of character ngrams feature is very important , increasing average accuracy on the CoNLL '09 datasets by about 0.5 % absolute .",0.7848101265822784,0.7916666666666666,0.935483870967742,experiments,approach
dependency_parsing,8,experiments,part of speech tagging,187,39,30,"The set of character ngrams feature is very important , increasing average accuracy on the CoNLL '09 datasets by about 0.5 % absolute .","Using beam search with a locally normalized model does not help , but with global normalization it leads to a 7 % reduction in relative error , empirically demonstrating the effect of label bias .",This shows that characterlevel modeling can also be done with a simple feed - forward network without recurrence .,0.7890295358649789,0.8125,0.967741935483871,experiments,approach
dependency_parsing,8,experiments,dependency parsing,189,41,1,Dependency Parsing, , ,0.7974683544303798,0.8541666666666666,0.125,experiments,approach
dependency_parsing,8,experiments,dependency parsing,191,43,3,Results .,In dependency parsing the goal is to produce a directed tree representing the syntactic structure of the input sentence .,"Even though we do not use tri-training , our model compares favorably to the 94.26 % LAS and 92.41 % UAS reported by with tri-training .",0.8059071729957806,0.8958333333333334,0.375,experiments,approach
dependency_parsing,8,experiments,dependency parsing,192,44,4,"Even though we do not use tri-training , our model compares favorably to the 94.26 % LAS and 92.41 % UAS reported by with tri-training .",Results .,"As we show in Sec. 5 , these gains can be attributed to the full backpropagation training that differentiates our approach from that of and .",0.810126582278481,0.9166666666666666,0.5,experiments,baselines
dependency_parsing,8,experiments,dependency parsing,194,46,6,Our results also significantly outperform the LSTM - based approaches of .,"As we show in Sec. 5 , these gains can be attributed to the full backpropagation training that differentiates our approach from that of and .",Sentence Compression,0.8185654008438819,0.9583333333333334,0.75,experiments,baselines
document_classification,0,title,title,2,2,2,Bag of Tricks for Efficient Text Classification, , ,0.021505376344086,1.0,1.0,research-problem,approach
document_classification,0,introduction,introduction,14,8,8,"In this work , we explore ways to scale these baselines to very large corpus with a large output space , in the context of text classification .",They also have the potential to scale to very large corpus .,"Inspired by the recent work in efficient word representation learning , we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes , while achieving performance on par with the state - of - the - art .",0.1505376344086021,0.8,0.8,approach,experimental-setup
document_classification,0,introduction,introduction,15,9,9,"Inspired by the recent work in efficient word representation learning , we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes , while achieving performance on par with the state - of - the - art .","In this work , we explore ways to scale these baselines to very large corpus with a large output space , in the context of text classification .","We evaluate the quality of our approach fastText 1 on two different tasks , namely tag prediction and sentiment analysis .",0.1612903225806451,0.9,0.9,approach,baselines
document_classification,0,experiments,sentiment analysis,53,6,1,Sentiment analysis, , ,0.5698924731182796,0.15,0.0625,experiments,approach
document_classification,0,experiments,sentiment analysis,60,13,8,"We use 10 hidden units and run fastText for 5 epochs with a learning rate selected on a validation set from { 0.05 , 0.1 , 0.25 , 0.5 } .",We present the results in .,"On this task , adding bigram information improves the performance by 1 - 4 % .",0.6451612903225806,0.325,0.5,experiments,experimental-setup
document_classification,0,experiments,sentiment analysis,61,14,9,"On this task , adding bigram information improves the performance by 1 - 4 % .","We use 10 hidden units and run fastText for 5 epochs with a learning rate selected on a validation set from { 0.05 , 0.1 , 0.25 , 0.5 } .","Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .",0.6559139784946236,0.35,0.5625,experiments,approach
document_classification,0,experiments,sentiment analysis,62,15,10,"Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .","On this task , adding bigram information improves the performance by 1 - 4 % .","Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .",0.6666666666666666,0.375,0.625,experiments,approach
document_classification,0,experiments,sentiment analysis,63,16,11,"Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .","Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .","Finally , shows that our method is competitive with the methods presented in .",0.6774193548387096,0.4,0.6875,experiments,baselines
document_classification,0,experiments,sentiment analysis,65,18,13,We tune the hyperparameters on the validation set and observe that using n-grams up to 5 leads to the best performance .,"Finally , shows that our method is competitive with the methods presented in .","Unlike , fastText does not use pre-trained word embeddings , which can be explained the 1 % difference in accuracy .",0.6989247311827957,0.45,0.8125,experiments,approach
document_classification,0,experiments,tag prediction,69,22,1,Tag prediction, , ,0.7419354838709677,0.55,0.0526315789473684,experiments,approach
document_classification,0,experiments,tag prediction,79,32,11,We consider a frequency - based baseline which predicts the most frequent tag .,We report precision at 1 .,"We also compare with Tagspace ( Weston et al. , 2014 ) , which is a tag prediction model similar to ours , but based on the Wsabie model of .",0.8494623655913979,0.8,0.5789473684210527,experiments,approach
document_classification,0,experiments,tag prediction,80,33,12,"We also compare with Tagspace ( Weston et al. , 2014 ) , which is a tag prediction model similar to ours , but based on the Wsabie model of .",We consider a frequency - based baseline which predicts the most frequent tag .,"While the Tagspace model is described using convolutions , we consider the linear version , which achieves comparable performance but is much faster .",0.8602150537634409,0.825,0.631578947368421,experiments,baselines
document_classification,0,experiments,tag prediction,82,35,14,Results and training time . and 200 .,"While the Tagspace model is described using convolutions , we consider the linear version , which achieves comparable performance but is much faster .","Both models achieve a similar performance with a small hidden layer , but adding bigrams gives us a significant boost in accuracy .",0.8817204301075269,0.875,0.7368421052631579,experiments,baselines
document_classification,0,experiments,tag prediction,83,36,15,"Both models achieve a similar performance with a small hidden layer , but adding bigrams gives us a significant boost in accuracy .",Results and training time . and 200 .,"At test time , Tagspace needs to compute the scores for all the classes which makes it relatively slow , while our fast inference gives a significant speed - up when the number of classes is large ( more than 300 K here ) .",0.8924731182795699,0.9,0.7894736842105263,experiments,approach
document_classification,1,title,title,2,2,2,BRIDGING THE DOMAIN GAP IN CROSS - LINGUAL DOCUMENT CLASSIFICATION, , ,0.0070422535211267,1.0,1.0,research-problem,approach
document_classification,1,abstract,abstract,5,3,3,"Recent developments in cross - lingual understanding ( XLU ) has made progress in this area , trying to bridge the language barrier using language universal representations .",The scarcity of labeled training data often prohibits the internationalization of NLP models to multiple languages .,"However , even if the language problem was resolved , models trained in one language would not transfer to another language perfectly due to the natural domain drift across languages and cultures .",0.0176056338028169,0.3,0.3,research-problem,baselines
document_classification,1,abstract,abstract,8,6,6,We combine state - of - the - art cross - lingual methods with recently proposed methods for weakly supervised learning such as unsupervised pre-training and unsupervised data augmentation to simultaneously close both the language gap and the domain gap in XLU .,"We consider the setting of semi-supervised cross - lingual understanding , where labeled data is available in a source language ( English ) , but only unlabeled data is available in the target language .",We show that addressing the domain gap is crucial .,0.028169014084507,0.6,0.6,research-problem,baselines
document_classification,1,introduction,introduction,38,26,26,"In particular , we focus on two approaches for domain adaptation .","Using this unlabeled data , we combine the aforementioned cross - lingual methods with recently proposed unsupervised domain adaptation and weak supervision techniques on the task of cross - lingual document classification .",The first method is based on masked language model ( MLM ) pre-training ( as in ) using unlabeled target language corpora .,0.1338028169014084,0.7428571428571429,0.7428571428571429,model,approach
document_classification,1,introduction,introduction,39,27,27,The first method is based on masked language model ( MLM ) pre-training ( as in ) using unlabeled target language corpora .,"In particular , we focus on two approaches for domain adaptation .",Such methods have been shown to improve over general purpose pre-trained models such as BERT in the weakly supervised setting ; ) .,0.1373239436619718,0.7714285714285715,0.7714285714285715,model,ablation-analysis
document_classification,1,introduction,introduction,41,29,29,"The second method is unsupervised data augmentation ( UDA ) ) , where synthetic paraphrases are generated from the unlabeled corpus , and the model is trained on a label consistency loss .",Such methods have been shown to improve over general purpose pre-trained models such as BERT in the weakly supervised setting ; ) .,"While both of these techniques were proposed previously , in both cases it is non-trivial to extend them to the cross - lingual setting .",0.1443661971830986,0.8285714285714286,0.8285714285714286,model,ablation-analysis
document_classification,1,experiments,main results,152,32,3,Fine-tune ( Ft ) : Fine - tuning the pre-trained model with the source - domain training set .,We compare the follwing models :,"In the case of XLM ft , the training set is translated into the target language .",0.5352112676056338,0.3368421052631579,0.0612244897959183,baselines,baselines
document_classification,1,experiments,main results,154,34,5,Fine - tune with UDA ( UDA ) :,"In the case of XLM ft , the training set is translated into the target language .",This method utilizes the unlabeled data from the target domain by optimizing the UDA loss function ( Eq. ) .,0.5422535211267606,0.3578947368421053,0.1020408163265306,baselines,approach
document_classification,1,experiments,main results,155,35,6,This method utilizes the unlabeled data from the target domain by optimizing the UDA loss function ( Eq. ) .,Fine - tune with UDA ( UDA ) :,Self - training based on the UDA model ( UDA + Self ) :,0.545774647887324,0.3684210526315789,0.1224489795918367,baselines,baselines
document_classification,1,experiments,main results,156,36,7,Self - training based on the UDA model ( UDA + Self ) :,This method utilizes the unlabeled data from the target domain by optimizing the UDA loss function ( Eq. ) .,"We first train the Ft model and UDA model , and choose the better one as the teacher model .",0.5492957746478874,0.3789473684210526,0.1428571428571428,baselines,approach
document_classification,1,experiments,main results,157,37,8,"We first train the Ft model and UDA model , and choose the better one as the teacher model .",Self - training based on the UDA model ( UDA + Self ) :,"The teacher model is used to train a new XLM student using only unlabeled data U tgt in the target domain , as described above .",0.5528169014084507,0.3894736842105263,0.1632653061224489,baselines,approach
document_classification,1,experiments,main results,158,38,9,"The teacher model is used to train a new XLM student using only unlabeled data U tgt in the target domain , as described above .","We first train the Ft model and UDA model , and choose the better one as the teacher model .",We report the results of applying these three methods on both the original XLM model and the XLM ft model .,0.5563380281690141,0.4,0.1836734693877551,baselines,baselines
document_classification,1,experiments,main results,171,51,22,"Looking at Ft ( XLM ) results , it is clear that without the help of unlabeled data from the target domain , there still exists a substantial gap between the model performance of the cross -lingual settings and the monolingual baselines , even when using state - of - the - art pre-trained cross -lingual representations .",We can summarize our findings as follows :,Both the UDA algorithm and MLM pre-training can offer significant improvements by utilizing the unlabeled data .,0.602112676056338,0.5368421052631579,0.4489795918367347,results,baselines
document_classification,1,experiments,main results,172,52,23,Both the UDA algorithm and MLM pre-training can offer significant improvements by utilizing the unlabeled data .,"Looking at Ft ( XLM ) results , it is clear that without the help of unlabeled data from the target domain , there still exists a substantial gap between the model performance of the cross -lingual settings and the monolingual baselines , even when using state - of - the - art pre-trained cross -lingual representations .","In the sentiment classification task , where the unlabeled data size is larger , Ft ( XLM ft ) model usnig MLM pre-training consistently provides larger improvements compared with the UDA method .",0.6056338028169014,0.5473684210526316,0.4693877551020408,results,baselines
document_classification,1,experiments,main results,173,53,24,"In the sentiment classification task , where the unlabeled data size is larger , Ft ( XLM ft ) model usnig MLM pre-training consistently provides larger improvements compared with the UDA method .",Both the UDA algorithm and MLM pre-training can offer significant improvements by utilizing the unlabeled data .,"On the other hand , the MLM method is relatively more resource intensive and takes longer to converge ( see Appendix A.5 ) .",0.6091549295774648,0.5578947368421052,0.4897959183673469,results,baselines
document_classification,1,experiments,main results,174,54,25,"On the other hand , the MLM method is relatively more resource intensive and takes longer to converge ( see Appendix A.5 ) .","In the sentiment classification task , where the unlabeled data size is larger , Ft ( XLM ft ) model usnig MLM pre-training consistently provides larger improvements compared with the UDA method .","In contrast , in the MLdoc dataset , when the size of the unlabeled samples is limited , the UDA method is more helpful .",0.6126760563380281,0.5684210526315789,0.5102040816326531,results,baselines
document_classification,1,experiments,main results,175,55,26,"In contrast , in the MLdoc dataset , when the size of the unlabeled samples is limited , the UDA method is more helpful .","On the other hand , the MLM method is relatively more resource intensive and takes longer to converge ( see Appendix A.5 ) .",The combination of both methods - as in the UDA ( XLM ft ) model - consistently outperforms either method alone .,0.6161971830985915,0.5789473684210527,0.5306122448979592,results,approach
document_classification,1,experiments,main results,178,58,29,"In the sentiment classification task , we observe the self - training technique consistently improves over its teacher model .","In this case the additional improvement provided by the UDA algorithm is smaller , but still consistent .",It offers best results in both XLM and XLM ft based classifiers .,0.6267605633802817,0.6105263157894737,0.5918367346938775,results,approach
document_classification,1,experiments,main results,179,59,30,It offers best results in both XLM and XLM ft based classifiers .,"In the sentiment classification task , we observe the self - training technique consistently improves over its teacher model .",The results demonstrate that self - training process is able to alleviate the train - test distribution mismatch problem and provide better generalization ability .,0.6302816901408451,0.6210526315789474,0.6122448979591837,results,approach
document_classification,1,experiments,main results,181,61,32,"In the MLdoc dataset , self - training also achieves the best results over all , however the gains are less clear .",The results demonstrate that self - training process is able to alleviate the train - test distribution mismatch problem and provide better generalization ability .,We hypothesize that this technique is not as useful without enough number of unlabeled samples .,0.6373239436619719,0.6421052631578947,0.6530612244897959,results,approach
document_classification,1,experiments,main results,183,63,34,"Finally , comparing with the best cross - lingual results and monolingual fine - tune baseline , we are able to completely close the performance gap by utilizing unlabeled data in the target language .",We hypothesize that this technique is not as useful without enough number of unlabeled samples .,"Furthermore , our framework reaches new state - of - the - art results , improving over vanilla XLM baselines by 44 % on average .",0.6443661971830986,0.6631578947368421,0.6938775510204082,results,approach
document_classification,1,experiments,main results,184,64,35,"Furthermore , our framework reaches new state - of - the - art results , improving over vanilla XLM baselines by 44 % on average .","Finally , comparing with the best cross - lingual results and monolingual fine - tune baseline , we are able to completely close the performance gap by utilizing unlabeled data in the target language .","Furthermore , we provide an additional baseline , which only uses English samples to perform semisupervised learning , whose details are in Appendix A.2 .",0.647887323943662,0.6736842105263158,0.7142857142857143,results,baselines
document_classification,1,experiments,ablation study augmentation strategies,214,94,16,"Leveraging the unlabeled data from other domains does not offer consistent improvement , however can provide additional value in isolated cases .","From the results , we conclude that t2t is the best performing approach , as it 's the best matched to the target domain .","We include additional ablations regarding translation system in the appendix , including the application of translate - train method in our experiments ( section A.3 ) and effects of hyper - parameters ( section A.4 ) .",0.7535211267605634,0.9894736842105264,0.9411764705882352,ablation-analysis,approach
document_classification,10,title,title,2,2,2,Neural Attentive Bag - of - Entities Model for Text Classification, , ,0.0121951219512195,1.0,1.0,research-problem,approach
document_classification,10,abstract,abstract,9,7,7,The source code of the proposed model is available online at https://github.com/wikipedia2vec/wikipedia2vec.,"As a result , our model achieved state - of - the - art results on all datasets .", ,0.0548780487804878,1.0,1.0,code,baselines
document_classification,10,introduction,introduction,21,12,12,"This study proposes the Neural Attentive Bagof - Entities ( NABoE ) model , which is a neural network model that addresses the text classification problem by modeling the semantics in the target documents using entities in the KB .","However , this approach is problematic because ( 1 ) entity linking systems produce dis ambiguation errors , and ( 2 ) entities appearing in a document are not necessarily relevant to the given document .","For each entity name in a document ( e.g. , "" Apple "" ) , our model first detects entities that maybe referred to by this name ( e.g. , Apple Inc. , Apple ( food ) ) , and then represents the document using the weighted average of the embeddings of these entities .",0.1280487804878048,0.631578947368421,0.631578947368421,model,ablation-analysis
document_classification,10,introduction,introduction,22,13,13,"For each entity name in a document ( e.g. , "" Apple "" ) , our model first detects entities that maybe referred to by this name ( e.g. , Apple Inc. , Apple ( food ) ) , and then represents the document using the weighted average of the embeddings of these entities .","This study proposes the Neural Attentive Bagof - Entities ( NABoE ) model , which is a neural network model that addresses the text classification problem by modeling the semantics in the target documents using entities in the KB .",The weights are computed using a novel neural attention mechanism that enables the model to focus on a small subset of the entities that are less ambiguous in meaning and more relevant to the document .,0.1341463414634146,0.6842105263157895,0.6842105263157895,model,approach
document_classification,10,introduction,introduction,23,14,14,The weights are computed using a novel neural attention mechanism that enables the model to focus on a small subset of the entities that are less ambiguous in meaning and more relevant to the document .,"For each entity name in a document ( e.g. , "" Apple "" ) , our model first detects entities that maybe referred to by this name ( e.g. , Apple Inc. , Apple ( food ) ) , and then represents the document using the weighted average of the embeddings of these entities .","In other words , the attention mechanism is designed to compute weights by jointly addressing entity linking and entity salience detection tasks .",0.1402439024390244,0.7368421052631579,0.7368421052631579,model,approach
document_classification,10,introduction,introduction,24,15,15,"In other words , the attention mechanism is designed to compute weights by jointly addressing entity linking and entity salience detection tasks .",The weights are computed using a novel neural attention mechanism that enables the model to focus on a small subset of the entities that are less ambiguous in meaning and more relevant to the document .,"Furthermore , the attention mechanism improves the interpretability of the model because it enables us to inspect the small number of entities that strongly affect the classification decisions .",0.1463414634146341,0.7894736842105263,0.7894736842105263,model,approach
document_classification,10,experimental setup,setup,81,21,11,The model was trained using mini-batch SGD with its learning rate controlled by Adam and its mini-batch size set to 32 .,The settings we used to train the model were the same as those in the previous experiment ( see Section 4.1 ) .,"We used words and entities that were detected three times or more in the dataset , and ignored the other words and entities .",0.4939024390243902,0.3,0.7333333333333333,hyperparameters,approach
document_classification,10,experimental setup,setup,83,23,13,The size of the embeddings of words and entities was set to d = 300 .,"We used words and entities that were detected three times or more in the dataset , and ignored the other words and entities .","As in past work , we report the accuracy score , and the score on the development set was used for early stopping . :",0.5060975609756098,0.3285714285714285,0.8666666666666667,hyperparameters,experiments
document_classification,10,experimental setup,baselines,86,26,1,Baselines, , ,0.5243902439024389,0.3714285714285713,0.0714285714285714,baselines,baselines
document_classification,10,experimental setup,baselines,88,28,3,BoW,We used the following baseline models :,This model is based on a logistic regression classifier with conventional binary BoW features .,0.5365853658536586,0.4,0.2142857142857142,baselines,baselines
document_classification,10,experimental setup,baselines,89,29,4,This model is based on a logistic regression classifier with conventional binary BoW features .,BoW,FTS- BRNN,0.5426829268292683,0.4142857142857143,0.2857142857142857,baselines,experimental-setup
document_classification,10,experimental setup,baselines,90,30,5,FTS- BRNN,This model is based on a logistic regression classifier with conventional binary BoW features .,This model is based on a bidirectional RNN with gated recurrent units ( GRU ) .,0.5487804878048781,0.4285714285714285,0.3571428571428571,baselines,experimental-setup
document_classification,10,experimental setup,baselines,91,31,6,This model is based on a bidirectional RNN with gated recurrent units ( GRU ) .,FTS- BRNN,It uses the logistic regression classifier with the features derived by the RNN .,0.5548780487804879,0.4428571428571429,0.4285714285714285,baselines,ablation-analysis
document_classification,10,experimental setup,baselines,93,33,8,NTEE This model is a state - of - the - art model that uses a multi - layer perceptron classifier with the features computed using the embeddings of words and entities trained on Wikipedia using the neural network model proposed in their paper .,It uses the logistic regression classifier with the features derived by the RNN .,"Similar to our previous experiment , we also add SWEM - concat , and the variants of our NABoEentity and NABoE - full models based on Wikifier and TAGME ( see Section 4.2 ) .",0.5670731707317073,0.4714285714285714,0.5714285714285714,baselines,ablation-analysis
document_classification,10,experimental setup,results,100,40,1,Results, , ,0.6097560975609756,0.5714285714285714,0.1,results,baselines
document_classification,10,experimental setup,results,101,41,2,"Relative to the baselines , our models yielded enhanced over all performance on both datasets .", ,The NABoE - full model outperformed all baseline models in terms of both measures on both datasets .,0.6158536585365854,0.5857142857142857,0.2,results,approach
document_classification,10,experimental setup,results,102,42,3,The NABoE - full model outperformed all baseline models in terms of both measures on both datasets .,"Relative to the baselines , our models yielded enhanced over all performance on both datasets .","Furthermore , the NABoE-entity model outperformed all the baseline models in terms of both measures on the 20NG dataset , and the F 1 score on the R8 dataset .",0.6219512195121951,0.6,0.3,results,baselines
document_classification,10,experimental setup,results,103,43,4,"Furthermore , the NABoE-entity model outperformed all the baseline models in terms of both measures on the 20NG dataset , and the F 1 score on the R8 dataset .",The NABoE - full model outperformed all baseline models in terms of both measures on both datasets .,"Moreover , our attention mechanism consistently improved the performance .",0.6280487804878049,0.6142857142857143,0.4,results,baselines
document_classification,11,title,title,2,2,2,Task - oriented Word Embedding for Text Classification, , ,0.0095238095238095,1.0,1.0,research-problem,approach
document_classification,11,introduction,introduction,30,14,14,"In this paper , we propose a task - oriented word embedding method ( denoted as ToWE ) to solve the aforementioned problem .","Apparently , using word embedding directly from Word2 Vec would not obtain good performance on the text classification task due to the fact that words ' functional features in the real tasks are ignored in the training process .",It learns the distributed representation of words according to the given specific :,0.1428571428571428,0.4117647058823529,0.4117647058823529,model,experimental-setup
document_classification,11,introduction,introduction,38,22,22,"In our method , the words ' contextual information and task information are inherently jointed to construct the word embeddings .","Specifically , we focus on text classification .","In the joint learning framework , the contextual information is captured following the context prediction task introduced by .",0.1809523809523809,0.6470588235294118,0.6470588235294118,model,experimental-setup
document_classification,11,introduction,introduction,40,24,24,"To model the task information , we regularize the distribution of the salient words to have a clear classification boundary , and then adjust the distribution of the other words in the embedding space correspondingly .","In the joint learning framework , the contextual information is captured following the context prediction task introduced by .","To give an intuitive understanding on how our method works from the classification perspective , we design a 5 Abstracts",0.1904761904761904,0.7058823529411765,0.7058823529411765,model,approach
document_classification,11,experiments,datasets,146,18,17,"To evaluate our method , we consider the following baselines : ( 1 ) the BOW method is employed as a basic baseline .",Baseline Methods,It represents each document as a bag of words and the weighting scheme is TFIDF .,0.6952380952380952,0.2222222222222222,0.2125,baselines,ablation-analysis
document_classification,11,experiments,datasets,147,19,18,It represents each document as a bag of words and the weighting scheme is TFIDF .,"To evaluate our method , we consider the following baselines : ( 1 ) the BOW method is employed as a basic baseline .","We select the top 2,000 words according to the TFIDF scores as features ;",0.7,0.2345679012345679,0.225,baselines,approach
document_classification,11,experiments,datasets,149,21,20,( 2 ) the Word2 Vec method is a neural network language method which learns word embeddings by maximizing the conditional probability leveraging contextual information .,"We select the top 2,000 words according to the TFIDF scores as features ;","It comprises two models , i.e. , CBOW which predicts the target word using context information , and the Skip - gram ( denoted as SG ) which predicts each context word using the target word ; ( 3 ) the Glo Ve method is a state - of - the - art matrix factorization method .",0.7095238095238096,0.2592592592592592,0.25,baselines,experimental-setup
document_classification,11,experiments,datasets,183,55,54,"( 1 ) Our method performs better than the other methods , and are proved to be highly reliable for the text classification task .","Based on the experiment results , we make several observations :","In particular , the ToWE - SG method significantly outperforms the other baselines on the 20 New s Group , 5 Abstract s Group , and MR .",0.8714285714285714,0.6790123456790124,0.675,results,approach
document_classification,11,experiments,datasets,184,56,55,"In particular , the ToWE - SG method significantly outperforms the other baselines on the 20 New s Group , 5 Abstract s Group , and MR .","( 1 ) Our method performs better than the other methods , and are proved to be highly reliable for the text classification task .","This is mainly attributed to the task - specific modeling mechanism , which enables our models to capture functional features among words , therefore , it can more accurately distinguish classes .",0.8761904761904762,0.6913580246913579,0.6875,results,baselines
document_classification,11,experiments,datasets,186,58,57,"( 2 ) The word embedding methods outperform the basic bag - of - words methods in most cases , indicating the superiority of distributed word representation over the one - hot representation .","This is mainly attributed to the task - specific modeling mechanism , which enables our models to capture functional features among words , therefore , it can more accurately distinguish classes .","Moreover , the manager ( Business ) layer ( AI ) congress ( Law ) poverty ( Sociology ) accident methods which integrate the abundant information discovered from the datasets ( i.e. , TWE and ToWE ) achieve better performance compared to those that only consider contextual information , such as GloVe , CBOW , and SG .",0.8857142857142857,0.7160493827160493,0.7125,results,baselines
document_classification,11,experiments,datasets,190,62,61,"Our method achieves better performance over Retrofit method , indicating that the task - specific features could be more effective compared with general semantic relations constructed by humans in the knowledge bases .",( 3 ) The Retrofit method is the knowledge - base enhanced word embedding method .,"( 4 ) In sentence classification , such as the MR and SST datasets , it is obvious that TWE achieves a relatively lower performance .",0.9047619047619048,0.7654320987654321,0.7625,results,approach
document_classification,11,experiments,datasets,193,65,64,"Our method outperforms the TWE method on both the document - level and sentence - level tasks , which shows the stability and reliability of modeling taskspecific features in real - world applications .",This observation shows that topical information enhanced word embedding does not accurately represent a short text .,Case Study,0.919047619047619,0.8024691358024691,0.8,results,baselines
document_classification,12,title,title,2,2,2,Graph Convolutional Networks for Text Classification, , ,0.009009009009009,1.0,1.0,research-problem,experimental-setup
document_classification,12,introduction,introduction,22,10,10,"In this work , we propose a new graph neural networkbased method for text classification .",Graph neural networks have been effective at tasks thought to have rich relational structure and can preserve global structure information of a graph in graph embeddings .,"We construct a single large graph from an entire corpus , which contains words and documents as nodes .",0.0990990990990991,0.1041666666666666,0.1492537313432835,model,experimental-setup
document_classification,12,introduction,introduction,23,11,11,"We construct a single large graph from an entire corpus , which contains words and documents as nodes .","In this work , we propose a new graph neural networkbased method for text classification .","We model the graph with a Graph Convolutional Network ( GCN ) , a simple and effective graph neural network that captures high order neighborhoods information .",0.1036036036036036,0.1145833333333333,0.1641791044776119,model,experimental-setup
document_classification,12,introduction,introduction,24,12,12,"We model the graph with a Graph Convolutional Network ( GCN ) , a simple and effective graph neural network that captures high order neighborhoods information .","We construct a single large graph from an entire corpus , which contains words and documents as nodes .",The edge between two word nodes is built byword co-occurrence information and the edge between a word node and document node is built using word frequency and word 's document frequency .,0.1081081081081081,0.125,0.1791044776119403,model,ablation-analysis
document_classification,12,introduction,introduction,25,13,13,The edge between two word nodes is built byword co-occurrence information and the edge between a word node and document node is built using word frequency and word 's document frequency .,"We model the graph with a Graph Convolutional Network ( GCN ) , a simple and effective graph neural network that captures high order neighborhoods information .",We then turn text classification problem into anode classification problem .,0.1126126126126126,0.1354166666666666,0.1940298507462686,model,experimental-setup
document_classification,12,introduction,introduction,26,14,14,We then turn text classification problem into anode classification problem .,The edge between two word nodes is built byword co-occurrence information and the edge between a word node and document node is built using word frequency and word 's document frequency .,The method can achieve strong classification performances with a small proportion of labeled documents and learn interpretable word and document node embeddings .,0.1171171171171171,0.1458333333333333,0.2089552238805969,model,approach
document_classification,12,introduction,introduction,28,16,16,Our source code is available at https://github. com/yao8839836/text_gcn .,The method can achieve strong classification performances with a small proportion of labeled documents and learn interpretable word and document node embeddings .,"To summarize , our contributions are as follows :",0.1261261261261261,0.1666666666666666,0.2388059701492537,code,baselines
document_classification,12,experiment,experiment,114,6,6,Baselines .,Can our model learn predictive word and document embeddings ?,We compare our Text GCN with multiple stateof - the - art text classification and embedding methods as follows :,0.5135135135135135,0.0526315789473684,0.0526315789473684,baselines,baselines
document_classification,12,experiment,experiment,115,7,7,We compare our Text GCN with multiple stateof - the - art text classification and embedding methods as follows :,Baselines .,TF - IDF + LR : bag - of - words model with term frequencyinverse document frequency weighting .,0.5180180180180181,0.0614035087719298,0.0614035087719298,baselines,baselines
document_classification,12,experiment,experiment,116,8,8,TF - IDF + LR : bag - of - words model with term frequencyinverse document frequency weighting .,We compare our Text GCN with multiple stateof - the - art text classification and embedding methods as follows :,Logistic Regression is used as the classifier .,0.5225225225225225,0.0701754385964912,0.0701754385964912,baselines,baselines
document_classification,12,experiment,experiment,117,9,9,Logistic Regression is used as the classifier .,TF - IDF + LR : bag - of - words model with term frequencyinverse document frequency weighting .,CNN : Convolutional Neural Network ( Kim 2014 ) .,0.527027027027027,0.0789473684210526,0.0789473684210526,baselines,approach
document_classification,12,experiment,experiment,118,10,10,CNN : Convolutional Neural Network ( Kim 2014 ) .,Logistic Regression is used as the classifier .,We explored CNN -rand which uses randomly initialized word embeddings and CNN - non- static which uses pre-trained word embeddings .,0.5315315315315315,0.087719298245614,0.087719298245614,baselines,baselines
document_classification,12,experiment,experiment,119,11,11,We explored CNN -rand which uses randomly initialized word embeddings and CNN - non- static which uses pre-trained word embeddings .,CNN : Convolutional Neural Network ( Kim 2014 ) .,LSTM : The LSTM model defined in which uses the last hidden state as the representation of the whole text .,0.536036036036036,0.0964912280701754,0.0964912280701754,baselines,approach
document_classification,12,experiment,experiment,120,12,12,LSTM : The LSTM model defined in which uses the last hidden state as the representation of the whole text .,We explored CNN -rand which uses randomly initialized word embeddings and CNN - non- static which uses pre-trained word embeddings .,We also experimented with the model with / without pre-trained word embeddings .,0.5405405405405406,0.1052631578947368,0.1052631578947368,baselines,experimental-setup
document_classification,12,experiment,experiment,122,14,14,"Bi- LSTM : a bi-directional LSTM , commonly used in text classification .",We also experimented with the model with / without pre-trained word embeddings .,We input pre-trained word embeddings to Bi - LSTM .,0.5495495495495496,0.1228070175438596,0.1228070175438596,baselines,baselines
document_classification,12,experiment,experiment,124,16,16,"PV - DBOW : a paragraph vector model proposed by , the orders of words in text are ignored .",We input pre-trained word embeddings to Bi - LSTM .,We used Logistic Regression as the classifier .,0.5585585585585585,0.1403508771929824,0.1403508771929824,baselines,experimental-setup
document_classification,12,experiment,experiment,125,17,17,We used Logistic Regression as the classifier .,"PV - DBOW : a paragraph vector model proposed by , the orders of words in text are ignored .","PV - DM : a paragraph vector model proposed by , which considers the word order .",0.5630630630630631,0.1491228070175438,0.1491228070175438,baselines,approach
document_classification,12,experiment,experiment,126,18,18,"PV - DM : a paragraph vector model proposed by , which considers the word order .",We used Logistic Regression as the classifier .,We used Logistic Regression as the classifier .,0.5675675675675675,0.1578947368421052,0.1578947368421052,baselines,model
document_classification,12,experiment,experiment,127,19,19,We used Logistic Regression as the classifier .,"PV - DM : a paragraph vector model proposed by , which considers the word order .","PTE : predictive text embedding , which firstly learns word embedding based on heterogeneous text network containing words , documents and labels as nodes , then averages word embeddings as document embeddings for text classification .",0.5720720720720721,0.1666666666666666,0.1666666666666666,baselines,approach
document_classification,12,experiment,experiment,128,20,20,"PTE : predictive text embedding , which firstly learns word embedding based on heterogeneous text network containing words , documents and labels as nodes , then averages word embeddings as document embeddings for text classification .",We used Logistic Regression as the classifier .,"fast Text : a simple and efficient text classification method , which treats the average of word / n- grams embeddings as document embeddings , then feeds document embeddings into a linear classifier .",0.5765765765765766,0.175438596491228,0.175438596491228,baselines,approach
document_classification,12,experiment,experiment,129,21,21,"fast Text : a simple and efficient text classification method , which treats the average of word / n- grams embeddings as document embeddings , then feeds document embeddings into a linear classifier .","PTE : predictive text embedding , which firstly learns word embedding based on heterogeneous text network containing words , documents and labels as nodes , then averages word embeddings as document embeddings for text classification .",We evaluated it with and without bigrams .,0.581081081081081,0.1842105263157894,0.1842105263157894,baselines,approach
document_classification,12,experiment,experiment,131,23,23,"SWEM : simple word embedding models , which employs simple pooling strategies operated over word embeddings .",We evaluated it with and without bigrams .,"LEAM : label - embedding attentive models , which embeds the words and labels in the same joint space for text classification .",0.5900900900900901,0.2017543859649122,0.2017543859649122,baselines,approach
document_classification,12,experiment,experiment,132,24,24,"LEAM : label - embedding attentive models , which embeds the words and labels in the same joint space for text classification .","SWEM : simple word embedding models , which employs simple pooling strategies operated over word embeddings .",It utilizes label descriptions .,0.5945945945945946,0.2105263157894736,0.2105263157894736,baselines,approach
document_classification,12,experiment,experiment,133,25,25,It utilizes label descriptions .,"LEAM : label - embedding attentive models , which embeds the words and labels in the same joint space for text classification .","Graph - CNN - C : a graph CNN model that operates convolutions over word embedding similarity graphs ( Defferrard , Bresson , and Vandergheynst 2016 ) , in which Chebyshev filter is used .",0.5990990990990991,0.219298245614035,0.219298245614035,baselines,approach
document_classification,12,experiment,experiment,134,26,26,"Graph - CNN - C : a graph CNN model that operates convolutions over word embedding similarity graphs ( Defferrard , Bresson , and Vandergheynst 2016 ) , in which Chebyshev filter is used .",It utilizes label descriptions .,Graph - CNN - S : the same as Graph - CNN - C but using Spline filter ) .,0.6036036036036037,0.2280701754385965,0.2280701754385965,baselines,baselines
document_classification,12,experiment,experiment,135,27,27,Graph - CNN - S : the same as Graph - CNN - C but using Spline filter ) .,"Graph - CNN - C : a graph CNN model that operates convolutions over word embedding similarity graphs ( Defferrard , Bresson , and Vandergheynst 2016 ) , in which Chebyshev filter is used .",Graph - CNN - F : the same as Graph - CNN - C but using Fourier filter .,0.6081081081081081,0.2368421052631579,0.2368421052631579,baselines,experimental-setup
document_classification,12,experiment,experiment,136,28,28,Graph - CNN - F : the same as Graph - CNN - C but using Fourier filter .,Graph - CNN - S : the same as Graph - CNN - C but using Spline filter ) .,Datasets .,0.6126126126126126,0.2456140350877193,0.2456140350877193,baselines,experimental-setup
document_classification,12,experiment,experiment,155,47,47,"For Text GCN , we set the embedding size of the first convolution layer as 200 and set the window size as 20 .",Settings .,We also experimented with other settings and found that small changes did not change the results much .,0.6981981981981982,0.412280701754386,0.412280701754386,hyperparameters,experimental-setup
document_classification,12,experiment,experiment,157,49,49,"We tuned other parameters and set the learning rate as 0.02 , dropout For baseline models , we used default parameter settings as in their original papers or implementations .",We also experimented with other settings and found that small changes did not change the results much .,"For baseline models using pre-trained word embeddings , we used 300 dimensional Glo Ve word embeddings ( Pennington , Socher , and Manning 2014 )",0.7072072072072072,0.4298245614035088,0.4298245614035088,hyperparameters,approach
document_classification,12,experiment,experiment,158,50,50,"For baseline models using pre-trained word embeddings , we used 300 dimensional Glo Ve word embeddings ( Pennington , Socher , and Manning 2014 )","We tuned other parameters and set the learning rate as 0.02 , dropout For baseline models , we used default parameter settings as in their original papers or implementations .",7 .,0.7117117117117117,0.4385964912280701,0.4385964912280701,hyperparameters,baselines
document_classification,12,experiment,experiment,161,53,53,"Text GCN performs the best and significantly outperforms all baseline models ( p < 0.05 based on student t- test ) on four datasets , which showcases the effectiveness of the proposed method on long text datasets .",Test Performance . presents test accuracy of each model .,"For more in - depth performance analysis , we note that TF - IDF + LR performs well on long text datasets like 20 NG and can outperform CNN with randomly initialized word embeddings .",0.7252252252252253,0.4649122807017544,0.4649122807017544,results,baselines
document_classification,12,experiment,experiment,163,55,55,"When pre-trained Glo Ve word embeddings are provided , CNN performs much better , especially on Ohsumed and 20 NG .","For more in - depth performance analysis , we note that TF - IDF + LR performs well on long text datasets like 20 NG and can outperform CNN with randomly initialized word embeddings .","CNN also achieves the best results on short text dataset MR with pre-trained word embeddings , which shows it can 7 http://nlp.stanford.edu/data/glove.6B.zip model consecutive and short - distance semantics well .",0.7342342342342343,0.4824561403508772,0.4824561403508772,results,baselines
document_classification,12,experiment,experiment,165,57,57,"Similarly , LSTM - based models also rely on pre-trained word embeddings and tend to perform better when documents are shorter .","CNN also achieves the best results on short text dataset MR with pre-trained word embeddings , which shows it can 7 http://nlp.stanford.edu/data/glove.6B.zip model consecutive and short - distance semantics well .","PV - DBOW achieves comparable results to strong baselines on 20 NG and Ohsumed , but the results on shorter text are clearly inferior to others .",0.7432432432432432,0.5,0.5,results,baselines
document_classification,12,experiment,experiment,166,58,58,"PV - DBOW achieves comparable results to strong baselines on 20 NG and Ohsumed , but the results on shorter text are clearly inferior to others .","Similarly , LSTM - based models also rely on pre-trained word embeddings and tend to perform better when documents are shorter .",This is likely due to the fact that word orders are important in short text or sentiment classification .,0.7477477477477478,0.5087719298245614,0.5087719298245614,results,baselines
document_classification,12,experiment,experiment,168,60,60,"PV - DM performs worse than PV - DBOW , the only comparable results are on MR , where word orders are more essential .",This is likely due to the fact that word orders are important in short text or sentiment classification .,The results of PV - DBOW and PV - DM indicate that unsupervised document embeddings are not very discriminative in text classification .,0.7567567567567568,0.5263157894736842,0.5263157894736842,results,baselines
document_classification,12,experiment,experiment,172,64,64,Graph - CNN models also show competitive performances .,"The two recent methods SWEM and LEAM perform quite well , which demonstrates the effectiveness of simple pooling methods and label descriptions / embeddings .","This suggests that building word similarity graph using pretrained word embeddings can preserve syntactic and semantic relations among words , which can provide additional information in large external text data .",0.7747747747747747,0.5614035087719298,0.5614035087719298,results,approach
document_classification,13,title,title,2,2,2,Deep Pyramid Convolutional Neural Networks for Text Categorization, , ,0.0088495575221238,1.0,1.0,research-problem,experimental-setup
document_classification,13,introduction,introduction,29,19,19,"We call it deep pyramid CNN ( DPCNN ) , as the computation time per layer decreases exponentially in a ' pyramid shape ' .",We carefully studied deepening of word - level CNNs in the large - data setting and found a deep but low - complexity network architecture with which the best accuracy can be obtained by increasing the depth but not the order of computation time - the total computation time is bounded by a constant .,"After converting discrete text to continuous representation , the DPCNN architecture simply alternates a convolution block and a downsampling layer over and over 1 , leading to a deep network in which internal data size ( as well as per-layer computation ) shrinks in a pyramid shape .",0.1283185840707964,0.2,0.5277777777777778,model,experimental-setup
document_classification,13,introduction,introduction,30,20,20,"After converting discrete text to continuous representation , the DPCNN architecture simply alternates a convolution block and a downsampling layer over and over 1 , leading to a deep network in which internal data size ( as well as per-layer computation ) shrinks in a pyramid shape .","We call it deep pyramid CNN ( DPCNN ) , as the computation time per layer decreases exponentially in a ' pyramid shape ' .",The network depth can be treated as a meta-parameter .,0.1327433628318584,0.2105263157894736,0.5555555555555556,model,ablation-analysis
document_classification,13,introduction,introduction,31,21,21,The network depth can be treated as a meta-parameter .,"After converting discrete text to continuous representation , the DPCNN architecture simply alternates a convolution block and a downsampling layer over and over 1 , leading to a deep network in which internal data size ( as well as per-layer computation ) shrinks in a pyramid shape .",The computational complexity of this network is bounded to be no more than twice that of one convolution block .,0.1371681415929203,0.2210526315789473,0.5833333333333334,model,approach
document_classification,13,introduction,introduction,32,22,22,The computational complexity of this network is bounded to be no more than twice that of one convolution block .,The network depth can be treated as a meta-parameter .,"At the same time , as described later , the ' pyramid ' enables efficient discovery of long - range associations in the text ( and so more global information ) , as the network is deepened .",0.1415929203539823,0.231578947368421,0.6111111111111112,model,approach
document_classification,13,introduction,introduction,36,26,26,We show that DPCNN with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic classification ..,"Moreover , DPCNN can be regarded as a deep extension of ShallowCNN , which we proposed in and later tested with large datasets in .","The first layer performs text region embedding , which generalizes commonly used word embedding to the embedding of text regions covering one or more words .",0.1592920353982301,0.2736842105263158,0.7222222222222222,model,approach
document_classification,13,introduction,introduction,37,27,27,"The first layer performs text region embedding , which generalizes commonly used word embedding to the embedding of text regions covering one or more words .",We show that DPCNN with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic classification ..,It is followed by stacking of convolution blocks ( two convolution layers and a shortcut ) interleaved with pooling layers with stride 2 for downsampling .,0.163716814159292,0.2842105263157895,0.75,model,model
document_classification,13,introduction,introduction,38,28,28,It is followed by stacking of convolution blocks ( two convolution layers and a shortcut ) interleaved with pooling layers with stride 2 for downsampling .,"The first layer performs text region embedding , which generalizes commonly used word embedding to the embedding of text regions covering one or more words .",The final pooling layer aggregates internal data for each document into one vector .,0.168141592920354,0.2947368421052631,0.7777777777777778,model,model
document_classification,13,introduction,introduction,39,29,29,The final pooling layer aggregates internal data for each document into one vector .,It is followed by stacking of convolution blocks ( two convolution layers and a shortcut ) interleaved with pooling layers with stride 2 for downsampling .,We use max pooling for all pooling layers .,0.1725663716814159,0.3052631578947368,0.8055555555555556,model,model
document_classification,13,introduction,introduction,40,30,30,We use max pooling for all pooling layers .,The final pooling layer aggregates internal data for each document into one vector .,The key features of DPCNN are as follows .,0.1769911504424778,0.3157894736842105,0.8333333333333334,model,experimental-setup
document_classification,13,experiments,results,150,45,1,Results, , ,0.6637168141592921,0.3846153846153846,0.0136986301369863,results,baselines
document_classification,13,experiments,results,155,50,6,Large data results,Main results,We first report the error rates of our full model ( DPCNN with 15 weight layers plus unsupervised embeddings ) on the larger five datasets .,0.6858407079646017,0.4273504273504273,0.0821917808219178,results,approach
document_classification,13,experiments,results,159,54,10,"On all the five datasets , DPCNN outperforms all of the previous results , which validates the effectiveness of our approach .",The previous results are roughly sorted in the order of error rates from best to worst .,"DPCNN can be regarded as a deep extension of ShallowCNN ( row 2 ) , sharing region embedding enhancement with diverse unsupervised embeddings .",0.7035398230088495,0.4615384615384616,0.136986301369863,results,baselines
document_classification,13,experiments,results,191,86,42,Small data results,"is on Yelp.f , and we observed the same performance trend on the other four large datasets .",Now we turn to the results on the three smaller datasets in .,0.8451327433628318,0.7350427350427351,0.5753424657534246,results,approach
document_classification,13,experiments,results,194,89,45,"For these small datasets , the DPCNN performances with 100 - dim unsupervised embed - dings are shown , which turned out to be as good as those with 300 - dim unsupervised embeddings .","Again , the previous models are roughly sorted from best to worst .",One difference from the large dataset results is that the strength of shallow models stands out .,0.8584070796460177,0.7606837606837606,0.6164383561643836,results,baselines
document_classification,13,experiments,results,196,91,47,"ShallowCNN ( row 2 ) rivals DPCNN ( row 1 ) , and Zhang et al. 's best linear model ( row 3 ) moved up from the worst performer to the third best performer .",One difference from the large dataset results is that the strength of shallow models stands out .,"The results are inline with the general fact that more complex models require more training data , and with the paucity of training data , simpler models can outperform more complex ones .",0.8672566371681416,0.7777777777777778,0.6438356164383562,results,baselines
document_classification,14,title,title,2,2,2,Supervised and Semi- Supervised Text Categorization using LSTM for Region Embeddings, , ,0.0078125,1.0,1.0,research-problem,experimental-setup
document_classification,14,abstract,abstract,4,2,2,"One - hot CNN ( convolutional neural network ) has been shown to be effective for text categorization ( Johnson & Zhang , 2015a ; b ) .", ,We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of ' text region embedding + pooling ' .,0.015625,0.2222222222222222,0.2222222222222222,research-problem,baselines
document_classification,14,introduction,introduction,31,20,20,"In this work , we build on the general framework of ' region embedding + pooling ' and explore a more sophisticated region embedding via Long Short - Term Memory ( LSTM ) , seeking to overcome the shortcomings above , in the supervised and semi-supervised settings .","This enables a larger region , but at the expense of losing word order in the region and so its use maybe limited .",LSTM ) is a recurrent neural network .,0.12109375,0.3125,0.5714285714285714,model,ablation-analysis
document_classification,14,introduction,introduction,35,24,24,It is designed to enable learning of dependencies over larger time lags than feasible with traditional recurrent networks .,"1 . Therefore , the output from each time step can be regarded as the embedding of the sequence of words that have been seen so far ( or a relevant part of it ) .","That is , an LSTM can be used to embed text regions of variable ( and possibly large ) sizes .",0.13671875,0.375,0.6857142857142857,model,experimental-setup
document_classification,14,introduction,introduction,36,25,25,"That is , an LSTM can be used to embed text regions of variable ( and possibly large ) sizes .",It is designed to enable learning of dependencies over larger time lags than feasible with traditional recurrent networks .,"We pursue the best use of LSTM for our purpose , and then compare the resulting model with the previous best methods including one - hot CNN and previous LSTM .",0.140625,0.390625,0.7142857142857143,model,approach
document_classification,14,introduction,introduction,38,27,27,"Our strategy is to simplify the model as much as possible , including elimination of a word embedding layer routinely used to produce input to LSTM .","We pursue the best use of LSTM for our purpose , and then compare the resulting model with the previous best methods including one - hot CNN and previous LSTM .",Our findings are threefold .,0.1484375,0.421875,0.7714285714285715,model,ablation-analysis
document_classification,14,introduction,introduction,46,35,35,Our code and experimental details are available at http://riejohnson.com/cnn download.html .,We report performances exceeding the previous best results on four benchmark datasets ., ,0.1796875,0.546875,1.0,code,baselines
document_classification,14,supervised lstm for text categorization,experiments supervised,124,49,1,Experiments ( supervised ), , ,0.484375,0.5632183908045977,0.0256410256410256,experiments,baselines
document_classification,14,supervised lstm for text categorization,experiments supervised,135,60,12,Optimization was done with SGD with mini-batch size 50 or 100 with momentum or optionally rmsprop for acceleration .,zero mean and standard deviation 0.01 .,"Hyper parameters such as learning rates were chosen based on the performance on the development data , which was a held - out portion of the training data , and training was redone using all the training data with the chosen parameters .",0.52734375,0.6896551724137931,0.3076923076923077,experiments,approach
document_classification,14,supervised lstm for text categorization,experiments supervised,139,64,16,"Comparing the two types of LSTM in , we see that our one - hot bidirectional LSTM with pooling ( oh - 2 LSTMp ) outperforms word - vector LSTM ( wv - LSTM ) on all the datasets , confirming the effectiveness of our approach .","The pooling settings chosen based on the performance on the development data are the same as JZ15a , which are max - pooling with k= 1 on IMDB and Elec and average - pooling with k=10 on RCV1 ; on 20 NG , max - pooling with k = 10 was chosen .",Now we review the non -LSTM baseline methods .,0.54296875,0.7356321839080459,0.4102564102564102,experiments,baselines
document_classification,14,supervised lstm for text categorization,experiments supervised,140,65,17,Now we review the non -LSTM baseline methods .,"Comparing the two types of LSTM in , we see that our one - hot bidirectional LSTM with pooling ( oh - 2 LSTMp ) outperforms word - vector LSTM ( wv - LSTM ) on all the datasets , confirming the effectiveness of our approach .",The last row of shows the best one - hot CNN results within the constraints above .,0.546875,0.7471264367816092,0.4358974358974359,experiments,experimental-setup
document_classification,14,supervised lstm for text categorization,experiments supervised,143,68,20,"In , on three out of the four datasets , oh - 2 LSTMp outperforms SVM and the CNN .","They were obtained by bow - CNN ( whose input to the embedding function is a bow vector of the region ) with region size 20 on RCV1 , and seq -CNN ( with the regular concatenation input ) with region size 3 on the others .","However , on RCV1 , it underperforms both .",0.55859375,0.7816091954022989,0.5128205128205128,experiments,baselines
document_classification,14,supervised lstm for text categorization,experiments supervised,147,72,24,"Only on RCV1 , n-gram SVM is no better than bag - of - word SVM , and only on RCV1 , bow - CNN outperforms seq-CNN .",This point can also be observed in the SVM and CNN performances .,"That is , on RCV1 , bags of words in a window of 20 at every location are more useful than words in strict order .",0.57421875,0.8275862068965517,0.6153846153846154,experiments,baselines
document_classification,14,supervised lstm for text categorization,experiments supervised,158,83,35,"Overall , one - hot CNN works surprising well considering its simplicity , and this observation motivates the idea of combining the two types of region embeddings , discussed later .",This maybe because the amount of training data is not sufficient enough to learn the relevance of longer word sequences .,Comparison with the previous best results on 20 NG,0.6171875,0.9540229885057472,0.8974358974358975,experiments,baselines
document_classification,14,supervised lstm for text categorization,experiments supervised,160,85,37,"The previous best performance on 20NG is 15.3 ( not shown in the table ) of DL15 , obtained by pre-training wv - LSTM of 1024 units with labeled training data .",Comparison with the previous best results on 20 NG,"Our oh - 2 LSTMp achieved 13.32 , which is 2 % better .",0.625,0.9770114942528736,0.9487179487179488,experiments,baselines
document_classification,14,supervised lstm for text categorization,experiments supervised,161,86,38,"Our oh - 2 LSTMp achieved 13.32 , which is 2 % better .","The previous best performance on 20NG is 15.3 ( not shown in the table ) of DL15 , obtained by pre-training wv - LSTM of 1024 units with labeled training data .","The previous best results on the other datasets use unlabeled data , and we will review them with our semi-supervised results .",0.62890625,0.9885057471264368,0.9743589743589745,experiments,baselines
document_classification,14,semi supervised lstm,semi supervised experiments,195,33,1,Semi-supervised experiments, , ,0.76171875,0.3928571428571429,0.0222222222222222,experiments,approach
document_classification,14,semi supervised lstm,semi supervised experiments,210,48,16,"Although the pre-trained wv - LSTM clearly outperformed the supervised wv - LSTM , it underperformed the models with region tv-embeddings .","We used the same configuration on Elec as DL15 ; however , on RCV1 , which has 55 classes , 30 hidden units turned out to be too few and we changed it to 1000 .","Previous studies on LSTM for text often convert words into pre-trained word vectors , and word2vec is a popular choice for this purpose .",0.8203125,0.5714285714285714,0.3555555555555556,experiments,baselines
document_classification,14,semi supervised lstm,semi supervised experiments,216,54,22,"On our tasks , wv - 2 LSTMp using the Google News vectors ( row # 2 ) performed relatively poorly .",The Google News word vectors were trained by word2vec on a huge ( 100 billion - word ) news corpus and are provided publicly .,"When word2vec was trained with the domain unlabeled data , better results were observed after we scaled word vectors appropriately ) .",0.84375,0.6428571428571429,0.4888888888888889,experiments,baselines
document_classification,14,semi supervised lstm,semi supervised experiments,220,58,26,"Now we review the performance of one - hot CNN with one 200 - dim CNN tv-embedding row # 5 ) , which is comparable with our LSTM with two 100 - dim LSTM tv-embeddings ( row # 4 ) in terms of the dimensionality of tv-embeddings .","We attribute the superiority of the models with tv-embeddings to the fact that they learn , from unlabeled data , embeddings of text regions , which can convey higher - level concepts than single words in isolation .",The LSTM ( row # 4 ) rivals or outperforms the CNN ( row # 5 ) on IMDB / Elec but underperforms it on RCV1 .,0.859375,0.6904761904761905,0.5777777777777777,experiments,baselines
document_classification,14,semi supervised lstm,semi supervised experiments,221,59,27,The LSTM ( row # 4 ) rivals or outperforms the CNN ( row # 5 ) on IMDB / Elec but underperforms it on RCV1 .,"Now we review the performance of one - hot CNN with one 200 - dim CNN tv-embedding row # 5 ) , which is comparable with our LSTM with two 100 - dim LSTM tv-embeddings ( row # 4 ) in terms of the dimensionality of tv-embeddings .","Increasing the dimensionality of LSTM tvembeddings from 100 to 300 on RCV1 , we obtain 8.62 , but it still does not reach 7.97 of the CNN .",0.86328125,0.7023809523809523,0.6,experiments,baselines
document_classification,14,semi supervised lstm,semi supervised experiments,222,60,28,"Increasing the dimensionality of LSTM tvembeddings from 100 to 300 on RCV1 , we obtain 8.62 , but it still does not reach 7.97 of the CNN .",The LSTM ( row # 4 ) rivals or outperforms the CNN ( row # 5 ) on IMDB / Elec but underperforms it on RCV1 .,"As discussed earlier , we attribute the superiority of one - hot CNN on RCV1 to its unique way of representing parts of documents via bow input .",0.8671875,0.7142857142857143,0.6222222222222222,experiments,baselines
document_classification,15,title,title,2,2,2,ADVERSARIAL TRAINING METHODS FOR SEMI - SUPERVISED TEXT CLASSIFICATION, , ,0.0098522167487684,1.0,1.0,research-problem,experimental-setup
document_classification,15,introduction,introduction,18,10,10,Previous work has primarily applied adversarial and virtual adversarial training to image classification tasks .,Virtual adversarial training achieves good generalization performance for both supervised and semi-supervised learning tasks .,"In this work , we extend these techniques to text classification tasks and sequence models .",0.0886699507389162,0.4166666666666667,0.4166666666666667,approach,experimental-setup
document_classification,15,introduction,introduction,19,11,11,"In this work , we extend these techniques to text classification tasks and sequence models .",Previous work has primarily applied adversarial and virtual adversarial training to image classification tasks .,Adversarial perturbations typically consist of making small modifications to very many real - valued inputs .,0.0935960591133004,0.4583333333333333,0.4583333333333333,approach,experimental-setup
document_classification,15,introduction,introduction,20,12,12,Adversarial perturbations typically consist of making small modifications to very many real - valued inputs .,"In this work , we extend these techniques to text classification tasks and sequence models .","For text classification , the input is discrete , and usually represented as a series of highdimensional one - hot vectors .",0.0985221674876847,0.5,0.5,approach,ablation-analysis
document_classification,15,introduction,introduction,21,13,13,"For text classification , the input is discrete , and usually represented as a series of highdimensional one - hot vectors .",Adversarial perturbations typically consist of making small modifications to very many real - valued inputs .,"Because the set of high - dimensional one - hot vectors does not admit infinitesimal perturbation , we define the perturbation on continuous word embeddings instead of discrete word inputs .",0.1034482758620689,0.5416666666666666,0.5416666666666666,approach,ablation-analysis
document_classification,15,introduction,introduction,22,14,14,"Because the set of high - dimensional one - hot vectors does not admit infinitesimal perturbation , we define the perturbation on continuous word embeddings instead of discrete word inputs .","For text classification , the input is discrete , and usually represented as a series of highdimensional one - hot vectors .",Traditional adversarial and virtual adversarial training can be interpreted both as a regularization strategy and as defense against an adversary who can supply malicious inputs .,0.1083743842364532,0.5833333333333334,0.5833333333333334,approach,ablation-analysis
document_classification,15,introduction,introduction,25,17,17,We thus propose this approach exclusively as a means of regularizing a text classifier by stabilizing the classification function .,"Since the perturbed embedding does not map to any word and the adversary presumably does not have access to the word embedding layer , our proposed training strategy is no longer intended as a defense against an adversary .","We show that our approach with neural language model unsupervised pretraining as proposed by achieves state of the art performance for multiple semi-supervised text classification tasks , including sentiment classification and topic classification .",0.1231527093596059,0.7083333333333334,0.7083333333333334,approach,approach
document_classification,15,experimental settings,experimental settings,100,2,2,All experiments used TensorFlow on GPUs ., ,Code will be available at https://github.com/tensorflow/models/tree/master/adversarial_text.,0.4926108374384237,0.04,0.0952380952380952,experimental-setup,experiments
document_classification,15,experimental settings,experimental settings,101,3,3,Code will be available at https://github.com/tensorflow/models/tree/master/adversarial_text.,All experiments used TensorFlow on GPUs .,"To compare our method with other text classification methods , we tested on 5 different text datasets .",0.4975369458128079,0.06,0.1428571428571428,code,baselines
document_classification,15,experimental settings,experimental settings,112,14,14,"We trained for 100,000 steps .","There are some duplicated reviews in the original Elec dataset , and we used the dataset with removal of the duplicated reviews , provided by , thus there are slightly fewer examples shown in than the ones in previous works , with batch size 256 , an initial learning rate of 0.001 , and a 0.9999 learning rate exponential decay factor at each training step .",We applied gradient clipping with norm set to 1.0 on all the parameters except word embeddings .,0.5517241379310345,0.28,0.6666666666666666,experimental-setup,baselines
document_classification,15,experimental settings,experimental settings,113,15,15,We applied gradient clipping with norm set to 1.0 on all the parameters except word embeddings .,"We trained for 100,000 steps .","To reduce runtime on GPU , we used truncated backpropagation up to 400 words from each end of the sequence .",0.5566502463054187,0.3,0.7142857142857143,experimental-setup,experiments
document_classification,15,experimental settings,experimental settings,115,17,17,"For regularization of the recurrent language model , we applied dropout on the word embedding layer with 0.5 dropout rate .","To reduce runtime on GPU , we used truncated backpropagation up to 400 words from each end of the sequence .","For the bidirectional LSTM model , we used 512 hidden units LSTM for both the standard order and reversed order sequences , and we used 256 dimensional word embeddings which are shared with both of the LSTMs .",0.5665024630541872,0.34,0.8095238095238095,experimental-setup,approach
document_classification,15,experimental settings,experimental settings,116,18,18,"For the bidirectional LSTM model , we used 512 hidden units LSTM for both the standard order and reversed order sequences , and we used 256 dimensional word embeddings which are shared with both of the LSTMs .","For regularization of the recurrent language model , we applied dropout on the word embedding layer with 0.5 dropout rate .",The other hyperparameters are the same as for the unidirectional LSTM .,0.5714285714285714,0.36,0.8571428571428571,experimental-setup,approach
document_classification,15,results,test performance on imdb dataset and model analysis,163,15,14,We saw that cosine distance on adversarial and virtual adversarial training ( 0.159-0.331 ) were much smaller than ones on the baseline and random perturbation method ( 0.244-0.399 ) .,We also investigated the 15 nearest neighbors to ' great ' and its cosine distances with the trained embeddings .,9.99 % NBSVM- bigrams 8.78 % Paragraph Vectors 7.42 % SA - LSTM 7.24 % One - hot bi -LSTM * 5.94 % The much weaker positive word ' good ' also moved from the 3rd nearest neighbor to the 15th after virtual adversarial training .,0.8029556650246306,0.4545454545454545,0.4516129032258064,results,model
document_classification,15,results,test performance on imdb dataset and model analysis,165,17,16,shows the test performance on the Elec and RCV1 datasets .,9.99 % NBSVM- bigrams 8.78 % Paragraph Vectors 7.42 % SA - LSTM 7.24 % One - hot bi -LSTM * 5.94 % The much weaker positive word ' good ' also moved from the 3rd nearest neighbor to the 15th after virtual adversarial training .,"We can see our proposed method improved test performance on the baseline method and achieved state of the art performance on both datasets , even though the state of the art method uses a combination of CNN and bidirectional LSTM models .",0.8128078817733991,0.5151515151515151,0.5161290322580645,results,baselines
document_classification,15,results,test performance on imdb dataset and model analysis,166,18,17,"We can see our proposed method improved test performance on the baseline method and achieved state of the art performance on both datasets , even though the state of the art method uses a combination of CNN and bidirectional LSTM models .",shows the test performance on the Elec and RCV1 datasets .,Our unidirectional LSTM model improves on the state of the art method and our method with a bidirectional LSTM further improves results on RCV1 .,0.8177339901477833,0.5454545454545454,0.5483870967741935,results,baselines
document_classification,15,results,test performance on imdb dataset and model analysis,167,19,18,Our unidirectional LSTM model improves on the state of the art method and our method with a bidirectional LSTM further improves results on RCV1 .,"We can see our proposed method improved test performance on the baseline method and achieved state of the art performance on both datasets , even though the state of the art method uses a combination of CNN and bidirectional LSTM models .","The reason why the bidirectional models have better performance on the RCV1 dataset would be that , on the RCV1 dataset , there are some very long sentences compared with the other datasets , and the bidirectional model could better handle such long sentences with the shorter dependencies from the reverse order sentences .",0.8226600985221675,0.5757575757575758,0.5806451612903226,results,baselines
document_classification,15,results,test performance on imdb dataset and model analysis,169,21,20,shows test performance on the Rotten Tomatoes dataset .,"The reason why the bidirectional models have better performance on the RCV1 dataset would be that , on the RCV1 dataset , there are some very long sentences compared with the other datasets , and the bidirectional model could better handle such long sentences with the shorter dependencies from the reverse order sentences .","Adversarial training was able to improve over the baseline method , and with both adversarial and virtual adversarial cost , achieved almost the same performance as the current state of the art method .",0.8325123152709359,0.6363636363636364,0.6451612903225806,results,baselines
document_classification,15,results,test performance on imdb dataset and model analysis,170,22,21,"Adversarial training was able to improve over the baseline method , and with both adversarial and virtual adversarial cost , achieved almost the same performance as the current state of the art method .",shows test performance on the Rotten Tomatoes dataset .,However the test performance of only virtual adversarial training was worse than the baseline .,0.8374384236453202,0.6666666666666666,0.6774193548387096,results,baselines
document_classification,15,results,test performance on imdb dataset and model analysis,171,23,22,However the test performance of only virtual adversarial training was worse than the baseline .,"Adversarial training was able to improve over the baseline method , and with both adversarial and virtual adversarial cost , achieved almost the same performance as the current state of the art method .",We speculate that this is because the Rotten Tomatoes dataset has very few labeled sentences and the labeled sentences are very short .,0.8423645320197044,0.696969696969697,0.7096774193548387,results,baselines
document_classification,16,title,title,2,2,2,A C - LSTM Neural Network for Text Classification, , ,0.0091743119266055,1.0,1.0,research-problem,experimental-setup
document_classification,16,introduction,introduction,29,19,19,"In this paper , we introduce a new architecture short for C - LSTM by combining CNN and LSTM to model sentences .",We explore training the LSTM model directly from sequences of higherlevel representaions while preserving the sequence order of these representaions .,"To benefit from the advantages of both CNN and RNN , we design a simple end - to - end , unified architecture by feeding the output of a one - layer CNN into LSTM .",0.1330275229357798,0.7037037037037037,0.7037037037037037,model,experimental-setup
document_classification,16,introduction,introduction,30,20,20,"To benefit from the advantages of both CNN and RNN , we design a simple end - to - end , unified architecture by feeding the output of a one - layer CNN into LSTM .","In this paper , we introduce a new architecture short for C - LSTM by combining CNN and LSTM to model sentences .",The CNN is constructed on top of the pre-trained word vectors from massive unlabeled text data to learn higher - level representions of n-grams .,0.1376146788990825,0.7407407407407407,0.7407407407407407,model,ablation-analysis
document_classification,16,introduction,introduction,31,21,21,The CNN is constructed on top of the pre-trained word vectors from massive unlabeled text data to learn higher - level representions of n-grams .,"To benefit from the advantages of both CNN and RNN , we design a simple end - to - end , unified architecture by feeding the output of a one - layer CNN into LSTM .","Then to learn sequential correlations from higher - level suqence representations , the feature maps of CNN are organized as sequential window features to serve as the input of LSTM .",0.1422018348623853,0.7777777777777778,0.7777777777777778,model,ablation-analysis
document_classification,16,introduction,introduction,32,22,22,"Then to learn sequential correlations from higher - level suqence representations , the feature maps of CNN are organized as sequential window features to serve as the input of LSTM .",The CNN is constructed on top of the pre-trained word vectors from massive unlabeled text data to learn higher - level representions of n-grams .,"In this way , instead of constructing LSTM directly from the input sentence , we first transform each sentence into successive window ( n- gram ) features to help disentangle factors of variations within sentences .",0.146788990825688,0.8148148148148148,0.8148148148148148,model,model
document_classification,16,introduction,introduction,34,24,24,"We choose sequence - based input other than relying on the syntactic parse trees before feeding in the neural network , thus our model does n't rely on any external language knowledge and complicated pre-processing .","In this way , instead of constructing LSTM directly from the input sentence , we first transform each sentence into successive window ( n- gram ) features to help disentangle factors of variations within sentences .","In our experiments , we evaluate the semantic sentence representations learned from C - LSTM with two tasks : sentiment classification and 6 - way question classification .",0.1559633027522936,0.8888888888888888,0.8888888888888888,model,experimental-setup
document_classification,16,experiments,experimental settings,142,20,2,"We implement our model based on Theano ) - a python library , which supports efficient symbolic differentiation and transparent use of a GPU .", ,"To benefit from the efficiency of parallel computation of the tensors , we train the model on a GPU .",0.6513761467889908,0.425531914893617,0.0689655172413793,experimental-setup,approach
document_classification,16,experiments,experimental settings,143,21,3,"To benefit from the efficiency of parallel computation of the tensors , we train the model on a GPU .","We implement our model based on Theano ) - a python library , which supports efficient symbolic differentiation and transparent use of a GPU .","For text preprocessing , we only convert all characters in the dataset to lowercase .",0.6559633027522935,0.4468085106382978,0.1034482758620689,experimental-setup,approach
document_classification,16,experiments,experimental settings,147,25,7,"In our final settings , we only use one convolutional layer and one LSTM layer for both tasks .","For TREC , we holdout 1000 samples from the training dataset for hyperparameter search and train the model using the remaining data .","For the filter size , we investigated filter lengths of 2 , 3 and 4 in two cases : a ) single convolutional layer with the same filter length , and b ) multiple convolutional layers with different lengths of filters in parallel .",0.6743119266055045,0.5319148936170213,0.2413793103448276,experimental-setup,approach
document_classification,16,experiments,experimental settings,167,45,27,"For TREC , the number of filters is set to be 300 and the memory dimension is set to be 300 .",The word vector layer and the LSTM layer are dropped outwith a probability of 0.5 .,The word vector layer and the LSTM layer are dropped outwith a probability of 0.5 .,0.7660550458715596,0.9574468085106383,0.9310344827586208,experimental-setup,approach
document_classification,16,experiments,experimental settings,168,46,28,The word vector layer and the LSTM layer are dropped outwith a probability of 0.5 .,"For TREC , the number of filters is set to be 300 and the memory dimension is set to be 300 .",We also add L2 regularization with a factor of 0.001 to the weights in the softmax layer for both tasks .,0.7706422018348624,0.9787234042553192,0.9655172413793104,experimental-setup,approach
document_classification,16,experiments,experimental settings,169,47,29,We also add L2 regularization with a factor of 0.001 to the weights in the softmax layer for both tasks .,The word vector layer and the LSTM layer are dropped outwith a probability of 0.5 ., ,0.7752293577981652,1.0,1.0,experimental-setup,approach
document_classification,16,results and model analysis,results and model analysis,170,1,1,Results and Model Analysis, , ,0.7798165137614679,0.0232558139534883,0.3333333333333333,results,baselines
document_classification,16,results and model analysis,sentiment classification,173,4,1,Sentiment Classification, , ,0.7935779816513762,0.0930232558139534,0.0555555555555555,results,baselines
document_classification,16,results and model analysis,sentiment classification,184,15,12,"To the best of our knowledge , we achieve the fourth best published result for the 5 - class classification task on this dataset .","For other baseline methods , we compare against SVM with unigram and bigram features , NBoW with average word vector features and paragraph vector that infers the new paragraph vector for unseen documents .","For the binary classification task , we achieve comparable results with respect to the state - of - the - art ones .",0.8440366972477065,0.3488372093023256,0.6666666666666666,results,approach
document_classification,16,results and model analysis,sentiment classification,185,16,13,"For the binary classification task , we achieve comparable results with respect to the state - of - the - art ones .","To the best of our knowledge , we achieve the fourth best published result for the 5 - class classification task on this dataset .","From , we have the following observations :",0.8486238532110092,0.372093023255814,0.7222222222222222,results,approach
document_classification,16,results and model analysis,question type classification,191,22,1,Question Type Classification, , ,0.8761467889908257,0.5116279069767442,0.0909090909090909,results,baselines
document_classification,16,results and model analysis,question type classification,198,29,8,"( 1 ) Our result consistently outperforms all published neural baseline models , which means that C - LSTM captures intentions of TREC questions well .","From , we have the following observations :",( 2 ) Our result is close to that of the state - of - the - art SVM that depends on highly engineered features .,0.908256880733945,0.6744186046511628,0.7272727272727273,results,baselines
document_classification,16,results and model analysis,question type classification,199,30,9,( 2 ) Our result is close to that of the state - of - the - art SVM that depends on highly engineered features .,"( 1 ) Our result consistently outperforms all published neural baseline models , which means that C - LSTM captures intentions of TREC questions well .","Such engineered features not only demands human laboring but also leads to the error propagation in the existing NLP tools , thus could n't generalize well in other datasets and tasks .",0.9128440366972476,0.6976744186046512,0.8181818181818182,results,approach
document_classification,16,results and model analysis,model analysis,206,37,5,"However , we found in our experiments that single convolutional layer with filter length 3 always outperforms the other cases .","Intuitively , multiple convolutional layers in parallel with differ - ent filter sizes should perform better than single convolutional layers with the same length filters in that different filter sizes could exploit features of different n-grams .",We show in the prediction accuracies on the 6 - way question classification task using different filter configurations .,0.9449541284403672,0.8604651162790697,0.4545454545454545,ablation-analysis,approach
document_classification,16,results and model analysis,model analysis,210,41,9,It it shown that single convolutional layer with filter length 3 performs best among all filter configurations .,"For each filter configuration , we report in the best result under extensive grid - search on hyperparameters .","For the case of multiple convolutional layers in parallel , it is shown that filter configurations with filter length 3 performs better that those without tri-gram filters , which further confirms that tri-gram features do play a significant role in capturing local features in our tasks .",0.963302752293578,0.9534883720930232,0.8181818181818182,ablation-analysis,approach
document_classification,16,results and model analysis,model analysis,211,42,10,"For the case of multiple convolutional layers in parallel , it is shown that filter configurations with filter length 3 performs better that those without tri-gram filters , which further confirms that tri-gram features do play a significant role in capturing local features in our tasks .",It it shown that single convolutional layer with filter length 3 performs best among all filter configurations .,We conjecture that LSTM could learn better semantic sentence representations from sequences of tri-gram features .,0.9678899082568808,0.9767441860465116,0.9090909090909092,ablation-analysis,approach
document_classification,17,title,title,2,2,2,Very Deep Convolutional Networks for Text Classification, , ,0.0086580086580086,1.0,1.0,research-problem,experimental-setup
document_classification,17,introduction,introduction,34,26,26,"We believe that a challenge in NLP is to develop deep architectures which are able to learn hierarchical representations of whole sentences , jointly with the task .","Texts have similar properties : characters combine to form n-grams , stems , words , phrase , sentences etc .","In this paper , we propose to use deep architectures of many convolutional layers to approach this goal , using up to 29 layers .",0.1471861471861472,0.7222222222222222,0.7222222222222222,approach,baselines
document_classification,17,introduction,introduction,35,27,27,"In this paper , we propose to use deep architectures of many convolutional layers to approach this goal , using up to 29 layers .","We believe that a challenge in NLP is to develop deep architectures which are able to learn hierarchical representations of whole sentences , jointly with the task .","The design of our architecture is inspired by recent progress in computer vision , in particular .",0.1515151515151515,0.75,0.75,approach,experimental-setup
document_classification,17,experimental evaluation,common model settings,196,23,5,"The dictionary consists of the following characters "" abcdefghijklmnopqrstuvwxyz0123456 789-,;.!?:'"" / | # $ % & *' +=<>( ) [ ]{} "" plus a special padding , space and unknown token which add up to a total of 69 tokens .","Following , all processing is done at the character level which is the atomic representation of a sentence , same as pixels for images .","The input text is padded to a fixed size of 1014 , larger text are truncated .",0.8484848484848485,0.5227272727272727,0.3333333333333333,experimental-setup,model
document_classification,17,experimental evaluation,common model settings,197,24,6,"The input text is padded to a fixed size of 1014 , larger text are truncated .","The dictionary consists of the following characters "" abcdefghijklmnopqrstuvwxyz0123456 789-,;.!?:'"" / | # $ % & *' +=<>( ) [ ]{} "" plus a special padding , space and unknown token which add up to a total of 69 tokens .",The character embedding is of size 16 .,0.8528138528138528,0.5454545454545454,0.4,experimental-setup,model
document_classification,17,experimental evaluation,common model settings,198,25,7,The character embedding is of size 16 .,"The input text is padded to a fixed size of 1014 , larger text are truncated .","Training is performed with SGD , using a mini-batch of size 128 , an initial learning rate of 0.01 and momentum of 0.9 .",0.8571428571428571,0.5681818181818182,0.4666666666666667,experimental-setup,experiments
document_classification,17,experimental evaluation,common model settings,199,26,8,"Training is performed with SGD , using a mini-batch of size 128 , an initial learning rate of 0.01 and momentum of 0.9 .",The character embedding is of size 16 .,We follow the same training procedure as in .,0.8614718614718615,0.5909090909090909,0.5333333333333333,experimental-setup,baselines
document_classification,17,experimental evaluation,common model settings,204,31,13,The implementation is done using Torch 7 .,It took between 10 to 15 epoches to converge .,All experiments are performed on a single NVidia K40 GPU .,0.8831168831168831,0.7045454545454546,0.8666666666666667,experimental-setup,approach
document_classification,17,experimental evaluation,common model settings,205,32,14,All experiments are performed on a single NVidia K40 GPU .,The implementation is done using Torch 7 .,"Unlike previous research on the use of ConvNets for text processing , we use temporal batch norm without dropout .",0.8874458874458875,0.7272727272727273,0.9333333333333332,experimental-setup,baselines
document_classification,17,experimental evaluation,common model settings,206,33,15,"Unlike previous research on the use of ConvNets for text processing , we use temporal batch norm without dropout .",All experiments are performed on a single NVidia K40 GPU ., ,0.8917748917748918,0.75,1.0,experimental-setup,baselines
document_classification,17,experimental evaluation,experimental results,210,37,4,"Our deep architecture works well on big data sets in particular , even for small depths .","Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with small temporal convolution filters with different types of pooling , which shows that a significant improvement on the state - of - the - art configurations can be achieved on text classification tasks by pushing the depth to 29 convolutional layers .","shows the test errors for depths 9 , 17 and 29 and for each type of pooling : convolution with stride 2 , k- max pooling and temporal max - pooling .",0.9090909090909092,0.8409090909090909,0.3636363636363637,results,approach
document_classification,17,experimental evaluation,experimental results,212,39,6,"For the smallest depth we use ( 9 convolutional layers ) , we see that our model already performs better than Zhang 's convolutional baselines ( which includes 6 convolutional layers and has a different architecture ) on the biggest data sets :","shows the test errors for depths 9 , 17 and 29 and for each type of pooling : convolution with stride 2 , k- max pooling and temporal max - pooling .","Yelp Full , Yahoo Answers and Amazon Full and Polarity .",0.9177489177489178,0.8863636363636364,0.5454545454545454,results,baselines
document_classification,17,experimental evaluation,experimental results,214,41,8,The most important decrease in classification error can be observed on the largest data set Amazon Full which has more than 3 Million training samples . :,"Yelp Full , Yahoo Answers and Amazon Full and Polarity .",Best published results from previous work .,0.9264069264069263,0.9318181818181818,0.7272727272727273,results,baselines
document_classification,17,experimental evaluation,experimental results,217,44,11,"We also observe that for a small depth , temporal max - pooling works best on all data sets .",best results use a Thesaurus data augmentation technique ( marked with an * ) . 's hierarchical methods is particularly adapted to datasets whose samples contain multiple sentences ., ,0.9393939393939394,1.0,1.0,results,baselines
document_classification,18,title,title,2,2,2,Character - level Convolutional Networks for Text Classification, ,*,0.0088495575221238,0.6666666666666666,0.6666666666666666,research-problem,experimental-setup
document_classification,18,introduction,introduction,18,7,7,"In this article we explore treating text as a kind of raw signal at character level , and applying temporal ( one-dimensional ) ConvNets to it .","In particular , time - delay networks used in the early days of deep learning research are essentially convolutional networks that model sequential data .",For this article we only used a classification task as away to exemplify ConvNets ' ability to understand texts .,0.079646017699115,0.4375,0.4375,model,ablation-analysis
document_classification,18,comparison models,traditional methods,103,4,1,Traditional Methods, , ,0.4557522123893805,0.0930232558139534,0.0555555555555555,baselines,baselines
document_classification,18,comparison models,traditional methods,106,7,4,Bag - of - words and its TFIDF .,The classifier used is a multinomial logistic regression in all these models .,"For each dataset , the bag - of - words model is constructed by selecting 50,000 most frequent words from the training subset .",0.4690265486725664,0.1627906976744186,0.2222222222222222,baselines,approach
document_classification,18,comparison models,traditional methods,107,8,5,"For each dataset , the bag - of - words model is constructed by selecting 50,000 most frequent words from the training subset .",Bag - of - words and its TFIDF .,"For the normal bag - of - words , we use the counts of each word as the features .",0.4734513274336283,0.1860465116279069,0.2777777777777778,baselines,approach
document_classification,18,comparison models,traditional methods,112,13,10,Bag - of - ngrams and its TFIDF .,The features are normalized by dividing the largest feature value .,"The bag - of - ngrams models are constructed by selecting the 500,000 most frequent n-grams ( up to 5 - grams ) from the training subset for each dataset .",0.4955752212389381,0.3023255813953488,0.5555555555555556,baselines,baselines
document_classification,18,comparison models,traditional methods,113,14,11,"The bag - of - ngrams models are constructed by selecting the 500,000 most frequent n-grams ( up to 5 - grams ) from the training subset for each dataset .",Bag - of - ngrams and its TFIDF .,The feature values are computed the same way as in the bag - of - words model .,0.5,0.3255813953488372,0.6111111111111112,baselines,baselines
document_classification,18,comparison models,traditional methods,115,16,13,Bag - of - means on word embedding .,The feature values are computed the same way as in the bag - of - words model .,"We also have an experimental model that uses k-means on word2vec learnt from the training subset of each dataset , and then use these learnt means as representatives of the clustered words .",0.5088495575221239,0.372093023255814,0.7222222222222222,baselines,experiments
document_classification,18,comparison models,traditional methods,116,17,14,"We also have an experimental model that uses k-means on word2vec learnt from the training subset of each dataset , and then use these learnt means as representatives of the clustered words .",Bag - of - means on word embedding .,We take into consideration all the words that appeared more than 5 times in the training subset .,0.5132743362831859,0.3953488372093023,0.7777777777777778,baselines,baselines
document_classification,18,comparison models,deep learning methods,121,22,1,Deep Learning Methods, , ,0.5353982300884956,0.5116279069767442,0.0555555555555555,baselines,baselines
document_classification,18,comparison models,deep learning methods,124,25,4,Word - based ConvNets .,"We choose two simple and representative models for comparison , in which one is word - based ConvNet and the other a simple long - short term memory ( LSTM ) recurrent neural network model .","Among the large number of recent works on word - based ConvNets for text classification , one of the differences is the choice of using pretrained or end - to - end learned word representations .",0.5486725663716814,0.5813953488372093,0.2222222222222222,baselines,baselines
document_classification,18,comparison models,deep learning methods,131,32,11,Long - short term memory .,LSTM LSTM LSTM ... : long - short term memory,"We also offer a comparison with a recurrent neural network model , namely long - short term memory ( LSTM ) .",0.5796460176991151,0.7441860465116279,0.6111111111111112,baselines,approach
document_classification,18,large scale datasets and results,large scale datasets and results,194,52,52,The most important conclusion from our experiments is that character - level ConvNets could work for text classification without the need for words .,Character - level ConvNet is an effective method .,This is a strong indication that language could also bethought of as a signal no different from any other kind .,0.8584070796460177,0.6666666666666666,0.6666666666666666,results,approach
document_classification,18,large scale datasets and results,large scale datasets and results,199,57,57,"Traditional methods like n-grams TFIDF remain strong candidates for dataset of size up to several hundreds of thousands , and only until the dataset goes to the scale of several millions do we observe that character - level ConvNets start to do better .",The most obvious trend coming from all the plots in is that the larger datasets tend to perform better .,Conv Nets may work well for user - generated data .,0.8805309734513275,0.7307692307692307,0.7307692307692307,results,baselines
document_classification,18,large scale datasets and results,large scale datasets and results,200,58,58,Conv Nets may work well for user - generated data .,"Traditional methods like n-grams TFIDF remain strong candidates for dataset of size up to several hundreds of thousands , and only until the dataset goes to the scale of several millions do we observe that character - level ConvNets start to do better .",User- generated data vary in the degree of how well the texts are curated .,0.8849557522123894,0.7435897435897436,0.7435897435897436,results,approach
document_classification,18,large scale datasets and results,large scale datasets and results,207,65,65,Choice of alphabet makes a difference .,"However , further analysis is needed to validate the hypothesis that ConvNets are truly good at identifying exotic character combinations such as misspellings and emoticons , as our experiments alone do not show any explicit evidence .",shows that changing the alphabet by distinguishing between uppercase and lowercase letters could make a difference .,0.915929203539823,0.8333333333333334,0.8333333333333334,results,approach
document_classification,18,large scale datasets and results,large scale datasets and results,211,69,69,Semantics of tasks may not matter .,"One possible explanation is that there is a regularization effect , but this is to be validated .",Our datasets consist of two kinds of tasks : sentiment analysis ( Yelp and Amazon reviews ) and topic classification ( all others ) .,0.9336283185840708,0.8846153846153846,0.8846153846153846,results,results
document_classification,19,title,title,2,2,2,Text Classification Improved by Integrating Bidirectional LSTM with Two - dimensional Max Pooling, , ,0.0082644628099173,1.0,1.0,research-problem,experimental-setup
document_classification,19,introduction,introduction,31,17,17,"Above all , this paper proposes Bidirectional Long Short - Term Memory Networks with Two - Dimensional Max Pooling ( BLSTM - 2DPooling ) to capture features on both the time - step dimension and the feature vector dimension .",It is a good choice to utilize 2D convolution and 2D pooling to sample more meaningful features on both the time - step dimension and the feature vector dimension for text classification .,It first utilizes Bidirectional Long Short - Term Memory Networks ( BLSTM ) to transform the text into vectors .,0.128099173553719,0.5,0.5,model,ablation-analysis
document_classification,19,introduction,introduction,32,18,18,It first utilizes Bidirectional Long Short - Term Memory Networks ( BLSTM ) to transform the text into vectors .,"Above all , this paper proposes Bidirectional Long Short - Term Memory Networks with Two - Dimensional Max Pooling ( BLSTM - 2DPooling ) to capture features on both the time - step dimension and the feature vector dimension .",And then 2D max pooling operation is utilized to obtain a fixed - length vector .,0.1322314049586777,0.5294117647058824,0.5294117647058824,model,ablation-analysis
document_classification,19,introduction,introduction,33,19,19,And then 2D max pooling operation is utilized to obtain a fixed - length vector .,It first utilizes Bidirectional Long Short - Term Memory Networks ( BLSTM ) to transform the text into vectors .,This paper also applies 2D convolution ( BLSTM - 2DCNN ) to capture more meaningful features to represent the input text .,0.1363636363636363,0.5588235294117647,0.5588235294117647,model,model
document_classification,19,introduction,introduction,34,20,20,This paper also applies 2D convolution ( BLSTM - 2DCNN ) to capture more meaningful features to represent the input text .,And then 2D max pooling operation is utilized to obtain a fixed - length vector .,The contributions of this paper can be summarized as follows :,0.140495867768595,0.5882352941176471,0.5882352941176471,model,approach
document_classification,19,experimental setup,hyper parameter settings,170,27,5,"The dimension of word embeddings is 300 , the hidden units of LSTM is 300 .",The final hyper -parameters are as follows .,"We use 100 convolutional filters each for window sizes of ( 3 , 3 ) , 2D pooling size of ( 2 , 2 ) .",0.7024793388429752,0.84375,0.5,hyperparameters,experiments
document_classification,19,experimental setup,hyper parameter settings,171,28,6,"We use 100 convolutional filters each for window sizes of ( 3 , 3 ) , 2D pooling size of ( 2 , 2 ) .","The dimension of word embeddings is 300 , the hidden units of LSTM is 300 .",We set the mini-batch size as 10 and the learning rate of AdaDelta as the default value 1.0 .,0.7066115702479339,0.875,0.6,hyperparameters,experimental-setup
document_classification,19,experimental setup,hyper parameter settings,172,29,7,We set the mini-batch size as 10 and the learning rate of AdaDelta as the default value 1.0 .,"We use 100 convolutional filters each for window sizes of ( 3 , 3 ) , 2D pooling size of ( 2 , 2 ) .","For regularization , we employ Dropout operation with dropout rate of 0.5 for the word embeddings , 0.2 for the BLSTM layer and 0.4 for the penultimate layer , we also use l 2 penalty with coefficient 10 ? 5 over the parameters .",0.7107438016528925,0.90625,0.7,hyperparameters,approach
document_classification,19,experimental setup,hyper parameter settings,173,30,8,"For regularization , we employ Dropout operation with dropout rate of 0.5 for the word embeddings , 0.2 for the BLSTM layer and 0.4 for the penultimate layer , we also use l 2 penalty with coefficient 10 ? 5 over the parameters .",We set the mini-batch size as 10 and the learning rate of AdaDelta as the default value 1.0 .,These values are chosen via a grid search on the SST - 1 development set .,0.7148760330578512,0.9375,0.8,hyperparameters,experimental-setup
document_classification,19,results,results,176,1,1,Results, , ,0.7272727272727273,0.0169491525423728,1.0,results,baselines
document_classification,19,results,overall performance,179,4,3,The BLSTM - 2DCNN model achieves excellent performance on 4 out of 6 tasks .,"This work implements four models , BLSTM , BLSTM - Att , BLSTM - 2DPooling , and BLSTM - 2DCNN . presents the performance of the four models and other state - of - the - art models on six classification tasks .","Especially , it achieves 52.4 % and 89.5 % test accuracies on SST - 1 and SST - 2 respectively .",0.7396694214876033,0.0677966101694915,0.0681818181818181,results,baselines
document_classification,19,results,overall performance,180,5,4,"Especially , it achieves 52.4 % and 89.5 % test accuracies on SST - 1 and SST - 2 respectively .",The BLSTM - 2DCNN model achieves excellent performance on 4 out of 6 tasks .,BLSTM - 2DPooling performs worse than the state - of - the - art models .,0.743801652892562,0.0847457627118644,0.0909090909090909,results,approach
document_classification,19,results,overall performance,181,6,5,BLSTM - 2DPooling performs worse than the state - of - the - art models .,"Especially , it achieves 52.4 % and 89.5 % test accuracies on SST - 1 and SST - 2 respectively .","While we expect performance gains through the use of 2D convolution , we are surprised at the magnitude of the gains .",0.7479338842975206,0.1016949152542373,0.1136363636363636,results,baselines
document_classification,19,results,overall performance,183,8,7,"BLSTM - CNN beats all baselines on SST - 1 , SST - 2 , and TREC datasets .","While we expect performance gains through the use of 2D convolution , we are surprised at the magnitude of the gains .","As for Subj and MR datasets , BLSTM - 2DCNN gets a second higher accuracies .",0.756198347107438,0.135593220338983,0.1590909090909091,results,baselines
document_classification,19,results,overall performance,184,9,8,"As for Subj and MR datasets , BLSTM - 2DCNN gets a second higher accuracies .","BLSTM - CNN beats all baselines on SST - 1 , SST - 2 , and TREC datasets .","Some of the previous techniques only work on sentences , but not paragraphs / documents with several sentences .",0.7603305785123967,0.1525423728813559,0.1818181818181818,results,approach
document_classification,19,results,overall performance,188,13,12,"Compared with RCNN , BLSTM - 2DCNN achieves a comparable result .","For that purpose , this paper tests the four models on document - level dataset 20 Ng , by treating the document as along sentence .","Besides , this paper also compares with ReNN , RNN , CNN and other neural networks :",0.7768595041322314,0.2203389830508474,0.2727272727272727,results,baselines
document_classification,19,results,overall performance,190,15,14,"Compared with ReNN , the proposed two models do not depend on external language - specific features such as dependency parse trees .","Besides , this paper also compares with ReNN , RNN , CNN and other neural networks :","CNN extracts features from word embeddings of the input text , while BLSTM - 2DPooling and BLSTM - 2DCNN captures features from the output of BLSTM layer , which has already extracted features from the original input text .",0.7851239669421488,0.2542372881355932,0.3181818181818182,results,approach
document_classification,19,results,overall performance,194,19,18,"Compared with DSCNN , BLSTM - 2DCNN outperforms it on five datasets .","Ada Sent utilizes a more complicated model to form a hierarchy of representations , and it outperforms BLSTM - 2DCNN on Subj and MR datasets .","Compared with these results , 2D convolution and 2D max pooling operation are more effective for modeling sentence , even document .",0.8016528925619835,0.3220338983050847,0.4090909090909091,results,approach
document_classification,2,title,title,2,2,2,Rethinking Complex Neural Network Architectures for Document Classification, , ,0.015625,1.0,1.0,research-problem,experimental-setup
document_classification,2,introduction,introduction,21,12,12,"Like the papers cited above , we question the need for overly complex neural architectures , focusing on the problem of document classification .","Complex architectures are more difficult to train , more sensitive to hyperparameters , and brittle with respect to domains with different data characteristics - thus both exacerbating the "" crisis of reproducibility "" and making it difficult for practitioners to deploy networks that tackle real - world problems in production environments .","Starting with a large - scale reproducibility study of several recent neural models , we find that a simple bi-directional LSTM ( BiLSTM ) architecture with appropriate regularization yields accuracy and F 1 that are either competitive or exceed the state of the art on four standard benchmark datasets .",0.1640625,0.75,0.75,approach,experimental-setup
document_classification,2,introduction,introduction,22,13,13,"Starting with a large - scale reproducibility study of several recent neural models , we find that a simple bi-directional LSTM ( BiLSTM ) architecture with appropriate regularization yields accuracy and F 1 that are either competitive or exceed the state of the art on four standard benchmark datasets .","Like the papers cited above , we question the need for overly complex neural architectures , focusing on the problem of document classification .","As the closest comparison point , we find no benefit to the hierarchical modeling proposed by and we are able to achieve good classification results without attention mechanisms .",0.171875,0.8125,0.8125,approach,baselines
document_classification,2,experimental setup,experimental setup,60,2,2,"We conduct a large - scale reproducibility study involving HAN , XML - CNN , KimCNN , and SGM .", ,"These are compared to our proposed model , referred to as LSTM reg , as well as an ablated variant without regularization , denoted LSTM base .",0.46875,0.0476190476190476,0.2,experimental-setup,approach
document_classification,2,experimental setup,experimental setup,64,6,6,"In addition , we compare the neural approaches to logistic regression ( LR ) and support vector machines ( SVMs ) .",1,"The LR model is trained using a one - vs - rest multi-label objective , while the SVM is trained with a linear kernel .",0.5,0.1428571428571428,0.6,experimental-setup,approach
document_classification,2,experimental setup,experimental setup,65,7,7,"The LR model is trained using a one - vs - rest multi-label objective , while the SVM is trained with a linear kernel .","In addition , we compare the neural approaches to logistic regression ( LR ) and support vector machines ( SVMs ) .",Both of these methods use word - level tf - idf vectors of the documents as features .,0.5078125,0.1666666666666666,0.7,experimental-setup,experimental-setup
document_classification,2,experimental setup,experimental setup,67,9,9,"All of our experiments are performed on Nvidia GTX 1080 and RTX 2080 Ti GPUs , with PyTorch 0.4.1 as the backend framework .",Both of these methods use word - level tf - idf vectors of the documents as features .,We use Scikitlearn 0.19.2 for computing the tf - idf vectors and implementing LR and SVMs .,0.5234375,0.2142857142857142,0.9,experimental-setup,experiments
document_classification,2,experimental setup,experimental setup,68,10,10,We use Scikitlearn 0.19.2 for computing the tf - idf vectors and implementing LR and SVMs .,"All of our experiments are performed on Nvidia GTX 1080 and RTX 2080 Ti GPUs , with PyTorch 0.4.1 as the backend framework .", ,0.53125,0.2380952380952381,1.0,experimental-setup,approach
document_classification,2,results and discussion,results and discussion,111,11,11,"We see that our simple LSTM reg model achieves state of the art on Reuters and IMDB ( see , rows 9 and 10 ) , establishing mean scores of 87.0 and 52.8 for F 1 score and accuracy on the test sets of Reuters and IMDB , respectively .",Baseline Comparison .,This highlights the efficacy of proper regularization and optimization techniques for the task .,0.8671875,0.5,0.5,results,baselines
document_classification,2,results and discussion,results and discussion,113,13,13,"We observe that LSTM reg consistently improves upon the performance of LSTM base across all of the tasks - see rows 9 and 10 , where , on average , regularization yields increases of 1.5 and 0.5 points for F 1 score and accuracy , respectively .",This highlights the efficacy of proper regularization and optimization techniques for the task .,A few of our LSTM reg runs attain state - of - theart test F 1 scores on AAPD .,0.8828125,0.5909090909090909,0.5909090909090909,results,baselines
document_classification,2,results and discussion,results and discussion,114,14,14,A few of our LSTM reg runs attain state - of - theart test F 1 scores on AAPD .,"We observe that LSTM reg consistently improves upon the performance of LSTM base across all of the tasks - see rows 9 and 10 , where , on average , regularization yields increases of 1.5 and 0.5 points for F 1 score and accuracy , respectively .","However , in the interest of robustness , we report the mean value , as mentioned in Section 4.2 .",0.890625,0.6363636363636364,0.6363636363636364,results,baselines
document_classification,2,results and discussion,results and discussion,119,19,19,"Interestingly , the non-neural LR and SVM baselines perform remarkably well .",We can not rule out that these disparities are caused by the absence of any widely - accepted splits for evaluation on Yelp 2014 and IMDB ( as opposed to model or implementation differences ) .,"On Reuters , for example , the SVM beats many neural baselines , including our non-regularized LSTM base ( rows 2 - 9 ) .",0.9296875,0.8636363636363636,0.8636363636363636,results,baselines
document_classification,2,results and discussion,results and discussion,120,20,20,"On Reuters , for example , the SVM beats many neural baselines , including our non-regularized LSTM base ( rows 2 - 9 ) .","Interestingly , the non-neural LR and SVM baselines perform remarkably well .","On AAPD , the SVM either ties or beats the other models , losing only to SGM ( rows 2 - 8 ) .",0.9375,0.9090909090909092,0.9090909090909092,results,baselines
document_classification,2,results and discussion,results and discussion,121,21,21,"On AAPD , the SVM either ties or beats the other models , losing only to SGM ( rows 2 - 8 ) .","On Reuters , for example , the SVM beats many neural baselines , including our non-regularized LSTM base ( rows 2 - 9 ) .","Compared to the SVM , the LR baseline appears better suited for the single - label datasets IMDB and Yelp 2014 , where it achieves better accuracy than the SVM does .",0.9453125,0.9545454545454546,0.9545454545454546,results,baselines
document_classification,2,results and discussion,results and discussion,122,22,22,"Compared to the SVM , the LR baseline appears better suited for the single - label datasets IMDB and Yelp 2014 , where it achieves better accuracy than the SVM does .","On AAPD , the SVM either ties or beats the other models , losing only to SGM ( rows 2 - 8 ) .", ,0.953125,1.0,1.0,results,baselines
document_classification,20,title,title,2,2,2,Practical Text Classification With Large Pre-Trained Language Models, , ,0.0093457943925233,1.0,1.0,research-problem,experimental-setup
document_classification,20,introduction,introduction,17,6,6,"In this work , we train both mLSTM and Transformer language models on a large 40 GB text dataset , then transfer those models to two text classification problems : binary sentiment ( including Neutral labels ) , and multidimensional emotion classification based on the Plutchik wheel of emotions .",All rights reserved .,"We examine our performance on these tasks , both against large academic datasets , and on an original text dataset that we compiled from social media messages about several specific topics , such as video games .",0.0794392523364486,0.375,0.375,model,ablation-analysis
document_classification,20,results,results,139,1,1,Results, , ,0.6495327102803738,0.0344827586206896,1.0,results,baselines
document_classification,20,results,binary sentiment tweets,140,2,1,Binary Sentiment Tweets, , ,0.6542056074766355,0.0689655172413793,0.2,results,baselines
document_classification,20,results,binary sentiment tweets,143,5,4,"While the Transformer gets close but does not exceed the state of the art on the SST dataset , it exceeds both the mL - STM and ELMo baseline as well as both Watson and Google Sentiment APIs on the company tweets .",See.,This is despite optimally calibrating the API results on the test set .,0.6682242990654206,0.1724137931034483,0.8,results,baselines
document_classification,20,results,multi label emotion tweets,145,7,1,Multi - Label Emotion Tweets, , ,0.677570093457944,0.2413793103448276,0.0434782608695652,results,baselines
document_classification,20,results,multi label emotion tweets,148,10,4,We find that our models outperform Watson on every emotion category .,We compare our models to Watson on these categories for both the SemEval dataset and the company tweets in .,Sem Eval Tweets,0.6915887850467289,0.3448275862068966,0.1739130434782608,results,baselines
document_classification,20,results,multi label emotion tweets,149,11,5,Sem Eval Tweets,We find that our models outperform Watson on every emotion category .,"We submitted our finetuned Transformer model to the SemEval Task1:E - C challenge , as seen in Table 6 .",0.6962616822429907,0.3793103448275862,0.217391304347826,results,approach
document_classification,20,results,multi label emotion tweets,152,14,8,"Our model achieved the top macro-averaged F1 score among all submission , with competitive but lower scores for the micro -average F1 an the Jaccard Index accuracy 8 .","These results were computed by the organizers on a golden test set , for which we do not have access to the truth labels .","This suggests that our model out - performs the other top submission on rare and difficult categories , since macroaverage weighs performance on all classes equally , and the most common categories of Joy , Anger , Disgust and Optimism get relatively higher F 1 scores across all models .",0.7102803738317757,0.4827586206896552,0.3478260869565217,results,baselines
document_classification,20,results,multi label emotion tweets,154,16,10,We also compare the deep learning architectures of the Transformer and m LSTM on this dataset in and find that the Transformer outperforms the m LSTM across Plutchik categories .,"This suggests that our model out - performs the other top submission on rare and difficult categories , since macroaverage weighs performance on all classes equally , and the most common categories of Joy , Anger , Disgust and Optimism get relatively higher F 1 scores across all models .","The winner of the Task 1:E - c challenge ) trained a bidirectional LSTM with an 800,000 word embedding vocabulary derived from training word vectors ) on a dataset of 550 million tweets .",0.7196261682242991,0.5517241379310345,0.4347826086956521,results,baselines
document_classification,20,results,multi label emotion tweets,162,24,18,Our models gets lower F 1 scores on the company tweets dataset than on equivalent Se -m Eval categories .,Plutchik on Company Tweets,"As with the SemEval challenge tweets , the Transformer outperformed the mLSTM .",0.7570093457943925,0.8275862068965517,0.7826086956521741,results,approach
document_classification,3,title,title,2,2,2,Squeezed Very Deep Convolutional Neural Networks for Text Classification, , ,0.0104712041884816,1.0,1.0,research-problem,experimental-setup
document_classification,3,abstract,abstract,30,28,28,"In this paper , we investigate modifications on the network proposed by Conneau et al. with the aim of reducing its number of parameters , storage size and latency with minimal performance degradation .",Such advantages would boost the usage of deep neural models in text - based applications for embedded platforms .,To achieve these improvements we used Temporal Depthwise Separable Convolution and Global Average Pooling techniques .,0.1570680628272251,0.7567567567567568,0.7567567567567568,model,baselines
document_classification,3,abstract,abstract,31,29,29,To achieve these improvements we used Temporal Depthwise Separable Convolution and Global Average Pooling techniques .,"In this paper , we investigate modifications on the network proposed by Conneau et al. with the aim of reducing its number of parameters , storage size and latency with minimal performance degradation .","Therefore , our main contribution is to propose the Squeezed Very Deep Convolutional Neural Networks ( SVDCNN ) , a text classification model which requires significantly fewer parameters compared to the stateof - the - art CNNs .",0.1623036649214659,0.7837837837837838,0.7837837837837838,model,baselines
document_classification,3,abstract,abstract,32,30,30,"Therefore , our main contribution is to propose the Squeezed Very Deep Convolutional Neural Networks ( SVDCNN ) , a text classification model which requires significantly fewer parameters compared to the stateof - the - art CNNs .",To achieve these improvements we used Temporal Depthwise Separable Convolution and Global Average Pooling techniques .,Section II provides an overview of the state - of - the - art in CNNs for text classification .,0.1675392670157068,0.8108108108108109,0.8108108108108109,model,ablation-analysis
document_classification,3,experiments,experiments,141,5,5,"For SVDCNN and Char - CNN , we calculated the abovementioned number from the network architecture implemented in PyTorch .","The source code of the proposed model is available in the GitHub repository SVDCNN The original VDCNN paper reported the number of parameters of the convolutional layers , in which we reproduce in this article .","As for the FC layer 's parameters , the number is obtained as the summation of the product of the input and output size of each FC layer for each CNN .",0.7382198952879581,0.25,0.25,experimental-setup,approach
document_classification,3,experiments,experiments,146,10,10,"The SVDCNN experimental settings are similar to the original VDCNN paper , using the same dictionary and the same embedding size of 16 .","Regarding the inference time , its average and standard deviation were calculated as the time to predict one instance of the AG's News dataset throughout 1,000 repetitions .","The training is also performed with SGD , utilizing size batch of 64 , with a maximum of 100 epochs .",0.7643979057591623,0.5,0.5,experimental-setup,approach
document_classification,3,experiments,experiments,147,11,11,"The training is also performed with SGD , utilizing size batch of 64 , with a maximum of 100 epochs .","The SVDCNN experimental settings are similar to the original VDCNN paper , using the same dictionary and the same embedding size of 16 .","We use an initial learning rate of 0.01 , a momentum of 0.9 and a weight decay of 0.001 .",0.7696335078534031,0.55,0.55,experimental-setup,approach
document_classification,3,experiments,experiments,148,12,12,"We use an initial learning rate of 0.01 , a momentum of 0.9 and a weight decay of 0.001 .","The training is also performed with SGD , utilizing size batch of 64 , with a maximum of 100 epochs .",All the experiments were performed on an NVIDIA GTX 1060 GPU + Intel Core i 7 4770s CPU .,0.774869109947644,0.6,0.6,experimental-setup,approach
document_classification,3,experiments,experiments,149,13,13,All the experiments were performed on an NVIDIA GTX 1060 GPU + Intel Core i 7 4770s CPU .,"We use an initial learning rate of 0.01 , a momentum of 0.9 and a weight decay of 0.001 .",The model 's performance is evaluated on three large - scale public datasets also used by Zhang et al. in the introduction of Char - CNN and VDCNN models .,0.7801047120418848,0.65,0.65,experimental-setup,approach
document_classification,3,results,results,158,2,2,The network reduction obtained by the GAP is even more representative since both compared models use three FC layers for their classification tasks ., ,"Considering a dataset with four target classes , and comparing SVDCNN with VDCNN , the number of parameters of the FC layers has passed from 12.59 to 0.02 million parameters , representing a reduction of 99.84 % .",0.8272251308900523,0.0714285714285714,0.0714285714285714,results,approach
document_classification,3,results,results,159,3,3,"Considering a dataset with four target classes , and comparing SVDCNN with VDCNN , the number of parameters of the FC layers has passed from 12.59 to 0.02 million parameters , representing a reduction of 99.84 % .",The network reduction obtained by the GAP is even more representative since both compared models use three FC layers for their classification tasks .,"Following with the same comparison , but to Char - CNN , the proposed model is 99.82 % smaller , 0.02 against 11.36 million of FC parameters .",0.8324607329842932,0.1071428571428571,0.1071428571428571,results,baselines
document_classification,3,results,results,160,4,4,"Following with the same comparison , but to Char - CNN , the proposed model is 99.82 % smaller , 0.02 against 11.36 million of FC parameters .","Considering a dataset with four target classes , and comparing SVDCNN with VDCNN , the number of parameters of the FC layers has passed from 12.59 to 0.02 million parameters , representing a reduction of 99.84 % .",The reduction of the total parameters impacts directly on the storage size of the networks .,0.837696335078534,0.1428571428571428,0.1428571428571428,results,approach
document_classification,3,results,results,162,6,6,"While our most in - depth model ( 29 ) occupies only 6 MB , VDCNN with the same depth occupies 64. 16 MB of storage .",The reduction of the total parameters impacts directly on the storage size of the networks .,"Likewise , Char- CNN ( which has depth 6 ) occupies 43.25 MB .",0.8481675392670157,0.2142857142857142,0.2142857142857142,results,baselines
document_classification,3,results,results,166,10,10,"Regarding accuracy results , usually , a model with such parameter reduction should present some loss of accuracy in comparison to the original model .","For example , FPGAs often have less than 10 MB of on - chip memory and no off - chip memory or storage .","Nevertheless , the performance difference between VDCNN and SVDCNN models varies between 0.4 and 1.3 % , which is pretty modest considering the parameters and storage size reduction aforementioned .",0.8691099476439791,0.3571428571428571,0.3571428571428571,results,model
document_classification,3,results,results,167,11,11,"Nevertheless , the performance difference between VDCNN and SVDCNN models varies between 0.4 and 1.3 % , which is pretty modest considering the parameters and storage size reduction aforementioned .","Regarding accuracy results , usually , a model with such parameter reduction should present some loss of accuracy in comparison to the original model .","In , it is possible to see the accuracy scores obtained by the compared models .",0.8743455497382199,0.3928571428571429,0.3928571428571429,results,baselines
document_classification,4,title,title,2,2,2,Joint Embedding of Words and Labels for Text Classification, , ,0.0075471698113207,1.0,1.0,research-problem,approach
document_classification,4,introduction,our contribution,35,22,2,"Our primary contribution is therefore to propose such a solution by making use of the label embedding framework , and propose the Label - Embedding Attentive Model ( LEAM ) to improve text classification .", ,"While there is an abundant literature in the NLP community on word embeddings ( how to describe a word ) for text representations , much less work has been devoted in comparison to label embeddings ( how to describe a class ) .",0.1320754716981132,0.7333333333333333,0.2,model,ablation-analysis
document_classification,4,introduction,our contribution,37,24,4,"The proposed LEAM is implemented by jointly embedding the word and label in the same latent space , and the text representations are constructed directly using the text - label compatibility .","While there is an abundant literature in the NLP community on word embeddings ( how to describe a word ) for text representations , much less work has been devoted in comparison to label embeddings ( how to describe a class ) .","Our label embedding framework has the following salutary properties : ( i ) Label - attentive text representation is informative for the downstream classification task , as it directly learns from a shared joint space , whereas traditional methods proceed in multiple steps by solving intermediate problems .",0.1396226415094339,0.8,0.4,model,ablation-analysis
document_classification,4,introduction,our contribution,38,25,5,"Our label embedding framework has the following salutary properties : ( i ) Label - attentive text representation is informative for the downstream classification task , as it directly learns from a shared joint space , whereas traditional methods proceed in multiple steps by solving intermediate problems .","The proposed LEAM is implemented by jointly embedding the word and label in the same latent space , and the text representations are constructed directly using the text - label compatibility .","( ii ) The LEAM learning procedure only involves a series of basic algebraic operations , and hence it retains the interpretability of simple models , especially when the label description is available .",0.1433962264150943,0.8333333333333334,0.5,model,baselines
document_classification,4,introduction,our contribution,39,26,6,"( ii ) The LEAM learning procedure only involves a series of basic algebraic operations , and hence it retains the interpretability of simple models , especially when the label description is available .","Our label embedding framework has the following salutary properties : ( i ) Label - attentive text representation is informative for the downstream classification task , as it directly learns from a shared joint space , whereas traditional methods proceed in multiple steps by solving intermediate problems .","( iii ) Our attention mechanism ( derived from the text - label compatibility ) has fewer parameters and less computation than related methods , and thus is much cheaper in both training and testing , compared with sophisticated deep attention models .",0.1471698113207547,0.8666666666666667,0.6,model,model
document_classification,4,experimental results,experimental results,199,2,2,Setup We use 300 - dimensional Glo Ve word embeddings as initialization for word embeddings and label embeddings in our model ., ,"Out - Of - Vocabulary ( OOV ) words are initialized from a uniform distribution with range [ ? 0.01 , 0.01 ] .",0.7509433962264151,0.032258064516129,0.2222222222222222,experimental-setup,baselines
document_classification,4,experimental results,experimental results,200,3,3,"Out - Of - Vocabulary ( OOV ) words are initialized from a uniform distribution with range [ ? 0.01 , 0.01 ] .",Setup We use 300 - dimensional Glo Ve word embeddings as initialization for word embeddings and label embeddings in our model .,The final classifier is implemented as an MLP layer followed by a sigmoid or softmax function depending on specific task .,0.7547169811320755,0.0483870967741935,0.3333333333333333,experimental-setup,experimental-setup
document_classification,4,experimental results,experimental results,201,4,4,The final classifier is implemented as an MLP layer followed by a sigmoid or softmax function depending on specific task .,"Out - Of - Vocabulary ( OOV ) words are initialized from a uniform distribution with range [ ? 0.01 , 0.01 ] .","We train our model 's parameters with the Adam Optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 , and a minibatch size of 100 .",0.7584905660377359,0.064516129032258,0.4444444444444444,experimental-setup,approach
document_classification,4,experimental results,experimental results,202,5,5,"We train our model 's parameters with the Adam Optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 , and a minibatch size of 100 .",The final classifier is implemented as an MLP layer followed by a sigmoid or softmax function depending on specific task .,"Dropout regularization is employed on the final MLP layer , with dropout rate 0.5 .",0.7622641509433963,0.0806451612903225,0.5555555555555556,experimental-setup,approach
document_classification,4,experimental results,experimental results,203,6,6,"Dropout regularization is employed on the final MLP layer , with dropout rate 0.5 .","We train our model 's parameters with the Adam Optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 , and a minibatch size of 100 .",The model is implemented using Tensorflow and is trained on GPU Titan X.,0.7660377358490567,0.0967741935483871,0.6666666666666666,experimental-setup,baselines
document_classification,4,experimental results,experimental results,204,7,7,The model is implemented using Tensorflow and is trained on GPU Titan X.,"Dropout regularization is employed on the final MLP layer , with dropout rate 0.5 .",The code to reproduce the experimental results is at https://github.com/guoyinwang/LEAM :,0.769811320754717,0.1129032258064516,0.7777777777777778,experimental-setup,baselines
document_classification,4,experimental results,experimental results,205,8,8,The code to reproduce the experimental results is at https://github.com/guoyinwang/LEAM :,The model is implemented using Tensorflow and is trained on GPU Titan X.,"Summary statistics of five datasets , including the number of classes , number of training samples and number of testing samples .",0.7735849056603774,0.1290322580645161,0.8888888888888888,code,baselines
document_classification,4,experimental results,applications to clinical text,249,52,11,"We compare against the three baselines : a logistic regression model with bag - ofwords , a bidirectional gated recurrent unit ( Bi - GRU ) and a single - layer 1 D convolutional network .",Results,"We also compare with three recent methods for multi-label classification of clinical text , including Condensed Memory Networks ( C - MemNN ) , Attentive LSTM and Convolutional Attention ( CAML ) .",0.939622641509434,0.8387096774193549,0.5238095238095238,baselines,ablation-analysis
document_classification,4,experimental results,applications to clinical text,250,53,12,"We also compare with three recent methods for multi-label classification of clinical text , including Condensed Memory Networks ( C - MemNN ) , Attentive LSTM and Convolutional Attention ( CAML ) .","We compare against the three baselines : a logistic regression model with bag - ofwords , a bidirectional gated recurrent unit ( Bi - GRU ) and a single - layer 1 D convolutional network .","To quantify the prediction performance , we follow to consider the micro-averaged and macro-averaged F1 and area under the ROC curve ( AUC ) , as well as the precision at n ( P@n ) .",0.9433962264150944,0.8548387096774194,0.5714285714285714,baselines,experimental-setup
document_classification,4,experimental results,applications to clinical text,256,59,18,"LEAM provides the best AUC score , and better F1 and P@5 values than all methods except CNN .",The results are shown in .,"CNN consistently outperforms the basic Bi - GRU architecture , and the logistic regression baseline performs worse than all deep learning architectures .",0.9660377358490566,0.9516129032258064,0.8571428571428571,results,baselines
document_classification,4,experimental results,applications to clinical text,257,60,19,"CNN consistently outperforms the basic Bi - GRU architecture , and the logistic regression baseline performs worse than all deep learning architectures .","LEAM provides the best AUC score , and better F1 and P@5 values than all methods except CNN .",We emphasize that the learned attention can be very useful to reduce a doctor 's reading burden .,0.969811320754717,0.967741935483871,0.9047619047619048,results,baselines
document_classification,5,title,title,2,2,2,HDLTex : Hierarchical Deep Learning for Text Classification, , ,0.0075757575757575,1.0,1.0,research-problem,approach
document_classification,5,abstract,abstract,5,3,3,"Central to these information processing methods is document classification , which has become an important application for supervised learning .","Increasingly large document collections require improved information processing methods for searching , retrieving , and organizing text .",Recently the performance of traditional supervised classifiers has degraded as the number of documents has increased .,0.0189393939393939,0.12,0.12,research-problem,experimental-setup
document_classification,5,abstract,abstract,9,7,7,Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification ( HDLTex ) .,This paper approaches this problem differently from current document classification methods that view the problem as multi-class classification .,HDLTex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy .,0.034090909090909,0.28,0.28,research-problem,model
document_classification,5,abstract,abstract,24,22,22,This paper presents a new approach to hierarchical document classification that we call Hierarchical Deep Learning for Text classification ( HDLTex ) .,"Further , the combination of top - level fields and all sub-fields presents current document classification approaches with a combinatorially increasing number of class labels that they can not handle .",HDLTex combines deep learning architectures to allow both over all and specialized learning by level of the document hierarchy .,0.0909090909090909,0.88,0.88,model,experimental-setup
document_classification,5,abstract,abstract,25,23,23,HDLTex combines deep learning architectures to allow both over all and specialized learning by level of the document hierarchy .,This paper presents a new approach to hierarchical document classification that we call Hierarchical Deep Learning for Text classification ( HDLTex ) .,"This paper reports our experiments with HDLTex , which exhibits improved accuracy over traditional document classification methods .",0.0946969696969697,0.92,0.92,model,baselines
document_classification,5,results,results,231,20,20,The following results were obtained using a combination of central processing units ( CPUs ) and graphical processing units ( GPUs ) .,B. Hardware and Implementation,"The processing was done on a Xeon E5 ? 2640 ( 2.6 GHz ) with 32 cores and 64GB memory , and the GPU cards were N vidia Quadro K620 and N vidia Tesla K20c .",0.875,0.4878048780487805,0.4878048780487805,experimental-setup,approach
document_classification,5,results,results,232,21,21,"The processing was done on a Xeon E5 ? 2640 ( 2.6 GHz ) with 32 cores and 64GB memory , and the GPU cards were N vidia Quadro K620 and N vidia Tesla K20c .",The following results were obtained using a combination of central processing units ( CPUs ) and graphical processing units ( GPUs ) .,"We implemented our approaches in Python using the Compute Unified Device Architecture ( CUDA ) , which is a parallel computing platform and Application Programming Interface ( API ) model created by N vidia .",0.8787878787878788,0.5121951219512195,0.5121951219512195,experimental-setup,baselines
document_classification,5,results,results,233,22,22,"We implemented our approaches in Python using the Compute Unified Device Architecture ( CUDA ) , which is a parallel computing platform and Application Programming Interface ( API ) model created by N vidia .","The processing was done on a Xeon E5 ? 2640 ( 2.6 GHz ) with 32 cores and 64GB memory , and the GPU cards were N vidia Quadro K620 and N vidia Tesla K20c .","We also used Keras and Tensor Flow libraries for creating the neural networks , .",0.8825757575757576,0.5365853658536586,0.5365853658536586,experimental-setup,baselines
document_classification,5,results,results,234,23,23,"We also used Keras and Tensor Flow libraries for creating the neural networks , .","We implemented our approaches in Python using the Compute Unified Device Architecture ( CUDA ) , which is a parallel computing platform and Application Programming Interface ( API ) model created by N vidia .",shows the results from our experiments .,0.8863636363636364,0.5609756097560976,0.5609756097560976,experimental-setup,baselines
document_classification,5,results,results,236,25,25,"The baseline tests compare three conventional document classification approaches ( nave Bayes and two versions of SVM ) and stacking SVM with three deep learning approaches ( DNN , RNN , and CNN ) .",shows the results from our experiments .,In this set of tests the RNN outperforms the others for all three W OS data sets .,0.8939393939393939,0.6097560975609756,0.6097560975609756,baselines,approach
document_classification,5,results,results,237,26,26,In this set of tests the RNN outperforms the others for all three W OS data sets .,"The baseline tests compare three conventional document classification approaches ( nave Bayes and two versions of SVM ) and stacking SVM with three deep learning approaches ( DNN , RNN , and CNN ) .",CNN performs secondbest for three data sets .,0.8977272727272727,0.6341463414634146,0.6341463414634146,results,approach
document_classification,5,results,results,238,27,27,CNN performs secondbest for three data sets .,In this set of tests the RNN outperforms the others for all three W OS data sets .,SVM with term weighting is third for the first two sets while the multi-word approach of is in third place for the third data set .,0.9015151515151516,0.6585365853658537,0.6585365853658537,results,baselines
document_classification,5,results,results,239,28,28,SVM with term weighting is third for the first two sets while the multi-word approach of is in third place for the third data set .,CNN performs secondbest for three data sets .,The third data set is the smallest of the three and has the fewest labels so the differences among the three best performers are not large .,0.9053030303030304,0.6829268292682927,0.6829268292682927,results,baselines
document_classification,5,results,results,242,31,31,"Overall , nave Bayes does much worse than the other methods throughout these tests .",These results show that over all performance improvement for general document classification is obtainable with deep learning approaches compared to traditional methods .,"As for the tests of classifying these documents within a hierarchy , the HDLTex approaches with stacked , deep learning architectures clearly provide superior performance .",0.9166666666666666,0.7560975609756098,0.7560975609756098,results,approach
document_classification,5,results,results,243,32,32,"As for the tests of classifying these documents within a hierarchy , the HDLTex approaches with stacked , deep learning architectures clearly provide superior performance .","Overall , nave Bayes does much worse than the other methods throughout these tests .","For data set W OS ? 11967 , the best accuracy is obtained by the combination RNN for the first level of classification and DNN for the second level .",0.9204545454545454,0.7804878048780488,0.7804878048780488,results,approach
document_classification,5,results,results,244,33,33,"For data set W OS ? 11967 , the best accuracy is obtained by the combination RNN for the first level of classification and DNN for the second level .","As for the tests of classifying these documents within a hierarchy , the HDLTex approaches with stacked , deep learning architectures clearly provide superior performance .","This gives accuracies of 94 % for the first level , 92 % for the second level and 86 % over all .",0.9242424242424242,0.8048780487804879,0.8048780487804879,results,approach
document_classification,5,results,results,245,34,34,"This gives accuracies of 94 % for the first level , 92 % for the second level and 86 % over all .","For data set W OS ? 11967 , the best accuracy is obtained by the combination RNN for the first level of classification and DNN for the second level .",This is significantly better than all of the others except for the combination of CNN and DNN .,0.928030303030303,0.8292682926829268,0.8292682926829268,results,approach
document_classification,5,results,results,247,36,36,For data set W OS ? 46985 the best scores are again achieved by RNN for level one but this time with RNN for level 2 .,This is significantly better than all of the others except for the combination of CNN and DNN .,"The closest scores to this are obtained by CNN and RNN in levels 1 and 2 , respectively .",0.9356060606060606,0.8780487804878049,0.8780487804878049,results,baselines
document_classification,6,title,title,2,2,2,Explicit Interaction Model towards Text Classification, , ,0.0079365079365079,1.0,1.0,research-problem,approach
document_classification,6,introduction,introduction,31,20,20,"To address the aforementioned problems , we introduce the interaction mechanism ( Wang and Jiang 2016 b ) , which is capable of incorporating the word - level matching signals for text classification .","As such , the probability of the text belonging to a class is largely determined by their over all matching score regardless of word - level matching signals , which would provide explicit signals for classification ( e.g. , missile strongly indicates the topic of military ) .",The key idea behind the interaction mechanism is to explicitly calculate the matching scores between the words and classes .,0.123015873015873,0.5882352941176471,0.5882352941176471,model,ablation-analysis
document_classification,6,introduction,introduction,35,24,24,"Based upon the interaction mechanism , we devise an EXplicit interAction Model ( dubbed as EXAM ) .","By taking the interaction matrix as a text representation , the later classification layer could incorporate fine - grained word level signals for the finer classification rather than simply making the text - level matching .","Specifically , the proposed framework consists of three main components : word - level encoder , interaction layer , and aggregation layer .",0.1388888888888889,0.7058823529411765,0.7058823529411765,model,ablation-analysis
document_classification,6,introduction,introduction,36,25,25,"Specifically , the proposed framework consists of three main components : word - level encoder , interaction layer , and aggregation layer .","Based upon the interaction mechanism , we devise an EXplicit interAction Model ( dubbed as EXAM ) .",The word - level encoder projects the textual contents into the word - level representations .,0.1428571428571428,0.7352941176470589,0.7352941176470589,model,approach
document_classification,6,introduction,introduction,37,26,26,The word - level encoder projects the textual contents into the word - level representations .,"Specifically , the proposed framework consists of three main components : word - level encoder , interaction layer , and aggregation layer .","Hereafter , the interaction layer calculates the matching scores between the words and classes ( i.e. , constructs the interaction matrix ) .",0.1468253968253968,0.7647058823529411,0.7647058823529411,model,experimental-setup
document_classification,6,introduction,introduction,38,27,27,"Hereafter , the interaction layer calculates the matching scores between the words and classes ( i.e. , constructs the interaction matrix ) .",The word - level encoder projects the textual contents into the word - level representations .,"Then , the last layer aggregates those matching scores into predictions over each class , respectively .",0.1507936507936508,0.7941176470588235,0.7941176470588235,model,model
document_classification,6,introduction,introduction,39,28,28,"Then , the last layer aggregates those matching scores into predictions over each class , respectively .","Hereafter , the interaction layer calculates the matching scores between the words and classes ( i.e. , constructs the interaction matrix ) .",We justify our proposed EXAM model over both the multi-label and multi-class text classifications .,0.1547619047619047,0.8235294117647058,0.8235294117647058,model,model
document_classification,6,experiments,experiments,129,1,1,Experiments, , ,0.5119047619047619,0.0104166666666666,1.0,experiments,experimental-setup
document_classification,6,experiments,multi class classification,130,2,1,Multi - Class Classification, , ,0.5158730158730159,0.0208333333333333,0.0217391304347826,experiments,approach
document_classification,6,experiments,multi class classification,141,13,12,"For the multi -class task , we chose region embedding as the Encoder in EXAM .",Hyperparameters,The region size is 7 and embedding size is 128 .,0.5595238095238095,0.1354166666666666,0.2608695652173913,experiments,approach
document_classification,6,experiments,multi class classification,142,14,13,The region size is 7 and embedding size is 128 .,"For the multi -class task , we chose region embedding as the Encoder in EXAM .",We used adam ( Kingma and Ba 2014 ) as the optimizer with the initial learning rate 0.0001 and the batch size is set to 16 .,0.5634920634920635,0.1458333333333333,0.2826086956521739,experiments,experiments
document_classification,6,experiments,multi class classification,143,15,14,We used adam ( Kingma and Ba 2014 ) as the optimizer with the initial learning rate 0.0001 and the batch size is set to 16 .,The region size is 7 and embedding size is 128 .,"As for the aggregation MLP , we set the size of the hidden layer as 2 times interaction feature length .",0.5674603174603174,0.15625,0.3043478260869565,experiments,experiments
document_classification,6,experiments,multi class classification,144,16,15,"As for the aggregation MLP , we set the size of the hidden layer as 2 times interaction feature length .",We used adam ( Kingma and Ba 2014 ) as the optimizer with the initial learning rate 0.0001 and the batch size is set to 16 .,Our models are implemented and trained by MXNet ( Chen et al. ) with a single NVIDIA TITAN Xp .,0.5714285714285714,0.1666666666666666,0.3260869565217391,experiments,approach
document_classification,6,experiments,multi class classification,145,17,16,Our models are implemented and trained by MXNet ( Chen et al. ) with a single NVIDIA TITAN Xp .,"As for the aggregation MLP , we set the size of the hidden layer as 2 times interaction feature length .",Baselines,0.5753968253968254,0.1770833333333333,0.3478260869565217,experiments,approach
document_classification,6,experiments,multi class classification,146,18,17,Baselines,Our models are implemented and trained by MXNet ( Chen et al. ) with a single NVIDIA TITAN Xp .,"To demonstrate the effectiveness of our proposed EXAM , we compared it with several state - of - the - art baselines .",0.5793650793650794,0.1875,0.3695652173913043,experiments,experimental-setup
document_classification,6,experiments,multi class classification,148,20,19,The baselines are mainly in three variants :,"To demonstrate the effectiveness of our proposed EXAM , we compared it with several state - of - the - art baselines .",1 ) models based on feature engineering ;,0.5873015873015873,0.2083333333333333,0.4130434782608696,experiments,approach
document_classification,6,experiments,multi class classification,149,21,20,1 ) models based on feature engineering ;,The baselines are mainly in three variants :,"2 ) Char - based deep models , and 3 ) Word - based deep models .",0.5912698412698413,0.21875,0.4347826086956521,experiments,approach
document_classification,6,experiments,multi class classification,150,22,21,"2 ) Char - based deep models , and 3 ) Word - based deep models .",1 ) models based on feature engineering ;,"The first category uses the feature from the text to conduct the classification , and we reported the results from BoW Overall Performance We compared our EXAM to several state - of - the - art baselines with respect to accuracy .",0.5952380952380952,0.2291666666666666,0.4565217391304348,experiments,approach
document_classification,6,experiments,multi class classification,154,26,25,Models based on feature engineering get the worst results on all the five datasets compared to the other methods .,Four points are observed as following :,The main reason is that the feature engineering can not take full advantage of the supervision from the training set and it also suffers from the data sparsity .,0.6111111111111112,0.2708333333333333,0.5434782608695652,experiments,approach
document_classification,6,experiments,multi class classification,156,28,27,Char - based models get the highest over all scores on the two Amazon datasets .,The main reason is that the feature engineering can not take full advantage of the supervision from the training set and it also suffers from the data sparsity .,"There are possibly two reasons , 1 ) compared to the word - based models , char - based models enrich the supervision from characters and the characters are combined to form N-grams , stems , words and phrase which are helpful in the sentimental classification .",0.6190476190476191,0.2916666666666667,0.5869565217391305,experiments,baselines
document_classification,6,experiments,multi class classification,160,32,31,Word - based baselines exceed the other variants on three datasets and lose on the two Amazon datasets .,"For the three char - based baselines , VDCNN gets the best performance on almost all the datasets because it has 29 convolutional layers allowing the model to learn more combinations of characters .","The main reason is that the three tasks like news classification conduct categorization mainly via key words , and the wordbased models are able to directly use the word embedding without combining the characters .",0.6349206349206349,0.3333333333333333,0.6739130434782609,experiments,approach
document_classification,6,experiments,multi class classification,162,34,33,"For the five baselines , W.C Region Emb performs the best , because it learns the region embedding to utilize the N- grams feature from the text .","The main reason is that the three tasks like news classification conduct categorization mainly via key words , and the wordbased models are able to directly use the word embedding without combining the characters .","It is clear to see that EXAM achieves the best performance over the three datasets : AG , Yah. A. and DBP .",0.6428571428571429,0.3541666666666667,0.7173913043478259,experiments,baselines
document_classification,6,experiments,multi class classification,163,35,34,"It is clear to see that EXAM achieves the best performance over the three datasets : AG , Yah. A. and DBP .","For the five baselines , W.C Region Emb performs the best , because it learns the region embedding to utilize the N- grams feature from the text .","For the Yah.A. , EXAM improves the best performance by 1.1 % .",0.6468253968253969,0.3645833333333333,0.7391304347826086,experiments,baselines
document_classification,6,experiments,multi class classification,164,36,35,"For the Yah.A. , EXAM improves the best performance by 1.1 % .","It is clear to see that EXAM achieves the best performance over the three datasets : AG , Yah. A. and DBP .","Additionally , as a word - based model , EXAM beats all the word - based baselines on the other two Amazon datasets with a performance gain of 1.0 % on the Amazon Full , because our EXAM considers more fine - grained interaction features between classes and words , which is quite helpful in this task .",0.6507936507936508,0.375,0.7608695652173914,experiments,baselines
document_classification,6,experiments,multi label classification,176,48,1,Multi - Label Classification, , ,0.6984126984126984,0.5,0.0204081632653061,experiments,approach
document_classification,6,experiments,multi label classification,194,66,19,We implemented the baseline models and EXAM by MXNet .,Hyperparameters,"We used the matrix trained by word2vec to initialize the embedding layer , and the embedding size is 256 .",0.7698412698412699,0.6875,0.3877551020408163,experiments,approach
document_classification,6,experiments,multi label classification,195,67,20,"We used the matrix trained by word2vec to initialize the embedding layer , and the embedding size is 256 .",We implemented the baseline models and EXAM by MXNet .,"We adopted GRU as the Encoder , and each GRU Cell has 1,024 hidden states .",0.7738095238095238,0.6979166666666666,0.4081632653061225,experiments,experiments
document_classification,6,experiments,multi label classification,196,68,21,"We adopted GRU as the Encoder , and each GRU Cell has 1,024 hidden states .","We used the matrix trained by word2vec to initialize the embedding layer , and the embedding size is 256 .",The accumulated MLP has 60 hidden units .,0.7777777777777778,0.7083333333333334,0.4285714285714285,experiments,baselines
document_classification,6,experiments,multi label classification,197,69,22,The accumulated MLP has 60 hidden units .,"We adopted GRU as the Encoder , and each GRU Cell has 1,024 hidden states .",We applied Adam to optimize models on one NVIDIA TITAN Xp with the batch size of 1000 and the initial learning rate is 0.001 .,0.7817460317460317,0.71875,0.4489795918367347,experiments,approach
document_classification,6,experiments,multi label classification,198,70,23,We applied Adam to optimize models on one NVIDIA TITAN Xp with the batch size of 1000 and the initial learning rate is 0.001 .,The accumulated MLP has 60 hidden units .,The validation set is applied for early - stopping to avoid overfitting .,0.7857142857142857,0.7291666666666666,0.4693877551020408,experiments,experimental-setup
document_classification,6,experiments,multi label classification,199,71,24,The validation set is applied for early - stopping to avoid overfitting .,We applied Adam to optimize models on one NVIDIA TITAN Xp with the batch size of 1000 and the initial learning rate is 0.001 .,All hyperparameters are chosen empirically .,0.7896825396825397,0.7395833333333334,0.4897959183673469,experiments,approach
document_classification,6,experiments,multi label classification,210,82,35,Word - based models are better than char - based models in Kanshan - Cup dataset .,We observed the following from the :,That maybe because in Chinese the words can offer more supervisions than characters and the question tagging task needs more word supervision .,0.8333333333333334,0.8541666666666666,0.7142857142857143,experiments,approach
document_classification,6,experiments,multi label classification,213,85,38,Our models achieve the state - of - the - art performance over two different datasets though we only slightly modified Text RNN to build EXAM .,"For word - based baseline models , all the baselines have similar performance which corroborates the conclusion in FastText ) that simple network is on par with deep learning classifiers in text classification .","Different from the traditional models which encode the whole text into a vector , in EXAM , the representations of classes firstly interact with words to get more fine - grained features as shown in .",0.8452380952380952,0.8854166666666666,0.7755102040816326,experiments,approach
document_classification,7,title,title,2,2,2,A Corpus for Multilingual Document Classification in Eight Languages, , ,0.014388489208633,1.0,1.0,research-problem,approach
document_classification,7,abstract,abstract,4,2,2,Cross - lingual document classification aims at training a document classifier on resources in one language and transferring it to a different language without any additional resources ., ,Several approaches have been proposed in the literature and the current best practice is to evaluate them on a subset of the Reuters Corpus Volume,0.0287769784172661,0.1818181818181818,0.1818181818181818,research-problem,experimental-setup
document_classification,7,introduction,introduction,31,18,18,"We extend previous works and use the data in the Reuters Corpus Volume 2 to define new cross - lingual document classification tasks for eight very different languages , namely English , French , Spanish , Italian , German , Russian , Chinese and Japanese .",The contributions of this work are as follows .,"For each language , we define a train , development and test corpus .",0.2230215827338129,0.8181818181818182,0.8181818181818182,approach,experimental-setup
document_classification,7,introduction,introduction,32,19,19,"For each language , we define a train , development and test corpus .","We extend previous works and use the data in the Reuters Corpus Volume 2 to define new cross - lingual document classification tasks for eight very different languages , namely English , French , Spanish , Italian , German , Russian , Chinese and Japanese .","We also provide strong reference results for all transfer directions between the eight languages , e.g. not limited to the transfer between a foreign language and English .",0.2302158273381295,0.8636363636363636,0.8636363636363636,approach,approach
document_classification,7,baseline results,multilingual sentence representations,114,44,11,Zero - shot cross - lingual document classification,"For comparison , we have evaluated its performance on the original subset of RCV2 as used in previous publications on cross - lingual document classification : we are able to outperform the current state - of - the - art in three out of six transfer directions .",The classification accuracy for zero - shot transfer on the test set of our Multilingual Document Classification Corpus are summarized in .,0.8201438848920863,0.6875,0.4230769230769231,experiments,experimental-setup
document_classification,7,baseline results,multilingual sentence representations,116,46,13,The classifiers based on the MultiCCA embeddings perform very well on the development corpus ( accuracies close or exceeding 90 % ) .,The classification accuracy for zero - shot transfer on the test set of our Multilingual Document Classification Corpus are summarized in .,"The system trained on English also achieves excellent results when transfered to a different languages , it scores best for three out of seven languages ( DE , IT and ZH ) .",0.8345323741007195,0.71875,0.5,experiments,baselines
document_classification,7,baseline results,multilingual sentence representations,117,47,14,"The system trained on English also achieves excellent results when transfered to a different languages , it scores best for three out of seven languages ( DE , IT and ZH ) .",The classifiers based on the MultiCCA embeddings perform very well on the development corpus ( accuracies close or exceeding 90 % ) .,3,0.8417266187050358,0.734375,0.5384615384615384,experiments,baselines
document_classification,7,baseline results,multilingual sentence representations,119,49,16,"However , the transfer accuracies are quite low when training the classifiers on other languages than English , in particular for Russian , Chinese and Japanese .",3,The systems using multilingual sentence embeddings seem to be over all more robust and less language specific .,0.8561151079136691,0.765625,0.6153846153846154,experiments,baselines
document_classification,7,baseline results,multilingual sentence representations,120,50,17,The systems using multilingual sentence embeddings seem to be over all more robust and less language specific .,"However , the transfer accuracies are quite low when training the classifiers on other languages than English , in particular for Russian , Chinese and Japanese .","They score best for four out of seven languages ( EN , ES , FR and RU ) .",0.8633093525179856,0.78125,0.6538461538461539,experiments,approach
document_classification,7,baseline results,multilingual sentence representations,122,52,19,Training on German or French actually leads to better transfer performance than training on English .,"They score best for four out of seven languages ( EN , ES , FR and RU ) .",Crosslingual transfer between very different languages like Chinese and Russian also achieves remarkable results .,0.8776978417266187,0.8125,0.7307692307692307,experiments,baselines
document_classification,7,baseline results,multilingual sentence representations,123,53,20,Crosslingual transfer between very different languages like Chinese and Russian also achieves remarkable results .,Training on German or French actually leads to better transfer performance than training on English .,Targeted cross - lingual document classification,0.8848920863309353,0.828125,0.7692307692307693,experiments,baselines
document_classification,7,baseline results,joint multilingual document classification,130,60,1,Joint multilingual document classification, , ,0.9352517985611508,0.9375,0.2,experiments,baselines
document_classification,7,baseline results,joint multilingual document classification,134,64,5,"This leads to important improvement for all languages , in comparison to zero - shot or targeted transfer learning .",One could argue that the data collection and annotation cost for such a corpus would be the same than producing a corpus of the same size in one language only ., ,0.9640287769784172,1.0,1.0,experiments,baselines
document_classification,8,title,title,2,2,2,Disconnected Recurrent Neural Networks for Text Categorization, , ,0.0077519379844961,1.0,1.0,research-problem,experimental-setup
document_classification,8,introduction,introduction,23,14,14,"In this paper , we incorporate positioninvariance into RNN and propose a novel model named Disconnected Recurrent Neural Network ( DRNN ) .","RNN , however , does n't address such issues well because the representation of the key phrase relies on all the previous terms and the representation changes as the key phrase moves .","Concretely , we disconnect the information transmission of RNN and limit the maximal transmission step length as a fixed value k , so that the representation at each step only depends on the previous k ?",0.0891472868217054,0.3888888888888889,0.3888888888888889,model,experimental-setup
document_classification,8,introduction,introduction,27,18,18,"To maintain the position - invariance , we utilize max pooling to extract the important information , which has been suggested by .","In this way , DRNN can also alleviate the burden of modeling the entire document .",Our proposed model can also be regarded as a special 1D CNN where convolution kernels are replaced with recurrent units .,0.1046511627906976,0.5,0.5,model,model
document_classification,8,introduction,introduction,28,19,19,Our proposed model can also be regarded as a special 1D CNN where convolution kernels are replaced with recurrent units .,"To maintain the position - invariance , we utilize max pooling to extract the important information , which has been suggested by .","Therefore , the maximal transmission step length can also be consid-ered as the window size in CNN .",0.1085271317829457,0.5277777777777778,0.5277777777777778,model,experimental-setup
document_classification,8,method,drnn for text classification,156,78,35,"We utilize the 300D Glo Ve 840B vectors ( Pennington et al. , 2014 ) as our pre-trained word embeddings .",also shows the window sizes that we set for these datasets .,"For words that do not appear in Glo Ve , we average the vector representations of 8 words around the word in training dataset as its word vector , which has been applied by .",0.6046511627906976,0.4333333333333333,0.7608695652173914,hyperparameters,experimental-setup
document_classification,8,method,drnn for text classification,159,81,38,"We use Adadelta ( Zeiler , 2012 ) to optimize all the trainable parameters .","When training our model , word embeddings are updated along with other parameters .",The hyperparameter of Adadelta is set as Zeiler ( 2012 ) suggest that is 1 e ? 6 and ? is 0.95 .,0.6162790697674418,0.45,0.8260869565217391,hyperparameters,approach
document_classification,8,method,drnn for text classification,160,82,39,The hyperparameter of Adadelta is set as Zeiler ( 2012 ) suggest that is 1 e ? 6 and ? is 0.95 .,"We use Adadelta ( Zeiler , 2012 ) to optimize all the trainable parameters .","To avoid the gradient explosion problem , we apply gradient norm clipping .",0.6201550387596899,0.4555555555555556,0.8478260869565217,hyperparameters,experimental-setup
document_classification,8,method,drnn for text classification,161,83,40,"To avoid the gradient explosion problem , we apply gradient norm clipping .",The hyperparameter of Adadelta is set as Zeiler ( 2012 ) suggest that is 1 e ? 6 and ? is 0.95 .,The batch size is set to 128 and all the dimensions of input vectors and hidden shows that our proposed model significantly outperforms all the other models in 7 datasets .,0.624031007751938,0.4611111111111112,0.8695652173913043,hyperparameters,experimental-setup
document_classification,8,method,drnn for text classification,162,84,41,The batch size is set to 128 and all the dimensions of input vectors and hidden shows that our proposed model significantly outperforms all the other models in 7 datasets .,"To avoid the gradient explosion problem , we apply gradient norm clipping .",DRNN does not have too many hyperparameters .,0.627906976744186,0.4666666666666667,0.8913043478260869,hyperparameters,baselines
document_classification,8,method,experimental results,172,94,5,We can see that very deep CNN ( VDCNN ) performs well in large datasets .,Hierarchical attention network ( HAN ) is a hierarchical GRU model with attentive pooling .,"However , VDCNN is a CNN model with 29 convolutional layers , which needs to be tuned more carefully .",0.6666666666666666,0.5222222222222223,0.3846153846153846,results,baselines
document_classification,8,method,experimental results,180,102,13,shows that our model achieves 10 - 50 % relative error reduction compared with char - CRNN in these datasets .,"Compared with their model , our model can better maintain the position - invariance by max pooling .", ,0.6976744186046512,0.5666666666666667,1.0,results,approach
document_classification,8,method,comparison with rnn and cnn,186,108,6,shows that DRNN performs far better than CNN .,we firstly compare DRNN with CNN on AG dataset .,"In addition , the optimal window size of CNN is 3 , while for DRNN the optimal window size is 15 .",0.7209302325581395,0.6,0.0769230769230769,results,baselines
document_classification,8,method,comparison with rnn and cnn,193,115,13,Our model DRNN achieves much better performance than GRU and LSTM .,The experimental results are shown in .,Qualitative Analysis,0.748062015503876,0.6388888888888888,0.1666666666666666,results,baselines
document_classification,9,title,title,2,2,2,Investigating Capsule Networks with Dynamic Routing for Text Classification, , ,0.0082304526748971,1.0,1.0,research-problem,approach
document_classification,9,abstract,abstract,10,8,8,1 Codes are publicly available at : https : //github.com/andyweizhao/capsule_text_ classification .,"To the best of our knowledge , this is the first work that capsule networks have been empirically investigated for text modeling 1 . * Corresponding author ( min.yang@siat.ac.cn )",Introduction,0.0411522633744856,0.2222222222222222,0.2222222222222222,code,baselines
document_classification,9,abstract,abstract,31,29,29,A recent method called capsule network introduced by possesses this attractive potential to address the aforementioned issue .,Improving the efficiency to encode spatial patterns while keeping the flexibility of their representation capability is thus a central issue .,They introduce an iterative routing process to decide the credit attribution between nodes from lower and higher layers .,0.1275720164609053,0.8055555555555556,0.8055555555555556,approach,baselines
document_classification,9,abstract,abstract,32,30,30,They introduce an iterative routing process to decide the credit attribution between nodes from lower and higher layers .,A recent method called capsule network introduced by possesses this attractive potential to address the aforementioned issue .,A metaphor ( also as an argument ) they made is that human visual system intelligently assigns parts to wholes at the inference time without hard - coding patterns to be perspective relevant .,0.1316872427983539,0.8333333333333334,0.8333333333333334,approach,approach
document_classification,9,abstract,abstract,36,34,34,"Three strategies are proposed to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain "" background "" information such as stop words and the words thatare unrelated to specific categories .","In our work , we follow a similar spirit to use this technique in modeling texts .",We conduct a series of experiments with capsule networks on top of the pre-trained word vectors for six text classification benchmarks .,0.1481481481481481,0.9444444444444444,0.9444444444444444,approach,model
document_classification,9,experimental setup,implementation details,139,7,2,"In the experiments , we use 300 - dimensional word2vec vectors to initialize embedding vectors .", ,We conduct mini-batch with size 50 for AG 's news and size 25 for other datasets .,0.5720164609053497,0.5833333333333334,0.4,hyperparameters,experimental-setup
document_classification,9,experimental setup,implementation details,140,8,3,We conduct mini-batch with size 50 for AG 's news and size 25 for other datasets .,"In the experiments , we use 300 - dimensional word2vec vectors to initialize embedding vectors .",We use Adam optimization algorithm with 1e - 3 learning rate to train the model .,0.5761316872427984,0.6666666666666666,0.6,hyperparameters,approach
document_classification,9,experimental setup,implementation details,141,9,4,We use Adam optimization algorithm with 1e - 3 learning rate to train the model .,We conduct mini-batch with size 50 for AG 's news and size 25 for other datasets .,We use 3 iteration of routing for all datasets since it optimizes the loss faster and converges to a lower loss at the end .,0.5802469135802469,0.75,0.8,hyperparameters,approach
document_classification,9,experimental setup,baseline methods,144,12,2,"In the experiments , we evaluate and compare our model with several strong baseline methods including : LSTM / Bi - LSTM , tree - structured LSTM ( Tree - LSTM ) , LSTM regularized by linguistic knowledge ( LR - LSTM ) , CNNrand / CNN - static / CNN - non-static ( Kim , 2014 ) , very deep convolutional network ( VD - CNN ) , and character - level convolutional network ( CL - CNN ) .", , ,0.5925925925925926,1.0,1.0,baselines,approach
document_classification,9,experimental results,experimental results,145,1,1,Experimental Results, , ,0.5967078189300411,0.0161290322580645,1.0,results,approach
document_classification,9,experimental results,quantitative evaluation,149,5,4,"From the results , we observe that the capsule networks achieve best results on 4 out of 6 benchmarks , which verifies the effectiveness of the capsule networks .",We summarize the experimental results in .,"In particular , our model substantially and consistently outperforms",0.6131687242798354,0.0806451612903225,0.8,results,baselines
document_classification,9,experimental results,ablation study,156,12,6,Single - Label to Multi - Label Text Classification,More comprehensive comparison results are demonstrated in . 4 in Supplementary Material .,Capsule network demonstrates promising performance in single - label text classification which as - signs a label from a predefined set to a text ( see ) .,0.6419753086419753,0.1935483870967742,0.2068965517241379,ablation-analysis,approach
document_classification,9,experimental results,ablation study,163,19,13,"In this section , we investigate the capability of capsule network on multi-label text classification by using only the single - label samples as training data .","How deep neural networks ( e.g. , CNN and LSTM ) best cope with multi-label text classification still remains a problem since obtaining large scale of multi-label dataset is a timeconsuming and expensive process .","With feature property as part of the information extracted by capsules , we may generalize the model better to multi-label text classification without an over extensive amount of labeled data .",0.6707818930041153,0.3064516129032258,0.4482758620689655,ablation-analysis,experimental-setup
document_classification,9,experimental results,ablation study,175,31,25,"From the results , we can observe that the capsule networks have substantial and significant improvement in terms of all four evaluation metrics over the strong baseline methods on the test sets in both Reuters - Multi-label and Reuters - Full datasets .",The experimental results are summarized in .,"In particular , larger improvement is achieved on Reuters - Multi - label dataset which only contains the multi-label documents in the test set .",0.7201646090534979,0.5,0.8620689655172413,ablation-analysis,experimental-setup
document_classification,9,experimental results,ablation study,176,32,26,"In particular , larger improvement is achieved on Reuters - Multi - label dataset which only contains the multi-label documents in the test set .","From the results , we can observe that the capsule networks have substantial and significant improvement in terms of all four evaluation metrics over the strong baseline methods on the test sets in both Reuters - Multi-label and Reuters - Full datasets .",This is within our expectation since the capsule network is capable of preserving the instantiated parameters of the categories trained by singlelabel documents .,0.7242798353909465,0.5161290322580645,0.8965517241379308,ablation-analysis,approach
document_classification,9,experimental results,ablation study,178,34,28,The capsule network has much stronger transferring capability than the conventional deep neural networks .,This is within our expectation since the capsule network is capable of preserving the instantiated parameters of the categories trained by singlelabel documents .,"In addition , the good results on Reuters - Full also indicate that the capsule network has robust superiority over competitors on single - label documents .",0.7325102880658436,0.5483870967741935,0.9655172413793104,ablation-analysis,approach
document_classification,9,experimental results,ablation study,179,35,29,"In addition , the good results on Reuters - Full also indicate that the capsule network has robust superiority over competitors on single - label documents .",The capsule network has much stronger transferring capability than the conventional deep neural networks ., ,0.7366255144032922,0.5645161290322581,1.0,ablation-analysis,approach
document_classification,9,experimental results,connection strength visualization,180,36,1,Connection Strength Visualization, , ,0.7407407407407407,0.5806451612903226,0.037037037037037,ablation-analysis,approach
document_classification,9,experimental results,connection strength visualization,188,44,9,"From the results , we observe that capsule networks can correctly recognize and cluster the important phrases with respect to the text categories .","The stronger the connection strength , the bigger the font size .","The histograms are used to show the intensity of connection strengths between primary capsules and the fully connected capsules , as shown in To experimentally verify the convergence of the routing algorithm , we also plot learning curve to show the training loss overtime with different iterations of routing .",0.7736625514403292,0.7096774193548387,0.3333333333333333,ablation-analysis,approach
entity_linking,0,title,title,2,2,2,Deep Joint Entity Disambiguation with Local Neural Attention, , ,0.0077821011673151,1.0,1.0,research-problem,experimental-setup
entity_linking,0,introduction,introduction,9,2,2,Entity disambiguation ( ED ) is an important stage in text understanding which automatically resolves references to entities in a given knowledge base ( KB ) ., ,This task is challenging due to the inherent ambiguity between surface form mentions such as names and the entities they refer to .,0.0350194552529182,0.1666666666666666,0.1666666666666666,research-problem,ablation-analysis
entity_linking,0,introduction,introduction,12,5,5,"ED research has largely focused on two types of contextual information for disambiguation : local information based on words that occur in a context window around an entity mention , and , global information , exploiting document - level coherence of the referenced entities .",This many - to - many ambiguity can often be captured partially by name- entity co-occurrence counts extracted from entity - linked corpora .,"Many stateof - the - art methods aim to combine the benefits of both , which is also the philosophy we follow in this paper .",0.046692607003891,0.4166666666666667,0.4166666666666667,research-problem,ablation-analysis
entity_linking,0,introduction,introduction,18,11,11,The explicit goal of our work is to use deep learning in order to learn basic features and their combinations from scratch .,Such features often rely on domain knowledge and may fail to capture all relevant statistical dependencies and interactions .,"To the best of our knowledge , our approach is the first to carryout this program with full rigor .",0.0700389105058365,0.9166666666666666,0.9166666666666666,model,experimental-setup
entity_linking,0,experiments,training details and hyper parameters,179,11,3,All models are implemented in the Torch framework .,We explain training details of our approach .,Entity Vectors Training & Relatedness Evaluation .,0.6964980544747081,0.1235955056179775,0.088235294117647,experimental-setup,approach
entity_linking,0,experiments,training details and hyper parameters,181,13,5,"For entity embeddings only , we use Wikipedia ( Feb 2014 ) corpus for training .",Entity Vectors Training & Relatedness Evaluation .,Entity vectors are initialized randomly from a 0 mean normal distribution with standard deviation 1 .,0.7042801556420234,0.146067415730337,0.1470588235294117,experimental-setup,approach
entity_linking,0,experiments,training details and hyper parameters,182,14,6,Entity vectors are initialized randomly from a 0 mean normal distribution with standard deviation 1 .,"For entity embeddings only , we use Wikipedia ( Feb 2014 ) corpus for training .",We first train each entity vector on the entity 's Wikipedia canonical description page ( title words included ) for 400 iterations .,0.7081712062256809,0.1573033707865168,0.1764705882352941,experimental-setup,experimental-setup
entity_linking,0,experiments,training details and hyper parameters,183,15,7,We first train each entity vector on the entity 's Wikipedia canonical description page ( title words included ) for 400 iterations .,Entity vectors are initialized randomly from a 0 mean normal distribution with standard deviation 1 .,"Subsequently , Wikipedia hyperlinks of the respective entities are used for learning until validation score ( described below ) stops improving .",0.7120622568093385,0.1685393258426966,0.2058823529411765,experimental-setup,baselines
entity_linking,0,experiments,training details and hyper parameters,186,18,10,We use Adagrad with a learning rate of 0.3 .,"In each iteration , 20 positive words , each with 5 negative words , are sampled and used for optimization as explained in Section 3 .","We choose embedding size d = 300 , pre-trained ( fixed ) Word2 Vec word vectors 8 , ? = 0.6 , ? = 0.1 and window size of 20 for the hyperlinks .",0.7237354085603113,0.2022471910112359,0.2941176470588235,experimental-setup,approach
entity_linking,0,experiments,training details and hyper parameters,187,19,11,"We choose embedding size d = 300 , pre-trained ( fixed ) Word2 Vec word vectors 8 , ? = 0.6 , ? = 0.1 and window size of 20 for the hyperlinks .",We use Adagrad with a learning rate of 0.3 .,We remove stop words before training .,0.7276264591439688,0.2134831460674156,0.3235294117647059,experimental-setup,experimental-setup
entity_linking,0,experiments,training details and hyper parameters,190,22,14,Training of those takes 20 hours on a single TitanX GPU with 12 GB of memory .,"Since our method allows to train the embedding of each entity independently of other entities , we decide for efficiency reasons ( and without loss of generality ) to learn only the vectors of all entities appearing as mention candidates in all the test datasets described in Sec. 7.1 , a total of 270000 entities .",We test and validate our entity embeddings on the entity relatedness dataset of .,0.7392996108949417,0.247191011235955,0.4117647058823529,experimental-setup,approach
entity_linking,0,experiments,training details and hyper parameters,199,31,23,"Our local and global ED models are trained on AIDA - train ( multiple epochs ) , validated on AIDA - A and tested on AIDA - B and other datasets mentioned in Section 7.1 .",Local and Global Model Training .,"We use Adam with learning rate of 1e - 4 until validation accuracy exceeds 90 % , afterwards setting it to 1e - 5 .",0.77431906614786,0.348314606741573,0.6764705882352942,experimental-setup,experimental-setup
entity_linking,0,experiments,training details and hyper parameters,200,32,24,"We use Adam with learning rate of 1e - 4 until validation accuracy exceeds 90 % , afterwards setting it to 1e - 5 .","Our local and global ED models are trained on AIDA - train ( multiple epochs ) , validated on AIDA - A and tested on AIDA - B and other datasets mentioned in Section 7.1 .",Variable size mini-batches consisting of all mentions in a document are used during training .,0.7782101167315175,0.3595505617977528,0.7058823529411765,experimental-setup,approach
entity_linking,0,experiments,training details and hyper parameters,206,38,30,"To regularize , we use early stopping , i.e. we stop learning if the validation accuracy does not increase after 500 epochs .",Validation accuracy is computed after each 5 epochs .,"Training on a single GPU takes , on average , 2 ms per mention , or 16 hours for 1250 epochs over AIDA - train .",0.8015564202334631,0.4269662921348314,0.8823529411764706,experimental-setup,approach
entity_linking,0,experiments,training details and hyper parameters,207,39,31,"Training on a single GPU takes , on average , 2 ms per mention , or 16 hours for 1250 epochs over AIDA - train .","To regularize , we use early stopping , i.e. we stop learning if the validation accuracy does not increase after 500 epochs .","By using diagonal matrices A , B , C , we keep the number of parameters very low ( approx. 1.2 K parameters ) .",0.8054474708171206,0.4382022471910113,0.9117647058823528,experimental-setup,baselines
entity_linking,0,experiments,ed baselines results,228,60,9,We obtain state of the art accuracy on AIDA which is the largest and hardest ( by the accuracy of thep ( e |m ) baseline ) manually created ED dataset .,"We run our system 5 times , each time we pick the best model on the validation set , and report results on the test set for these models .",We are also competitive on the other datasets .,0.8871595330739299,0.6741573033707865,0.5625,results,baselines
entity_linking,0,experiments,ed baselines results,232,64,13,"To gain further insight , we analyzed the accuracy on the AIDA - B dataset for situations where gold entities have low frequency or mention prior .","The merit of our proposed method is to show that , with the exception of thep ( e |m ) feature , a neural network is able to learn the best features for ED without requiring expert input .",shows that our method performs well in these harder cases . :,0.9027237354085604,0.7191011235955056,0.8125,results,ablation-analysis
entity_linking,0,experiments,ed baselines results,233,65,14,shows that our method performs well in these harder cases . :,"To gain further insight , we analyzed the accuracy on the AIDA - B dataset for situations where gold entities have low frequency or mention prior .",Examples of context words selected by our local attention mechanism .,0.906614785992218,0.7303370786516854,0.875,results,model
entity_linking,0,experiments,error analysis,257,89,13,Our code and data are publicly available : http://github.com/dalab/deep-ed,"In the future , we would like to extend this system to perform nil detection , coreference resolution and mention detection .", ,1.0,1.0,1.0,code,experimental-setup
entity_linking,1,title,title,2,2,2,Pre-training of Deep Contextualized Embeddings of Words and Entities for Named Entity Disambiguation, , ,0.0157480314960629,1.0,1.0,research-problem,experimental-setup
entity_linking,1,abstract,abstract,5,3,3,"In this paper , we propose a new contextualized embedding model of words and entities for named entity disambiguation ( NED ) .","Deep contextualized embeddings trained using unsupervised language modeling ( e.g. , ELMo and BERT ) are successful in a wide range of NLP tasks .",Our model is based on the bidirectional transformer encoder and produces contextualized embeddings for words and entities in the input text .,0.0393700787401574,0.375,0.375,research-problem,experimental-setup
entity_linking,1,abstract,abstract,9,7,7,We evaluated our model by addressing NED using a simple NED model based on the trained contextualized embeddings .,We trained the model using entity - annotated texts obtained from Wikipedia .,"As a result , we achieved stateof - the - art or competitive results on several standard NED datasets .",0.0708661417322834,0.875,0.875,research-problem,approach
entity_linking,1,introduction,introduction,18,8,8,"In this paper , we describe a new contextualized embedding model for words and entities for NED .","For instance , proposed Masked Language Model ( MLM ) , which aims to train the embeddings by predicting randomly masked words in the text .","Following , the proposed model is based on the bidirectional transformer encoder .",0.1417322834645669,0.5,0.5,model,experimental-setup
entity_linking,1,introduction,introduction,19,9,9,"Following , the proposed model is based on the bidirectional transformer encoder .","In this paper , we describe a new contextualized embedding model for words and entities for NED .","It takes a sequence of words and entities in the input text , and produces a contextualized embedding for each word and entity .",0.1496062992125984,0.5625,0.5625,model,model
entity_linking,1,introduction,introduction,20,10,10,"It takes a sequence of words and entities in the input text , and produces a contextualized embedding for each word and entity .","Following , the proposed model is based on the bidirectional transformer encoder .","Inspired by MLM , we propose masked entity prediction , a new task that aims to train the embedding model by predicting randomly masked entities based on words and non-masked entities in the input text .",0.1574803149606299,0.625,0.625,model,approach
entity_linking,1,introduction,introduction,21,11,11,"Inspired by MLM , we propose masked entity prediction , a new task that aims to train the embedding model by predicting randomly masked entities based on words and non-masked entities in the input text .","It takes a sequence of words and entities in the input text , and produces a contextualized embedding for each word and entity .",We trained the model using texts and their entity annotations retrieved from Wikipedia .,0.1653543307086614,0.6875,0.6875,model,ablation-analysis
entity_linking,1,introduction,introduction,22,12,12,We trained the model using texts and their entity annotations retrieved from Wikipedia .,"Inspired by MLM , we propose masked entity prediction , a new task that aims to train the embedding model by predicting randomly masked entities based on words and non-masked entities in the input text .",We evaluated the proposed model by addressing NED using an NED model based on trained contextualized embeddings .,0.1732283464566929,0.75,0.75,model,approach
entity_linking,1,background and related work,results,113,87,4,"As shown , our models outperformed all previously proposed models .","6 Several models , including our models , report the 95 % confidence intervals obtained over five runs .","Furthermore , using pseudo entity annotations boosted the accuracy by 0.3 % .",0.8897637795275591,0.9157894736842104,0.3333333333333333,results,approach
entity_linking,1,background and related work,results,114,88,5,"Furthermore , using pseudo entity annotations boosted the accuracy by 0.3 % .","As shown , our models outperformed all previously proposed models .",Methods,0.8976377952755905,0.9263157894736842,0.4166666666666667,results,approach
entity_linking,1,background and related work,results,118,92,9,"Our models achieved new state - of - the - art results on four of the five datasets , namely MSNBC , AQUAINT , ACE2004 , and WNED - WIKI , and performed competitive on the WNED - CLUEWEB dataset .",94.3 0.25 The results of the out - domain scenario are shown in .,"Furthermore , using pseudo entity annotations improved the performance on the AQUAINT and ACE2004 datasets .",0.9291338582677166,0.9684210526315792,0.75,results,baselines
entity_linking,1,background and related work,results,119,93,10,"Furthermore , using pseudo entity annotations improved the performance on the AQUAINT and ACE2004 datasets .","Our models achieved new state - of - the - art results on four of the five datasets , namely MSNBC , AQUAINT , ACE2004 , and WNED - WIKI , and performed competitive on the WNED - CLUEWEB dataset .","Note that unlike all the past models shown in except , our models used in the out - domain scenario were trained only on entity annotations retrieved from Wikipedia .",0.937007874015748,0.9789473684210528,0.8333333333333334,results,approach
entity_linking,10,title,title,2,2,2,Deep contextualized word representations, , ,0.0093457943925233,1.0,1.0,research-problem,approach
entity_linking,10,abstract,abstract,4,2,2,"We introduce a new type of deep contextualized word representation that models both ( 1 ) complex characteristics of word use ( e.g. , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i.e. , to model polysemy ) .", ,"Our word vectors are learned functions of the internal states of a deep bidirectional language model ( biLM ) , which is pretrained on a large text corpus .",0.0186915887850467,0.4,0.4,research-problem,baselines
entity_linking,10,introduction,introduction,9,2,2,Pre-trained word representations are a key component in many neural language understanding models ., ,"However , learning high quality representations can be challenging .",0.0420560747663551,0.0212765957446808,0.0512820512820512,research-problem,experimental-setup
entity_linking,10,introduction,introduction,12,5,5,"In this paper , we introduce a new type of deep contextualized word representation that directly addresses both challenges , can be easily integrated into existing models , and significantly improves the state of the art in every considered case across a range of challenging language understanding problems .","They should ideally model both ( 1 ) complex characteristics of word use ( e.g. , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i.e. , to model polysemy ) .",Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,0.0560747663551401,0.0531914893617021,0.1282051282051282,model,ablation-analysis
entity_linking,10,introduction,introduction,13,6,6,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,"In this paper , we introduce a new type of deep contextualized word representation that directly addresses both challenges , can be easily integrated into existing models , and significantly improves the state of the art in every considered case across a range of challenging language understanding problems .",We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,0.0607476635514018,0.0638297872340425,0.1538461538461538,model,experimental-setup
entity_linking,10,introduction,introduction,14,7,7,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .",0.0654205607476635,0.0744680851063829,0.1794871794871795,model,ablation-analysis
entity_linking,10,introduction,introduction,15,8,8,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .",We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,"Unlike previous approaches for learning contextualized word vectors , ELMo representations are deep , in the sense that they are a function of all of the internal layers of the biLM .",0.0700934579439252,0.0851063829787234,0.2051282051282051,model,approach
entity_linking,10,introduction,introduction,16,9,9,"Unlike previous approaches for learning contextualized word vectors , ELMo representations are deep , in the sense that they are a function of all of the internal layers of the biLM .","For this reason , we call them ELMo ( Embeddings from Language Models ) representations .","More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .",0.0747663551401869,0.0957446808510638,0.2307692307692308,model,ablation-analysis
entity_linking,10,introduction,introduction,17,10,10,"More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .","Unlike previous approaches for learning contextualized word vectors , ELMo representations are deep , in the sense that they are a function of all of the internal layers of the biLM .",Combining the internal states in this manner allows for very rich word representations .,0.0794392523364486,0.1063829787234042,0.2564102564102564,model,baselines
entity_linking,10,introduction,introduction,19,12,12,"Using intrinsic evaluations , we show that the higher - level LSTM states capture context - dependent aspects of word meaning ( e.g. , they can be used without modification to perform well on supervised word sense disambiguation tasks ) while lowerlevel states model aspects of syntax ( e.g. , they can be used to do part - of - speech tagging ) .",Combining the internal states in this manner allows for very rich word representations .,"Simultaneously exposing all of these signals is highly beneficial , allowing the learned models select the types of semi-supervision that are most useful for each end task .",0.0887850467289719,0.1276595744680851,0.3076923076923077,model,approach
entity_linking,10,evaluation,evaluation,103,2,2,Question answering, ,The Stanford Question Answering Dataset ( SQuAD ) contains 100K + crowd sourced questionanswer pairs where the answer is a span in a given Wikipedia paragraph .,0.4813084112149533,0.0606060606060606,0.0606060606060606,experiments,baselines
entity_linking,10,evaluation,evaluation,107,6,6,"After adding ELMo to the baseline model , test set F 1 improved by 4.7 % from 81.1 % to 85.8 % , a 24.9 % relative error reduction over the baseline , and improving the overall single model state - of - the - art by 1.4 % .","It adds a self - attention layer after the bidirectional attention component , simplifies some of the pooling operations and substitutes the LSTMs for gated recurrent units ( GRU s ; .","A 11 member ensemble pushes F 1 to 87.4 , the overall state - of - the - art at time of submission to the leaderboard .",0.5,0.1818181818181818,0.1818181818181818,experiments,baselines
entity_linking,10,evaluation,evaluation,109,8,8,The increase of 4.7 % with ELMo is also significantly larger then the 1.8 % improvement from adding CoVe to a baseline model .,"A 11 member ensemble pushes F 1 to 87.4 , the overall state - of - the - art at time of submission to the leaderboard .","Due to the small test sizes for NER and SST - 5 , we report the mean and standard deviation across five runs with different random seeds .",0.5093457943925234,0.2424242424242424,0.2424242424242424,experiments,baselines
entity_linking,10,evaluation,evaluation,112,11,11,Textual entailment,"The "" increase "" column lists both the absolute and relative improvements over our baseline .","Textual entailment is the task of determining whether a "" hypothesis "" is true , given a "" premise "" .",0.5233644859813084,0.3333333333333333,0.3333333333333333,experiments,approach
entity_linking,10,evaluation,evaluation,116,15,15,"Overall , adding ELMo to the ESIM model improves accuracy by an average of 0.7 % across five random seeds .","Our baseline , the ESIM sequence model from , uses a biL - STM to encode the premise and hypothesis , followed by a matrix attention layer , a local inference layer , another biLSTM inference composition layer , and finally a pooling operation before the output layer .","A five member ensemble pushes the overall accuracy to 89.3 % , exceeding the previous ensemble best of 88.9 % .",0.5420560747663551,0.4545454545454545,0.4545454545454545,experiments,approach
entity_linking,10,evaluation,evaluation,118,17,17,Semantic role labeling,"A five member ensemble pushes the overall accuracy to 89.3 % , exceeding the previous ensemble best of 88.9 % .","A semantic role labeling ( SRL ) system models the predicate - argument structure of a sentence , and is often described as answering "" Who did what to whom "" .",0.5514018691588785,0.5151515151515151,0.5151515151515151,experiments,approach
entity_linking,10,evaluation,evaluation,124,23,23,"In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task , adding ELMo improved the average F 1 by 3.2 % from 67.2 to 70.4 , establishing a new state of the art , again improving over the previous best ensemble result by 1.6 % F 1 .",It uses a biLSTM and attention mechanism to first compute span representations and then applies a softmax mention ranking model to find coreference chains .,Named entity extraction,0.5794392523364486,0.696969696969697,0.696969696969697,experiments,baselines
entity_linking,10,evaluation,evaluation,125,24,24,Named entity extraction,"In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task , adding ELMo improved the average F 1 by 3.2 % from 67.2 to 70.4 , establishing a new state of the art , again improving over the previous best ensemble result by 1.6 % F 1 .","The CoNLL 2003 NER task consists of newswire from the Reuters RCV1 corpus tagged with four different entity types ( PER , LOC , ORG , MISC ) .",0.5841121495327103,0.7272727272727273,0.7272727272727273,experiments,approach
entity_linking,10,evaluation,evaluation,128,27,27,"As shown in , our ELMo enhanced biLSTM - CRF achieves 92. 22 % F 1 averaged over five runs .","Following recent state - of - the - art systems , the baseline model uses pre-trained word embeddings , a character - based CNN representation , two biLSTM layers and a conditional random field ( CRF ) loss , similar to .","The key difference between our system and the previous state of the art from is that we allowed the task model to learn a weighted average of all biLM layers , whereas Peters et al .",0.5981308411214953,0.8181818181818182,0.8181818181818182,experiments,baselines
entity_linking,11,title,title,2,2,2,Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation, , ,0.0103626943005181,1.0,1.0,research-problem,experimental-setup
entity_linking,11,abstract,abstract,5,3,3,"We propose two different methods that greatly reduce the size of neural WSD models , with the benefit of improving their coverage without additional training data , and without impacting their precision .","In this article , we tackle the issue of the limited quantity of manually sense annotated corpora for the task of word sense disambiguation , by exploiting the semantic relationships between senses such as synonymy , hypernymy and hyponymy , in order to compress the sense vocabulary of Princeton WordNet , and thus reduce the number of different sense tags that must be observed to disambiguate all words of the lexical database .","In addition to our methods , we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperforms the state of the art on all WSD evaluation tasks .",0.0259067357512953,0.75,0.75,research-problem,baselines
entity_linking,11,abstract,abstract,6,4,4,"In addition to our methods , we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperforms the state of the art on all WSD evaluation tasks .","We propose two different methods that greatly reduce the size of neural WSD models , with the benefit of improving their coverage without additional training data , and without impacting their precision .", ,0.0310880829015544,1.0,1.0,research-problem,baselines
entity_linking,11,introduction,introduction,8,2,2,"Word Sense Disambiguation ( WSD ) is a task which aims to clarify a text by assigning to each of its words the most suitable sense labels , given a predefined sense inventory .", ,"Various approaches have been proposed to achieve WSD : Knowledge - based methods rely on dictionaries , lexical databases , thesauri or knowledge graphs as primary resources , and use algorithms such as lexical similarity measures or graph - based measures .",0.0414507772020725,0.0952380952380952,0.0952380952380952,research-problem,ablation-analysis
entity_linking,11,introduction,introduction,17,11,11,"In this work , the idea is to solve this issue by taking advantage of the semantic relationships between senses included in WordNet , such as the hypernymy , the hyponymy , the meronymy , the antonymy , etc .","Many works try to leverage this problem by creating new sense annotated corpora , either automatically , semi-automatically , or through crowdsourcing .","Our method is based on the observation that a sense and its closest related senses ( it s hypernym or it s hyponyms for instance ) all share a common idea or concept , and so a word can sometimes be disambiguated using only related concepts .",0.0880829015544041,0.5238095238095238,0.5238095238095238,model,approach
entity_linking,11,introduction,introduction,18,12,12,"Our method is based on the observation that a sense and its closest related senses ( it s hypernym or it s hyponyms for instance ) all share a common idea or concept , and so a word can sometimes be disambiguated using only related concepts .","In this work , the idea is to solve this issue by taking advantage of the semantic relationships between senses included in WordNet , such as the hypernymy , the hyponymy , the meronymy , the antonymy , etc .","Consequently , we do not need to know every sense of WordNet to disambiguate all words of WordNet .",0.0932642487046632,0.5714285714285714,0.5714285714285714,model,experimental-setup
entity_linking,11,related work,implementation details,132,105,2,"For BERT , we used the model named "" bert - largecased "" of the PyTorch implementation 3 , which consists of vectors of dimension 1024 , trained on Book s Corpus and English Wikipedia .", ,"Due to the fact that BERT 's internal tokenizer sometimes split words in multiples tokens ( i.e. [ "" rodent "" ] becomes [ "" rode "" , "" # # nt "" ] ) , we trained our system to predict a sense tag on the first token only of a splitted annotated word .",0.6839378238341969,0.660377358490566,0.4,experimental-setup,baselines
entity_linking,11,related work,implementation details,134,107,4,"For the Transformer encoder layers , we used the same parameters as the "" base "" model of , that is 6 layers with 8 attention heads , a hidden size of 2048 , and a dropout of 0.1 .","Due to the fact that BERT 's internal tokenizer sometimes split words in multiples tokens ( i.e. [ "" rodent "" ] becomes [ "" rode "" , "" # # nt "" ] ) , we trained our system to predict a sense tag on the first token only of a splitted annotated word .","Finally , because BERT already encodes the position of the words inside their vectors , we did not add any positional encoding .",0.6943005181347149,0.6729559748427673,0.8,experimental-setup,experimental-setup
entity_linking,11,related work,evaluation,164,137,13,"In the results in , we first observe that our systems that use the sense vocabulary compression through hypernyms or through all relations obtain scores that are overall equivalent to the systems that do not use it .",""" Backoff on Monosemics "" means that monosemic words are considered annotated .",Our methods greatly improves their coverage on the evaluation tasks however .,0.8497409326424871,0.8616352201257862,0.7222222222222222,results,baselines
entity_linking,11,related work,evaluation,169,142,18,"In comparison to the other works , thanks to the Princeton WordNet Gloss Corpus added to the training data and the use of BERT as input embeddings , we outperform systematically the state of the art on every task .","If we exclude the monosemic words , the system based on our compression method through all relations miss only one word ( the adverb "" eloquently "" ) when trained on the SemCor , and has a coverage to 100 % when the WNGC is addded .", ,0.8756476683937824,0.8930817610062893,1.0,results,approach
entity_linking,11,related work,ablation study,180,153,11,"As we can see in , the additional training corpus ( WNGC ) and even more the use of BERT as input embeddings both have a major impact on our results and lead to scores above the state of the art .","For the ensembling method , we either perform ensembling as in our main results , by averaging the prediction of 8 models trained separately or we give the mean and the standard deviation of the scores of the 8 models evaluated separately .","Using BERT instead of ELMo or Glo Ve improves respectively the score by approximately 3 and 5 points in every experiment , and adding the WNGC to the training data improves it by approximately 2 points .",0.932642487046632,0.9622641509433962,0.6470588235294118,ablation-analysis,approach
entity_linking,11,related work,ablation study,181,154,12,"Using BERT instead of ELMo or Glo Ve improves respectively the score by approximately 3 and 5 points in every experiment , and adding the WNGC to the training data improves it by approximately 2 points .","As we can see in , the additional training corpus ( WNGC ) and even more the use of BERT as input embeddings both have a major impact on our results and lead to scores above the state of the art .","Finally , using ensembles adds roughly another 1 point to the final F1 score .",0.9378238341968912,0.9685534591194968,0.7058823529411765,ablation-analysis,model
entity_linking,11,related work,ablation study,182,155,13,"Finally , using ensembles adds roughly another 1 point to the final F1 score .","Using BERT instead of ELMo or Glo Ve improves respectively the score by approximately 3 and 5 points in every experiment , and adding the WNGC to the training data improves it by approximately 2 points .",5 https://allennlp.org/elmo,0.9430051813471504,0.9748427672955976,0.7647058823529411,ablation-analysis,approach
entity_linking,11,related work,ablation study,186,159,17,"However , the compression method through all relations seems to negatively impact the results in some cases ( when using ELMo or GloVe especially ) .","Finally , through the scores obtained by invidual models ( without ensemble ) , we can observe on the standard deviations that the vocabulary compression method through hypernyms never impact significantly the final score .", ,0.9637305699481864,1.0,1.0,ablation-analysis,approach
entity_linking,12,incorporating glosses into neural word sense disambiguation,incorporating glosses into neural word sense disambiguation,2,1,1,Incorporating Glosses into Neural Word Sense Disambiguation, , ,0.0075187969924812,1.0,1.0,research-problem,experimental-setup
entity_linking,12,abstract,abstract,4,2,2,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context ., ,Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge - based methods .,0.0150375939849624,0.2222222222222222,0.2222222222222222,research-problem,ablation-analysis
entity_linking,12,abstract,abstract,5,3,3,Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge - based methods .,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .,"However , previous neural networks for WSD always rely on massive labeled data ( context ) , ignoring lexical resources like glosses ( sense definitions ) .",0.018796992481203,0.3333333333333333,0.3333333333333333,research-problem,baselines
entity_linking,12,introduction,introduction,30,19,19,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .","The figure shows that bed 2 is a kind of plot 2 , and bed 2 includes flowerbed 1 , seedbed 1 , etc .",GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,0.112781954887218,0.6785714285714286,0.6785714285714286,model,model
entity_linking,12,introduction,introduction,31,20,20,GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .","In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .",0.1165413533834586,0.7142857142857143,0.7142857142857143,model,baselines
entity_linking,12,introduction,introduction,32,21,21,"In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .",GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,The main contributions of this paper are listed as follows :,0.1203007518796992,0.75,0.75,model,ablation-analysis
entity_linking,12,incorporating glosses into neural word sense disambiguation,incorporating glosses into neural word sense disambiguation,65,1,1,Incorporating Glosses into Neural Word Sense Disambiguation, , ,0.2443609022556391,0.0086206896551724,0.3333333333333333,research-problem,experimental-setup
entity_linking,12,experiments and evaluation,implementation details,195,15,3,"We use pre-trained word embeddings with 300 dimensions 9 , and keep them fixed during the training process .","We use the validation set ( SE7 ) to find the optimal settings of our framework : the hidden state size n , the number of passes | T M | , the optimizer , etc .","We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .",0.7330827067669173,0.1923076923076923,0.3,hyperparameters,experimental-setup
entity_linking,12,experiments and evaluation,implementation details,196,16,4,"We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .","We use pre-trained word embeddings with 300 dimensions 9 , and keep them fixed during the training process .","Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .",0.7368421052631579,0.2051282051282051,0.4,hyperparameters,experimental-setup
entity_linking,12,experiments and evaluation,implementation details,197,17,5,"Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .","We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .",We assign gloss expansion depth K the value of 4 .,0.7406015037593985,0.2179487179487179,0.5,hyperparameters,experimental-setup
entity_linking,12,experiments and evaluation,implementation details,198,18,6,We assign gloss expansion depth K the value of 4 .,"Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .","We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .",0.7443609022556391,0.2307692307692308,0.6,hyperparameters,approach
entity_linking,12,experiments and evaluation,implementation details,199,19,7,"We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .",We assign gloss expansion depth K the value of 4 .,We use Adam optimizer in the training process with 0.001 initial learning rate .,0.7481203007518797,0.2435897435897436,0.7,hyperparameters,approach
entity_linking,12,experiments and evaluation,implementation details,200,20,8,We use Adam optimizer in the training process with 0.001 initial learning rate .,"We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .","In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .",0.7518796992481203,0.2564102564102564,0.8,hyperparameters,approach
entity_linking,12,experiments and evaluation,implementation details,201,21,9,"In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .",We use Adam optimizer in the training process with 0.001 initial learning rate .,Training runs for up to 100 epochs with early stopping if the validation loss does n't improve within the last 10 epochs .,0.7556390977443609,0.2692307692307692,0.9,hyperparameters,approach
entity_linking,12,experiments and evaluation,implementation details,202,22,10,Training runs for up to 100 epochs with early stopping if the validation loss does n't improve within the last 10 epochs .,"In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .", ,0.7593984962406015,0.282051282051282,1.0,hyperparameters,approach
entity_linking,12,experiments and evaluation,systems to be compared,210,30,8,Knowledge - based Systems,"The fives blocks list the MFS baseline , two knowledge - based systems , two supervised systems ( feature - based ) , three neuralbased systems and our models , respectively .",. gloss information via its semantic relations can help to WSD .,0.7894736842105263,0.3846153846153846,0.3809523809523809,baselines,approach
entity_linking,12,experiments and evaluation,systems to be compared,212,32,10,Babelfy : exploits the semantic network structure from BabelNet and builds a unified graph - based architecture for WSD and Entity Linking .,. gloss information via its semantic relations can help to WSD .,Supervised Systems,0.7969924812030075,0.4102564102564102,0.4761904761904762,baselines,model
entity_linking,12,experiments and evaluation,systems to be compared,213,33,11,Supervised Systems,Babelfy : exploits the semantic network structure from BabelNet and builds a unified graph - based architecture for WSD and Entity Linking .,The supervised systems mentioned in this paper refers to traditional feature - based systems which train a dedicated classifier for every word individually ( word expert ) .,0.8007518796992481,0.4230769230769231,0.5238095238095238,baselines,approach
entity_linking,12,experiments and evaluation,systems to be compared,215,35,13,"IMS : Zhi and Ng ( 2010 ) selects a linear Support Vector Machine ( SVM ) as its classifier and makes use of a set of features surrounding the target word within a limited window , such as POS tags , local words and local collocations .",The supervised systems mentioned in this paper refers to traditional feature - based systems which train a dedicated classifier for every word individually ( word expert ) .,IMS +emb : selects IMS as the underlying framework and makes use of word embeddings as features which makes it hard to beat inmost of WSD datasets .,0.8082706766917294,0.4487179487179487,0.6190476190476191,baselines,ablation-analysis
entity_linking,12,experiments and evaluation,systems to be compared,216,36,14,IMS +emb : selects IMS as the underlying framework and makes use of word embeddings as features which makes it hard to beat inmost of WSD datasets .,"IMS : Zhi and Ng ( 2010 ) selects a linear Support Vector Machine ( SVM ) as its classifier and makes use of a set of features surrounding the target word within a limited window , such as POS tags , local words and local collocations .",Neural - based Systems,0.8120300751879699,0.4615384615384616,0.6666666666666666,baselines,model
entity_linking,12,experiments and evaluation,systems to be compared,217,37,15,Neural - based Systems,IMS +emb : selects IMS as the underlying framework and makes use of word embeddings as features which makes it hard to beat inmost of WSD datasets .,Neural - based systems aim to build an end - to - end unified neural network for all the polysemous words in texts .,0.8157894736842105,0.4743589743589744,0.7142857142857143,baselines,approach
entity_linking,12,experiments and evaluation,systems to be compared,219,39,17,Bi- LSTM : leverages a bidirectional LSTM network which shares model parameters among all words .,Neural - based systems aim to build an end - to - end unified neural network for all the polysemous words in texts .,Note that this model is equivalent to our model if we remove the gloss module and memory module of GAS .,0.8233082706766918,0.5,0.8095238095238095,baselines,model
entity_linking,12,experiments and evaluation,systems to be compared,221,41,19,"Bi-LSTM +att.+ LEX and it s variant Bi- LSTM +att.+ LEX+P OS : transfers WSD into a sequence learning task and propose a multi - task learning framework for WSD , POS tagging and coarse - grained semantic labels ( LEX ) .",Note that this model is equivalent to our model if we remove the gloss module and memory module of GAS .,"These two models have used the external knowledge , for the LEX is based on lexicographer files in WordNet .",0.8308270676691729,0.5256410256410257,0.9047619047619048,baselines,model
entity_linking,12,experiments and evaluation,results and discussion,225,45,2,English all - words results, ,"In this section , we show the performance of our proposed model in the English all - words task .",0.8458646616541353,0.5769230769230769,0.0571428571428571,results,approach
entity_linking,12,experiments and evaluation,results and discussion,230,50,7,GAS and GAS ext achieves the state - of - theart performance on the concatenation of all test datasets .,The last block lists the performance of our proposed model GAS and its variant GAS ext which extends the gloss module in GAS .,"Although there is no one system always performs best on all the test sets 10 , we can find that GAS ext with concatenation memory updating strategy achieves the best results 70.6 on the concatenation of the four test datasets .",0.8646616541353384,0.6410256410256411,0.2,results,baselines
entity_linking,12,experiments and evaluation,results and discussion,231,51,8,"Although there is no one system always performs best on all the test sets 10 , we can find that GAS ext with concatenation memory updating strategy achieves the best results 70.6 on the concatenation of the four test datasets .",GAS and GAS ext achieves the state - of - theart performance on the concatenation of all test datasets .,Compared with other three neural - based methods in the Context :,0.8684210526315791,0.6538461538461539,0.2285714285714285,results,baselines
entity_linking,12,experiments and evaluation,results and discussion,234,54,11,It shows that appropriate number of passes can boost the performance as well as avoid over - fitting of the model .,He plays a pianist in the film Glosses Pass 1 Pass 2 Pass 3 Pass 4 Pass 5 g 1 : participate in games or sport g 2 : perform music on a instrument g 3 : act a role or part : F1-score ( % ) of different passes from 1 to 5 on the test data sets .,". fourth block , we can find that our best model outperforms the previous best neural network models on every individual test set .",0.8796992481203008,0.6923076923076923,0.3142857142857143,results,approach
entity_linking,12,experiments and evaluation,results and discussion,242,62,19,Multiple Passes Analysis,This proves that incorporating extended glosses through its hypernyms and hyponyms into the neural network models can boost the performance for WSD .,"To better illustrate the influence of multiple passes , we give an example in .",0.9097744360902256,0.7948717948717948,0.5428571428571428,results,approach
entity_linking,12,experiments and evaluation,results and discussion,252,72,29,"It shows that multiple passes operation performs better than one pass , though the improvement is not significant .","Furthermore , shows the effectiveness of multi-pass operation in the memory module .","The reason of this phenomenon is that for most target words , one main word sense accounts for the majority of their appearances .",0.9473684210526316,0.9230769230769232,0.8285714285714286,results,approach
entity_linking,12,experiments and evaluation,results and discussion,256,76,33,"In Table 3 , with the increasing number of passes , the F1 - score increases .",Case studies in show that the proposed multipass inference can help to recognize the infrequent senses like the third sense for wordplay .,"However , when the number of passes is larger than 3 , the F1- score stops increasing or even decreases due to over-fitting .",0.962406015037594,0.9743589743589745,0.9428571428571428,results,approach
entity_linking,12,experiments and evaluation,results and discussion,257,77,34,"However , when the number of passes is larger than 3 , the F1- score stops increasing or even decreases due to over-fitting .","In Table 3 , with the increasing number of passes , the F1 - score increases .",It shows that appropriate number of passes can boost the performance as well as avoid over - fitting of the model .,0.9661654135338346,0.9871794871794872,0.9714285714285714,results,baselines
entity_linking,13,title,title,2,2,2,Word Sense Disambiguation using a Bidirectional LSTM, , ,0.0172413793103448,1.0,1.0,research-problem,experimental-setup
entity_linking,13,introduction,introduction,14,4,4,"The task of assigning a word token in a text , e.g. rock , to a well defined word sense in a lexicon is called word sense disambiguation ( WSD ) .","For instance , the word rock can refer to both a stone and a music genre , but in the sentence "" Without the guitar , there would be no rock music "" the sense of rock is no longer ambiguous .",From the rock example above it is easy to see that the context surrounding the word is what disambiguates the sense .,0.1206896551724138,0.2,0.2666666666666666,research-problem,experimental-setup
entity_linking,13,introduction,introduction,19,9,9,"Improved WSD would be beneficial to many natural language processing ( NLP ) problems , e.g. machine translation , information Retrieval , information Extraction , and sense aware word representations .","With this in mind , our thesis is that to do WSD well we need to go beyond bag of words and into the territory of sequence modeling .","However , though much progress has been made in the area , many current WSD systems suffer from one or two of the following deficits .",0.1637931034482758,0.45,0.6,research-problem,ablation-analysis
entity_linking,13,introduction,introduction,23,13,13,"We aim to mitigate these problems by ( 1 ) modeling the sequence of words surrounding the target word , and ( 2 ) refrain from using any hand crafted features or external resources and instead represent the words using real valued vector representation , i.e. word embeddings .","( 2 ) Relying on complicated and potentially language specific hand crafted features and resources , which is a big problem particularly for resource poor languages .",Using word embeddings has previously been shown to improve WSD .,0.1982758620689655,0.65,0.8666666666666667,model,ablation-analysis
entity_linking,13,the model,experimental settings,83,39,3,"The source code , implemented using TensorFlow , has been released as open source 1 .","The hyperparameter settings used during the experiments , presented in , were tuned on a separate validation set with data picked from the SE2 training set .",Hyperparameter,0.7155172413793104,0.5909090909090909,0.1875,experimental-setup,approach
entity_linking,13,the model,experimental settings,87,43,7,The embeddings are initialized using a set of freely available 2 Glo Ve vectors trained on Wikipedia and Gigaword .,Embeddings,"Words not included in this set are initialized from N ( 0 , 0.1 ) .",0.75,0.6515151515151515,0.4375,experimental-setup,approach
entity_linking,13,the model,experimental settings,88,44,8,"Words not included in this set are initialized from N ( 0 , 0.1 ) .",The embeddings are initialized using a set of freely available 2 Glo Ve vectors trained on Wikipedia and Gigaword .,To keep the input noise proportional to the embeddings it is scaled by ?,0.7586206896551724,0.6666666666666666,0.5,experimental-setup,experimental-setup
entity_linking,13,the model,results,106,62,10,Our proposed model achieves the top score on SE2 and are tied with IMS + adapted CW on SE3 .,"However , it also relies on a rich set of other features including POS tags , collocations and surrounding words to achieve their reported result .","Moreover , we see that dropword consistently improves the results on both SE2 and SE3 .",0.913793103448276,0.9393939393939394,0.7142857142857143,results,baselines
entity_linking,13,the model,results,107,63,11,"Moreover , we see that dropword consistently improves the results on both SE2 and SE3 .",Our proposed model achieves the top score on SE2 and are tied with IMS + adapted CW on SE3 .,"Randomizing the order of the input words yields a substantially worse result , which provides evidence for our hypothesis that the order of the words are significant .",0.9224137931034484,0.9545454545454546,0.7857142857142857,results,baselines
entity_linking,13,the model,results,108,64,12,"Randomizing the order of the input words yields a substantially worse result , which provides evidence for our hypothesis that the order of the words are significant .","Moreover , we see that dropword consistently improves the results on both SE2 and SE3 .",We also see that the system effectively makes use of the information in the pre-trained word embeddings and that they are essential to the performance of our system on these datasets .,0.9310344827586208,0.9696969696969696,0.8571428571428571,results,baselines
entity_linking,13,the model,results,109,65,13,We also see that the system effectively makes use of the information in the pre-trained word embeddings and that they are essential to the performance of our system on these datasets .,"Randomizing the order of the input words yields a substantially worse result , which provides evidence for our hypothesis that the order of the words are significant .",F1 score,0.9396551724137931,0.9848484848484848,0.9285714285714286,results,baselines
entity_linking,14,title,title,2,2,2,Knowledge - based Word Sense Disambiguation using Topic Models, , ,0.0102564102564102,1.0,1.0,research-problem,approach
entity_linking,14,abstract,abstract,4,2,2,Disambiguation is an open problem in Natural Language Processing which is particularly challenging and useful in the unsupervised setting where all the words in any given text need to be disambiguated without using any labeled data ., ,Typically WSD systems use the sentence or a small window of words around the target word as the context for disambiguation because their computational complexity scales exponentially with the size of the context .,0.0205128205128205,0.25,0.25,research-problem,baselines
entity_linking,14,abstract,abstract,5,3,3,Typically WSD systems use the sentence or a small window of words around the target word as the context for disambiguation because their computational complexity scales exponentially with the size of the context .,Disambiguation is an open problem in Natural Language Processing which is particularly challenging and useful in the unsupervised setting where all the words in any given text need to be disambiguated without using any labeled data .,"In this paper , we leverage the formalism of topic model to design a WSD system that scales linearly with the number of words in the context .",0.0256410256410256,0.375,0.375,research-problem,model
entity_linking,14,introduction,introduction,12,2,2,Word Sense Disambiguation ( WSD ) is the task of mapping an ambiguous word in a given context to its correct meaning ., ,"WSD is an important problem in natural language processing ( NLP ) , both in its own right and as a steppingstone to more advanced tasks such as machine translation , information extraction and retrieval , and question answering .",0.0615384615384615,0.1052631578947368,0.1052631578947368,research-problem,ablation-analysis
entity_linking,14,introduction,introduction,22,12,12,"In this paper , we propose a novel knowledge - based WSD algorithm for the all - word WSD task , which utilizes the whole document as the context for a word , rather than just the current sentence used by most WSD systems .",These systems only require an external knowledge source ( such as WordNet ) but no labeled training data .,"In order to model the whole document for WSD , we leverage the formalism of topic models , especially Latent Dirichlet Allocation ( LDA ) .",0.1128205128205128,0.631578947368421,0.631578947368421,model,ablation-analysis
entity_linking,14,introduction,introduction,23,13,13,"In order to model the whole document for WSD , we leverage the formalism of topic models , especially Latent Dirichlet Allocation ( LDA ) .","In this paper , we propose a novel knowledge - based WSD algorithm for the all - word WSD task , which utilizes the whole document as the context for a word , rather than just the current sentence used by most WSD systems .",Our method is a variant of LDA in which the topic proportions for a document are replaced by synset proportions for a document .,0.1179487179487179,0.6842105263157895,0.6842105263157895,model,experimental-setup
entity_linking,14,introduction,introduction,24,14,14,Our method is a variant of LDA in which the topic proportions for a document are replaced by synset proportions for a document .,"In order to model the whole document for WSD , we leverage the formalism of topic models , especially Latent Dirichlet Allocation ( LDA ) .",We use a non-uniform prior for the synset distribution over words to model the frequency of words within a synset .,0.123076923076923,0.7368421052631579,0.7368421052631579,model,approach
entity_linking,14,introduction,introduction,25,15,15,We use a non-uniform prior for the synset distribution over words to model the frequency of words within a synset .,Our method is a variant of LDA in which the topic proportions for a document are replaced by synset proportions for a document .,"Furthermore , we also model the relationships between synsets by using a logisticnormal prior for drawing the synset proportions of the document .",0.1282051282051282,0.7894736842105263,0.7894736842105263,model,approach
entity_linking,14,introduction,introduction,26,16,16,"Furthermore , we also model the relationships between synsets by using a logisticnormal prior for drawing the synset proportions of the document .",We use a non-uniform prior for the synset distribution over words to model the frequency of words within a synset .,"This makes our model similar to the correlated topic model , with the difference that our priors are not learned but fixed .",0.1333333333333333,0.8421052631578947,0.8421052631578947,model,model
entity_linking,14,experiments results,experiments results,169,9,9,"The proposed method , denoted by WSD - TM in the tables referring to WSD using topic models , outperforms the state - of - the - art WSD system by a significant margin ( pvalue < 0.01 ) by achieving an overall F1 - score of 66.9 as compared to Moro14 's score of 65.5 .","We also provide the F 1 scores of MFS baseline , i.e. labeling each word with its most frequent sense ( MFS ) in labeled datasets , ) and OM - STI ( Taghipour and Ng 2015 ) .","We also observe that the performance of the proposed model is not much worse than the best supervised system , Melamud16 ( 69.4 ) .",0.8666666666666667,0.6923076923076923,0.6923076923076923,results,baselines
entity_linking,14,experiments results,experiments results,170,10,10,"We also observe that the performance of the proposed model is not much worse than the best supervised system , Melamud16 ( 69.4 ) .","The proposed method , denoted by WSD - TM in the tables referring to WSD using topic models , outperforms the state - of - the - art WSD system by a significant margin ( pvalue < 0.01 ) by achieving an overall F1 - score of 66.9 as compared to Moro14 's score of 65.5 .",In we report the F 1 scores on different parts of speech .,0.8717948717948718,0.7692307692307693,0.7692307692307693,results,baselines
entity_linking,14,experiments results,experiments results,172,12,12,The proposed system outperforms all previous knowledgebased systems overall parts of speech .,In we report the F 1 scores on different parts of speech .,This indicates that using document context helps in disambiguating words of all PoS tags .,0.8820512820512819,0.9230769230769232,0.9230769230769232,results,approach
entity_linking,15,title,title,2,2,2,Mixing Context Granularities for Improved Entity Linking on Question Answering Data across Entity Categories, , ,0.0077220077220077,1.0,1.0,research-problem,approach
entity_linking,15,introduction,introduction,11,3,3,"The first stage for every QA approach is entity linking ( EL ) , that is the identification of entity mentions in the question and linking them to entities in KB .",Knowledge base question answering ( QA ) requires a precise modeling of the question semantics through the entities and relations available in the knowledge base ( KB ) in order to retrieve the correct answer .,"In , two entity mentions are detected and linked to the knowledge base referents .",0.0424710424710424,0.0375,0.0375,research-problem,model
entity_linking,15,introduction,introduction,14,6,6,The state - of - the - art QA systems usually rely on off - the - shelf EL systems to extract entities from the question .,This step is crucial for QA since the correct answer must be connected via some path over KB to the entities mentioned in the question .,Multiple EL systems are freely available and can be readily applied what are taylor swift 's albums ?,0.054054054054054,0.075,0.075,research-problem,ablation-analysis
entity_linking,15,introduction,introduction,27,19,19,"In this paper , we present an approach that tackles the challenges listed above : we perform entity mention detection and entity disambiguation jointly in a single neural model that makes the whole process end - to - end differentiable .",The QA data also features many entities of various categories and differs in this respect from the Twitter datasets that are often used to evaluate EL systems .,"This ensures that any token n-gram can be considered as a potential entity mention , which is important to be able to link entities of different categories , such as movie titles and organization names .",0.1042471042471042,0.2375,0.2375,approach,baselines
entity_linking,15,introduction,introduction,29,21,21,"To overcome the noise in the data , we automatically learn features over a set of contexts of different granularity levels .","This ensures that any token n-gram can be considered as a potential entity mention , which is important to be able to link entities of different categories , such as movie titles and organization names .",Each level of granularity is handled by a separate component of the model .,0.1119691119691119,0.2625,0.2625,approach,experimental-setup
entity_linking,15,introduction,introduction,30,22,22,Each level of granularity is handled by a separate component of the model .,"To overcome the noise in the data , we automatically learn features over a set of contexts of different granularity levels .","A token - level component extracts higher - level features from the whole question context , whereas a character - level component builds lower - level features for the candidate n-gram .",0.1158301158301158,0.275,0.275,approach,approach
entity_linking,15,introduction,introduction,31,23,23,"A token - level component extracts higher - level features from the whole question context , whereas a character - level component builds lower - level features for the candidate n-gram .",Each level of granularity is handled by a separate component of the model .,"Simultaneously , we extract features from the knowledge base context of the candidate entity : character - level features are extracted for the entity label and higher - level features are produced based on the entities surrounding the candidate entity in the knowledge graph .",0.1196911196911196,0.2875,0.2875,approach,approach
entity_linking,15,introduction,introduction,32,24,24,"Simultaneously , we extract features from the knowledge base context of the candidate entity : character - level features are extracted for the entity label and higher - level features are produced based on the entities surrounding the candidate entity in the knowledge graph .","A token - level component extracts higher - level features from the whole question context , whereas a character - level component builds lower - level features for the candidate n-gram .",This information is aggregated and used to predict whether the n-gram is an entity mention and to what entity it should be linked .,0.1235521235521235,0.3,0.3,approach,model
entity_linking,15,introduction,introduction,33,25,25,This information is aggregated and used to predict whether the n-gram is an entity mention and to what entity it should be linked .,"Simultaneously , we extract features from the knowledge base context of the candidate entity : character - level features are extracted for the entity label and higher - level features are produced based on the entities surrounding the candidate entity in the knowledge graph .",Contributions,0.1274131274131274,0.3125,0.3125,approach,model
entity_linking,15,introduction,introduction,42,34,34,The complete code as well as the scripts that produce the evaluation data can be found here : https://github.com/UKPLab/ starsem2018-entity-linking.,Our system can be applied on any QA dataset .,"Several benchmarks exist for EL on Wikipedia texts and news articles , such as ACE and CoNLL - YAGO .",0.1621621621621621,0.425,0.425,code,approach
entity_linking,15,experiments,baselines,212,15,2,Existing systems, ,"In our experiments , we compare to DBPedia Spotlight that was used in several QA systems and represents a strong baseline for entity linking 4 .",0.8185328185328186,0.2727272727272727,0.1666666666666666,baselines,baselines
entity_linking,15,experiments,baselines,213,16,3,"In our experiments , we compare to DBPedia Spotlight that was used in several QA systems and represents a strong baseline for entity linking 4 .",Existing systems,"In addition , we are able to compare to the state - of - the - art S - MART system , since their output on the WebQSP datasets was publicly released 5 .",0.8223938223938224,0.2909090909090909,0.25,baselines,approach
entity_linking,15,experiments,baselines,214,17,4,"In addition , we are able to compare to the state - of - the - art S - MART system , since their output on the WebQSP datasets was publicly released 5 .","In our experiments , we compare to DBPedia Spotlight that was used in several QA systems and represents a strong baseline for entity linking 4 .","The S - MART system is not openly available , it was first trained on the NEEL 2014 Twitter dataset and later adapted to the QA data .",0.8262548262548263,0.3090909090909091,0.3333333333333333,baselines,approach
entity_linking,15,experiments,baselines,216,19,6,We also include a heuristics baseline that ranks candidate entities according to their frequency in Wikipedia .,"The S - MART system is not openly available , it was first trained on the NEEL 2014 Twitter dataset and later adapted to the QA data .",This baseline represents a reasonable lower bound fora Wikidata based approach .,0.8339768339768341,0.3454545454545454,0.5,baselines,approach
entity_linking,15,experiments,baselines,218,21,8,Simplified VCG,This baseline represents a reasonable lower bound fora Wikidata based approach .,"To test the effect of the end - toend context encoders of the VCG network , we define a model that instead uses a set of features commonly suggested in the literature for EL on noisy data .",0.8416988416988417,0.3818181818181817,0.6666666666666666,baselines,baselines
entity_linking,15,experiments,baselines,220,23,10,"In particular , we employ features that cover ( 1 ) frequency of the entity in Wikipedia , ( 2 ) edit distance between the label of the entity and the token n-gram , ( 3 ) number of entities and relations immediately connected to the entity in the KB , ( 4 ) word overlap between the input question and the labels of the connected entities and relations , ( 5 ) length of the n-gram .","To test the effect of the end - toend context encoders of the VCG network , we define a model that instead uses a set of features commonly suggested in the literature for EL on noisy data .","We also add an average of the word embeddings of the question tokens and , separately , an average of the embeddings of tokens of entities and relations connected to the entity candidate .",0.8494208494208494,0.4181818181818181,0.8333333333333334,baselines,approach
entity_linking,15,experiments,results,238,41,4,The VCG model shows the overall F- score result that is better than the DBPedia Spotlight baseline by a wide margin .,We can see that GraphQuestions provides a much more difficult benchmark for EL .,It is notable that again our model achieves higher precision values as compared to other approaches and manages to keep a satisfactory level of recall .,0.9189189189189192,0.7454545454545455,0.2222222222222222,results,baselines
entity_linking,15,experiments,results,239,42,5,It is notable that again our model achieves higher precision values as compared to other approaches and manages to keep a satisfactory level of recall .,The VCG model shows the overall F- score result that is better than the DBPedia Spotlight baseline by a wide margin .,Analysis,0.9227799227799228,0.7636363636363637,0.2777777777777778,results,approach
entity_linking,16,title,title,2,2,2,One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation of Text Data, , ,0.0127388535031847,1.0,1.0,research-problem,experimental-setup
entity_linking,16,abstract,abstract,5,3,3,"To mine these data properly , attributable to their innate ambiguity , a Word Sense Disambiguation ( WSD ) algorithm can avoid numbers of difficulties in Natural Language Processing ( NLP ) pipeline .","Due to recent technical and scientific advances , we have a wealth of information hidden in unstructured text data such as offline / online narratives , research articles , and clinical reports .","However , considering a large number of ambiguous words in one language or technical domain , we may encounter limiting constraints for proper deployment of existing WSD models .",0.0318471337579617,0.4285714285714285,0.4285714285714285,research-problem,baselines
entity_linking,16,abstract,abstract,6,4,4,"However , considering a large number of ambiguous words in one language or technical domain , we may encounter limiting constraints for proper deployment of existing WSD models .","To mine these data properly , attributable to their innate ambiguity , a Word Sense Disambiguation ( WSD ) algorithm can avoid numbers of difficulties in Natural Language Processing ( NLP ) pipeline .",This paper attempts to address the problem of oneclassifier - per - one - word WSD algorithms by proposing a single Bidirectional Long Short - Term Memory ( BLSTM ) network which by considering senses and context sequences works on all ambiguous words collectively .,0.0382165605095541,0.5714285714285714,0.5714285714285714,research-problem,model
entity_linking,16,introduction,introduction,16,7,7,"In this effort , we develop our supervised WSD model that leverages a Bidirectional Long Short - Term Memory ( BLSTM ) network .","Therefore , the ambiguous word cold is specified along with the sense set { S1 , S2 , S3 , S4 } and our goal is to identify the correct sense S4 ( as the closest meaning ) for this specific occurrence of cold after considering - the semantic and the syntactic information of - its context .","This network works with neural sense vectors ( i.e. sense embeddings ) , which are learned during model training , and employs neural word vectors ( i.e. word embeddings ) , which are learned through an unsupervised deep learning approach called GloVe ( Global Vectors for word representation ) for the context words .",0.1019108280254777,0.5,0.5,model,ablation-analysis
entity_linking,16,introduction,introduction,17,8,8,"This network works with neural sense vectors ( i.e. sense embeddings ) , which are learned during model training , and employs neural word vectors ( i.e. word embeddings ) , which are learned through an unsupervised deep learning approach called GloVe ( Global Vectors for word representation ) for the context words .","In this effort , we develop our supervised WSD model that leverages a Bidirectional Long Short - Term Memory ( BLSTM ) network .","By evaluating our onemodel - fits - all WSD network over the public gold standard dataset of SensEval - 3 , we demonstrate that the accuracy of our model in terms of F- measure is comparable with the state - of - the - art WSD algorithms '.",0.10828025477707,0.5714285714285714,0.5714285714285714,model,ablation-analysis
entity_linking,16,experiments,results,110,11,2,Between - all - models comparisons, ,- When SensEval - 3 task was launched 47 submissions ( supervised and unsupervised algorithms ) were received addressing this task .,0.7006369426751592,0.22,0.0666666666666666,results,baselines
entity_linking,16,experiments,results,114,15,6,"We show our single model sits among the 5 top - performing algorithms , considering that in other algorithms for each ambiguous word one separate classifier is trained ( i.e. in the same number of ambiguous words in a language there have to be classifiers ; which means 57 classifiers for this specific task ) .",We compare the result of our model with the top - performing and low - performing algorithms ( supervised ) .,shows the results of the top - performing and low - performing supervised algorithms .,0.7261146496815286,0.3,0.2,results,model
entity_linking,16,experiments,results,115,16,7,shows the results of the top - performing and low - performing supervised algorithms .,"We show our single model sits among the 5 top - performing algorithms , considering that in other algorithms for each ambiguous word one separate classifier is trained ( i.e. in the same number of ambiguous words in a language there have to be classifiers ; which means 57 classifiers for this specific task ) .",The first two algorithms represent the state - of - the - art models of supervised WSD when evaluated on SensEval - 3 .,0.732484076433121,0.32,0.2333333333333333,results,approach
entity_linking,16,experiments,results,128,29,20,Within - our - model comparisons,Considering their ranking scores we see that there are unsupervised methods that outperform these supervised algorithms .,"- Besides several internal experiments to examine the importance of some hyper - parameters to our network , we investigated if the sequential follow of cosine similarities computed between a true sense and its preceding and succeeding context words carries a pattern - like information that can be encoded with BLSTM .",0.8152866242038217,0.58,0.6666666666666666,results,baselines
entity_linking,16,experiments,results,136,37,28,"We observe if reverse the sequential follow of information into our Bidirectional LSTM , we shuffle the order of the context words , or even replace our Bidirectional LSTMs with two different fully - connected networks of the same size 50 ( the size of the LSTMs outputs ) , the achieved results were notably less than 72.5 % .","Generally , it is expected that the cosine similarities of closer words ( in the context ) to the true sense be larger than the incorrect senses ' ; however , if a series of cosine similarities can be encoded through an LSTM ( or BLSTM ) network should be experimented .","In the third section of the table , we report our changes to the hyper - parameters .",0.8662420382165605,0.74,0.9333333333333332,results,baselines
entity_linking,2,title,title,2,2,2,Neural Sequence Learning Models for Word Sense Disambiguation, , ,0.0098522167487684,1.0,1.0,research-problem,experimental-setup
entity_linking,2,introduction,introduction,9,2,2,"As one of the long - standing challenges in Natural Language Processing ( NLP ) , Word Sense Disambiguation , WSD ) has received considerable attention over recent years .", ,"Indeed , by dealing with lexical ambiguity an effective WSD model brings numerous benefits to a variety of downstream tasks and applications , from Information Retrieval and Extraction to Machine Translation .",0.0443349753694581,0.125,0.125,research-problem,ablation-analysis
entity_linking,2,introduction,introduction,18,11,11,"In this paper our focus is on supervised WSD , but we depart from previous approaches and adopt a different perspective on the task : instead of framing a separate classification problem for each given word , we aim at modeling the joint disambiguation of the target text as a whole in terms of a sequence labeling problem .","Such systems construct a model based only on the underlying resource , which is then able to handle multiple target words at the same time and disambiguate them jointly , whereas word experts are forced to treat each disambiguation target in isolation .","From this standpoint , WSD amounts to translating a sequence of words into a sequence of potentially sense - tagged tokens .",0.0886699507389162,0.6875,0.6875,model,ablation-analysis
entity_linking,2,introduction,introduction,20,13,13,"With this in mind , we design , analyze and compare experimentally various neural architectures of different complexities , ranging from a single bidirectional Long Short - Term Memory to a sequence - tosequence approach .","From this standpoint , WSD amounts to translating a sequence of words into a sequence of potentially sense - tagged tokens .","Each architecture reflects a particular way of modeling the disambiguation problem , but they all share some key features that set them apart from previous supervised approaches to WSD : they are trained end - to - end from sense - annotated text to sense labels , and learn a single all - words model from the training data , without fine tuning or explicit engineering of local features .",0.0985221674876847,0.8125,0.8125,model,ablation-analysis
entity_linking,2,introduction,introduction,21,14,14,"Each architecture reflects a particular way of modeling the disambiguation problem , but they all share some key features that set them apart from previous supervised approaches to WSD : they are trained end - to - end from sense - annotated text to sense labels , and learn a single all - words model from the training data , without fine tuning or explicit engineering of local features .","With this in mind , we design , analyze and compare experimentally various neural architectures of different complexities , ranging from a single bidirectional Long Short - Term Memory to a sequence - tosequence approach .",The contributions of this paper are twofold .,0.1034482758620689,0.875,0.875,model,ablation-analysis
entity_linking,2,experimental setup,experimental setup,144,11,11,"To set a level playing field with comparison systems on English all - words WSD , we followed and , for all our models , we used a layer of word embeddings pre-trained 8 on the English uk WaC corpus as initialization , and kept them fixed during the training process .",Architecture Details .,For all architectures we then employed 2 layers of bidirectional LSTM with 2048 hidden units ( 1024 units per direction ) .,0.7093596059113301,0.5789473684210527,0.5789473684210527,hyperparameters,approach
entity_linking,2,experimental setup,experimental setup,145,12,12,For all architectures we then employed 2 layers of bidirectional LSTM with 2048 hidden units ( 1024 units per direction ) .,"To set a level playing field with comparison systems on English all - words WSD , we followed and , for all our models , we used a layer of word embeddings pre-trained 8 on the English uk WaC corpus as initialization , and kept them fixed during the training process .","As regards multilingual all - words WSD ( Section 6.2 ) , we experimented , instead , with two different configurations of the embedding layer : the pre-trained bilingual embeddings by for all the language pairs of interest ( EN - IT , EN - FR , EN - DE , and EN - ES ) , and the pre-trained multilingual 512 - dimensional embeddings for 12 languages by .",0.7142857142857143,0.631578947368421,0.631578947368421,hyperparameters,approach
entity_linking,2,experimental results,experimental results,155,3,3,"We report the F1 - score on each in - dividual test set , as well as the F1- score obtained on the concatenation of all four test sets , divided by part - of - speech tag .","Throughout this section we identify the models based on the LSTM tagger ( Sections 3.1 - 3.2 ) by the label BLSTM , and the sequence - to - sequence models ( Section 3.3 ) by the label Seq2Seq. shows the performance of our models on the standardized benchmarks for all - words finegrained WSD .",We compared against the best supervised and knowledge - based systems evaluated on the same framework .,0.7635467980295566,0.0714285714285714,0.2727272727272727,results,baselines
entity_linking,2,experimental results,experimental results,157,5,5,"As supervised systems , we considered Context2 Vec and It Makes Sense , both the original implementation and the best configuration reported by , which also integrates word embeddings using exponential decay .",We compared against the best supervised and knowledge - based systems evaluated on the same framework .,10 All these supervised systems were trained on the standardized version of Sem - Cor .,0.7733990147783252,0.119047619047619,0.4545454545454545,results,approach
entity_linking,2,experimental results,experimental results,161,9,9,"11 Overall , both BLSTM and Seq2Seq achieved results that are either state - of - the - art or statistically equivalent ( unpaired t- test , p < 0.05 ) to the best supervised system in each benchmark , performing on par with word experts tuned over explicitly engineered features .",All these systems relied on the Most Frequent Sense ( MFS ) baseline as back - off strategy .,"Interestingly enough , BLSTM models tended consistently to outperform their Seq2Seq counterparts , suggesting that an encoder - decoder architecture , despite being more powerful , might be suboptimal for WSD .",0.7931034482758621,0.2142857142857142,0.8181818181818182,results,baselines
entity_linking,2,experimental results,experimental results,162,10,10,"Interestingly enough , BLSTM models tended consistently to outperform their Seq2Seq counterparts , suggesting that an encoder - decoder architecture , despite being more powerful , might be suboptimal for WSD .","11 Overall , both BLSTM and Seq2Seq achieved results that are either state - of - the - art or statistically equivalent ( unpaired t- test , p < 0.05 ) to the best supervised system in each benchmark , performing on par with word experts tuned over explicitly engineered features .","Furthermore , introducing LEX ( cf. Section 4 ) as auxiliary task was generally helpful ; on the other hand , POS did not seem to help , corroborating previous findings ( Alonso .",0.7980295566502463,0.2380952380952381,0.9090909090909092,results,baselines
entity_linking,2,experimental results,english all words wsd,164,12,1,English All - words WSD, , ,0.8078817733990148,0.2857142857142857,0.125,results,approach
entity_linking,2,experimental results,english all words wsd,169,17,6,"It is worth noting that RNN - based architectures outperformed classical supervised approaches when dealing with verbs , which are shown to be highly ambiguous .",els .,The performance on coarse - grained WSD followed the same trend ) .,0.8325123152709359,0.4047619047619048,0.75,results,baselines
entity_linking,2,experimental results,english all words wsd,171,19,8,"Both BLSTM and Seq2Seq outperformed UKB and IMS trained on SemCor , as well as recent supervised approaches based on distributional semantics and neural architectures .",The performance on coarse - grained WSD followed the same trend ) ., ,0.8423645320197044,0.4523809523809524,1.0,results,baselines
entity_linking,2,experimental results,multilingual all words wsd,172,20,1,Multilingual All - words WSD, , ,0.8472906403940886,0.4761904761904762,0.1111111111111111,results,approach
entity_linking,2,experimental results,multilingual all words wsd,179,27,8,"F - score figures show that bilingual and multilingual models , despite being trained only on English data , consistently outperformed the MFS baseline and achieved results that are competitive with the best participating systems in the task .","While doing this , we exploited BabelNet 's inter-resource mappings to convert WordNet sense labels ( used at training time ) into BabelNet synsets compliant with the sense inventory of the task .","We also note that the overall F- score performance did not change substantially ( and slightly improved ) when moving from bilingual to multilingual models , despite the increase in the number of target languages treated simultaneously .",0.8817733990147784,0.6428571428571429,0.8888888888888888,results,baselines
entity_linking,2,experimental results,multilingual all words wsd,180,28,9,"We also note that the overall F- score performance did not change substantially ( and slightly improved ) when moving from bilingual to multilingual models , despite the increase in the number of target languages treated simultaneously .","F - score figures show that bilingual and multilingual models , despite being trained only on English data , consistently outperformed the MFS baseline and achieved results that are competitive with the best participating systems in the task .", ,0.8866995073891626,0.6666666666666666,1.0,results,baselines
entity_linking,3,title,title,2,2,2,Does BERT Make Any Sense ? Interpretable Word Sense Disambiguation with Contextualized Embeddings, , ,0.009090909090909,1.0,1.0,research-problem,approach
entity_linking,3,abstract,abstract,7,5,5,"Since vectors of the same word type can vary depending on the respective context , they implicitly provide a model for word sense disambiguation ( WSD ) .","Their advantage over static word embeddings has been shown fora number of tasks , such as text classification , sequence tagging , or machine translation .",We introduce a simple but effective approach to WSD using a nearest neighbor classification on CWEs .,0.0318181818181818,0.0943396226415094,0.0943396226415094,research-problem,model
entity_linking,3,abstract,abstract,8,6,6,We introduce a simple but effective approach to WSD using a nearest neighbor classification on CWEs .,"Since vectors of the same word type can vary depending on the respective context , they implicitly provide a model for word sense disambiguation ( WSD ) .",We compare the performance of different CWE models for the task and can report improvements above the current state of the art for two standard WSD benchmark datasets .,0.0363636363636363,0.1132075471698113,0.1132075471698113,research-problem,baselines
entity_linking,3,abstract,abstract,49,47,47,We show that CWEs can be utilized directly to approach the WSD task due to their nature of providing distinct vector representations for the same token depending on its context .,Contribution :,"To learn the semantic capabilities of CWEs , we employ a simple , yet interpretable approach to WSD using a k -nearest neighbor classification ( kNN ) approach .",0.2227272727272727,0.8867924528301887,0.8867924528301887,model,baselines
entity_linking,3,abstract,abstract,50,48,48,"To learn the semantic capabilities of CWEs , we employ a simple , yet interpretable approach to WSD using a k -nearest neighbor classification ( kNN ) approach .",We show that CWEs can be utilized directly to approach the WSD task due to their nature of providing distinct vector representations for the same token depending on its context .,We compare the performance of three different CWE models on four standard benchmark datasets .,0.2272727272727273,0.9056603773584906,0.9056603773584906,model,baselines
entity_linking,3,experimental results,contextualized embeddings,134,6,1,Contextualized Embeddings, , ,0.6090909090909091,0.09375,0.0588235294117647,results,approach
entity_linking,3,experimental results,contextualized embeddings,137,9,4,Simple k NN with ELMo as well as BERT embeddings beats the state of the art of the lexical sample task SE - 2 ( cp. Table 3 ) .,shows a high variance in performance .,BERT also outperforms all others on the SE - 3 task .,0.6227272727272727,0.140625,0.2352941176470588,results,approach
entity_linking,3,experimental results,contextualized embeddings,138,10,5,BERT also outperforms all others on the SE - 3 task .,Simple k NN with ELMo as well as BERT embeddings beats the state of the art of the lexical sample task SE - 2 ( cp. Table 3 ) .,"However , we observe a major performance drop of our approach for the two all - words WSD tasks in which no training data is provided along with the test set .",0.6272727272727273,0.15625,0.2941176470588235,results,baselines
entity_linking,3,experimental results,contextualized embeddings,139,11,6,"However , we observe a major performance drop of our approach for the two all - words WSD tasks in which no training data is provided along with the test set .",BERT also outperforms all others on the SE - 3 task .,"For S7 - T7 and S7 - T17 , the content and structure of the out - of - domain SemCor and WNGT training datasets differ drastically from those in the test data , which prevents yielding near stateof - the - art results .",0.6318181818181818,0.171875,0.3529411764705882,results,baselines
entity_linking,3,experimental results,contextualized embeddings,141,13,8,"In fact , similarity of contextualized embeddings largely relies on semantically and structurally similar sentence contexts of polysemic target words .","For S7 - T7 and S7 - T17 , the content and structure of the out - of - domain SemCor and WNGT training datasets differ drastically from those in the test data , which prevents yielding near stateof - the - art results .","Hence , the more example sentences can be used fora sense , the higher are the chances 3 Unification of Sense Annotated Corpora and Tools .",0.6409090909090909,0.203125,0.4705882352941176,results,ablation-analysis
entity_linking,3,experimental results,contextualized embeddings,146,18,13,"As can be seen in , the SE - 2 and SE - 3 training datasets provide more CWEs for each word and sense , and our approach performs better with a growing number of CWEs , even with a higher average number of senses per word as is the casein SE - 3 .",that a nearest neighbor expresses the same sense .,"Thus , we conclude that the nearest neighbor approach suffers specifically from data sparseness .",0.6636363636363637,0.28125,0.7647058823529411,results,baselines
entity_linking,3,experimental results,contextualized embeddings,149,21,16,"Moreover , CWEs actually do not organize well in spherically shaped form in the embedding space .",The chances increase that aspects of similarity other than the sense of the target word in two compared sentence contexts drive the kNN decision .,"Although senses might be actually separable , the nonparametric kNN classification is unable to learn a complex decision boundary focusing only on the most informative aspects of the CWE .",0.6772727272727272,0.328125,0.9411764705882352,results,ablation-analysis
entity_linking,3,experimental results,nearest neighbors,151,23,1,Nearest Neighbors, , ,0.6863636363636364,0.359375,0.0294117647058823,results,approach
entity_linking,3,experimental results,nearest neighbors,152,24,2,K- Optimization :, ,"To attenuate for noise in the training data , we optimize fork to obtain a more robust nearest neighbor classification .",0.6909090909090909,0.375,0.0588235294117647,results,approach
entity_linking,3,experimental results,nearest neighbors,155,27,5,"For SensEval - 2 and SensEval - 3 , we achieve a new state - of - the - art result .",shows our best results using the BERT embeddings along with results from related works .,We observe convergence with higher k values since our k normalization heuristic is activated .,0.7045454545454546,0.421875,0.1470588235294117,results,approach
entity_linking,3,experimental results,nearest neighbors,157,29,7,"For the S7 - T * , we also achieve minor improvements with a higher k , but still drastically lack behind the state of the art .",We observe convergence with higher k values since our k normalization heuristic is activated .,Senses in CWE space :,0.7136363636363636,0.453125,0.2058823529411765,results,baselines
entity_linking,3,experimental results,nearest neighbors,158,30,8,Senses in CWE space :,"For the S7 - T * , we also achieve minor improvements with a higher k , but still drastically lack behind the state of the art .",We investigate how well different CWE models encode information such as distinguishable senses in their vector space .,0.7181818181818181,0.46875,0.2352941176470588,results,approach
entity_linking,3,experimental results,nearest neighbors,159,31,9,We investigate how well different CWE models encode information such as distinguishable senses in their vector space .,Senses in CWE space :,"shows T - SNE plots ( van der Maaten and Hinton , 2008 ) of six different senses of the word "" bank "" in the SE - 3 training dataset encoded by the three different CWE methods .",0.7227272727272728,0.484375,0.2647058823529412,results,ablation-analysis
entity_linking,3,experimental results,nearest neighbors,162,34,12,The Flair embeddings hardly allow to distinguish any clusters as most senses are scattered across the entire plot .,"For visualization purposes , we exclude senses with a frequency of less than two .","In the ELMo embedding space , the major senses are slightly more separated in different regions of the point cloud .",0.7363636363636363,0.53125,0.3529411764705882,results,model
entity_linking,3,experimental results,nearest neighbors,163,35,13,"In the ELMo embedding space , the major senses are slightly more separated in different regions of the point cloud .",The Flair embeddings hardly allow to distinguish any clusters as most senses are scattered across the entire plot .,"Only in the BERT embedding space , some senses form clearly separable clusters .",0.740909090909091,0.546875,0.3823529411764705,results,experimental-setup
entity_linking,4,title,title,2,2,2,Learning Distributed Representations of Texts and Entities from Knowledge Base, , ,0.0087336244541484,1.0,1.0,research-problem,approach
entity_linking,4,introduction,introduction,26,8,8,"In particular , we propose Neural Text - Entity Encoder ( NTEE ) , a neural network model to jointly learn distributed representations of texts ( i.e. , sentences and paragraphs ) and KB entities .","In this paper , we describe a novel method to bridge these two different approaches .","For every text in the KB , our model aims to predict its relevant entities , and places the text and the relevant entities close to each other in a continuous vector space .",0.1135371179039301,0.2222222222222222,0.2222222222222222,model,ablation-analysis
entity_linking,4,introduction,introduction,27,9,9,"For every text in the KB , our model aims to predict its relevant entities , and places the text and the relevant entities close to each other in a continuous vector space .","In particular , we propose Neural Text - Entity Encoder ( NTEE ) , a neural network model to jointly learn distributed representations of texts ( i.e. , sentences and paragraphs ) and KB entities .",We use humanedited entity annotations obtained from Wikipedia ( see ) as supervised data of relevant entities to the texts containing these annotations .,0.1179039301310043,0.25,0.25,model,approach
entity_linking,4,introduction,introduction,28,10,10,We use humanedited entity annotations obtained from Wikipedia ( see ) as supervised data of relevant entities to the texts containing these annotations .,"For every text in the KB , our model aims to predict its relevant entities , and places the text and the relevant entities close to each other in a continuous vector space .","Note that , KB entities have been conventionally used to model semantics of texts .",0.1222707423580786,0.2777777777777778,0.2777777777777778,model,approach
entity_linking,4,introduction,introduction,29,11,11,"Note that , KB entities have been conventionally used to model semantics of texts .",We use humanedited entity annotations obtained from Wikipedia ( see ) as supervised data of relevant entities to the texts containing these annotations .,"A representative example is Explicit Semantic Analysis ( ESA ) , which represents the semantics of a text using a sparse vector space , where each dimension corresponds to the relevance score of the text to each entity .",0.1266375545851528,0.3055555555555556,0.3055555555555556,model,approach
entity_linking,4,introduction,introduction,30,12,12,"A representative example is Explicit Semantic Analysis ( ESA ) , which represents the semantics of a text using a sparse vector space , where each dimension corresponds to the relevance score of the text to each entity .","Note that , KB entities have been conventionally used to model semantics of texts .","Essentially , ESA shows that text can be accurately represented using a small set of its relevant entities .",0.131004366812227,0.3333333333333333,0.3333333333333333,model,model
entity_linking,4,introduction,introduction,31,13,13,"Essentially , ESA shows that text can be accurately represented using a small set of its relevant entities .","A representative example is Explicit Semantic Analysis ( ESA ) , which represents the semantics of a text using a sparse vector space , where each dimension corresponds to the relevance score of the text to each entity .",Based on 2 Entity annotations in Wikipedia can be viewed as supervised data of relevant entities because Wikipedia instructs its contributors to create annotations only where they are relevant in its manual :,0.1353711790393013,0.3611111111111111,0.3611111111111111,model,model
entity_linking,4,introduction,introduction,35,17,17,"Furthermore , we also consider that placing texts and entities into the same vector space enables us to easily compute the similarity between texts and entities , which can be beneficial for various KB - related tasks .","wiki/Wikipedia:Manual_of_Style ar Xiv:1705.02494v3 [ cs.CL ] 7 Nov 2017 this fact , we hypothesize that we can use the annotations of relevant entities as the supervised data of learning text representations .","In order to test this hypothesis , we conduct three experiments involving both the unsupervised and the supervised tasks .",0.1528384279475982,0.4722222222222222,0.4722222222222222,model,baselines
entity_linking,4,introduction,introduction,54,36,36,We release our code and trained models to the community at https://github.com/ studio-ousia /ntee to facilitate further academic research .,An example of a sentence with entity annotations ., ,0.2358078602620087,1.0,1.0,code,baselines
entity_linking,4,our approach,semantic textual similarity,125,71,14,BOW is a conventional approach using a logistic regression ( LR ) classifier trained with binary BOW features to predict the correct answer .,The details of these models are as follows :,BOW - DT is based on the BOW baseline augmented with the feature set with dependency relation indicators .,0.5458515283842795,0.4765100671140939,0.4666666666666667,baselines,ablation-analysis
entity_linking,4,our approach,semantic textual similarity,126,72,15,BOW - DT is based on the BOW baseline augmented with the feature set with dependency relation indicators .,BOW is a conventional approach using a logistic regression ( LR ) classifier trained with binary BOW features to predict the correct answer .,QANTA is an approach based on a recursive neural network to derive the distributed representations of questions .,0.5502183406113537,0.4832214765100672,0.5,baselines,model
entity_linking,4,our approach,semantic textual similarity,127,73,16,QANTA is an approach based on a recursive neural network to derive the distributed representations of questions .,BOW - DT is based on the BOW baseline augmented with the feature set with dependency relation indicators .,The method also uses the LR classifier with the derived representations as features .,0.5545851528384279,0.4899328859060403,0.5333333333333333,baselines,ablation-analysis
entity_linking,4,our approach,semantic textual similarity,129,75,18,FTS - BRNN is based on the bidirectional recurrent neural network ( RNN ) with gated recurrent units ( GRU ) .,The method also uses the LR classifier with the derived representations as features .,"Similar to QANTA , the method adopts the LR classifier with the derived representations as features .",0.5633187772925764,0.5033557046979866,0.6,baselines,ablation-analysis
entity_linking,4,our approach,semantic textual similarity,137,83,26,"In particular , despite the simplicity of the neural network architecture of our method compared to the state - of - the - art methods ( i.e. , QANTA and FTS - BRNN ) , our method clearly outperformed these methods .",Accuracies of the proposed method and the state - of - the - art methods for the factoid QA task .,This demonstrates the effectiveness of our proposed representations as background knowledge for the QA task .,0.5982532751091703,0.5570469798657718,0.8666666666666667,results,baselines
entity_linking,4,our approach,semantic textual similarity,140,86,29,"Our observations indicated that our method mostly performed perfect in terms of predicting the types of target answers ( e.g. , locations , events , and people ) .",We also conducted a brief error analysis using the test set of the history dataset .,"However , our method erred in delicate cases such as predicting Henry II of England instead of Henry I of England , and Syracuse , Sicily instead of Sicily .",0.6113537117903929,0.5771812080536913,0.9666666666666668,results,approach
entity_linking,5,title,title,2,2,2,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships, , ,0.010204081632653,1.0,1.0,research-problem,experimental-setup
entity_linking,5,abstract,abstract,4,2,2,"In Word Sense Disambiguation ( WSD ) , the predominant approach generally involves a supervised system trained on sense annotated corpora .", ,The limited quantity of such corpora however restricts the coverage and the performance of these systems .,0.0204081632653061,0.3333333333333333,0.3333333333333333,research-problem,model
entity_linking,5,abstract,abstract,7,5,5,"Our method leads to state of the art results on most WSD evaluation tasks , while improving the coverage of supervised systems , reducing the training time and the size of the models , without additional training data .","In this article , we propose anew method that solves these issues by taking advantage of the knowledge present in WordNet , and especially the hypernymy and hyponymy relationships between synsets , in order to reduce the number of different sense tags that are necessary to disambiguate all words of the lexical database .","In addition , we exhibit results that significantly outperform the state of the art when our method is combined with an ensembling technique and the addition of the WordNet Gloss Tagged as training corpus .",0.0357142857142857,0.8333333333333334,0.8333333333333334,research-problem,baselines
entity_linking,5,introduction,introduction,27,19,19,We propose a method for reducing the vocabulary of senses of Word Net by selecting the minimal set of senses required for differentiating the meaning of every word .,Contributions :,"By using this technique , and converting the sense tags present in sense annotated corpora to the most generalized sense possible , we are able to greatly improve the coverage and the generalization ability of supervised systems .",0.1377551020408163,0.1862745098039216,0.76,model,experimental-setup
entity_linking,5,introduction,introduction,31,23,23,The code for using our system or reproducing our results is available at the following URL : https://github.com/getalp/disambiguate,Our method is then evaluated by measuring its contribution to a state of the art neural WSD system evaluated on classic WSD evaluation campaigns .,Neural Word Sense Disambiguation,0.1581632653061224,0.2254901960784313,0.92,code,experimental-setup
entity_linking,5,experiments,results,181,71,5,"In subsection 3.2 , we showed that our vocabulary reduction method improves the coverage of supervised systems overall WordNet vocabulary .",The scores obtained by our ensemble of models are given in .,"In , we can see that this coverage improvement holds true on the evaluation tasks , for both training sets .",0.923469387755102,0.8255813953488372,0.25,results,baselines
entity_linking,5,experiments,results,182,72,6,"In , we can see that this coverage improvement holds true on the evaluation tasks , for both training sets .","In subsection 3.2 , we showed that our vocabulary reduction method improves the coverage of supervised systems overall WordNet vocabulary .","On the total of 7 253 words to annotate for the corpus "" ALL "" , the baseline system trained on the SemCor only is incapable of annotating 491 of them , and with the vocabulary reduction applied this number drops to 91 .",0.9285714285714286,0.8372093023255814,0.3,results,approach
entity_linking,5,experiments,results,185,75,9,"Now if we look at the results in , the difference of scores obtained by our system using the sense vocabulary reduction or not is overall not significant ( regarding the "" ALL "" column ) .","When adding the WordNet Gloss Tagged to the training set , this number is 126 for the baseline system , and with the vocabulary reduction , only 12 words can not be annotated .","However we can notice a very large gap on the SemEval 2013 task , especially when the SemCor is used alone for training .",0.9438775510204082,0.8720930232558141,0.45,results,baselines
entity_linking,5,experiments,results,186,76,10,"However we can notice a very large gap on the SemEval 2013 task , especially when the SemCor is used alone for training .","Now if we look at the results in , the difference of scores obtained by our system using the sense vocabulary reduction or not is overall not significant ( regarding the "" ALL "" column ) .","This can be explained by the fact that this corpus is only composed of nouns , and our method for vocabulary reduction targets this part of speech principally .",0.9489795918367347,0.8837209302325582,0.5,results,approach
entity_linking,5,experiments,results,190,80,14,"When we add the WordNet Gloss Tagged to the training data however , we obtain systematically state of the art results on all tasks except on SensEval 3 .","In comparison with the other works , our systems trained on the SemCor alone expose results comparable with the best system of , which is trained on the same corpus and augmented with a semi-supervised method .","Once again , the sense reduction method does not consistently improves or decreases the score on every task , and in overall ( task "" ALL "" ) , the result is roughly the same as without sense reduction applied .",0.9693877551020408,0.9302325581395348,0.7,results,approach
entity_linking,5,experiments,results,191,81,15,"Once again , the sense reduction method does not consistently improves or decreases the score on every task , and in overall ( task "" ALL "" ) , the result is roughly the same as without sense reduction applied .","When we add the WordNet Gloss Tagged to the training data however , we obtain systematically state of the art results on all tasks except on SensEval 3 .","Finally , in we show the results of our system ensembling 20 models by averaging the output of their last layer .",0.9744897959183674,0.9418604651162792,0.75,results,approach
entity_linking,5,experiments,results,193,83,17,"As we can see , ensembling is a very efficient method in WSD as it improves systematically all our results .","Finally , in we show the results of our system ensembling 20 models by averaging the output of their last layer .","Interestingly , with ensembles , the scores are significantly higher when applying the vocabulary reduction algorithm .",0.9846938775510204,0.9651162790697676,0.85,results,approach
entity_linking,5,experiments,results,194,84,18,"Interestingly , with ensembles , the scores are significantly higher when applying the vocabulary reduction algorithm .","As we can see , ensembling is a very efficient method in WSD as it improves systematically all our results .","One possible interpretation is that individual models might be more frequently "" lost "" in the sense that with the sense vocabulary reduction applied , a lot of words are annotated with the same tag , and it can make the trained model "" unsure "" about the decisions it make .",0.9897959183673468,0.9767441860465116,0.9,results,approach
entity_linking,6,title,title,2,2,2,Incorporating Glosses into Neural Word Sense Disambiguation, , ,0.0074349442379182,1.0,1.0,research-problem,experimental-setup
entity_linking,6,abstract,abstract,4,2,2,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context ., ,Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge - based methods .,0.0148698884758364,0.2222222222222222,0.2222222222222222,research-problem,ablation-analysis
entity_linking,6,abstract,abstract,5,3,3,Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge - based methods .,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .,"However , previous neural networks for WSD always rely on massive labeled data ( context ) , ignoring lexical resources like glosses ( sense definitions ) .",0.0185873605947955,0.3333333333333333,0.3333333333333333,research-problem,baselines
entity_linking,6,introduction,introduction,31,20,20,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .",the related glosses of hypernyms and hyponyms into the neural network .,GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,0.1152416356877323,0.6896551724137931,0.6896551724137931,model,model
entity_linking,6,introduction,introduction,32,21,21,GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .","In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .",0.1189591078066914,0.7241379310344828,0.7241379310344828,model,baselines
entity_linking,6,introduction,introduction,33,22,22,"In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .",GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,The main contributions of this paper are listed as follows :,0.1226765799256505,0.7586206896551724,0.7586206896551724,model,ablation-analysis
entity_linking,6,experiments and evaluation,implementation details,196,15,3,"We use pre-trained word embeddings with 300 dimensions 10 , and keep them fixed during the training process .","We use the validation set ( SE7 ) to find the optimal settings of our framework : the hidden state size n , the number of passes | T M | , the optimizer , etc .","We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .",0.7286245353159851,0.1875,0.3,hyperparameters,experimental-setup
entity_linking,6,experiments and evaluation,implementation details,197,16,4,"We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .","We use pre-trained word embeddings with 300 dimensions 10 , and keep them fixed during the training process .","Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .",0.7323420074349443,0.2,0.4,hyperparameters,experimental-setup
entity_linking,6,experiments and evaluation,implementation details,198,17,5,"Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .","We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .",We assign gloss expansion depth K the value of 4 .,0.7360594795539034,0.2125,0.5,hyperparameters,experimental-setup
entity_linking,6,experiments and evaluation,implementation details,199,18,6,We assign gloss expansion depth K the value of 4 .,"Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .","We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .",0.7397769516728625,0.225,0.6,hyperparameters,approach
entity_linking,6,experiments and evaluation,implementation details,200,19,7,"We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .",We assign gloss expansion depth K the value of 4 .,We use Adam optimizer in the training process with 0.001 initial learning rate .,0.7434944237918215,0.2375,0.7,hyperparameters,approach
entity_linking,6,experiments and evaluation,implementation details,201,20,8,We use Adam optimizer in the training process with 0.001 initial learning rate .,"We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .","In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .",0.7472118959107806,0.25,0.8,hyperparameters,approach
entity_linking,6,experiments and evaluation,implementation details,202,21,9,"In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .",We use Adam optimizer in the training process with 0.001 initial learning rate .,Training runs for up to 100 epochs with early stopping if the validation loss does n't improve within the last 10 epochs .,0.7509293680297398,0.2625,0.9,hyperparameters,approach
entity_linking,6,experiments and evaluation,implementation details,203,22,10,Training runs for up to 100 epochs with early stopping if the validation loss does n't improve within the last 10 epochs .,"In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .", ,0.7546468401486989,0.275,1.0,hyperparameters,approach
entity_linking,6,experiments and evaluation,results and discussion,228,47,2,English all - words results, ,"In this section , we show the performance of our proposed model in the English all - words task .",0.8475836431226765,0.5875,0.0571428571428571,results,approach
entity_linking,6,experiments and evaluation,results and discussion,233,52,7,GAS and GAS ext achieves the state - of - theart performance on the concatenation of all test datasets .,The last block lists the performance of our proposed model GAS and its variant GAS ext which extends the gloss module in GAS .,Although there is no one system al - Context :,0.8661710037174721,0.65,0.2,results,baselines
entity_linking,6,experiments and evaluation,results and discussion,237,56,11,". ways performs best on all the test sets 11 , we can find that GAS ext with concatenation memory updating strategy achieves the best results 70.6 on the concatenation of the four test datasets .",It shows that appropriate number of passes can boost the performance as well as avoid over - fitting of the model .,"Compared with other three neural - based methods in the fourth block , we can find that our best model outperforms the previous best neural network models on every individual test set .",0.8810408921933085,0.7,0.3142857142857143,results,baselines
entity_linking,6,experiments and evaluation,results and discussion,238,57,12,"Compared with other three neural - based methods in the fourth block , we can find that our best model outperforms the previous best neural network models on every individual test set .",". ways performs best on all the test sets 11 , we can find that GAS ext with concatenation memory updating strategy achieves the best results 70.6 on the concatenation of the four test datasets .","The IMS +emb , which trains a dedicated classifier for each word individually ( word expert ) with massive manual designed features including word embeddings , is hard to beat for neural networks models .",0.8847583643122676,0.7125,0.3428571428571429,results,approach
entity_linking,6,experiments and evaluation,results and discussion,240,59,14,"However , our best model can also beat IMS + emb on the SE3 , SE13 and SE15 test sets .","The IMS +emb , which trains a dedicated classifier for each word individually ( word expert ) with massive manual designed features including word embeddings , is hard to beat for neural networks models .",Incorporating glosses into neural WSD can greatly improve the performance and extending the original gloss can further boost the results .,0.8921933085501859,0.7375,0.4,results,baselines
entity_linking,6,experiments and evaluation,results and discussion,241,60,15,Incorporating glosses into neural WSD can greatly improve the performance and extending the original gloss can further boost the results .,"However , our best model can also beat IMS + emb on the SE3 , SE13 and SE15 test sets .","Compared with the Bi - LSTM baseline which only uses labeled data , our proposed model greatly improves the WSD task by 2.2 % F1 - score with the help of gloss knowledge .",0.895910780669145,0.75,0.4285714285714285,results,approach
entity_linking,6,experiments and evaluation,results and discussion,242,61,16,"Compared with the Bi - LSTM baseline which only uses labeled data , our proposed model greatly improves the WSD task by 2.2 % F1 - score with the help of gloss knowledge .",Incorporating glosses into neural WSD can greatly improve the performance and extending the original gloss can further boost the results .,"Furthermore , compared with the GAS which only uses original gloss as the background knowledge , GAS ext can further improve the performance with the help of the extended glosses through the semantic relations .",0.8996282527881041,0.7625,0.4571428571428571,results,baselines
entity_linking,6,experiments and evaluation,results and discussion,243,62,17,"Furthermore , compared with the GAS which only uses original gloss as the background knowledge , GAS ext can further improve the performance with the help of the extended glosses through the semantic relations .","Compared with the Bi - LSTM baseline which only uses labeled data , our proposed model greatly improves the WSD task by 2.2 % F1 - score with the help of gloss knowledge .",This proves that incorporating extended glosses through its hypernyms and hyponyms into the neural network models can boost the performance for Because the source of the four datasets are extremely different which belongs to different domains .,0.9033457249070632,0.775,0.4857142857142857,results,approach
entity_linking,7,title,title,2,2,2,Semi-supervised Word Sense Disambiguation with Neural Models, , ,0.0089285714285714,1.0,1.0,research-problem,experimental-setup
entity_linking,7,abstract,abstract,4,2,2,Determining the intended sense of words in text - word sense disambiguation ( WSD ) - is a longstanding problem in natural language processing ., ,"Recently , researchers have shown promising results using word vectors extracted from a neural network language model as features in WSD algorithms .",0.0178571428571428,0.2222222222222222,0.2222222222222222,research-problem,experimental-setup
entity_linking,7,abstract,abstract,5,3,3,"Recently , researchers have shown promising results using word vectors extracted from a neural network language model as features in WSD algorithms .",Determining the intended sense of words in text - word sense disambiguation ( WSD ) - is a longstanding problem in natural language processing .,"However , a simple average or concatenation of word vectors for each word in a text loses the sequential and syntactic information of the text .",0.0223214285714285,0.3333333333333333,0.3333333333333333,research-problem,baselines
entity_linking,7,introduction,introduction,20,9,9,"In this paper , we describe two novel WSD algorithms .",show that this can substantially improve WSD performance and indeed that competitive performance can be attained using word embeddings alone .,The first is based on a Long Short Term Memory ( LSTM ) ) .,0.0892857142857142,0.5625,0.5625,model,approach
entity_linking,7,introduction,introduction,21,10,10,The first is based on a Long Short Term Memory ( LSTM ) ) .,"In this paper , we describe two novel WSD algorithms .","Since this model is able to take into account word order when classifying , it performs significantly better than an algorithm based on a continuous bag of words model ( Word2 vec ) , especially on verbs .",0.09375,0.625,0.625,model,model
entity_linking,7,introduction,introduction,22,11,11,"Since this model is able to take into account word order when classifying , it performs significantly better than an algorithm based on a continuous bag of words model ( Word2 vec ) , especially on verbs .",The first is based on a Long Short Term Memory ( LSTM ) ) .,We then present a semi-supervised algorithm which uses label propagation to label unlabeled sentences based on their similarity to labeled ones .,0.0982142857142857,0.6875,0.6875,model,baselines
entity_linking,7,introduction,introduction,23,12,12,We then present a semi-supervised algorithm which uses label propagation to label unlabeled sentences based on their similarity to labeled ones .,"Since this model is able to take into account word order when classifying , it performs significantly better than an algorithm based on a continuous bag of words model ( Word2 vec ) , especially on verbs .","This allows us to better estimate the distribution of word senses , obtaining more accurate decision boundaries and higher classification accuracy .",0.1026785714285714,0.75,0.75,model,experimental-setup
entity_linking,7,experiments,experiments,130,5,5,Sem Eval Tasks,In order to assess the effects of training corpus size and language model capacity we also evaluate our algorithms using the New Oxford American Dictionary ( NOAD ) inventory with or MASC 1 .,"In this section , we study the performance of our classifiers on Senseval2 , Senseval3 , , SemEval - 2013 Task 12 ) and SemEval - 2015 task 13 2 .",0.5803571428571429,0.0704225352112676,0.1470588235294117,experiments,baselines
entity_linking,7,experiments,experiments,136,11,11,Our proposed algorithms achieve the highest all - words F 1 scores except for Sem - Eval 2013 .,"In our LP classifiers , unlabeled data for each lemma consists either of 1000 sentences which contain the lemma , randomly sampled from the web , or all OMSTI sentences ( without labels ) which contain the lemma . shows the Sem - Eval results .","only disambiguates nouns , and it outperforms our algorithms on Sem - Eval 2013 by 4 % , but is ranked behind our algorithms on Senseval - 3 and SemEval - 7 tasks with an F1 score more than 6 % lower than our algorithms .",0.6071428571428571,0.1549295774647887,0.3235294117647059,experiments,approach
entity_linking,7,experiments,experiments,138,13,13,"Unified WSD has the highest F 1 score on Nouns ( Sem - Eval - 7 Coarse ) , but our algorithms outperform Unified WSD on other part - of - speech tags .","only disambiguates nouns , and it outperforms our algorithms on Sem - Eval 2013 by 4 % , but is ranked behind our algorithms on Senseval - 3 and SemEval - 7 tasks with an F1 score more than 6 % lower than our algorithms .",Settings,0.6160714285714286,0.1830985915492957,0.3823529411764705,experiments,approach
entity_linking,7,experiments,experiments,148,23,23,Word2 Vec vectors Vs. LSTM,We also downsample frequent terms in the same way as .,"To better compare LSTM with word vectors we also build a nearest neighbor classifier using Word2 Vec word embeddings and SemCor example sentences , Word2 Vec ( T : SemCor ) .",0.6607142857142857,0.323943661971831,0.6764705882352942,experiments,experimental-setup
entity_linking,7,experiments,experiments,150,25,25,"It performs similar to IMS + Word2 Vec ( T: SemCor ) , a SVM - based classifier studied in .","To better compare LSTM with word vectors we also build a nearest neighbor classifier using Word2 Vec word embeddings and SemCor example sentences , Word2 Vec ( T : SemCor ) .",shows that the LSTM classifier outperforms the Word2 Vec classifier across the board .,0.6696428571428571,0.3521126760563379,0.7352941176470589,experiments,baselines
entity_linking,7,experiments,experiments,151,26,26,shows that the LSTM classifier outperforms the Word2 Vec classifier across the board .,"It performs similar to IMS + Word2 Vec ( T: SemCor ) , a SVM - based classifier studied in .",Sem Cor Vs. OMSTI,0.6741071428571429,0.3661971830985916,0.7647058823529411,experiments,baselines
entity_linking,7,experiments,experiments,152,27,27,Sem Cor Vs. OMSTI,shows that the LSTM classifier outperforms the Word2 Vec classifier across the board .,"Contrary to the results observed in , the LSTM classifier trained with OMSTI performs worse than that trained with SemCor .",0.6785714285714286,0.380281690140845,0.7941176470588235,experiments,baselines
entity_linking,7,experiments,experiments,153,28,28,"Contrary to the results observed in , the LSTM classifier trained with OMSTI performs worse than that trained with SemCor .",Sem Cor Vs. OMSTI,It seems that the larger size of the OMSTI training data set is more than offset by noise present in its automatically generated labels .,0.6830357142857143,0.3943661971830985,0.8235294117647058,experiments,baselines
entity_linking,7,experiments,experiments,155,30,30,"While the SVM classifier studied in maybe able to learn a model which copes with this noise , our naive nearest neighbor classifiers do not have a learned model and deal less well with noisy labels .",It seems that the larger size of the OMSTI training data set is more than offset by noise present in its automatically generated labels .,Label propagation,0.6919642857142857,0.4225352112676056,0.8823529411764706,experiments,approach
entity_linking,7,experiments,noad eval,160,35,1,NOAD Eval, , ,0.7142857142857143,0.4929577464788733,0.027027027027027,experiments,baselines
entity_linking,7,experiments,noad eval,178,53,19,LSTM classifier,We evaluate all polysemous words in the evaluation corpus .,We compare our algorithms with two baseline algorithms :,0.7946428571428571,0.7464788732394366,0.5135135135135135,experiments,baselines
entity_linking,7,experiments,noad eval,180,55,21,Most frequent sense :,We compare our algorithms with two baseline algorithms :,Compute the sense frequency ( from a labeled corpus ) and label word w with w 's most frequent sense .,0.8035714285714286,0.7746478873239436,0.5675675675675675,experiments,approach
entity_linking,7,experiments,noad eval,184,59,25,"LSTM outperforms Word2Vec by more than 10 % overall words , where most of the gains are from verbs and adverbs .",compares the F 1 scores of the LSTM and baseline algorithms .,"The results suggest that syntactic information , which is well modeled by LSTM but ignored by Word2 Vec , is important to distinguishing word senses of verbs and adverbs .",0.8214285714285714,0.8309859154929577,0.6756756756756757,experiments,baselines
entity_linking,7,experiments,noad eval,186,61,27,Change of training data,"The results suggest that syntactic information , which is well modeled by LSTM but ignored by Word2 Vec , is important to distinguishing word senses of verbs and adverbs .","By default , the WSD classifier uses the NOAD example sentences as training data .",0.8303571428571429,0.8591549295774648,0.7297297297297297,experiments,approach
entity_linking,7,experiments,noad eval,191,66,32,The SemCor ( or MASC ) trained classifier is on a par with the NOAD trained classifier on F1 score .,We further test our algorithm by using SemCor ( or MASC ) as training data ( without NOAD examples ) .,"However , the macro F1 score of the former is much lower than the latter , as shown in , because of the limited coverage of rare senses and words in SemCor and MASC .",0.8526785714285714,0.9295774647887324,0.8648648648648649,experiments,baselines
entity_linking,7,experiments,noad eval,193,68,34,Change of language model capacity,"However , the macro F1 score of the former is much lower than the latter , as shown in , because of the limited coverage of rare senses and words in SemCor and MASC .","In this experiment , we change the LSTM model capacity by varying the number of hidden units hand the dimensions of the input embeddings p and measuring F1 . shows strong positive correlation between F1 and the capacity of the language model .",0.8616071428571429,0.9577464788732394,0.9189189189189192,experiments,baselines
entity_linking,7,experiments,noad eval,196,71,37,"To balance the accuracy and resource usage , we use the second best LSTM model ( h = 2048 and p = 512 ) by default .","However , larger models are slower to train and use more memory .", ,0.875,1.0,1.0,experiments,baselines
entity_linking,7,semi supervised wsd,semi supervised wsd,197,1,1,Semi-supervised WSD, , ,0.8794642857142857,0.0526315789473684,0.0526315789473684,experiments,model
entity_linking,7,semi supervised wsd,semi supervised wsd,201,5,5,"As can be observed from , LP did not yield clear benefits when using the Word2 Vec language model .",We evaluate the performance of the algorithm by comparing the predicted labels and the gold labels on eval nodes .,"We did see significant improvements , 6.3 % increase on SemCor and 7.3 % increase on MASC , using LP with the LSTM language model .",0.8973214285714286,0.2631578947368421,0.2631578947368421,experiments,baselines
entity_linking,7,semi supervised wsd,semi supervised wsd,202,6,6,"We did see significant improvements , 6.3 % increase on SemCor and 7.3 % increase on MASC , using LP with the LSTM language model .","As can be observed from , LP did not yield clear benefits when using the Word2 Vec language model .",We hypothesize that this is because LP is sensitive to the quality of the graph distance metric .,0.9017857142857144,0.3157894736842105,0.3157894736842105,experiments,baselines
entity_linking,7,semi supervised wsd,semi supervised wsd,204,8,8,Change of seed data :,We hypothesize that this is because LP is sensitive to the quality of the graph distance metric .,"As can be seen in , LP substantially improves classifier F1 when the training datasets are SemCor + NOAD or MASC + NOAD .",0.9107142857142856,0.4210526315789473,0.4210526315789473,experiments,model
entity_linking,7,semi supervised wsd,semi supervised wsd,205,9,9,"As can be seen in , LP substantially improves classifier F1 when the training datasets are SemCor + NOAD or MASC + NOAD .",Change of seed data :,"As discussed in Section 4 , the improvement may come from explicitly modeling the sense prior .",0.9151785714285714,0.4736842105263158,0.4736842105263158,experiments,baselines
entity_linking,7,semi supervised wsd,semi supervised wsd,208,12,12,Change of graph density :,We did not see much performance lift by increasing the number of unlabeled sentences per lemma .,"By default , we construct the LP graph by connecting two nodes if their affinity is above 95 % percentile .",0.9285714285714286,0.631578947368421,0.631578947368421,experiments,model
entity_linking,7,semi supervised wsd,semi supervised wsd,209,13,13,"By default , we construct the LP graph by connecting two nodes if their affinity is above 95 % percentile .",Change of graph density :,We also force each node to connect to at least 10 neighbors to prevent isolated nodes .,0.9330357142857144,0.6842105263157895,0.6842105263157895,experiments,model
entity_linking,7,semi supervised wsd,semi supervised wsd,212,16,16,"The F1 scores are relatively stable when the percentile ranges between 85 to 98 , but decrease when the percentile drops to 80 .",shows the performance of the LP algorithm by changing the percentile threshold .,"Also , it takes longer to run the LP algorithm on a denser graph .",0.9464285714285714,0.8421052631578947,0.8421052631578947,experiments,model
entity_linking,8,abstract,abstract,5,2,2,"Language modeling tasks , in which words , or word - pieces , are predicted on the basis of a local context , have been very effective for learning word embeddings and context dependent representations of phrases .", ,"Motivated by the observation that efforts to code world knowledge into machine readable knowledge bases or human readable encyclopedias tend to be entity - centric , we investigate the use of a fill - in - the - blank task to learn context independent representations of entities from the text contexts in which those entities were mentioned .",0.0247524752475247,0.4,0.4,research-problem,experimental-setup
entity_linking,8,introduction,introduction,21,13,13,"We present RELIC ( Representations of Entities Learned in Context ) , a table of independent entity embeddings that have been trained to match fixed length vector representations of the textual context in which those entities have been seen .","The central hypothesis of this paper is that , by matching entities to the contexts in which they are mentioned , we should be able to build a representation for Valentina Tereshkova that encodes the fact that she was the first woman in space , that she was a politician , etc. and that we should be able to use these representations across a wide variety of downstream entity - centric tasks .","We apply RELIC to entity typing ( mapping each entity to its properties in an external , curated , ontology ) ; entity linking ( identifying which entity is referred to by a textual context ) , and trivia question answering ( retrieving the entity that best answers a question ) .",0.1039603960396039,0.5,0.5,model,ablation-analysis
entity_linking,8,introduction,introduction,22,14,14,"We apply RELIC to entity typing ( mapping each entity to its properties in an external , curated , ontology ) ; entity linking ( identifying which entity is referred to by a textual context ) , and trivia question answering ( retrieving the entity that best answers a question ) .","We present RELIC ( Representations of Entities Learned in Context ) , a table of independent entity embeddings that have been trained to match fixed length vector representations of the textual context in which those entities have been seen .","Through these experiments , we show that :",0.1089108910891089,0.5384615384615384,0.5384615384615384,model,baselines
entity_linking,8,experimental setup,experimental setup,98,2,2,"To train RELIC , we obtain data from the 2018 - 10 - 22 dump of English Wikipedia .", ,We take E to be the set of all entities in Wikipedia ( of which there are over 5 million ) .,0.4851485148514851,0.2222222222222222,0.2222222222222222,experimental-setup,approach
entity_linking,8,experimental setup,experimental setup,101,5,5,We limit each context sentence to 128 tokens .,"For each occurrence of a hyperlink , we take the context as the surrounding sentence , replace all tokens in the anchor text with a single [ MASK ] symbol with probability m ( see Section 5.3 fora discussion of different masking rates ) and set the ground truth to be the linked entity .","In this way , we collect a high - quality corpus of over 112M ( context , entity ) pairs .",0.5,0.5555555555555556,0.5555555555555556,experimental-setup,approach
entity_linking,8,experimental setup,experimental setup,104,8,8,We set the entity embedding size to d = 300 .,"Note in particular that an entity never co-occurs with text on its own Wikipedia page , since a page will not hyperlink to itself .",We train the model using Tensor Flow,0.5148514851485149,0.8888888888888888,0.8888888888888888,experimental-setup,experiments
entity_linking,8,experimental setup,experimental setup,105,9,9,We train the model using Tensor Flow,We set the entity embedding size to d = 300 ., ,0.5198019801980198,1.0,1.0,experimental-setup,experimental-setup
entity_linking,8,evaluation,entity linking,113,8,1,ENTITY LINKING, , ,0.5594059405940595,0.0824742268041237,0.0344827586206896,experiments,approach
entity_linking,8,evaluation,entity linking,124,19,12,"However , when we do adopt the standard CoNLL - Aida training set and alias table , RELIC matches the state of the art on this benchmark , despite using far fewer hand engineered resources use the large Wikidata knowledge base to create entity representations ) .",We hypothesize that this is because of the dataset 's bias towards certain types of news content that is very unlike our Wikipedia pre-training data - specifically sports reports .,"We do not make use of the TAC - KBP 2010 training set or alias table , and we observe that RELIC is already competitive without these enhancements 5",0.6138613861386139,0.1958762886597938,0.4137931034482759,experiments,baselines
entity_linking,8,evaluation,entity linking,134,29,22,"Finally , we believe that RELIC 's entity linking performance could be boosted even higher through the adoption of commonly used entity linking features .","In the rest of this section , we will show further support for our hypothesis by recreating parts of the Freebase and Wikipedia ontologies , and by using RELIC to answer trivia questions .","As shown in , use a small set of well chosen discrete features to increase the performance of their embedding based approach by 10 points .",0.6633663366336634,0.2989690721649484,0.7586206896551724,experiments,approach
entity_linking,8,evaluation,entity linking,141,36,29,"RELIC outperforms prior work , even with only 5 % of the training data .","We report P@1 ( proportion of entities whose top ranked types are correct ) , Micro F1 aggregated overall ( entity , type ) compatibility decisions , and overall accuracy of entity labeling decisions .", ,0.698019801980198,0.3711340206185567,1.0,experiments,baselines
entity_linking,8,evaluation,entity level fine typing,142,37,1,ENTITY - LEVEL FINE TYPING, , ,0.7029702970297029,0.3814432989690721,0.1111111111111111,experiments,approach
entity_linking,8,evaluation,entity level fine typing,146,41,5,show that RELIC significantly outperforms prior results on both datasets .,We train a simple 2 - layer feedforward network that takes as input RELIC 's embedding f ( e ) of the entity e and outputs a binary vector indicating which types apply to that entity .,"For FIGMENT , is an ensemble of several standard representation learning techniques : word2 vec skip - gram contexts , structured skip - gram contexts , and FastText representations of the entity names .",0.7227722772277227,0.4226804123711339,0.5555555555555556,experiments,baselines
entity_linking,8,evaluation,entity level fine typing,148,43,7,"For TypeNet , aggregate mention - level types and train with a structured loss based on the TypeNet hierarchy , but is still outperformed by our flat classifier of binary labels .","For FIGMENT , is an ensemble of several standard representation learning techniques : word2 vec skip - gram contexts , structured skip - gram contexts , and FastText representations of the entity names .",We expect that including a hierarchical loss is orthogonal to our approach and could improve our results further .,0.7326732673267327,0.4432989690721649,0.7777777777777778,experiments,baselines
entity_linking,8,evaluation,effect of masking,151,46,1,EFFECT OF MASKING, , ,0.7475247524752475,0.4742268041237113,0.0769230769230769,experiments,approach
entity_linking,8,evaluation,effect of masking,154,49,4,"It is clear that masking mentions during training is beneficial for entity typing tasks , but detrimental for entity linking .",show the effect of training RELIC with different mask rates .,This is in accordance with our intuitions .,0.7623762376237624,0.5051546391752577,0.3076923076923077,experiments,approach
entity_linking,8,evaluation,effect of masking,159,54,9,"A higher mask rate leads to better performance , both in low and high - data situations .","Subsequently , our best typing models are those that are forced : TypeNet entity - level typing m AP on the development set for RELIC models trained with different masking rates .","to capture more of the context in which each entity is mentioned , because they are not allowed to rely on the mention itself .",0.7871287128712872,0.5567010309278351,0.6923076923076923,experiments,approach
entity_linking,8,evaluation,effect of masking,162,57,12,"However , we would like to point out that that a mask rate of 10 % , RELIC nears optimum performance on most tasks .",The divergence between the trends in suggests that there may not be one set of entity embeddings that are optimum for all tasks .,"The optimum mask rate is an open research question , that will likely depend on entity frequency as well as other data statistics .",0.801980198019802,0.5876288659793815,0.9230769230769232,experiments,approach
entity_linking,8,evaluation,few shot category completion,164,59,1,FEW - SHOT CATEGORY COMPLETION, , ,0.8118811881188119,0.6082474226804123,0.0588235294117647,experiments,approach
entity_linking,8,evaluation,few shot category completion,166,61,3,"Furthermore , due to the incompleteness of the the FIGMENT and TypeNet type systems , we also believe that RELIC 's performance is approaching the upper bound on both of these supervised tasks .",The entity - level typing tasks discussed above involve an in - domain training step .,"Therefore , to properly measure RELIC 's ability to capture complex types from fill - in - the - blank training alone , we propose :",0.8217821782178217,0.6288659793814433,0.1764705882352941,experiments,baselines
entity_linking,8,evaluation,trivia question answering,181,76,1,TRIVIA QUESTION ANSWERING, , ,0.8960396039603961,0.7835051546391752,0.0454545454545454,experiments,approach
entity_linking,8,evaluation,trivia question answering,196,91,16,We observe that the retrieve - then - read approach taken by ORQA outperforms the direct answer retrieval approach taken by RELIC .,We consider ORQA to be the most relevant point of comparison for RELIC .,"However , ORQA runs a BERT based reading comprehension model over multiple evidence passages at inference time and we are encouraged to see that RELIC 's much faster nearest neighbor lookup captures 80 % of ORQA 's performance .",0.9702970297029704,0.9381443298969072,0.7272727272727273,experiments,baselines
entity_linking,8,evaluation,trivia question answering,197,92,17,"However , ORQA runs a BERT based reading comprehension model over multiple evidence passages at inference time and we are encouraged to see that RELIC 's much faster nearest neighbor lookup captures 80 % of ORQA 's performance .",We observe that the retrieve - then - read approach taken by ORQA outperforms the direct answer retrieval approach taken by RELIC .,"It is also significant that RELIC outperforms 's reading comprehension baseline by 20 points , despite the fact that the baseline has access to a single document that is known to and TypeNet , even when only training on a small fraction of the task - specific training data .",0.9752475247524752,0.9484536082474226,0.7727272727272727,experiments,baselines
entity_linking,8,evaluation,trivia question answering,198,93,18,"It is also significant that RELIC outperforms 's reading comprehension baseline by 20 points , despite the fact that the baseline has access to a single document that is known to and TypeNet , even when only training on a small fraction of the task - specific training data .","However , ORQA runs a BERT based reading comprehension model over multiple evidence passages at inference time and we are encouraged to see that RELIC 's much faster nearest neighbor lookup captures 80 % of ORQA 's performance .","We then introduce a novel few - shot category reconstruction task and when comparing to , we found that RELIC is better able to capture complex compound types .",0.9801980198019802,0.9587628865979382,0.8181818181818182,experiments,baselines
entity_linking,9,title,title,2,2,2,Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation, , ,0.007380073800738,1.0,1.0,research-problem,experimental-setup
entity_linking,9,abstract,abstract,4,2,2,"Named Entity Disambiguation ( NED ) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base ( KB ) ( e.g. , Wikipedia ) .", ,"In this paper , we propose a novel embedding method specifically designed for NED .",0.014760147601476,0.2857142857142857,0.2857142857142857,research-problem,ablation-analysis
entity_linking,9,abstract,abstract,5,3,3,"In this paper , we propose a novel embedding method specifically designed for NED .","Named Entity Disambiguation ( NED ) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base ( KB ) ( e.g. , Wikipedia ) .",The proposed method jointly maps words and entities into the same continuous vector space .,0.018450184501845,0.4285714285714285,0.4285714285714285,research-problem,baselines
entity_linking,9,introduction,introduction,21,12,12,"In this paper , we propose a method to construct a novel embedding that jointly maps words and entities into the same continuous vector space .",The vectors are designed to capture the semantic similarity of words when similar words are placed near one another in a relatively low - dimensional vector space .,"In this model , similar words and entities are placed close to one another in a vector space .",0.077490774907749,0.0857142857142857,0.4,model,experimental-setup
entity_linking,9,introduction,introduction,22,13,13,"In this model , similar words and entities are placed close to one another in a vector space .","In this paper , we propose a method to construct a novel embedding that jointly maps words and entities into the same continuous vector space .","Hence , we can measure the similarity between any pair of items ( i.e. , words , entities , and a word and an entity ) by simply computing their cosine similarity .",0.081180811808118,0.0928571428571428,0.4333333333333333,model,approach
entity_linking,9,introduction,introduction,23,14,14,"Hence , we can measure the similarity between any pair of items ( i.e. , words , entities , and a word and an entity ) by simply computing their cosine similarity .","In this model , similar words and entities are placed close to one another in a vector space .","This enables us to easily measure the contextual information for NED , such as the similarity between a context word and a candidate entity , and the relatedness of entities required to model coherence .",0.084870848708487,0.1,0.4666666666666667,model,model
entity_linking,9,introduction,introduction,25,16,16,"Our model is based on the skip - gram model , a recently proposed embedding model that learns to predict each context word given the target word .","This enables us to easily measure the contextual information for NED , such as the similarity between a context word and a candidate entity , and the relatedness of entities required to model coherence .",Our model consists of the following three models based on the skip - gram model :,0.092250922509225,0.1142857142857142,0.5333333333333333,model,experimental-setup
entity_linking,9,introduction,introduction,26,17,17,Our model consists of the following three models based on the skip - gram model :,"Our model is based on the skip - gram model , a recently proposed embedding model that learns to predict each context word given the target word .","1 ) the conventional skip - gram model that learns to predict neighboring words given the target word in text corpora , 2 ) the KB graph model that learns to estimate neighboring entities given the target entity in the link graph of the KB , and 3 ) the anchor context model that learns to predict neighboring words given the target entity using anchors and their context words in the KB .",0.0959409594095941,0.1214285714285714,0.5666666666666667,model,approach
entity_linking,9,introduction,introduction,27,18,18,"1 ) the conventional skip - gram model that learns to predict neighboring words given the target word in text corpora , 2 ) the KB graph model that learns to estimate neighboring entities given the target entity in the link graph of the KB , and 3 ) the anchor context model that learns to predict neighboring words given the target entity using anchors and their context words in the KB .",Our model consists of the following three models based on the skip - gram model :,"By jointly optimizing these models , our method simultaneously learns the embedding of words and entities .",0.0996309963099631,0.1285714285714285,0.6,model,ablation-analysis
entity_linking,9,introduction,introduction,29,20,20,"Based on our proposed embedding , we also develop a straightforward NED method that computes two contexts using the proposed embedding : textual context similarity , and coherence .","By jointly optimizing these models , our method simultaneously learns the embedding of words and entities .",Textual context similarity is measured according to vector similarity between an entity and words in a document .,0.1070110701107011,0.1428571428571428,0.6666666666666666,model,approach
entity_linking,9,introduction,introduction,32,23,23,"Our NED method combines these contexts with several standard features ( e.g. , prior probability ) using supervised machine learning .",Coherence is measured based on the relatedness between the target entity and other entities in a document .,We tested the proposed method using two standard NED datasets : the CoNLL dataset and the TAC 2010 dataset .,0.1180811808118081,0.1642857142857142,0.7666666666666667,model,experimental-setup
entity_linking,9,experiments,named entity disambiguation,215,66,38,Our method successfully achieved enhanced performance on both the CoNLL and the TAC 2010 datasets .,shows the experimental results of our proposed method .,"Moreover , we found that the choice of candidate generation method considerably affected performance on the CoNLL dataset .",0.7933579335793358,0.7096774193548387,0.5846153846153846,results,baselines
entity_linking,9,experiments,named entity disambiguation,216,67,39,"Moreover , we found that the choice of candidate generation method considerably affected performance on the CoNLL dataset .",Our method successfully achieved enhanced performance on both the CoNLL and the TAC 2010 datasets .,"Further , shows the experimental results of our proposed method as well as those of stateof - the - art methods .",0.7970479704797048,0.7204301075268817,0.6,results,baselines
entity_linking,9,experiments,named entity disambiguation,218,69,41,Our method outperformed all the state - of - the - art methods on both datasets .,"Further , shows the experimental results of our proposed method as well as those of stateof - the - art methods .",Results,0.8044280442804428,0.7419354838709677,0.6307692307692307,results,baselines
face_alignment,0,abstract,abstract,4,2,2,"3D Morphable Models ( 3 DMMs ) are powerful statistical models of 3D facial shape and texture , and among the stateof - the - art methods for reconstructing facial shape from single images .", ,"With the advent of new 3D sensors , many 3 D facial datasets have been collected containing both neutral as well as expressive faces .",0.0168776371308016,0.1818181818181818,0.1818181818181818,research-problem,ablation-analysis
face_alignment,0,introduction,introduction,18,5,5,3 D facial shape estimation from single images has attracted the attention of many researchers the past twenty years .,"Nevertheless , discriminative techniques can not be applied for 3D facial shape estimation "" in - the -wild "" , due to lack of ground - truth data .",The two main lines of research are ( i ) fitting a 3D Morphable Model ( 3 DMM ) and ( ii ) applying Shape from Shading ( SfS ) techniques .,0.0759493670886076,0.1282051282051282,0.1282051282051282,research-problem,experimental-setup
face_alignment,0,introduction,introduction,41,28,28,"We propose a methodology for learning a statistical texture model from "" in - the -wild "" facial images , which is in full correspondence with a statistical shape prior that exhibits both identity and expression variations .","In particular , our contributions are :","Motivated by the success of feature - based ( e.g. , HOG , SIFT ) Active Appearance Models ( AAMs ) we further show how to learn featurebased texture models for 3 DMMs .",0.1729957805907173,0.7179487179487181,0.7179487179487181,model,experimental-setup
face_alignment,0,introduction,introduction,42,29,29,"Motivated by the success of feature - based ( e.g. , HOG , SIFT ) Active Appearance Models ( AAMs ) we further show how to learn featurebased texture models for 3 DMMs .","We propose a methodology for learning a statistical texture model from "" in - the -wild "" facial images , which is in full correspondence with a statistical shape prior that exhibits both identity and expression variations .","We show that the advantage of using the "" in - the -wild "" feature - based texture model is that the fitting strategy gets simplified since there is not need to optimize with respect to the illumination parameters .",0.1772151898734177,0.7435897435897436,0.7435897435897436,model,experimental-setup
face_alignment,0,introduction,introduction,43,30,30,"We show that the advantage of using the "" in - the -wild "" feature - based texture model is that the fitting strategy gets simplified since there is not need to optimize with respect to the illumination parameters .","Motivated by the success of feature - based ( e.g. , HOG , SIFT ) Active Appearance Models ( AAMs ) we further show how to learn featurebased texture models for 3 DMMs .","By capitalising on the recent advancements in fitting statistical deformable models , we propose a novel and fast algorithm for fitting "" in - the -wild "" 3 DMMs .",0.1814345991561181,0.7692307692307693,0.7692307692307693,model,baselines
face_alignment,0,experiments,experiments,210,6,6,3D Shape Recovery,reports additional measures .,"Herein , we evaluate our "" in - the -wild "" 3 DMM ( ITW ) in terms of 3D shape estimation accuracy against two popular state - of - the - art alternative 3 DMM formulations .",0.8860759493670886,0.2222222222222222,0.2857142857142857,experiments,experimental-setup
face_alignment,0,experiments,experiments,222,18,18,"The Classic model struggles to fit to the "" in - the -wild "" conditions present in the test set , and performs the worst .",Under the Curve ( AUC ) and failure rates .,"The texture - free Linear model does better , but the ITW model is most able to recover the facial shapes due to its ideal feature basis for the "" in - the -wild "" conditions .",0.9367088607594936,0.6666666666666666,0.8571428571428571,experiments,approach
face_alignment,0,experiments,experiments,223,19,19,"The texture - free Linear model does better , but the ITW model is most able to recover the facial shapes due to its ideal feature basis for the "" in - the -wild "" conditions .","The Classic model struggles to fit to the "" in - the -wild "" conditions present in the test set , and performs the worst .","demonstrates qualitative results on a wide range of fits of "" in - the - wild "" images drawn from the Helen and 300 W datasets that qualitatively highlight the effectiveness of the proposed technique .",0.940928270042194,0.7037037037037037,0.9047619047619048,experiments,baselines
face_alignment,0,experiments,experiments,225,21,21,"We note that in a wide variety of expression , identity , lighting and occlusion conditions our model is able to robustly reconstruct a realistic 3 D facial shape that stands up to scrutiny .","demonstrates qualitative results on a wide range of fits of "" in - the - wild "" images drawn from the Helen and 300 W datasets that qualitatively highlight the effectiveness of the proposed technique .", ,0.9493670886075948,0.7777777777777778,1.0,experiments,baselines
face_alignment,0,experiments,quantitative normal recovery,226,22,1,Quantitative Normal Recovery, , ,0.9535864978902954,0.8148148148148148,0.1666666666666666,experiments,baselines
face_alignment,0,experiments,quantitative normal recovery,231,27,6,ITW slightly outperforms IMM even though both IMM and PS - NL use all four available images of each subject .,In we show the cumulative error distribution in terms of the mean angular error ., ,0.9746835443037974,1.0,1.0,experiments,baselines
face_alignment,1,title,title,2,2,2,Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression, , ,0.0091324200913242,1.0,1.0,research-problem,experimental-setup
face_alignment,1,abstract,abstract,5,3,3,Abstract 3 D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty .,"Figure 1 : A few results from our VRN - Guided method , on a full range of pose , including large expressions .","Current systems often assume the availability of multiple facial images ( sometimes from the same subject ) as input , and must address a number of methodological challenges such as establishing dense correspondences across large facial poses , expressions , and non-uniform illumination .",0.0228310502283105,0.3,0.3,research-problem,baselines
face_alignment,1,introduction,main contributions,33,21,2,"We describe a very simple approach which bypasses many of the difficulties encountered in 3D face reconstruction by using a novel volumetric representation of the 3D facial geometry , and an appropriate CNN architecture that is trained to regress directly from a 2 D facial image to the corresponding 3D volume .", ,An overview of our method is shown in .,0.1506849315068493,0.6774193548387096,0.1666666666666666,approach,ablation-analysis
face_alignment,1,method,training,147,63,2,"Each of our architectures was trained end - to - end using RMSProp with an initial learning rate of 10 ? 4 , which was lowered after 40 epochs to 10 ?5 .", ,"During training , random augmentation was applied to each input sample ( face image ) and its corresponding target ( 3D volume ) : we applied in - plane rotation r ? [ ?45 , ... , 45 ] , translation t z , t y ? [ ? 15 , ... , 15 ] and scale s ? [ 0.85 , ... , 1.15 ] jitter .",0.6712328767123288,0.9402985074626866,0.3333333333333333,experimental-setup,approach
face_alignment,1,method,training,148,64,3,"During training , random augmentation was applied to each input sample ( face image ) and its corresponding target ( 3D volume ) : we applied in - plane rotation r ? [ ?45 , ... , 45 ] , translation t z , t y ? [ ? 15 , ... , 15 ] and scale s ? [ 0.85 , ... , 1.15 ] jitter .","Each of our architectures was trained end - to - end using RMSProp with an initial learning rate of 10 ? 4 , which was lowered after 40 epochs to 10 ?5 .","In 20 % of cases , the input and target were flipped horizontally .",0.6757990867579908,0.9552238805970148,0.5,experimental-setup,approach
face_alignment,1,method,training,149,65,4,"In 20 % of cases , the input and target were flipped horizontally .","During training , random augmentation was applied to each input sample ( face image ) and its corresponding target ( 3D volume ) : we applied in - plane rotation r ? [ ?45 , ... , 45 ] , translation t z , t y ? [ ? 15 , ... , 15 ] and scale s ? [ 0.85 , ... , 1.15 ] jitter .","Finally , the input samples were adjusted with some colour scaling on each RGB channel .",0.680365296803653,0.9701492537313432,0.6666666666666666,experimental-setup,approach
face_alignment,1,method,training,150,66,5,"Finally , the input samples were adjusted with some colour scaling on each RGB channel .","In 20 % of cases , the input and target were flipped horizontally .","In the case of the VRN - Guided , the landmark detection module was trained to regress Gaussians with standard deviation of approximately 3 pixels (? = 1 ) .",0.6849315068493149,0.9850746268656716,0.8333333333333334,experimental-setup,approach
face_alignment,1,method,training,151,67,6,"In the case of the VRN - Guided , the landmark detection module was trained to regress Gaussians with standard deviation of approximately 3 pixels (? = 1 ) .","Finally , the input samples were adjusted with some colour scaling on each RGB channel .", ,0.6894977168949772,1.0,1.0,experimental-setup,approach
face_alignment,1,results,results,159,8,8,Volumetric Regression Networks largely outperform,"From these results , we can conclude the following :","3DDFA and EOS on all datasets , verifying that directly regressing the 3D facial structure is a much easier problem for CNN learning .",0.726027397260274,0.2162162162162162,0.2162162162162162,results,baselines
face_alignment,1,results,results,160,9,9,"3DDFA and EOS on all datasets , verifying that directly regressing the 3D facial structure is a much easier problem for CNN learning .",Volumetric Regression Networks largely outperform,"2 . All VRNs perform well across the whole spectrum of facial poses , expressions and occlusions .",0.7305936073059359,0.2432432432432433,0.2432432432432433,results,baselines
face_alignment,1,results,results,161,10,10,"2 . All VRNs perform well across the whole spectrum of facial poses , expressions and occlusions .","3DDFA and EOS on all datasets , verifying that directly regressing the 3D facial structure is a much easier problem for CNN learning .","Also , there are no significant performance discrepancies across different datasets ( ALFW2000 - 3D seems to be slightly more difficult ) .",0.7351598173515982,0.2702702702702703,0.2702702702702703,results,baselines
face_alignment,1,results,results,163,12,12,"3 . The best performing VRN is the one guided by detected landmarks , however at the cost of higher computational complexity :","Also , there are no significant performance discrepancies across different datasets ( ALFW2000 - 3D seems to be slightly more difficult ) .",VRN - Guided uses another stacked hourglass network for landmark localization .,0.7442922374429224,0.3243243243243243,0.3243243243243243,results,approach
face_alignment,1,results,results,165,14,14,"4 . VRN - Multitask does not always perform particularly better than the plain VRN ( in fact on BU - 4 DFE it performs worse ) , not justifying the increase of network complexity .",VRN - Guided uses another stacked hourglass network for landmark localization .,It seems that it might be preferable to train a network to focus on the task in hand .,0.7534246575342466,0.3783783783783784,0.3783783783783784,results,approach
face_alignment,1,ablation studies,ablation studies,200,4,4,Effect of pose .,"For all experiments reported , we used the best performing VRN - Guided .","To measure the influence of pose on the reconstruction error , we measured the NME for different yaw angles using all of our Florence renderings .",0.91324200913242,0.2222222222222222,0.2222222222222222,ablation-analysis,baselines
face_alignment,1,ablation studies,ablation studies,202,6,6,"As shown in , the performance of our method decreases as the pose increases .","To measure the influence of pose on the reconstruction error , we measured the NME for different yaw angles using all of our Florence renderings .","This is to be expected , due to less of the face being visible which makes evaluation for the invisible part difficult .",0.9223744292237442,0.3333333333333333,0.3333333333333333,ablation-analysis,approach
face_alignment,1,ablation studies,ablation studies,205,9,9,Effect of expression .,We believe that our error is still very low considering these poses .,Certain expressions are usually considered harder to accurately reproduce in 3D face reconstruction .,0.9360730593607306,0.5,0.5,ablation-analysis,baselines
face_alignment,1,ablation studies,ablation studies,208,12,12,"This kind of extreme acted facial expressions generally do not occur in the training set , yet as shown in , the performance variation across different expressions is quite minor .","To measure the effect of facial expressions on performance , we rendered frontal images in difference expressions from BU - 4DFE ( since Florence only exhibits a neutral expression ) and measured the performance for each expression .",Effect of Gaussian size for guidance .,0.9497716894977168,0.6666666666666666,0.6666666666666666,ablation-analysis,experimental-setup
face_alignment,1,ablation studies,ablation studies,209,13,13,Effect of Gaussian size for guidance .,"This kind of extreme acted facial expressions generally do not occur in the training set , yet as shown in , the performance variation across different expressions is quite minor .","We trained a VRN - Guided , however , this time , the facial landmark detector network of the VRN - Guided regresses larger Gaussians ( ? = 2 as opposed to the normal ? = 1 ) .",0.9543378995433792,0.7222222222222222,0.7222222222222222,ablation-analysis,baselines
face_alignment,1,ablation studies,ablation studies,211,15,15,"The performance of the 3D reconstruction dropped by a negligible amount , suggesting that as long as the Gaussians are of a sensible size , guidance will always help .","We trained a VRN - Guided , however , this time , the facial landmark detector network of the VRN - Guided regresses larger Gaussians ( ? = 2 as opposed to the normal ? = 1 ) .",Normalised NME :,0.9634703196347032,0.8333333333333334,0.8333333333333334,ablation-analysis,model
face_alignment,10,title,title,2,2,2,Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks, , ,0.0068493150684931,1.0,1.0,research-problem,experimental-setup
face_alignment,10,introduction,introduction,14,2,2,"Facial landmark localisation , or face alignment , aims at finding the coordinates of a set of pre-defined key points for 2 D face images .", ,"A facial landmark usually has specific semantic meaning , e.g. nose tip or eye centre , which provides rich geometric information for other face analysis tasks such as face recognition , emotion estimation and 3D face reconstruction .",0.047945205479452,0.064516129032258,0.064516129032258,research-problem,ablation-analysis
face_alignment,10,introduction,introduction,34,22,22,"a novel loss function , namely the Wing loss , which is designed to improve the deep neural network training capability for small and medium range errors .","We empirically and theoretically compare L1 , L2 and smooth L1 loss functions and find that L1 and smooth L1 perform much better than the widely used L2 loss .","a data augmentation strategy , i.e. pose - based data balancing , that compensates the low frequency of occurrence of samples with large out - of - plane head rotations in the training set .",0.1164383561643835,0.7096774193548387,0.7096774193548387,approach,model
face_alignment,10,introduction,introduction,35,23,23,"a data augmentation strategy , i.e. pose - based data balancing , that compensates the low frequency of occurrence of samples with large out - of - plane head rotations in the training set .","a novel loss function , namely the Wing loss , which is designed to improve the deep neural network training capability for small and medium range errors .",a two - stage facial landmark localisation framework for performance boosting .,0.1198630136986301,0.7419354838709677,0.7419354838709677,approach,ablation-analysis
face_alignment,10,introduction,introduction,36,24,24,a two - stage facial landmark localisation framework for performance boosting .,"a data augmentation strategy , i.e. pose - based data balancing , that compensates the low frequency of occurrence of samples with large out - of - plane head rotations in the training set .",The paper is organised as follows .,0.1232876712328767,0.7741935483870968,0.7741935483870968,approach,model
face_alignment,10,experimental results,implementation details,209,7,2,"In our experiments , we used Matlab 2017a and the Mat - ConvNet toolbox 2 .", ,"The training and testing of our networks were conducted on a server running Ubuntu 16.04 with 2 Intel Xeon E5-2667 v4 CPU , 256 GB RAM and 4 NVIDIA GeForce GTX Titan X ( Pascal ) cards .",0.7157534246575342,0.0875,0.0357142857142857,experimental-setup,experimental-setup
face_alignment,10,experimental results,implementation details,210,8,3,"The training and testing of our networks were conducted on a server running Ubuntu 16.04 with 2 Intel Xeon E5-2667 v4 CPU , 256 GB RAM and 4 NVIDIA GeForce GTX Titan X ( Pascal ) cards .","In our experiments , we used Matlab 2017a and the Mat - ConvNet toolbox 2 .",Note that we only use one GPU card for measuring the run time .,0.7191780821917808,0.1,0.0535714285714285,experimental-setup,experimental-setup
face_alignment,10,experimental results,implementation details,211,9,4,Note that we only use one GPU card for measuring the run time .,"The training and testing of our networks were conducted on a server running Ubuntu 16.04 with 2 Intel Xeon E5-2667 v4 CPU , 256 GB RAM and 4 NVIDIA GeForce GTX Titan X ( Pascal ) cards .","We set the weight decay to 5 10 ? 4 , momentum to 0.9 and batch size to 8 for network training .",0.7226027397260274,0.1125,0.0714285714285714,experimental-setup,approach
face_alignment,10,experimental results,implementation details,212,10,5,"We set the weight decay to 5 10 ? 4 , momentum to 0.9 and batch size to 8 for network training .",Note that we only use one GPU card for measuring the run time .,Each model was trained for 120 k iterations .,0.726027397260274,0.125,0.0892857142857142,experimental-setup,experimental-setup
face_alignment,10,experimental results,implementation details,213,11,6,Each model was trained for 120 k iterations .,"We set the weight decay to 5 10 ? 4 , momentum to 0.9 and batch size to 8 for network training .","We did not use any other advanced techniques in our CNN - 6 and CNN - 7 networks , such as batch normalisation , dropout or residual blocks .",0.7294520547945206,0.1375,0.1071428571428571,experimental-setup,baselines
face_alignment,10,experimental results,implementation details,215,13,8,"The standard ReLu function was used for nonlinear activation , and Max pooling with the stride of 2 was used to downsize feature maps .","We did not use any other advanced techniques in our CNN - 6 and CNN - 7 networks , such as batch normalisation , dropout or residual blocks .","For the convolutional layer , we used 3 3 kernels with the stride of 1 .",0.7363013698630136,0.1625,0.1428571428571428,experimental-setup,baselines
face_alignment,10,experimental results,implementation details,216,14,9,"For the convolutional layer , we used 3 3 kernels with the stride of 1 .","The standard ReLu function was used for nonlinear activation , and Max pooling with the stride of 2 was used to downsize feature maps .","All our networks , except ResNet - 50 , were trained from scratch without any pre-training on any other dataset .",0.7397260273972602,0.175,0.1607142857142857,experimental-setup,experimental-setup
face_alignment,10,experimental results,implementation details,218,16,11,"For the proposed PDB strategy , the number of bins K was set to 17 for AFLW and 9 for 300W .","All our networks , except ResNet - 50 , were trained from scratch without any pre-training on any other dataset .","For CNN - 6 , the input image size is 64 64 3 . We reduced the learning rate from 3 10 ? 6 to 3 10 ?8 for the L2 loss , and from 3 10 ?5 to 3 10 ? 7 for the other loss functions .",0.7465753424657534,0.2,0.1964285714285714,experimental-setup,experimental-setup
face_alignment,10,experimental results,implementation details,219,17,12,"For CNN - 6 , the input image size is 64 64 3 . We reduced the learning rate from 3 10 ? 6 to 3 10 ?8 for the L2 loss , and from 3 10 ?5 to 3 10 ? 7 for the other loss functions .","For the proposed PDB strategy , the number of bins K was set to 17 for AFLW and 9 for 300W .","The parameters of the Wing loss were set tow = 10 and = 2 . For CNN - 7 , the input image size is 128 128 3 . We reduced the learning rate from 1 10 ? 6 to 1 10 ?8 for the L2 loss , and from 1 10 ? 5 to 1 10 ? 7 for the other loss functions .",0.75,0.2125,0.2142857142857142,experimental-setup,experiments
face_alignment,10,experimental results,implementation details,220,18,13,"The parameters of the Wing loss were set tow = 10 and = 2 . For CNN - 7 , the input image size is 128 128 3 . We reduced the learning rate from 1 10 ? 6 to 1 10 ?8 for the L2 loss , and from 1 10 ? 5 to 1 10 ? 7 for the other loss functions .","For CNN - 6 , the input image size is 64 64 3 . We reduced the learning rate from 3 10 ? 6 to 3 10 ?8 for the L2 loss , and from 3 10 ?5 to 3 10 ? 7 for the other loss functions .",The parameters of the Wing loss were set tow = 15 and = 3 .,0.7534246575342466,0.225,0.2321428571428571,experimental-setup,experiments
face_alignment,10,experimental results,implementation details,222,20,15,"To perform data augmentation , we randomly rotated each training image between [ ? 30 , 30 ] degrees for CNN - 6 and between [ ? 10 , 10 ] degrees for CNN - 7 .",The parameters of the Wing loss were set tow = 15 and = 3 .,"In addition , we randomly flipped each training image with the probability of 50 % .",0.7602739726027398,0.25,0.2678571428571428,experimental-setup,experimental-setup
face_alignment,10,experimental results,implementation details,224,22,17,"For bounding box perturbation , we applied random translations to the upper-left and bottom - right corners of the face bounding box within 5 % of the bounding .","In addition , we randomly flipped each training image with the probability of 50 % .",A comparison of the CED curves on the AFLW dataset .,0.7671232876712328,0.275,0.3035714285714285,experimental-setup,model
face_alignment,10,experimental results,implementation details,226,24,19,"We compare our method with a set of state - of - the - art approaches , including SDM , ERT , RCPR , CFSS , LBF , GRF , CCL , DAC - CSR and TR - DRN. box size .",A comparison of the CED curves on the AFLW dataset .,"Last , we randomly injected Gaussian blur (? = 1 ) to each training image with the probability of 50 % .",0.773972602739726,0.3,0.3392857142857143,baselines,baselines
face_alignment,10,experimental results,implementation details,227,25,20,"Last , we randomly injected Gaussian blur (? = 1 ) to each training image with the probability of 50 % .","We compare our method with a set of state - of - the - art approaches , including SDM , ERT , RCPR , CFSS , LBF , GRF , CCL , DAC - CSR and TR - DRN. box size .",Evaluation Metric :,0.7773972602739726,0.3125,0.3571428571428571,experimental-setup,experimental-setup
face_alignment,10,experimental results,implementation details,234,32,27,Comparison with state of the art 7.2.1 AFLW,"This protocol uses the inter-pupil distance as the normalisation term , which is different from the standard 300 W protocol that uses the outer eye corner distance .","We first evaluated our algorithm on the AFLW dataset , using the AFLW - Full protocol .",0.8013698630136986,0.4,0.4821428571428572,experiments,experimental-setup
face_alignment,10,experimental results,implementation details,246,44,39,"As shown in , our CNN - 6/7 network outperforms all the other approaches even when trained with the commonly used L2 loss function ( magenta solid line ) .","Note that the error is normalised by the inter-pupil distance , rather than the outer eye corner distance .",This validates the effectiveness of the proposed two - stage localisation framework and the PDB strategy .,0.8424657534246576,0.55,0.6964285714285714,experiments,baselines
face_alignment,10,experimental results,implementation details,248,46,41,"Second , by simply switching the loss function from L2 to L1 or smooth L1 , the performance of our method has been improved significantly ( red solid and black dashed lines ) .",This validates the effectiveness of the proposed two - stage localisation framework and the PDB strategy .,"Last , the use of our newly proposed Wing loss function further improves the accuracy ( black solid line ) .",0.8493150684931506,0.575,0.7321428571428571,experiments,baselines
face_alignment,10,experimental results,implementation details,249,47,42,"Last , the use of our newly proposed Wing loss function further improves the accuracy ( black solid line ) .","Second , by simply switching the loss function from L2 to L1 or smooth L1 , the performance of our method has been improved significantly ( red solid and black dashed lines ) .",The proportion of test samples ( Y-axis ) associated with a small to medium normalised mean error ( X-axis ) is increased .,0.8527397260273972,0.5875,0.75,experiments,baselines
face_alignment,10,experimental results,implementation details,251,49,44,300 W,The proportion of test samples ( Y-axis ) associated with a small to medium normalised mean error ( X-axis ) is increased .,"The 300W dataset is a collection of multiple face datasets , including LFPW , HELEN , AFW and XM2VTS .",0.8595890410958904,0.6125,0.7857142857142857,experiments,baselines
face_alignment,10,experimental results,implementation details,262,60,55,"As shown in , our two - stage landmark localisation framework with the PDB strategy and the newly proposed Wing loss function outperforms all the other stateof - the - art algorithms on the 300 W dataset inaccuracy .",The results obtained by our approach with different loss functions are reported in .,The error has been reduced by almost 20 % as compared to the current best result reported by the RAR algorithm .,0.8972602739726028,0.75,0.9821428571428572,experiments,baselines
face_alignment,10,experimental results,implementation details,263,61,56,The error has been reduced by almost 20 % as compared to the current best result reported by the RAR algorithm .,"As shown in , our two - stage landmark localisation framework with the PDB strategy and the newly proposed Wing loss function outperforms all the other stateof - the - art algorithms on the 300 W dataset inaccuracy .", ,0.9006849315068494,0.7625,1.0,experiments,baselines
face_alignment,11,title,title,2,2,2,Unsupervised Training for 3D Morphable Model Regression, , ,0.0075187969924812,1.0,1.0,research-problem,approach
face_alignment,11,introduction,introduction,20,13,13,This paper presents a method for training a regression network that removes both the need for supervised training data and the reliance on inverse rendering to reproduce image pixels .,"to - image autoencoder with a fixed , morphable - model - based decoder and an image - based loss .","Instead , the network learns to minimize a loss based on the facial identity features produced by a face recognition network such as VGG - Face or Google 's FaceNet .",0.075187969924812,0.6190476190476191,0.6190476190476191,model,experimental-setup
face_alignment,11,introduction,introduction,21,14,14,"Instead , the network learns to minimize a loss based on the facial identity features produced by a face recognition network such as VGG - Face or Google 's FaceNet .",This paper presents a method for training a regression network that removes both the need for supervised training data and the reliance on inverse rendering to reproduce image pixels .,"These features are robust to pose , expression , lighting , and even non-photorealistic inputs .",0.0789473684210526,0.6666666666666666,0.6666666666666666,model,model
face_alignment,11,introduction,introduction,23,16,16,We exploit this invariance to apply a loss that matches the identity features between the input photograph and a synthetic rendering of the predicted face .,"These features are robust to pose , expression , lighting , and even non-photorealistic inputs .","The synthetic rendering need not have the same pose , expression , or lighting of the photograph , allowing our network to predict only shape and texture .",0.0864661654135338,0.7619047619047619,0.7619047619047619,model,experimental-setup
face_alignment,11,introduction,introduction,26,19,19,"We alleviate the fooling problem by applying three novel losses : a batch distribution loss to match the statistics of each training batch to the statistics of the morphable model , a loopback loss to ensure the regression network can correctly reinterpret its own output , and a multi-view identity loss that combines features from multiple , independent views of the predicted shape .","Simply optimizing for similarity between identity features , however , can teach the regression network to fool the recognition network by producing faces that match closely in feature space but look unnatural .","Using this scheme , we train a 3D shape and texture regression network using only a face recognition network , a morphable face model , and a dataset of unlabeled face images .",0.0977443609022556,0.9047619047619048,0.9047619047619048,model,baselines
face_alignment,11,introduction,introduction,27,20,20,"Using this scheme , we train a 3D shape and texture regression network using only a face recognition network , a morphable face model , and a dataset of unlabeled face images .","We alleviate the fooling problem by applying three novel losses : a batch distribution loss to match the statistics of each training batch to the statistics of the morphable model , a loopback loss to ensure the regression network can correctly reinterpret its own output , and a multi-view identity loss that combines features from multiple , independent views of the predicted shape .","We show that despite learning from unlabeled photographs , the 3D face results improve on the accuracy of previous work and are often recognizable as the original subjects .",0.1015037593984962,0.9523809523809524,0.9523809523809524,model,experimental-setup
face_alignment,11,introduction,introduction,28,21,21,"We show that despite learning from unlabeled photographs , the 3D face results improve on the accuracy of previous work and are often recognizable as the original subjects .","Using this scheme , we train a 3D shape and texture regression network using only a face recognition network , a morphable face model , and a dataset of unlabeled face images .", ,0.1052631578947368,1.0,1.0,model,approach
face_alignment,11,experiments,neutral pose reconstruction on micc,184,20,1,Neutral Pose Reconstruction on MICC, , ,0.6917293233082706,0.273972602739726,0.0833333333333333,experiments,experimental-setup
face_alignment,11,experiments,neutral pose reconstruction on micc,194,30,11,"Our results indicate that we have improved absolute error to the ground truth by 20 - 25 % , and our results are more consistent from person to person , with less than half the standard deviation when compared to .","shows the symmetric point - to - plane distance in millimeters within the ICPdetermined region of intersection , rather than point - to - point distances , as the methods and ground truth have different vertex densities .","We are also more stable across changing environments , with similar results for all three test sets .",0.7293233082706767,0.410958904109589,0.9166666666666666,experiments,baselines
face_alignment,11,experiments,neutral pose reconstruction on micc,195,31,12,"We are also more stable across changing environments , with similar results for all three test sets .","Our results indicate that we have improved absolute error to the ground truth by 20 - 25 % , and our results are more consistent from person to person , with less than half the standard deviation when compared to .", ,0.7330827067669173,0.4246575342465753,1.0,experiments,baselines
face_alignment,11,experiments,face recognition results,196,32,1,Face Recognition Results, , ,0.7368421052631579,0.4383561643835616,0.037037037037037,experiments,approach
face_alignment,11,experiments,face recognition results,202,38,7,Our method achieves an average similarity between rendering and photo of 0.403 on MoFA test ( the dataset for which results for all methods are available ) .,"The similarity between all pairs of photographs in the LFW dataset , separated into same - person and differentperson distributions , is shown for comparison in , top .","By comparison , 22.7 % of pairs of photos of the same person in LFW have a score below 0.403 , and only 0.04 % of pairs of photos of different people have a score above 0.403 .",0.7593984962406015,0.5205479452054794,0.2592592592592592,experiments,baselines
face_alignment,11,experiments,face recognition results,205,41,10,"Our method 's results are closer to the same - person distribution than the differentperson distribution in all cases , while the other methods results ' are closer to the different - person distribution .","For additional validation , shows the Earth Mover 's distance between the all - pairs LFW distributions and the results of each method .","We conclude that ours is the first method that generates neutralpose , 3 D faces with recognizability approaching a photo .",0.7706766917293233,0.5616438356164384,0.3703703703703704,experiments,approach
face_alignment,11,experiments,face recognition results,211,47,16,"Notably , the distance between the GT distribution and the same - person LFW distribution is very low , with almost the same mean ( 0.51 vs 0.50 ) , indicating the VGG - Face network has little trouble bridging the domain gap between photograph and rendering , and that our method does not yet reach the ground - truth baseline .",and their input photographs provide a ceiling for similarity scores .,MoFA - Test,0.7932330827067671,0.6438356164383562,0.5925925925925926,experiments,baselines
face_alignment,12,dense face alignment,dense face alignment,2,1,1,Dense Face Alignment, , ,0.007380073800738,1.0,1.0,research-problem,baselines
face_alignment,12,abstract,abstract,4,2,2,Face alignment is a classic problem in the computer vision field ., ,"Previous works mostly focus on sparse alignment with a limited number of facial landmark points , i.e. , facial landmark detection .",0.014760147601476,0.25,0.25,research-problem,experimental-setup
face_alignment,12,introduction,introduction,38,28,28,"With the objective of addressing both challenges , we learn a CNN to fit a 3 D face model to the face image .","Therefore , the second challenge is to allow the proposed method to leverage multiple face datasets .","While the proposed method works for any face image , we mainly pay attention to faces with large poses .",0.1402214022140221,0.6829268292682927,0.6829268292682927,model,ablation-analysis
face_alignment,12,introduction,introduction,41,31,31,"To tackle first challenge of limited landmark labeling , we propose to employ additional constraints .","Large - pose face alignment is a relatively new topic , and the performances in still have room to improve .","We include contour constraint where the contour of the predicted shape should match the detected 2 D face boundary , and SIFT constraint where the SIFT key points detected on two face images of the same individual should map to the same vertexes on the 3D face model .",0.1512915129151291,0.7560975609756098,0.7560975609756098,model,ablation-analysis
face_alignment,12,introduction,introduction,42,32,32,"We include contour constraint where the contour of the predicted shape should match the detected 2 D face boundary , and SIFT constraint where the SIFT key points detected on two face images of the same individual should map to the same vertexes on the 3D face model .","To tackle first challenge of limited landmark labeling , we propose to employ additional constraints .","Both constraints are integrated into the CNN training as additional loss function terms , where the end - to - end training results in an enhanced CNN for 3 D face model fitting .",0.1549815498154981,0.7804878048780488,0.7804878048780488,model,model
face_alignment,12,introduction,introduction,43,33,33,"Both constraints are integrated into the CNN training as additional loss function terms , where the end - to - end training results in an enhanced CNN for 3 D face model fitting .","We include contour constraint where the contour of the predicted shape should match the detected 2 D face boundary , and SIFT constraint where the SIFT key points detected on two face images of the same individual should map to the same vertexes on the 3D face model .","For the second challenge of leveraging multiple datasets , the 3D face model fitting approach has the inherent advantage in handling multiple training databases .",0.1586715867158671,0.8048780487804879,0.8048780487804879,model,experimental-setup
face_alignment,12,introduction,introduction,44,34,34,"For the second challenge of leveraging multiple datasets , the 3D face model fitting approach has the inherent advantage in handling multiple training databases .","Both constraints are integrated into the CNN training as additional loss function terms , where the end - to - end training results in an enhanced CNN for 3 D face model fitting .","Regardless of the landmark labeling number in a particular dataset , we can always define the corresponding 3 D vertexes to guide the training .",0.1623616236162361,0.8292682926829268,0.8292682926829268,model,ablation-analysis
face_alignment,12,dense face alignment,dense face alignment,83,1,1,Dense Face Alignment, , ,0.3062730627306273,0.0112359550561797,0.0212765957446808,research-problem,baselines
face_alignment,12,experimental results,experimental setup,203,32,12,"To train the network , we use 20 , 10 , and 10 epochs for stage 1 to 3 .",Tab. 2 shows the datasets and .,"We set the initial global learning rate as 1 e ? 3 , and reduce the learning rate by a factor of 10 when the training error approaches a plateau .",0.7490774907749077,0.3478260869565217,0.5217391304347826,experimental-setup,experimental-setup
face_alignment,12,experimental results,experimental setup,204,33,13,"We set the initial global learning rate as 1 e ? 3 , and reduce the learning rate by a factor of 10 when the training error approaches a plateau .","To train the network , we use 20 , 10 , and 10 epochs for stage 1 to 3 .","The minibatch size is 32 , weight decay is 0.005 , and the leak factor for Leaky ReLU is 0.01 .",0.7527675276752768,0.3586956521739129,0.5652173913043478,experimental-setup,approach
face_alignment,12,experimental results,experimental setup,205,34,14,"The minibatch size is 32 , weight decay is 0.005 , and the leak factor for Leaky ReLU is 0.01 .","We set the initial global learning rate as 1 e ? 3 , and reduce the learning rate by a factor of 10 when the training error approaches a plateau .","In stage 2 , the regularization weights ?",0.7564575645756457,0.3695652173913043,0.6086956521739131,experimental-setup,baselines
face_alignment,12,experimental results,experiments on large pose datasets,218,47,4,"For AFLW - LFPA , our method outperforms the best methods with a large margin of 17.8 % improvement .","The results are presented in Tab. 3 , where the performance of the baseline methods is either reported from the published papers or by running the publicly available source code .","For AFLW2000 - 3D , our method also shows a large improvement .",0.8044280442804428,0.5108695652173914,0.4444444444444444,results,baselines
face_alignment,12,experimental results,experiments on large pose datasets,219,48,5,"For AFLW2000 - 3D , our method also shows a large improvement .","For AFLW - LFPA , our method outperforms the best methods with a large margin of 17.8 % improvement .","Specifically , for images with yaw angle in [ 60 , 90 ] , our method improves the performance by 28 % ( from 7.93 to 5.68 ) .",0.8081180811808119,0.5217391304347826,0.5555555555555556,results,baselines
face_alignment,12,experimental results,experiments on large pose datasets,220,49,6,"Specifically , for images with yaw angle in [ 60 , 90 ] , our method improves the performance by 28 % ( from 7.93 to 5.68 ) .","For AFLW2000 - 3D , our method also shows a large improvement .","For the IJB - A dataset , even though we are able to only compare the accuracy for the three labeled landmarks , our method still reaches a higher accuracy .",0.8118081180811808,0.532608695652174,0.6666666666666666,results,baselines
face_alignment,12,experimental results,experiments on large pose datasets,221,50,7,"For the IJB - A dataset , even though we are able to only compare the accuracy for the three labeled landmarks , our method still reaches a higher accuracy .","Specifically , for images with yaw angle in [ 60 , 90 ] , our method improves the performance by 28 % ( from 7.93 to 5.68 ) .","Note that the best performing baselines , 3DDFA and PAWF , share the similar overall approach in estimating m and p , and also aim for large - pose face alignment .",0.8154981549815498,0.5434782608695652,0.7777777777777778,results,baselines
face_alignment,12,experimental results,experiments on near frontal datasets,225,54,2,"Even though the proposed method can handle largepose alignment , to show its performance on the near- frontal datasets , we evaluate our method on the 300W dataset .", ,The result of the state - of - the - art method on the both common and challenging sets are shown in Tab .,0.8302583025830258,0.5869565217391305,0.2222222222222222,results,experimental-setup
face_alignment,12,experimental results,experiments on near frontal datasets,228,57,5,Our method is the second best method on the challenging set .,"4 . To find the corresponding landmarks on the cheek , we apply the landmark marching algorithm to move contour landmarks from self - occluded location to the silhouette .","In general , the performance of our method is comparable to other methods that are designed for near - frontal datasets , especially under the following consideration .",0.8413284132841329,0.6195652173913043,0.5555555555555556,results,baselines
face_alignment,12,experimental results,experiments on near frontal datasets,229,58,6,"In general , the performance of our method is comparable to other methods that are designed for near - frontal datasets , especially under the following consideration .",Our method is the second best method on the challenging set .,"That is , most prior face alignment methods do not employ shape constraints such as 3 DMM , which could bean advantage for near - frontal face alignment , but might be a disadvantage for large - pose face alignment .",0.8450184501845018,0.6304347826086957,0.6666666666666666,results,approach
face_alignment,12,experimental results,ablation study,238,67,6,The accuracy of our method on the AFLW2000 - 3D consistently improves by adding more datasets .,It shows the advantage and the ability of our method in leveraging more datasets .,"For the AFLW - PIFA dataset , our method achieves 9.5 % and 20 % relative improvement by utilizing the datasets in the stage 2 and stage 3 over the first stage , respectively .",0.8782287822878229,0.7282608695652174,0.1935483870967742,ablation-analysis,approach
face_alignment,12,experimental results,ablation study,239,68,7,"For the AFLW - PIFA dataset , our method achieves 9.5 % and 20 % relative improvement by utilizing the datasets in the stage 2 and stage 3 over the first stage , respectively .",The accuracy of our method on the AFLW2000 - 3D consistently improves by adding more datasets .,"If including the datasets from both the second and third stages , we can have 26 % relative improvement and achieve NME of 3.86 % .",0.8819188191881919,0.7391304347826086,0.2258064516129032,ablation-analysis,baselines
face_alignment,12,experimental results,ablation study,240,69,8,"If including the datasets from both the second and third stages , we can have 26 % relative improvement and achieve NME of 3.86 % .","For the AFLW - PIFA dataset , our method achieves 9.5 % and 20 % relative improvement by utilizing the datasets in the stage 2 and stage 3 over the first stage , respectively .",Comparing the second and third rows in Tab .,0.8856088560885609,0.75,0.2580645161290322,ablation-analysis,baselines
face_alignment,12,experimental results,ablation study,248,77,16,Comparing LFC + SPC and LFC + CFC performances shows that the CFC is more helpful than the SPC .,We show the results in the left of .,The reason is that CFC is more helpful in correcting the pose of the face and leads to more landmark error reduction .,0.915129151291513,0.8369565217391305,0.5161290322580645,ablation-analysis,model
face_alignment,12,experimental results,ablation study,250,79,18,Using all constraints achieves the best performance .,The reason is that CFC is more helpful in correcting the pose of the face and leads to more landmark error reduction .,"Finally , to evaluate the influence of using the SIFT pairing constraint ( SPC ) , we use all of the three stages datasets to train our method .",0.922509225092251,0.8586956521739131,0.5806451612903226,ablation-analysis,approach
face_alignment,12,experimental results,ablation study,254,83,22,This result shows that for the images with NME - lp between 5 % and 15 % the SPC is helpful .,"The right plot in illustrates the CED diagrams of NME - lp , for the trained models with and without the SIFT pairing constraint .",Part of the reason DeFA works well is that it receives .,0.9372693726937268,0.9021739130434784,0.7096774193548387,ablation-analysis,baselines
face_alignment,13,abstract,abstract,12,10,10,"We demonstrate the superior representation power of our nonlinear 3 DMM over its linear counterpart , and its contribution to face alignment and 3D reconstruction .",The entire network is end - to - end trainable with only weak supervision .,1,0.0404040404040404,0.9090909090909092,0.9090909090909092,research-problem,baselines
face_alignment,13,introduction,introduction,45,32,32,"Hence , we utilize two network decoders , instead of two PCA spaces , as the shape and texture model components , respectively .","As shown in , starting with an observation that the linear 3 DMM formulation is equivalent to a single layer network , using a deep network architecture naturally increases the model capacity .","With careful consideration of each component , we design different networks for shape and texture : the multi - layer perceptron ( MLP ) for shape and convolutional neural network ( CNN ) for texture .",0.1515151515151515,0.6666666666666666,0.6666666666666666,approach,approach
face_alignment,13,introduction,introduction,46,33,33,"With careful consideration of each component , we design different networks for shape and texture : the multi - layer perceptron ( MLP ) for shape and convolutional neural network ( CNN ) for texture .","Hence , we utilize two network decoders , instead of two PCA spaces , as the shape and texture model components , respectively .",Each decoder will take a shape or texture representation as input and output the dense 3 D face or a face texture .,0.1548821548821548,0.6875,0.6875,approach,experimental-setup
face_alignment,13,introduction,introduction,47,34,34,Each decoder will take a shape or texture representation as input and output the dense 3 D face or a face texture .,"With careful consideration of each component , we design different networks for shape and texture : the multi - layer perceptron ( MLP ) for shape and convolutional neural network ( CNN ) for texture .",These two decoders are essentially the nonlinear 3 DMM .,0.1582491582491582,0.7083333333333334,0.7083333333333334,approach,model
face_alignment,13,introduction,introduction,49,36,36,"Further , we learn the fitting algorithm to our nonlinear 3 DMM , which is formulated as a CNN encoder .",These two decoders are essentially the nonlinear 3 DMM .,"The encoder takes a 2 D face image as input and generates the shape and texture parameters , from which two decoders estimate the 3D face and texture .",0.1649831649831649,0.75,0.75,approach,approach
face_alignment,13,introduction,introduction,50,37,37,"The encoder takes a 2 D face image as input and generates the shape and texture parameters , from which two decoders estimate the 3D face and texture .","Further , we learn the fitting algorithm to our nonlinear 3 DMM , which is formulated as a CNN encoder .","The 3 D face and texture would perfectly reconstruct the input face , if the fitting algorithm and 3 DMM are well learnt .",0.1683501683501683,0.7708333333333334,0.7708333333333334,approach,experimental-setup
face_alignment,13,introduction,introduction,51,38,38,"The 3 D face and texture would perfectly reconstruct the input face , if the fitting algorithm and 3 DMM are well learnt .","The encoder takes a 2 D face image as input and generates the shape and texture parameters , from which two decoders estimate the 3D face and texture .","Therefore , we design a differentiable rendering layer to generate a reconstructed face by fusing the 3D face , texture , and the camera projection parameters estimated by the encoder .",0.1717171717171717,0.7916666666666666,0.7916666666666666,approach,model
face_alignment,13,introduction,introduction,52,39,39,"Therefore , we design a differentiable rendering layer to generate a reconstructed face by fusing the 3D face , texture , and the camera projection parameters estimated by the encoder .","The 3 D face and texture would perfectly reconstruct the input face , if the fitting algorithm and 3 DMM are well learnt .","Finally , the endto - end learning scheme is constructed where the encoder and two decoders are learnt jointly to minimize the difference between the reconstructed face and the input face .",0.1750841750841751,0.8125,0.8125,approach,baselines
face_alignment,13,introduction,introduction,53,40,40,"Finally , the endto - end learning scheme is constructed where the encoder and two decoders are learnt jointly to minimize the difference between the reconstructed face and the input face .","Therefore , we design a differentiable rendering layer to generate a reconstructed face by fusing the 3D face , texture , and the camera projection parameters estimated by the encoder .",Jointly learning the 3 DMM and the model fitting encoder allows us to leverage the large collection of unconstrained 2D images without relying on 3D scans .,0.1784511784511784,0.8333333333333334,0.8333333333333334,approach,model
face_alignment,13,introduction,introduction,54,41,41,Jointly learning the 3 DMM and the model fitting encoder allows us to leverage the large collection of unconstrained 2D images without relying on 3D scans .,"Finally , the endto - end learning scheme is constructed where the encoder and two decoders are learnt jointly to minimize the difference between the reconstructed face and the input face .",We show significantly improved shape and texture representation power over the linear 3 DMM .,0.1818181818181818,0.8541666666666666,0.8541666666666666,approach,baselines
face_alignment,13,introduction,introduction,55,42,42,We show significantly improved shape and texture representation power over the linear 3 DMM .,Jointly learning the 3 DMM and the model fitting encoder allows us to leverage the large collection of unconstrained 2D images without relying on 3D scans .,"Consequently , this also benefits other tasks such as 2 D face alignment and 3D reconstruction .",0.1851851851851852,0.875,0.875,approach,baselines
face_alignment,13,experimental results,experimental results,208,4,4,"The model is optimized using Adam optimizer with an initial learning rate of 0.001 when minimizing L 0 , and 0.0002 when minimizing L.","Using facial mesh triangle definition by Basel Face Model ( BFM ) , we train our 3 DMM using 300W - LP dataset .","We set the following parameters : Q = 53 , 215 , U = V = 128 , l S = l T = 160 . ? values are set to make losses to have similar magnitudes .",0.7003367003367004,0.0481927710843373,0.8,hyperparameters,baselines
face_alignment,13,experimental results,experimental results,209,5,5,"We set the following parameters : Q = 53 , 215 , U = V = 128 , l S = l T = 160 . ? values are set to make losses to have similar magnitudes .","The model is optimized using Adam optimizer with an initial learning rate of 0.001 when minimizing L 0 , and 0.0002 when minimizing L.", ,0.7037037037037037,0.0602409638554216,1.0,hyperparameters,approach
face_alignment,13,experimental results,representation power,226,22,1,Representation Power, , ,0.7609427609427609,0.2650602409638554,0.0416666666666666,results,approach
face_alignment,13,experimental results,representation power,230,26,5,"Alternatively , we can minimize the reconstruction error in the image space , through the rendering layer with the groundtruth S and m .","With the groundtruth texture , by using gradient descent , we can estimate a texture parameter f T whose decoded texture matches with the groundtruth .","Empirically , the two methods give similar performances but we choose the first option as it involves only one warping step , instead of rendering in every optimization iteration .",0.7744107744107744,0.3132530120481928,0.2083333333333333,results,model
face_alignment,13,experimental results,representation power,233,29,8,"As in , our nonlinear texture is closer to the groundtruth than the linear model , especially for in - the - wild images ( the first two rows ) .","For the linear model , we use the fitting results of Basel texture and Phong illumination model given by .",This is expected since the linear model is trained with controlled images .,0.7845117845117845,0.3493975903614458,0.3333333333333333,results,experimental-setup
face_alignment,13,experimental results,representation power,235,31,10,"Quantitatively , our nonlinear model has significantly lower L 1 reconstruction error than the lin - We also compare the power of nonlinear and linear 3 DMM in representing real - world 3D scans .",This is expected since the linear model is trained with controlled images .,"We compare with BFM , the most commonly used 3 DMM at present .",0.7912457912457912,0.3734939759036144,0.4166666666666667,results,experimental-setup
face_alignment,13,experimental results,representation power,247,43,22,"Our nonlinear model has a significantly smaller reconstruction error than the linear model , 0.0196 vs. 0.0241 ( Tab. 3 ) .","To quantify the difference , we use NME , averaged pervertex errors between the recovered and groundtruth shapes , normalized by inter-ocular distances .","Also , the non-linear models are more compact .",0.8316498316498316,0.5180722891566265,0.9166666666666666,results,experimental-setup
face_alignment,13,experimental results,applications,264,60,15,Quantitative evaluation of 3D reconstruction .,We achieve comparable visual quality in 3D reconstruction . :,We obtain a low error that is comparable to optimization - based methods .,0.8888888888888888,0.7228915662650602,0.4838709677419355,results,approach
face_alignment,13,experimental results,applications,265,61,16,We obtain a low error that is comparable to optimization - based methods .,Quantitative evaluation of 3D reconstruction .,"of CNNs that iteratively refines its estimation in multiple steps , meanwhile ours is a single - pass of E and D S .",0.8922558922558923,0.7349397590361446,0.5161290322580645,results,approach
face_alignment,13,experimental results,applications,271,67,22,3D Face Reconstruction .,Again we observe the higher error from the linear model - based fitting .,We compare our approach to recent works : the CNN - based iterative supervised regressor of Richardson et al. and unsupervised regressor method of Tewari et al ..,0.9124579124579124,0.8072289156626506,0.7096774193548387,results,approach
face_alignment,13,experimental results,applications,280,76,31,"We achieve on - par results with Garrido et al. , an offline optimization method , while surpassing all other regression methods ] .","Following the same setting in , we also quantitatively compare our method with prior works on 9 subjects of FaceWarehouse database ) .", ,0.9427609427609428,0.9156626506024096,1.0,results,baselines
face_alignment,13,experimental results,ablation on texture learning,284,80,4,Using a global image - based discriminator is redundant as the global structure is guaranteed by the rendering layer .,The rendering layer opens a possibility to apply adversarial loss in addition to global L 1 loss .,"Also , we empirically find that using global image - based discriminator can cause severe artifacts in the resultant texture .",0.9562289562289562,0.9638554216867472,0.5714285714285714,ablation-analysis,experimental-setup
face_alignment,13,experimental results,ablation on texture learning,285,81,5,"Also , we empirically find that using global image - based discriminator can cause severe artifacts in the resultant texture .",Using a global image - based discriminator is redundant as the global structure is guaranteed by the rendering layer .,visualizes outputs of our network with different options of adversarial loss .,0.9595959595959596,0.9759036144578314,0.7142857142857143,ablation-analysis,experimental-setup
face_alignment,13,experimental results,ablation on texture learning,287,83,7,"Clearly , patchGAN offers higher realism and fewer artifacts .",visualizes outputs of our network with different options of adversarial loss ., ,0.9663299663299664,1.0,1.0,ablation-analysis,approach
face_alignment,14,title,title,2,2,2,Faster Than Real - time Facial Alignment : A 3D Spatial Transformer Network Approach in Unconstrained Poses, , ,0.0075471698113207,1.0,1.0,research-problem,approach
face_alignment,14,introduction,introduction,14,2,2,Robust face recognition and analysis are contingent upon accurate localization of facial features ., ,"When modeling faces , the landmark points of interest consist of points that lie along the shape boundaries of facial features , e.g. eyes , lips , mouth , etc .",0.0528301886792452,0.0714285714285714,0.0833333333333333,research-problem,ablation-analysis
face_alignment,14,introduction,introduction,33,21,21,"In our method , we follow this idea and observe that fairly accurate 3 D models can be generated by using a simple mean shape deformed to the input image at a relatively low computational cost compared to other approaches .",claim that the depth information of a face is not extremely discriminative when factoring out the 2D spatial location of facial features .,"We take the approach of using a simple mean shape and using a parametric , non-linear warping of that shape through alignment on the image to be able to model any unseen example .",0.1245283018867924,0.75,0.875,model,approach
face_alignment,14,experiments,implementation details,203,23,2,Our network is implemented in the Caffe framework ., ,"A new layer is created consisting of the 3D TPS transformation module , the camera projection module and the bilinear sampler module .",0.7660377358490567,0.2875,0.1818181818181818,experimental-setup,approach
face_alignment,14,experiments,implementation details,204,24,3,"A new layer is created consisting of the 3D TPS transformation module , the camera projection module and the bilinear sampler module .",Our network is implemented in the Caffe framework .,All modules are differentiable so that the whole network can be trained end - to - end .,0.769811320754717,0.3,0.2727272727272727,experimental-setup,model
face_alignment,14,experiments,implementation details,206,26,5,"We adopt two architectures , AlexNet and VGG - 16 , as the pre-trained models for our shared feature extraction networks in , i.e. we use the convolution layers from the pre-trained models to initialize ours .",All modules are differentiable so that the whole network can be trained end - to - end .,"Since these networks already extract informative low - level features and we do not want to lose this information , we freeze some of the earlier convolution layers and finetune the rest .",0.7773584905660378,0.325,0.4545454545454545,experimental-setup,experimental-setup
face_alignment,14,experiments,implementation details,208,28,7,"For the AlexNet architecture , we freeze the first layer while for the VGG - 16 architecture , the first 4 layers are frozen .","Since these networks already extract informative low - level features and we do not want to lose this information , we freeze some of the earlier convolution layers and finetune the rest .",The 2D landmark regression is implemented by attaching additional layers on top of the last convolution layer .,0.7849056603773585,0.35,0.6363636363636364,experimental-setup,experimental-setup
face_alignment,14,experiments,implementation details,209,29,8,The 2D landmark regression is implemented by attaching additional layers on top of the last convolution layer .,"For the AlexNet architecture , we freeze the first layer while for the VGG - 16 architecture , the first 4 layers are frozen .","With N landmarks to regress , we need NFC layers to compute the offsets for each individual landmark .",0.7886792452830189,0.3625,0.7272727272727273,experimental-setup,experimental-setup
face_alignment,15,title,title,2,2,2,Face Alignment Across Large Poses : A 3D Solution, , ,0.0069930069930069,1.0,1.0,research-problem,experimental-setup
face_alignment,15,abstract,abstract,4,2,2,"Face alignment , which fits a face model to an image and extracts the semantic meanings of facial pixels , has been an important topic in CV community .", ,"However , most algorithms are designed for faces in small to medium poses ( below 45 ) , lacking the ability to align faces in large poses up to 90 .",0.0139860139860139,0.2,0.2,research-problem,experimental-setup
face_alignment,15,introduction,introduction,45,33,33,"poses , we propose to fit the 3D dense face model rather than the sparse landmark shape model to the image .",To address the problem of invisible landmarks in large,"By incorporating 3D information , the appearance variations and self - occlusion caused by 3D transformations can be inherently addressed .",0.1573426573426573,0.7021276595744681,0.7021276595744681,model,ablation-analysis
face_alignment,15,introduction,introduction,47,35,35,We call this method 3D Dense Face Alignment ( 3DDFA ) .,"By incorporating 3D information , the appearance variations and self - occlusion caused by 3D transformations can be inherently addressed .",Some results are shown in .,0.1643356643356643,0.7446808510638298,0.7446808510638298,model,experimental-setup
face_alignment,15,introduction,introduction,50,38,38,"To resolve the fitting process in 3 DDFA , we propose a cascaded convolutional neutral network ( CNN ) based regression method .",2 .,CNN has been proved of excellent capability to extract useful information from images with large variations in object detection and image classification .,0.1748251748251748,0.8085106382978723,0.8085106382978723,model,ablation-analysis
face_alignment,15,introduction,introduction,52,40,40,"In this work , we adopt CNN to fit the 3D face model with a specifically designed feature , namely Projected Normalized Coordinate Code ( PNCC ) .",CNN has been proved of excellent capability to extract useful information from images with large variations in object detection and image classification .,"Besides , Weighted Parameter Distance Cost ( WPDC ) is proposed as the cost function .",0.1818181818181818,0.851063829787234,0.851063829787234,model,baselines
face_alignment,15,introduction,introduction,53,41,41,"Besides , Weighted Parameter Distance Cost ( WPDC ) is proposed as the cost function .","In this work , we adopt CNN to fit the 3D face model with a specifically designed feature , namely Projected Normalized Coordinate Code ( PNCC ) .","To the best of our knowledge , this is the first attempt to solve the 3D face alignment with CNN .",0.1853146853146853,0.8723404255319149,0.8723404255319149,model,model
face_alignment,15,introduction,introduction,54,42,42,"To the best of our knowledge , this is the first attempt to solve the 3D face alignment with CNN .","Besides , Weighted Parameter Distance Cost ( WPDC ) is proposed as the cost function .",3 .,0.1888111888111888,0.8936170212765957,0.8936170212765957,model,approach
face_alignment,15,introduction,introduction,56,44,44,"To enable the training of the 3DDFA , we construct a face database containing pairs of 2D face images and 3D face models .",3 .,We further propose a face profiling algorithm to synthesize 60 k + training samples across large poses .,0.1958041958041958,0.9361702127659576,0.9361702127659576,model,experimental-setup
face_alignment,15,introduction,introduction,57,45,45,We further propose a face profiling algorithm to synthesize 60 k + training samples across large poses .,"To enable the training of the 3DDFA , we construct a face database containing pairs of 2D face images and 3D face models .",The synthesized samples well simulate the face appearances in large poses and boost the performance of both prior and our proposed face alignment algorithms .,0.1993006993006993,0.9574468085106383,0.9574468085106383,model,approach
face_alignment,15,introduction,introduction,58,46,46,The synthesized samples well simulate the face appearances in large poses and boost the performance of both prior and our proposed face alignment algorithms .,We further propose a face profiling algorithm to synthesize 60 k + training samples across large poses .,"The database , face profiling code and 3 DDFA code are released at http://www.cbsr.ia.ac.cn/users / xiangyuzhu/.",0.2027972027972028,0.9787234042553192,0.9787234042553192,model,baselines
face_alignment,15,introduction,introduction,59,47,47,"The database , face profiling code and 3 DDFA code are released at http://www.cbsr.ia.ac.cn/users / xiangyuzhu/.",The synthesized samples well simulate the face appearances in large poses and boost the performance of both prior and our proposed face alignment algorithms ., ,0.2062937062937063,1.0,1.0,code,experimental-setup
face_alignment,15,experiments,performance analysis,221,23,2,Error Reduction in Cascade :, ,To analyze the error reduction process in cascade and evaluate the effect of initialization regeneration .,0.7727272727272727,0.2804878048780488,0.1111111111111111,experiments,approach
face_alignment,15,experiments,performance analysis,225,27,6,"As observed , the testing error is reduced due to initialization regeneration .","shows the training and testing errors at each iteration , with and without initialization regeneration .",In the generic cascade process the training and testing errors converge fast after 2 iterations .,0.7867132867132867,0.3292682926829268,0.3333333333333333,experiments,approach
face_alignment,15,experiments,performance analysis,226,28,7,In the generic cascade process the training and testing errors converge fast after 2 iterations .,"As observed , the testing error is reduced due to initialization regeneration .","While with initialization regeneration , the training error is updated at the beginning of each iteration and the testing error continues to descend .",0.7902097902097902,0.3414634146341464,0.3888888888888889,experiments,approach
face_alignment,15,experiments,performance analysis,227,29,8,"While with initialization regeneration , the training error is updated at the beginning of each iteration and the testing error continues to descend .",In the generic cascade process the training and testing errors converge fast after 2 iterations .,"During testing , 3 DDFA takes 25.24ms for each iteration , 17.49 ms for PNCC construction on 3.40 GHZ CPU and 7.75 ms for CNN on GTX TITAN Black GPU .",0.7937062937062938,0.3536585365853658,0.4444444444444444,experiments,model
face_alignment,15,experiments,performance analysis,231,33,12,Performance with Different Costs :,Considering both effectiveness and efficiency we choose 3 iterations in 3DDFA .,"In this experiment , we demonstrate the performance with different costs including PDC , VDC and WPDC .",0.8076923076923077,0.4024390243902439,0.6666666666666666,experiments,experimental-setup
face_alignment,15,experiments,performance analysis,235,37,16,It is shown that PDC can not well model the fitting error and converges to an unsatisfied result .,All the networks are trained until convergence .,"VDC is better than PDC , but the pathological curvature problem makes it only concentrate on a small set of parameters , which limits its performance .",0.8216783216783217,0.451219512195122,0.8888888888888888,experiments,approach
face_alignment,15,experiments,performance analysis,237,39,18,"WPDC explicitly models the priority of each parameter and adaptively optimizes them with the parameter weights , leading to the best result .","VDC is better than PDC , but the pathological curvature problem makes it only concentrate on a small set of parameters , which limits its performance .", ,0.8286713286713286,0.475609756097561,1.0,experiments,baselines
face_alignment,15,experiments,comparison experiments,240,42,3,Large Pose Face Alignment in AFLW Protocol :,"In this paper , we test the performance of 3DDFA on three different tasks , including the large - pose face alignment on AFLW , 3 D face alignment on AFLW2000 - 3D and mediumpose face alignment on 300W .","In this experiment , we regard 300 W and 300W - LP as the training set respectively and the whole AFLW as the testing set .",0.8391608391608392,0.5121951219512195,0.0697674418604651,experiments,approach
face_alignment,15,experiments,comparison experiments,258,60,21,"Firstly , the results indicate that all the methods benefits substantially from face profiling when dealing with large poses .",Results :,"The improvements in [ 60 , 90 ] are 44.06 % for RCPR , 40.36 % for ESR and 42.10 % for SDM .",0.902097902097902,0.7317073170731707,0.4883720930232558,experiments,approach
face_alignment,15,experiments,comparison experiments,262,64,25,"Secondly , 3DDFA reaches the state of the art above all the 2D methods especially beyond medium poses .",Thus the fidelity of the face profiling method can be well demonstrated .,The minimum standard deviation of 3DDFA also demonstrates its robustness to pose variations .,0.9160839160839158,0.7804878048780488,0.5813953488372093,experiments,approach
face_alignment,15,experiments,comparison experiments,263,65,26,The minimum standard deviation of 3DDFA also demonstrates its robustness to pose variations .,"Secondly , 3DDFA reaches the state of the art above all the 2D methods especially beyond medium poses .","Finally , the performance of 3DDFA can be further improved with the SDM landmark refinement in Section 5.2 .",0.9195804195804196,0.7926829268292683,0.6046511627906976,experiments,approach
face_alignment,15,experiments,comparison experiments,264,66,27,"Finally , the performance of 3DDFA can be further improved with the SDM landmark refinement in Section 5.2 .",The minimum standard deviation of 3DDFA also demonstrates its robustness to pose variations .,3D Face Alignment in AFLW2000-3D,0.9230769230769232,0.8048780487804879,0.627906976744186,experiments,approach
face_alignment,15,experiments,comparison experiments,265,67,28,3D Face Alignment in AFLW2000-3D,"Finally , the performance of 3DDFA can be further improved with the SDM landmark refinement in Section 5.2 .","As described in Section 6.1 , 3 D face alignment evaluation can be degraded to all - landmark evaluation considering both visible and invisible ones .",0.9265734265734266,0.8170731707317073,0.6511627906976745,experiments,experimental-setup
face_alignment,15,experiments,comparison experiments,270,72,33,"For all the methods , despite with ground truth bounding boxes the performance in [ 60 , 90 ] and the standard deviation are obviously reduced when considering all the landmarks .","Compared with the results in AFLW , we can seethe defect of barely evaluating visible landmarks .","We think for 3 D face alignment which depends on both visible and invisible landmarks , evaluating all the landmarks are necessary .",0.944055944055944,0.8780487804878049,0.7674418604651163,experiments,approach
face_alignment,15,experiments,comparison experiments,279,81,42,Common Challenging Full TSPM,Method,"8 that even as a generic face alignment algorithm , 3 DDFA still demonstrates competitive performance on the common set and state - of - the - art performance on the challenging set .",0.9755244755244756,0.9878048780487804,0.9767441860465116,experiments,baselines
face_alignment,15,experiments,comparison experiments,280,82,43,"8 that even as a generic face alignment algorithm , 3 DDFA still demonstrates competitive performance on the common set and state - of - the - art performance on the challenging set .",Common Challenging Full TSPM, ,0.9790209790209792,1.0,1.0,experiments,baselines
face_alignment,16,title,title,2,2,2,Deep Multi- Center Learning for Face Alignment, , ,0.0060790273556231,1.0,1.0,research-problem,approach
face_alignment,16,abstract,abstract,11,9,9,The code for our method is available at https://github.com/ZhiwenShao/MCNet-Extension .,Extensive experiments demonstrate that our method is effective for handling complex occlusions and appearance variations with real - time performance .,"Index Terms - Multi- Center Learning , Model Assembling , Face Alignment * Corresponding author .",0.033434650455927,0.1914893617021277,0.1914893617021277,code,baselines
face_alignment,16,abstract,abstract,35,33,33,"In this work 1 , we propose a novel deep learning framework named Multi - Center Learning ( MCL ) to exploit the strong correlations among landmarks .","Motivated by this fact , facial landmarks are divided into several clusters based on their semantic relevance .","In particular , our network uses multiple shape prediction layers to predict the locations of landmarks , and each shape prediction layer emphasizes on the detection of a certain cluster of landmarks respectively .",0.1063829787234042,0.7021276595744681,0.7021276595744681,model,ablation-analysis
face_alignment,16,abstract,abstract,36,34,34,"In particular , our network uses multiple shape prediction layers to predict the locations of landmarks , and each shape prediction layer emphasizes on the detection of a certain cluster of landmarks respectively .","In this work 1 , we propose a novel deep learning framework named Multi - Center Learning ( MCL ) to exploit the strong correlations among landmarks .","By weighting the loss of each landmark , challenging landmarks are focused firstly , and each cluster of landmarks is further optimized respectively .",0.1094224924012158,0.7234042553191491,0.7234042553191491,model,approach
face_alignment,16,abstract,abstract,37,35,35,"By weighting the loss of each landmark , challenging landmarks are focused firstly , and each cluster of landmarks is further optimized respectively .","In particular , our network uses multiple shape prediction layers to predict the locations of landmarks , and each shape prediction layer emphasizes on the detection of a certain cluster of landmarks respectively .","Moreover , to decrease the model complexity , we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer .",0.1124620060790273,0.7446808510638298,0.7446808510638298,model,approach
face_alignment,16,abstract,abstract,38,36,36,"Moreover , to decrease the model complexity , we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer .","By weighting the loss of each landmark , challenging landmarks are focused firstly , and each cluster of landmarks is further optimized respectively .",The entire framework reinforces the learning process of each landmark with a low model complexity .,0.1155015197568389,0.7659574468085106,0.7659574468085106,model,baselines
face_alignment,16,abstract,abstract,39,37,37,The entire framework reinforces the learning process of each landmark with a low model complexity .,"Moreover , to decrease the model complexity , we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer .",The main contributions of this study can be summarized as follows :,0.1185410334346504,0.7872340425531915,0.7872340425531915,model,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,210,108,108,"Uniform scaling and translation with different extents on face bounding boxes are further conducted , in which each newly generated face bounding box is used to crop the face .","In particular , for each training face , we firstly perform multiple rotations , and attain a tight face bounding box covering the ground truth locations of landmarks of each rotated result respectively .",Finally training samples are augmented through horizontal flip and JPEG compression .,0.6382978723404256,0.4954128440366973,0.4954128440366973,experimental-setup,experimental-setup
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,211,109,109,Finally training samples are augmented through horizontal flip and JPEG compression .,"Uniform scaling and translation with different extents on face bounding boxes are further conducted , in which each newly generated face bounding box is used to crop the face .",It is beneficial for avoiding overfitting and improving the robustness of learned models by covering various patterns .,0.6413373860182371,0.5,0.5,experimental-setup,experimental-setup
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,213,111,111,We train our MCL using an open source deep learning framework Caffe .,It is beneficial for avoiding overfitting and improving the robustness of learned models by covering various patterns .,"The input face patch is a 50 50 grayscale image , and each pixel value is normalized to [ ?1 , 1 ) by subtracting 128 and multiplying 0.0078125 .",0.6474164133738601,0.5091743119266054,0.5091743119266054,experimental-setup,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,214,112,112,"The input face patch is a 50 50 grayscale image , and each pixel value is normalized to [ ?1 , 1 ) by subtracting 128 and multiplying 0.0078125 .",We train our MCL using an open source deep learning framework Caffe .,"A more complex model is needed for a labeling pattern with more facial landmarks , so Dis set to be 512/512/1 , 024 for 5/29/68 facial landmarks .",0.6504559270516718,0.5137614678899083,0.5137614678899083,experimental-setup,experimental-setup
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,215,113,113,"A more complex model is needed for a labeling pattern with more facial landmarks , so Dis set to be 512/512/1 , 024 for 5/29/68 facial landmarks .","The input face patch is a 50 50 grayscale image , and each pixel value is normalized to [ ?1 , 1 ) by subtracting 128 and multiplying 0.0078125 .","The type of solver is SGD with a mini-batch size of 64 , a momentum of 0.9 , and a weight decay of 0.0005 .",0.6534954407294833,0.518348623853211,0.518348623853211,experimental-setup,experimental-setup
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,216,114,114,"The type of solver is SGD with a mini-batch size of 64 , a momentum of 0.9 , and a weight decay of 0.0005 .","A more complex model is needed for a labeling pattern with more facial landmarks , so Dis set to be 512/512/1 , 024 for 5/29/68 facial landmarks .","The maximum learning iterations of pre-training and each finetuning step are 1810 4 and 610 4 respectively , and the initial learning rates of pre-training and each fine - tuning step are 0.02 and 0.001 respectively .",0.6565349544072948,0.5229357798165137,0.5229357798165137,experimental-setup,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,217,115,115,"The maximum learning iterations of pre-training and each finetuning step are 1810 4 and 610 4 respectively , and the initial learning rates of pre-training and each fine - tuning step are 0.02 and 0.001 respectively .","The type of solver is SGD with a mini-batch size of 64 , a momentum of 0.9 , and a weight decay of 0.0005 .",Note that the initial learning rate of fine - tuning should below to preserve some representational structures learned in the pre-training stage and avoid missing good intermediate solutions .,0.6595744680851063,0.5275229357798165,0.5275229357798165,experimental-setup,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,219,117,117,"The learning rate is multiplied by a factor of 0.3 at every 3 10 4 iterations , and the remaining parameter ?",Note that the initial learning rate of fine - tuning should below to preserve some representational structures learned in the pre-training stage and avoid missing good intermediate solutions .,is set to be 125 .,0.6656534954407295,0.536697247706422,0.536697247706422,experimental-setup,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,230,128,128,"We compare our work MCL against state - of - the - art methods including ESR , SDM , Cascaded CNN , RCPR , CFAN , LBF , c GPRT , CFSS , TCDCN , , ALR , CFT , RFLD , RecNet , RAR , and FLD + PDE .",B. Comparison with State - of - the - Art Methods,All the methods are evaluated on testing images using the face bounding boxes provided by benchmarks .,0.6990881458966566,0.5871559633027523,0.5871559633027523,baselines,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,237,135,135,"Our method MCL outperforms most of the state - of - the - art methods , especially on AFLW dataset where a relative error reduction of 3.93 % is achieved compared to RecNet .",reports the results of our method and previous works on three benchmarks .,Cascaded CNN estimates the location of each The result is acquired by running the code at https://github.com/seetaface/SeetaFaceEngine/tree/master/FaceAlignment .,0.7203647416413373,0.6192660550458715,0.6192660550458715,results,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,239,137,137,We compare with other methods on several challenging images from AFLW and COFW respectively in .,Cascaded CNN estimates the location of each The result is acquired by running the code at https://github.com/seetaface/SeetaFaceEngine/tree/master/FaceAlignment .,Our method MCL indicates higher accuracy in the details than previous works .,0.7264437689969605,0.6284403669724771,0.6284403669724771,results,experiments
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,242,140,140,"MCL demonstrates a superior capability of handling severe occlusions and complex variations of pose , expression , illumination .",More examples on challenging IBUG are presented in .,The CED curves of MCL and several state - of - the - art methods are shown in .,0.7355623100303952,0.6422018348623854,0.6422018348623854,results,model
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,244,142,142,It is observed that MCL achieves competitive performance on all three benchmarks .,The CED curves of MCL and several state - of - the - art methods are shown in .,The average running speed of deep learning methods for detecting 68 facial landmarks are presented in .,0.7416413373860182,0.6513761467889908,0.6513761467889908,results,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,245,143,143,The average running speed of deep learning methods for detecting 68 facial landmarks are presented in .,It is observed that MCL achieves competitive performance on all three benchmarks .,Except for the methods tested on the i5-6200U,0.7446808510638298,0.6559633027522935,0.6559633027522935,results,experimental-setup
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,256,154,154,1 ) Global Average Pooling vs. Full Connection :,C. Ablation Study,"Based on the previous version of our work , the last maxpooling layer and the D-dimensional fully - connected layer are replaced with a convolutional layer and a Global Average Pooling layer .",0.7781155015197568,0.7064220183486238,0.7064220183486238,ablation-analysis,experimental-setup
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,259,157,157,It can be seen that BM performs better on IBUG and COFW but worse on AFLW than pre-BM .,The results of the mean error of BM and the previous version ( pre - BM ) are shown in .,It demonstrates that Global Average Pooling is more advantageous for more complex problems with more facial landmarks .,0.7872340425531915,0.7201834862385321,0.7201834862385321,ablation-analysis,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,260,158,158,It demonstrates that Global Average Pooling is more advantageous for more complex problems with more facial landmarks .,It can be seen that BM performs better on IBUG and COFW but worse on AFLW than pre-BM .,There are higher requirements for learned features when localizing more facial landmarks .,0.7902735562310029,0.7247706422018348,0.7247706422018348,ablation-analysis,approach
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,266,164,164,2 ) Robustness of Weighting :,"Therefore , BM has a stronger feature learning ability with fewer parameters than pre -BM .","To verify the robustness of weighting , random perturbations are added to the weights of landmarks .",0.8085106382978723,0.7522935779816514,0.7522935779816514,ablation-analysis,approach
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,273,171,171,"When ? is 0.4 , WM can still achieves good performance .",shows the variations of mean error of WM with the increase of ?.,"Therefore , weighting the loss of each landmark is robust to random perturbations .",0.8297872340425532,0.7844036697247706,0.7844036697247706,ablation-analysis,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,276,174,174,3 ) Analysis of Shape Prediction Layers :,"Even if different weights are obtained , the results will not be affected as long as the relative sizes of weights are identical .",Our method learns each shape prediction layer respectively with a certain cluster of landmarks being emphasized .,0.8389057750759878,0.7981651376146789,0.7981651376146789,ablation-analysis,approach
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,279,177,177,"Compared to WM , the left eye model and the right eye model both reduce the alignment errors of their corresponding clusters .",The results of WM and two shape prediction layers with respect to the left eye and the right eye on IBUG benchmark are shown in .,"As a result , the assembled AM can improve the detection accuracy of landmarks of the left eye and the right eye on the basis of WM .",0.8480243161094225,0.8119266055045872,0.8119266055045872,ablation-analysis,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,280,178,178,"As a result , the assembled AM can improve the detection accuracy of landmarks of the left eye and the right eye on the basis of WM .","Compared to WM , the left eye model and the right eye model both reduce the alignment errors of their corresponding clusters .",Note that the two models also improve the localization precision of other clusters .,0.851063829787234,0.8165137614678899,0.8165137614678899,ablation-analysis,approach
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,281,179,179,Note that the two models also improve the localization precision of other clusters .,"As a result , the assembled AM can improve the detection accuracy of landmarks of the left eye and the right eye on the basis of WM .","Taking the left eye model as an example , it additionally reduces the errors of landmarks of right eye , mouth , and chin , which is due to the correlations among different facial parts .",0.8541033434650456,0.8211009174311926,0.8211009174311926,ablation-analysis,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,285,183,183,4 ) Integration of Weighting Fine - Tuning and Multi - Center Fine - Tuning :,It can be concluded that each shape prediction layer emphasizes on the corresponding cluster respectively .,Here we validate the effectiveness of weighting fine - tuning by removing the weighting fine - tuning stage to learn a Simplified AM from BM . presents the results of mean error of Simplified AM and AM respectively on COFW and IBUG .,0.8662613981762918,0.8394495412844036,0.8394495412844036,ablation-analysis,approach
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,288,186,186,"The accuracy of AM is superior to that of Simplified AM especially on challenging IBUG , which is attributed to the integration of two stages .","Note that Simplified AM has already acquired good results , which verifies the effectiveness of the multicenter fine - tuning stage .","A Weighting Simplified AM from Simplified AM using the weighting finetuning stage is also learned , whose results are shown in .",0.8753799392097265,0.8532110091743119,0.8532110091743119,ablation-analysis,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,290,188,188,It can be seen that Weighting Simplified AM improves slightly on COFW but fails to search a better solution on IBUG .,"A Weighting Simplified AM from Simplified AM using the weighting finetuning stage is also learned , whose results are shown in .","Therefore , we choose to use the multi-center finetuning stage after the weighting fine - tuning stage .",0.8814589665653495,0.8623853211009175,0.8623853211009175,ablation-analysis,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,293,191,191,It can be observed that AM has higher accuracy and stronger robustness than BM and WM .,"summarizes the results of mean error and failure rate of BM , WM , and AM .",depicts the enhancement from WM to AM for several examples of COFW .,0.8905775075987842,0.8761467889908257,0.8761467889908257,ablation-analysis,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,295,193,193,The localization accuracy of facial landmarks from each cluster is improved in the details .,depicts the enhancement from WM to AM for several examples of COFW .,It is because each shape prediction layer increases the detection precision of corresponding cluster respectively .,0.8966565349544073,0.8853211009174312,0.8853211009174312,ablation-analysis,experimental-setup
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,297,195,195,D. MCL for Partially Occluded Faces,It is because each shape prediction layer increases the detection precision of corresponding cluster respectively .,The correlations among different facial parts are very useful for face alignment especially for partially occluded faces .,0.9027355623100304,0.8944954128440367,0.8944954128440367,ablation-analysis,experimental-setup
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,298,196,196,The correlations among different facial parts are very useful for face alignment especially for partially occluded faces .,D. MCL for Partially Occluded Faces,"To investigate the influence of occlusions , we directly use trained WM and AM without any additional processing for partially occluded faces .",0.905775075987842,0.8990825688073395,0.8990825688073395,ablation-analysis,approach
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,303,201,201,"After processing testing faces with occlusions , the mean error results of both WM and AM increase .","Note that our method does not process occlusions explicitly , in which the training data is not performed handcrafted occlusions .","Besides the results of landmarks from the left eye cluster , the results of remaining landmarks from other clusters become worse slightly .",0.9209726443768996,0.9220183486238532,0.9220183486238532,ablation-analysis,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,304,202,202,"Besides the results of landmarks from the left eye cluster , the results of remaining landmarks from other clusters become worse slightly .","After processing testing faces with occlusions , the mean error results of both WM and AM increase .",This is because different facial parts have correlations and the occlusions of the left eye influences results of other facial parts .,0.9240121580547112,0.9266055045871558,0.9266055045871558,ablation-analysis,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,306,204,204,"Note that WM and AM still perform well on occluded left eyes with the mean error of 6.60 and 6.50 respectively , due to the following reasons .",This is because different facial parts have correlations and the occlusions of the left eye influences results of other facial parts .,"First , WM weights each landmark proportional to its alignment error , which exploits correlations among landmarks .",0.9300911854103344,0.9357798165137616,0.9357798165137616,ablation-analysis,baselines
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,310,208,208,E. Weighting Fine - Tuning for State - of - the - Art Frameworks,"Q i in Eq. 9 for remaining landmarks , respectively , where correlations among landmarks are further exploited .","Most recently , there area few well - designed and welltrained deep learning frameworks advancing the performance of face alignment , in which DAN is atypical work .",0.9422492401215804,0.9541284403669724,0.9541284403669724,ablation-analysis,experimental-setup
face_alignment,16,multi center learning for face alignment,multi center learning for face alignment,317,215,215,It can be seen that the mean error of re -DAN is reduced from 7.97 to 7.81 after using our proposed weighting fine - tuning .,"For a fair comparison , the results of mean error of DAN , re - DAN , and DAN - WM on IBUG benchmark are all shown in .","Note that our method uses only a single neural network , which has a concise structure with low model complexity .",0.9635258358662614,0.9862385321100916,0.9862385321100916,ablation-analysis,baselines
face_alignment,17,abstract,abstract,5,2,2,"Facial landmark detection , or face alignment , is a fundamental task that has been extensively studied .", ,"In this paper , we investigate a new perspective of facial landmark detection and demonstrate it leads to further notable improvement .",0.017921146953405,0.25,0.25,research-problem,experimental-setup
face_alignment,17,abstract,abstract,11,8,8,The code is made publicly available at https://github.com/thesouthfrog/stylealign.,Our proposed structure is general and could be assembled into any face alignment frameworks ., ,0.039426523297491,1.0,1.0,code,baselines
face_alignment,17,introduction,introduction,30,19,19,"To this end , we propose a new framework to augment training for facial landmark detection without using extra knowledge .",It is a natural conjecture that face alignment could be more robust if we augment images only regarding their styles .,"Instead of directly generating images , we first map face images into the space of structure and style .",0.1075268817204301,0.5277777777777778,0.5277777777777778,model,experimental-setup
face_alignment,17,introduction,introduction,31,20,20,"Instead of directly generating images , we first map face images into the space of structure and style .","To this end , we propose a new framework to augment training for facial landmark detection without using extra knowledge .","To guarantee the disentanglement of these two spaces , we design a conditional variational auto - encoder model , in which Kullback - Leiber ( KL ) divergence loss and skip connections are incorporated for compact representation of style and structure respectively .",0.1111111111111111,0.5555555555555556,0.5555555555555556,model,experimental-setup
face_alignment,17,introduction,introduction,32,21,21,"To guarantee the disentanglement of these two spaces , we design a conditional variational auto - encoder model , in which Kullback - Leiber ( KL ) divergence loss and skip connections are incorporated for compact representation of style and structure respectively .","Instead of directly generating images , we first map face images into the space of structure and style .","By factoring these features , we perform visual style translation between existing facial geometry .",0.1146953405017921,0.5833333333333334,0.5833333333333334,model,approach
face_alignment,17,introduction,introduction,33,22,22,"By factoring these features , we perform visual style translation between existing facial geometry .","To guarantee the disentanglement of these two spaces , we design a conditional variational auto - encoder model , in which Kullback - Leiber ( KL ) divergence loss and skip connections are incorporated for compact representation of style and structure respectively .","Given existing facial structure , faces with glasses , of poor quality , under blur or strong lighting are rerendered from corresponding style , which are used to further train the facial landmark detectors for a rather general and robust system to recognize facial geometry .",0.1182795698924731,0.6111111111111112,0.6111111111111112,model,ablation-analysis
face_alignment,17,introduction,introduction,34,23,23,"Given existing facial structure , faces with glasses , of poor quality , under blur or strong lighting are rerendered from corresponding style , which are used to further train the facial landmark detectors for a rather general and robust system to recognize facial geometry .","By factoring these features , we perform visual style translation between existing facial geometry .",Our main contribution is as follows .,0.1218637992831541,0.6388888888888888,0.6388888888888888,model,model
face_alignment,17,experiments,experimental setting,154,18,7,"Implementation Details Before training , all images are cropped and resized to 256 256 using provided bounding boxes .","For other datasets , we follow the protocol used in and apply inter-ocular distance for normalization .","For the detailed conditional variational autoencoder network structures , we use a two - branch encoderdecoder structure as shown in .",0.5519713261648745,0.1914893617021277,0.4666666666666667,hyperparameters,experimental-setup
face_alignment,17,experiments,experimental setting,156,20,9,"We use 6 residual encoder blocks for downsampling the input feature maps , where batch normalization is removed for better synthetic results .","For the detailed conditional variational autoencoder network structures , we use a two - branch encoderdecoder structure as shown in .","The facial landmark detector backbone is substitutable and different detectors are usable to achieve improvement , which we will discuss later .",0.5591397849462365,0.2127659574468085,0.6,hyperparameters,approach
face_alignment,17,experiments,experimental setting,158,22,11,"For training of the disentangling step , we use Adam with an initial learning rate of 0.01 , which descends linearly to 0.0001 with no augmentation .","The facial landmark detector backbone is substitutable and different detectors are usable to achieve improvement , which we will discuss later .","For training of detectors , we first augment each landmark map with k random styles sampled from other face images .",0.5663082437275986,0.2340425531914893,0.7333333333333333,hyperparameters,approach
face_alignment,17,experiments,experimental setting,159,23,12,"For training of detectors , we first augment each landmark map with k random styles sampled from other face images .","For training of the disentangling step , we use Adam with an initial learning rate of 0.01 , which descends linearly to 0.0001 with no augmentation .",The number is set to 8 if not specially mentioned in experiments .,0.5698924731182796,0.2446808510638297,0.8,hyperparameters,experimental-setup
face_alignment,17,experiments,experimental setting,161,25,14,"For the detector architecture , a simple baseline network based on ResNet - 18 is chosen by changing the output dimension of the last FC layers to landmark 2 to demonstrate the increase brought by style translation .",The number is set to 8 if not specially mentioned in experiments .,"To compare with state - of - thearts and further validate the effectiveness of our approach , we replace our baseline model with similar structures proposed in , with the same affine augmentation .",0.5770609318996416,0.2659574468085106,0.9333333333333332,hyperparameters,approach
face_alignment,17,experiments,comparison with state of the arts,164,28,2,WFLW, ,We evaluate our approach on WFLW dataset .,0.5878136200716846,0.2978723404255319,0.0555555555555555,experiments,baselines
face_alignment,17,experiments,comparison with state of the arts,170,34,8,The light - weight Res - 18 is improved by 13.8 % .,"To further verify the effectiveness and generality of using style information , we replace the network by two strong baselines and report the result in .","By utilizing a stronger baseline , our model achieves 4.39 % NME under style - augmented training , outperforms state - of the - art entries by a large margin .",0.6093189964157706,0.3617021276595745,0.2222222222222222,experiments,baselines
face_alignment,17,experiments,comparison with state of the arts,171,35,9,"By utilizing a stronger baseline , our model achieves 4.39 % NME under style - augmented training , outperforms state - of the - art entries by a large margin .",The light - weight Res - 18 is improved by 13.8 % .,"In particular , for the strong baselines , our method also brings 15.9 % improvement to SAN model , and 9 % boost to LAB from 5.27 % NME to 4.76 % .",0.6129032258064516,0.3723404255319149,0.25,experiments,baselines
face_alignment,17,experiments,comparison with state of the arts,172,36,10,"In particular , for the strong baselines , our method also brings 15.9 % improvement to SAN model , and 9 % boost to LAB from 5.27 % NME to 4.76 % .","By utilizing a stronger baseline , our model achieves 4.39 % NME under style - augmented training , outperforms state - of the - art entries by a large margin .","The elevation is also determined by the model capacity . , we report different facial landmark detector performance ( in terms of normalized mean error ) on 300 W dataset .",0.6164874551971327,0.3829787234042553,0.2777777777777778,experiments,baselines
face_alignment,17,experiments,comparison with state of the arts,175,39,13,"With additional "" style- augmented "" synthetic training samples , our model based on a simple backbone outperforms previous state - of - the - art methods .",The baseline network follows Res - 18 structure .,"We also report results of models that are trained on original data , which reflect the performance gain brought by our approach .",0.6272401433691757,0.4148936170212766,0.3611111111111111,experiments,approach
face_alignment,17,experiments,comparison with state of the arts,177,41,15,300W,"We also report results of models that are trained on original data , which reflect the performance gain brought by our approach .",In,0.6344086021505376,0.4361702127659575,0.4166666666666667,experiments,baselines
face_alignment,17,experiments,comparison with state of the arts,183,47,21,"However , our model still yields 1.8 % and 3.1 % improvement on LAB and SAN respectively , which manifest the consistent benefit when using the "" style - augmented "" strategy .","We train the models from scratch , which perform less well than those reported in their original papers .",Method,0.6559139784946236,0.5,0.5833333333333334,experiments,baselines
face_alignment,17,experiments,comparison with state of the arts,185,49,23,Common Cross - dataset Evaluation on COFW,Method,"To comprehensively evaluate the robustness of our method towards occlusion , COFW - 68 is also utilized for cross - dataset evaluation .",0.6630824372759857,0.5212765957446809,0.6388888888888888,experiments,approach
face_alignment,17,experiments,comparison with state of the arts,188,52,26,"Our model performs the best with 4.43 % mean error and 2.82 % failure rate , which indicates high robustness to occlusion due to our proper utilization of style translation .",We perform comparison against several state - of - theart methods in .,AFLW,0.6738351254480287,0.5531914893617021,0.7222222222222222,experiments,approach
face_alignment,17,experiments,comparison with state of the arts,189,53,27,AFLW,"Our model performs the best with 4.43 % mean error and 2.82 % failure rate , which indicates high robustness to occlusion due to our proper utilization of style translation .",We further evaluate our algorithm on the AFLW dataset following the AFLW Full protocol .,0.6774193548387096,0.5638297872340425,0.75,experiments,baselines
face_alignment,17,experiments,comparison with state of the arts,196,60,34,"Exploiting style information also boosts landmark detectors with a large - scale training set ( 25 , 000 images in AFLW ) .","We compare our approach with several models in Table 3 , by re-implementing their algorithms on the new dataset along with our style - augmented samples .","Interestingly , our method improves SAN baseline in terms of NME on Full set from 6.94 % to 6.01 % , which indicates that augmenting in style level brings promising improvement on solving large pose variation .",0.7025089605734767,0.6382978723404256,0.9444444444444444,experiments,baselines
face_alignment,17,experiments,comparison with state of the arts,197,61,35,"Interestingly , our method improves SAN baseline in terms of NME on Full set from 6.94 % to 6.01 % , which indicates that augmenting in style level brings promising improvement on solving large pose variation .","Exploiting style information also boosts landmark detectors with a large - scale training set ( 25 , 000 images in AFLW ) .",The visual comparison in shows hidden face part is better modeled with our strategy .,0.7060931899641577,0.6489361702127661,0.9722222222222222,experiments,baselines
face_alignment,17,experiments,comparison with state of the arts,198,62,36,The visual comparison in shows hidden face part is better modeled with our strategy .,"Interestingly , our method improves SAN baseline in terms of NME on Full set from 6.94 % to 6.01 % , which indicates that augmenting in style level brings promising improvement on solving large pose variation .", ,0.7096774193548387,0.6595744680851063,1.0,experiments,approach
face_alignment,17,experiments,ablation study,201,65,3,Disentanglement of style and structure is the key that influences quality of style - augmented samples .,Improvement on Limited Data,We evaluate the completeness of disentanglement especially when the training samples are limited .,0.7204301075268817,0.6914893617021277,0.09375,ablation-analysis,model
face_alignment,17,experiments,ablation study,210,74,12,"Style - augmented synthetic images improve detectors ' performance by a large margin , while the improvement is even larger when the number of training images is quite small .",show the relative improvement on different training samples .,"In , a stronger baseline SAN is chosen .",0.7526881720430108,0.7872340425531915,0.375,ablation-analysis,approach
face_alignment,17,experiments,ablation study,225,89,27,We evaluate our method by adding the number of random sampled styles k of each annotated landmarks on a ResNet - 50 baseline .,"In this section , we experiment with choosing the style augmenting factor k and test the upper bound of style translation .",The result is reported in .,0.8064516129032258,0.946808510638298,0.84375,ablation-analysis,approach
face_alignment,17,experiments,ablation study,227,91,29,"By adding a number of augmented styles , the model continue gaining improvement .",The result is reported in .,"However , when k ? 8 , the performance grow slows down .",0.8136200716845878,0.9680851063829788,0.90625,ablation-analysis,approach
face_alignment,18,title,title,2,2,2,Deep Alignment Network : A convolutional neural network for robust face alignment, , ,0.0082304526748971,1.0,1.0,research-problem,experimental-setup
face_alignment,18,introduction,introduction,12,2,2,"The goal of face alignment is to localize a set of predefined facial landmarks ( eye corners , mouth corners etc. ) in an image of a face .", ,"Face alignment is an important component of many computer vision applications , such as face verification , facial emotion recognition , humancomputer interaction and facial motion capture .",0.0493827160493827,0.0555555555555555,0.0555555555555555,research-problem,model
face_alignment,18,introduction,introduction,19,9,9,"In this work , we address the above shortcoming by proposing a novel face alignment method which we dub Deep Alignment Network ( DAN ) .",We believe that this is due to the fact that for the most difficult images the features extracted at disjoint patches do not provide enough information and can lead the method into a local minimum .,"It is based on a multistage neural network where each stage refines the landmark positions estimated at the previous stage , iteratively improving the landmark locations .",0.0781893004115226,0.25,0.25,model,ablation-analysis
face_alignment,18,introduction,introduction,20,10,10,"It is based on a multistage neural network where each stage refines the landmark positions estimated at the previous stage , iteratively improving the landmark locations .","In this work , we address the above shortcoming by proposing a novel face alignment method which we dub Deep Alignment Network ( DAN ) .",The input to each stage of our algorithm ( except the first stage ) area face image normalized to a canonical pose and an image learned from the dense layer of the previous stage .,0.0823045267489712,0.2777777777777778,0.2777777777777778,model,model
face_alignment,18,introduction,introduction,21,11,11,The input to each stage of our algorithm ( except the first stage ) area face image normalized to a canonical pose and an image learned from the dense layer of the previous stage .,"It is based on a multistage neural network where each stage refines the landmark positions estimated at the previous stage , iteratively improving the landmark locations .","To make use of the entire face image during the process of face alignment , we additionally input at each stage a landmark heatmap , which is a key element of our system .",0.0864197530864197,0.3055555555555556,0.3055555555555556,model,experimental-setup
face_alignment,18,introduction,introduction,22,12,12,"To make use of the entire face image during the process of face alignment , we additionally input at each stage a landmark heatmap , which is a key element of our system .",The input to each stage of our algorithm ( except the first stage ) area face image normalized to a canonical pose and an image learned from the dense layer of the previous stage .,A landmark heatmap is an image with high intensity values around landmark locations where intensity decreases with the distance from the nearest landmark .,0.0905349794238683,0.3333333333333333,0.3333333333333333,model,model
face_alignment,18,experiments,experiments,193,43,43,During data augmentation a total of 10 images are created from each input image in the training set .,"Data augmentation is performed by mirroring around the Y axis as well as random translation , rotation and scaling , all sampled from normal distributions .",Both models ( DAN and DAN - Menpo ) consist of two stages .,0.7942386831275721,0.5058823529411764,0.8269230769230769,experimental-setup,model
face_alignment,18,experiments,experiments,195,45,45,Training is performed using Theano 0.9.0 and Lasagne 0.2 .,Both models ( DAN and DAN - Menpo ) consist of two stages .,For optimization we use Adam stochastic optimization with an initial step size of 0.001 and mini batch size of 64 .,0.8024691358024691,0.5294117647058824,0.8653846153846154,experimental-setup,approach
face_alignment,18,experiments,experiments,196,46,46,For optimization we use Adam stochastic optimization with an initial step size of 0.001 and mini batch size of 64 .,Training is performed using Theano 0.9.0 and Lasagne 0.2 .,For validation we use a random subset of 100 images from the training set .,0.8065843621399177,0.5411764705882353,0.8846153846153846,experimental-setup,approach
face_alignment,18,experiments,experiments,197,47,47,For validation we use a random subset of 100 images from the training set .,For optimization we use Adam stochastic optimization with an initial step size of 0.001 and mini batch size of 64 .,The Python implementation runs at 73 fps for images processed in parallel and at 45 fps for images processed sequentially on a GeForce GTX 1070 GPU .,0.8106995884773662,0.5529411764705883,0.903846153846154,experimental-setup,experimental-setup
face_alignment,18,experiments,experiments,198,48,48,The Python implementation runs at 73 fps for images processed in parallel and at 45 fps for images processed sequentially on a GeForce GTX 1070 GPU .,For validation we use a random subset of 100 images from the training set .,"We believe that the processing speed can be further improved by optimizing the implementation of some of our custom layers , most notably the Image Transform layer .",0.8148148148148148,0.5647058823529412,0.9230769230769232,experimental-setup,baselines
face_alignment,18,experiments,comparison with state of the art,210,60,8,"a failure rate reduction of 60 % on the 300 W private test set ,","All of the experiments performed on the two most difficult test subsets ( the challenging subset and the 300 W private test set ) show state - of - the - art results , including :","a failure rate reduction of 72 % on the 300W public test set ,",0.8641975308641975,0.7058823529411765,0.7272727272727273,results,approach
face_alignment,18,experiments,comparison with state of the art,211,61,9,"a failure rate reduction of 72 % on the 300W public test set ,","a failure rate reduction of 60 % on the 300 W private test set ,",a 9 % improvement of the mean error on the challenging subset .,0.8683127572016461,0.7176470588235294,0.8181818181818182,results,approach
face_alignment,18,experiments,comparison with state of the art,212,62,10,a 9 % improvement of the mean error on the challenging subset .,"a failure rate reduction of 72 % on the 300W public test set ,",This shows that the proposed DAN is particularly suited for handling difficult face images with a high degree of occlusion and variation in pose and illumination .,0.8724279835390947,0.7294117647058823,0.9090909090909092,results,approach
face_alignment,2,title,title,2,2,2,DeCaFA : Deep Convolutional Cascade for Face Alignment In The Wild, , ,0.0097087378640776,1.0,1.0,research-problem,experimental-setup
face_alignment,2,abstract,abstract,4,2,2,"Face Alignment is an active computer vision domain , that consists in localizing a number of facial landmarks that vary across datasets .", ,"State - of - the - art face alignment methods either consist in end - to - end regression , or in refining the shape in a cascaded manner , starting from an initial guess .",0.0194174757281553,0.2222222222222222,0.2222222222222222,research-problem,ablation-analysis
face_alignment,2,introduction,introduction,22,11,11,"In this paper , we introduce a Deep convolutional Cascade for Face Alignment ( DeCaFA ) .","However , these representations are usually ad hoc and do not guarantee to be optimal to address landmark localization tasks .","DeCaFA is composed of several stages that each produce landmark - wise attention maps , relatively to heterogeneous annotation markups .",0.1067961165048543,0.55,0.55,model,experimental-setup
face_alignment,2,introduction,introduction,23,12,12,"DeCaFA is composed of several stages that each produce landmark - wise attention maps , relatively to heterogeneous annotation markups .","In this paper , we introduce a Deep convolutional Cascade for Face Alignment ( DeCaFA ) .",shows attention maps extracted by the subsequent DeCaFA stages ( horizontally ) and for three different markups ( vertically ) .,0.1116504854368932,0.6,0.6,model,ablation-analysis
face_alignment,2,introduction,introduction,25,14,14,"It illustrates how these attention maps are refined through the successive stages , and how the different prediction tasks can benefit from each other .",shows attention maps extracted by the subsequent DeCaFA stages ( horizontally ) and for three different markups ( vertically ) .,The contributions of this paper are tree - fold :,0.1213592233009708,0.7,0.7,model,approach
face_alignment,2,decafa overview,implementation details,130,72,2,"The DeCaFA models that will be investigated below use 1 to 4 stages that each contains 12 3 3 convolutional layers with 64 ? 64 ? 128 ? 128 ? 256 ? 256 channels for the downsampling portion , and vice - versa for the upsampling portion .", ,The input images are resized to 128 128 grayscale images prior to being processed by the network .,0.6310679611650486,0.935064935064935,0.2857142857142857,hyperparameters,experimental-setup
face_alignment,2,decafa overview,implementation details,131,73,3,The input images are resized to 128 128 grayscale images prior to being processed by the network .,"The DeCaFA models that will be investigated below use 1 to 4 stages that each contains 12 3 3 convolutional layers with 64 ? 64 ? 128 ? 128 ? 256 ? 256 channels for the downsampling portion , and vice - versa for the upsampling portion .",Each convolution is followed by a batch normalization layer with ReLU activation .,0.6359223300970874,0.948051948051948,0.4285714285714285,hyperparameters,experimental-setup
face_alignment,2,decafa overview,implementation details,132,74,4,Each convolution is followed by a batch normalization layer with ReLU activation .,The input images are resized to 128 128 grayscale images prior to being processed by the network .,In order to generate smooth feature maps we do not use transposed convolution but bilinear image upsampling followed with 3 3 convolutional layers .,0.6407766990291263,0.9610389610389608,0.5714285714285714,hyperparameters,experimental-setup
face_alignment,2,decafa overview,implementation details,133,75,5,In order to generate smooth feature maps we do not use transposed convolution but bilinear image upsampling followed with 3 3 convolutional layers .,Each convolution is followed by a batch normalization layer with ReLU activation .,The whole architecture is trained using ADAM optimizer with a 5e ? 4 learning rate with momentum 0.9 and learning rate annealing with power 0.9 .,0.6456310679611651,0.974025974025974,0.7142857142857143,hyperparameters,experimental-setup
face_alignment,2,decafa overview,implementation details,134,76,6,The whole architecture is trained using ADAM optimizer with a 5e ? 4 learning rate with momentum 0.9 and learning rate annealing with power 0.9 .,In order to generate smooth feature maps we do not use transposed convolution but bilinear image upsampling followed with 3 3 convolutional layers .,"We apply 400000 updates with batch size 8 for each database , with alternating updates between the databases .",0.6504854368932039,0.987012987012987,0.8571428571428571,hyperparameters,model
face_alignment,2,decafa overview,implementation details,135,77,7,"We apply 400000 updates with batch size 8 for each database , with alternating updates between the databases .",The whole architecture is trained using ADAM optimizer with a 5e ? 4 learning rate with momentum 0.9 and learning rate annealing with power 0.9 ., ,0.6553398058252428,1.0,1.0,hyperparameters,model
face_alignment,2,experiments,ablation study,153,18,4,"The accuracy steadily increases as we add more stages , and saturates after the third on LFPW and HELEN , which is a well - known behavior of cascaded models , showing that DeCaFA with weighted intermediate supervision indeed works as a cascade , by first providing coarse estimates and refining in the later stages .","shows CED curves for models with S = 1 , 2 , 3 and 4 cascade stages .","On IBUG , this difference is more conspicuous , thus there is for improvement by stacking more cascade stages .",0.7427184466019418,0.3,0.16,ablation-analysis,baselines
face_alignment,2,experiments,ablation study,156,21,7,"Coarsely annotated data ( 5 landmarks ) significantly helps the fine - grained landmark localization , as it is integrated a kind of weakly supervised scheme .","shows the interest of chaining multiple tasks , most notably on LFPW , that contains low - resolution images , and IBUG , which contains strong head pose variations as well as occlusions .",This will be discussed more thoroughly in Section 4.4 .,0.7572815533980582,0.35,0.28,ablation-analysis,approach
face_alignment,2,experiments,ablation study,160,25,11,"First , reinjecting the whole input image ( F 3 - Equation vs F 2 - Equation ) significantly improves the accuracy on challenging data such as 300 W - challenging or WFLW - pose , where the first cascade stages may commit errors .","We test our model on 300W ( full and challenging ) , WFLW ( All and challenging , i.e. pose subset ) as well as CelebA and report the average accuracy on those 5 subsets .",F 4 - Equation ( 7 ) and F 3 fusion ( cascaded models ) using local + global information rivals the basic deep approach F 1 - Equation ( 4 ) .,0.7766990291262136,0.4166666666666667,0.44,ablation-analysis,model
face_alignment,2,experiments,ablation study,161,26,12,F 4 - Equation ( 7 ) and F 3 fusion ( cascaded models ) using local + global information rivals the basic deep approach F 1 - Equation ( 4 ) .,"First , reinjecting the whole input image ( F 3 - Equation vs F 2 - Equation ) significantly improves the accuracy on challenging data such as 300 W - challenging or WFLW - pose , where the first cascade stages may commit errors .","Furthermore , F 5 - Equation fusion , which uses local and global cues is the best by a significant margin .",0.7815533980582524,0.4333333333333333,0.48,ablation-analysis,model
face_alignment,2,experiments,ablation study,162,27,13,"Furthermore , F 5 - Equation fusion , which uses local and global cues is the best by a significant margin .",F 4 - Equation ( 7 ) and F 3 fusion ( cascaded models ) using local + global information rivals the basic deep approach F 1 - Equation ( 4 ) .,"Furthermore , chaining the transfer layers is better than using independant transfer layers : likewise , in such a case , the first transfer layer benefits from the gradients from the subsequents layer at train time .",0.7864077669902912,0.45,0.52,ablation-analysis,baselines
face_alignment,2,experiments,ablation study,163,28,14,"Furthermore , chaining the transfer layers is better than using independant transfer layers : likewise , in such a case , the first transfer layer benefits from the gradients from the subsequents layer at train time .","Furthermore , F 5 - Equation fusion , which uses local and global cues is the best by a significant margin .","Last but not least , using increasing intermediate supervision weights in Equation ( 10 ) ( i.e. ? 1 = 1 /8 , ? 2 = 1 /4 , ? 3 = 1 /2 , ? 4 = 1 ) is better than both using constant weights ( ? 1 = ? 2 = ? 3 = ? 4 = 1 ) and decreasing weights (? 1 = 1 , ? 2 = 1 /2 , ? 3 = 1 / 4 , ? 4 = 1 / 8 ) , as it enables proper cascade - like training of the network , with the first stage outputting coarser attention maps that can be refined in the latter stages of the network .",0.7912621359223301,0.4666666666666667,0.56,ablation-analysis,model
face_alignment,2,experiments,comparisons with state of the art methods,176,41,2,"Finally , shows a comparison of our method and state - of - the - art approaches on Celeb A .", ,"As in we report the average point - to - point error on the test partition , normalized by the distance between the two eye centers .",0.8543689320388349,0.6833333333333333,0.2222222222222222,results,experimental-setup
face_alignment,2,experiments,comparisons with state of the art methods,181,46,7,"Overall , DeCaFA sets a new state - of - the - art on the three databases with several evaluation metrics .","Noteworthy , even though we use auxiliary data from 300 W and WFLW , we do not use data from the val partition of CelebA , contrary to , thus there is significant room for improvement .",Also notice that it embraces few parameters ( ?,0.8786407766990292,0.7666666666666667,0.7777777777777778,results,approach
face_alignment,3,title,title,2,2,2,Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression, , ,0.0059171597633136,1.0,1.0,research-problem,experimental-setup
face_alignment,3,abstract,abstract,13,11,11,Code will be made publicly available at https://github.com/protossw512/AdaptiveWingLoss.,"Besides , the Adaptive Wing loss also helps other heatmap regression tasks .", ,0.0384615384615384,1.0,1.0,code,baselines
face_alignment,3,introduction,introduction,15,2,2,"Face alignment , also known as facial landmark localization , seeks to localize pre-defined landmarks on human faces .", ,"Face alignment plays an essential role in many face related applications such as face recognition , face frontalization and 3D face reconstruction .",0.044378698224852,0.0714285714285714,0.0714285714285714,research-problem,ablation-analysis
face_alignment,3,introduction,introduction,30,17,17,"We thus propose a new loss function and name it Adaptive Wing loss ( Sec. , that is able to significantly improve the quality of heatmap regression results .",Small errors on background pixels will accumulate significant gradients and thus cause the training process to diverge .,"Due to the translation invariance of the convolution operation in bottom - up and top - down CNN structures such as stacked Hourglass ( HG ) , the network is notable to capture coordinate information , which we believe is useful for facial landmark localization , since the structure of human faces is relatively stable .",0.0887573964497041,0.6071428571428571,0.6071428571428571,model,model
face_alignment,3,introduction,introduction,31,18,18,"Due to the translation invariance of the convolution operation in bottom - up and top - down CNN structures such as stacked Hourglass ( HG ) , the network is notable to capture coordinate information , which we believe is useful for facial landmark localization , since the structure of human faces is relatively stable .","We thus propose a new loss function and name it Adaptive Wing loss ( Sec. , that is able to significantly improve the quality of heatmap regression results .","Inspired by the Coord - Conv layer proposed by Liu et al. , we encode into our model the full coordinate information and the information only on boundaries predicted from the previous HG module into our model .",0.0917159763313609,0.6428571428571429,0.6428571428571429,model,baselines
face_alignment,3,introduction,introduction,32,19,19,"Inspired by the Coord - Conv layer proposed by Liu et al. , we encode into our model the full coordinate information and the information only on boundaries predicted from the previous HG module into our model .","Due to the translation invariance of the convolution operation in bottom - up and top - down CNN structures such as stacked Hourglass ( HG ) , the network is notable to capture coordinate information , which we believe is useful for facial landmark localization , since the structure of human faces is relatively stable .",The encoded coordinate information further improves the performance of our approach .,0.0946745562130177,0.6785714285714286,0.6785714285714286,model,ablation-analysis
face_alignment,3,introduction,introduction,34,21,21,"To encode boundary coordinates , we also add a sub-task of boundary prediction by concatenating an additional boundary channel into the ground truth heatmap which is jointly trained with other channels .",The encoded coordinate information further improves the performance of our approach .,"In summary , our main contributions include :",0.1005917159763313,0.75,0.75,model,ablation-analysis
face_alignment,3,experiments,implementation details,231,28,7,"During training , we use RM - SProp with an initial learning rate of 1 10 ?4 .","We use four stacks of HG , same with other baselines .",We set the momentum to be 0 ( adopted from ) and the weight decay to be 1 10 ?5 .,0.6834319526627219,0.3218390804597701,0.3684210526315789,hyperparameters,baselines
face_alignment,3,experiments,implementation details,232,29,8,We set the momentum to be 0 ( adopted from ) and the weight decay to be 1 10 ?5 .,"During training , we use RM - SProp with an initial learning rate of 1 10 ?4 .","We train for 240 epoches , and the learning rate is reduced to 1 10 ?5 and 1 10 ? 6 after 80 and 160 epoches .",0.6863905325443787,0.3333333333333333,0.4210526315789473,hyperparameters,baselines
face_alignment,3,experiments,implementation details,233,30,9,"We train for 240 epoches , and the learning rate is reduced to 1 10 ?5 and 1 10 ? 6 after 80 and 160 epoches .",We set the momentum to be 0 ( adopted from ) and the weight decay to be 1 10 ?5 .,"Data augmentation is performed with random rotation ( 50 ) , translation ( 25 px ) , flipping ( 50 % ) , and rescaling ( 15 % ) .",0.6893491124260355,0.3448275862068966,0.4736842105263158,hyperparameters,approach
face_alignment,3,experiments,implementation details,234,31,10,"Data augmentation is performed with random rotation ( 50 ) , translation ( 25 px ) , flipping ( 50 % ) , and rescaling ( 15 % ) .","We train for 240 epoches , and the learning rate is reduced to 1 10 ?5 and 1 10 ? 6 after 80 and 160 epoches .","Random Gaussian blur , noise and occlusion are also used .",0.6923076923076923,0.3563218390804598,0.5263157894736842,hyperparameters,model
face_alignment,3,experiments,implementation details,235,32,11,"Random Gaussian blur , noise and occlusion are also used .","Data augmentation is performed with random rotation ( 50 ) , translation ( 25 px ) , flipping ( 50 % ) , and rescaling ( 15 % ) .",All models are trained from scratch .,0.6952662721893491,0.3678160919540229,0.5789473684210527,hyperparameters,approach
face_alignment,3,experiments,evaluation on 300w,244,41,1,Evaluation on 300W, , ,0.7218934911242604,0.4712643678160919,0.0769230769230769,results,approach
face_alignment,3,experiments,evaluation on 300w,245,42,2,"Our method is able to achieve the state - of - the - art performance on the 300W testing dataset , see .", ,"For the challenge subset ( iBug dataset ) , we are able to outperform",0.7248520710059172,0.4827586206896552,0.1538461538461538,results,baselines
face_alignment,3,experiments,evaluation on 300w,246,43,3,"For the challenge subset ( iBug dataset ) , we are able to outperform","Our method is able to achieve the state - of - the - art performance on the 300W testing dataset , see .","Wing by a significant margin , which also proves the robustness of our approach against occlusion and large pose variation .",0.7278106508875741,0.4942528735632184,0.2307692307692308,results,baselines
face_alignment,3,experiments,evaluation on 300w,247,44,4,"Wing by a significant margin , which also proves the robustness of our approach against occlusion and large pose variation .","For the challenge subset ( iBug dataset ) , we are able to outperform","Furthermore , on the 300 W private test dataset ) , we again outperform the previous state - of - theart on variant metrics including NME , AUC and FR measured with either 8 % NME and 10 % NME .",0.7307692307692307,0.5057471264367817,0.3076923076923077,results,baselines
face_alignment,3,experiments,evaluation on 300w,248,45,5,"Furthermore , on the 300 W private test dataset ) , we again outperform the previous state - of - theart on variant metrics including NME , AUC and FR measured with either 8 % NME and 10 % NME .","Wing by a significant margin , which also proves the robustness of our approach against occlusion and large pose variation .","Note that we more than halved the failure rate of the next best baseline to 0.83 % , which means only 5 faces out of 600 have an NME that is larger than 8 % .",0.7337278106508875,0.5172413793103449,0.3846153846153846,results,baselines
face_alignment,3,experiments,evaluation on 300w,250,47,7,Evaluation on WFLW,"Note that we more than halved the failure rate of the next best baseline to 0.83 % , which means only 5 faces out of 600 have an NME that is larger than 8 % .","Our method again achieves the best results on the WFLW dataset in , which is significantly more difficult than COFW and 300W ( see for visualizations ) .",0.7396449704142012,0.5402298850574713,0.5384615384615384,results,approach
face_alignment,3,experiments,evaluation on 300w,251,48,8,"Our method again achieves the best results on the WFLW dataset in , which is significantly more difficult than COFW and 300W ( see for visualizations ) .",Evaluation on WFLW,On every subset we outperform the previous state - of - the - art ap - :,0.7426035502958579,0.5517241379310345,0.6153846153846154,results,baselines
face_alignment,3,experiments,evaluation on 300w,252,49,9,On every subset we outperform the previous state - of - the - art ap - :,"Our method again achieves the best results on the WFLW dataset in , which is significantly more difficult than COFW and 300W ( see for visualizations ) .",Evaluation on the 300W testset proaches by a significant margin .,0.7455621301775148,0.5632183908045977,0.6923076923076923,results,baselines
face_alignment,3,experiments,evaluation on 300w,256,53,13,"All in all , our approach fails on only 2.84 % of all images , more than a two times improvement compared with 7.6 .",We are also able to reduce the failure rate and increase the AUC dramatically and hence improving the overall localization quality significantly ., ,0.7573964497041421,0.6091954022988506,1.0,results,baselines
face_alignment,4,title,title,2,2,2,Facial Landmarks Detection by Self - Iterative Regression based Landmarks - Attention Network, , ,0.0094786729857819,1.0,1.0,research-problem,approach
face_alignment,4,introduction,introduction,31,19,19,"In this paper , we develop a Self - Iterative Regression ( SIR ) framework to solve the above issues .","( 3 ) Subsequent regressors usually can not be activated for training until previous regressors finished their training process , which increases the system complexity .","By means of the powerful representation of Convolutional Neural Network ( CNN ) , we only train one regressor to learn the descent directions in coarse and fine stages together .",0.1469194312796208,0.6551724137931034,0.6551724137931034,model,approach
face_alignment,4,introduction,introduction,32,20,20,"By means of the powerful representation of Convolutional Neural Network ( CNN ) , we only train one regressor to learn the descent directions in coarse and fine stages together .","In this paper , we develop a Self - Iterative Regression ( SIR ) framework to solve the above issues .","The training data is obtained by random sampling in the parameter space , and in the test - ing process , parameters are updated iteratively by calling the same regressor , which is dubbed Self - Iterative Regression .",0.1516587677725118,0.6896551724137931,0.6896551724137931,model,ablation-analysis
face_alignment,4,introduction,introduction,36,24,24,"Moreover , to obtain discriminative landmarks features , we proposed a Landmarks - Attention Network ( LAN ) , which focuses on the appearance around landmarks .","The experimental results show that for deep learning based method , one regressor achieves comparable performance to state - of the - art multiple cascaded regressors and significantly reduce the training complexity .","It first concurrently extracts local landmarks ' features and then obtains the holistic increment , which significantly reduces the dimension of the final feature layer and the number of model parameters .",0.1706161137440758,0.8275862068965517,0.8275862068965517,model,ablation-analysis
face_alignment,4,introduction,introduction,37,25,25,"It first concurrently extracts local landmarks ' features and then obtains the holistic increment , which significantly reduces the dimension of the final feature layer and the number of model parameters .","Moreover , to obtain discriminative landmarks features , we proposed a Landmarks - Attention Network ( LAN ) , which focuses on the appearance around landmarks .",The contributions of this paper are summarized as follows :,0.1753554502369668,0.8620689655172413,0.8620689655172413,model,baselines
face_alignment,4,experiments,experiments,167,13,13,"We perform the experiments based on a machine with Core i7 - 5930 k CPU , 32 GB memory and GTX 1080 GPU with 8G video memory .","Similar as MDM ) , we consider mean point - to - point error greater than 0.08 as a failure , i.e. , ? = 0.08 . Implementation Detail .",The detected faces are resized into 256 256 and the location patch size is 57 57 .,0.7914691943127962,0.25,0.8125,experimental-setup,approach
face_alignment,4,experiments,experiments,168,14,14,The detected faces are resized into 256 256 and the location patch size is 57 57 .,"We perform the experiments based on a machine with Core i7 - 5930 k CPU , 32 GB memory and GTX 1080 GPU with 8G video memory .","For CNN structure , the Rectified Linear Unit ( ReLU ) is adopted as the activation function , and the optimizer is the Adadelta ( Zeiler 2012 ) approach , learning rate is set to 0.1 and weight decay is set to 1 e ?",0.7962085308056872,0.2692307692307692,0.875,experimental-setup,approach
face_alignment,4,experiments,experiments,169,15,15,"For CNN structure , the Rectified Linear Unit ( ReLU ) is adopted as the activation function , and the optimizer is the Adadelta ( Zeiler 2012 ) approach , learning rate is set to 0.1 and weight decay is set to 1 e ?",The detected faces are resized into 256 256 and the location patch size is 57 57 .,4 . Training the CNN requires around 2 days .,0.8009478672985783,0.2884615384615384,0.9375,experimental-setup,baselines
face_alignment,4,experiments,experiments,170,16,16,4 . Training the CNN requires around 2 days .,"For CNN structure , the Rectified Linear Unit ( ReLU ) is adopted as the activation function , and the optimizer is the Adadelta ( Zeiler 2012 ) approach , learning rate is set to 0.1 and weight decay is set to 1 e ?", ,0.8056872037914692,0.3076923076923077,1.0,experimental-setup,approach
face_alignment,4,experiments,comparison with state of the arts,175,21,5,The NME results shows that SIR performs comparatively with RAR ) and outperform other existing methods .,Thus the the full set ( 689 faces ) of the union of the common ( 554 faces ) and challenging subsets ( 135 faces ) .,"Besides , more visual results are also illustrated in .",0.8293838862559242,0.4038461538461538,0.4545454545454545,results,approach
face_alignment,4,experiments,comparison with state of the arts,177,23,7,"In the more challenging IBUG subset , our method achieves robust performance in large pose , expression and illumination environment .","Besides , more visual results are also illustrated in .","On the other hand , we evaluate SIR in the competition testing set .",0.8388625592417062,0.4423076923076923,0.6363636363636364,results,approach
face_alignment,4,experiments,comparison with state of the arts,179,25,9,"As shown in , the SIR method outperform the state - of - the - art methods according to the CED curve .","On the other hand , we evaluate SIR in the competition testing set .","Moreover , presents the quantitative results for both the 51 - point and 68 - point error metrics ( i.e. , AUC and Failure Rate at a threshold of 0.08 of the normalised error ) , compared to existing methods ) .",0.8483412322274881,0.4807692307692308,0.8181818181818182,results,experimental-setup
face_alignment,5,title,title,2,2,2,Look at Boundary : A Boundary - Aware Face Alignment Algorithm, , ,0.0070175438596491,1.0,1.0,research-problem,approach
face_alignment,5,abstract,abstract,14,12,12,Dataset and model will be publicly available at https://wywu.github.io/projects/LAB/LAB.html,"Moreover , we propose anew dataset WFLW to unify training and testing across different factors , including poses , expressions , illuminations , makeups , occlusions , and blurriness .", ,0.0491228070175438,1.0,1.0,code,baselines
face_alignment,5,introduction,introduction,16,2,2,"Face alignment , which refers to facial landmark detection in this work , serves as a key step for many face applications , e.g. , face recognition , face verification and face frontalisation .", ,The objective of this paper is to devise an effective face alignment algorithm to handle faces with unconstrained pose variation and occlusion across multiple datasets and annotation protocols . *,0.0561403508771929,0.054054054054054,0.054054054054054,research-problem,model
face_alignment,5,introduction,introduction,28,14,14,"To this end , we use well - defined facial boundaries to represent the geometric structure of the human face .",We believe the reasoning of a unique facial structure is the key to localise facial landmarks since human face does not include ambiguities .,It is easier to identify facial boundaries comparing to facial landmarks under large pose and occlusion .,0.0982456140350877,0.3783783783783784,0.3783783783783784,approach,experimental-setup
face_alignment,5,introduction,introduction,30,16,16,"In this work , we represent facial structure using 13 boundary lines .",It is easier to identify facial boundaries comparing to facial landmarks under large pose and occlusion .,"Each facial boundary line can be interpolated from a sufficient number of facial landmarks across multiple datasets , which will not suffer from inconsistency of the annotation schemes .",0.1052631578947368,0.4324324324324325,0.4324324324324325,approach,approach
face_alignment,5,introduction,introduction,32,18,18,Our boundary - aware face alignment algorithm contains two stages .,"Each facial boundary line can be interpolated from a sufficient number of facial landmarks across multiple datasets , which will not suffer from inconsistency of the annotation schemes .",We first estimate facial boundary heatmaps and then regress landmarks with the help of boundary heatmaps .,0.1122807017543859,0.4864864864864865,0.4864864864864865,approach,ablation-analysis
face_alignment,5,introduction,introduction,33,19,19,We first estimate facial boundary heatmaps and then regress landmarks with the help of boundary heatmaps .,Our boundary - aware face alignment algorithm contains two stages .,"As noticed in , facial landmarks of different annotation schemes can be derived from boundary heatmaps with the same definition .",0.1157894736842105,0.5135135135135135,0.5135135135135135,approach,ablation-analysis
face_alignment,5,introduction,introduction,35,21,21,"To explore the relationship between facial boundaries and landmarks , we introduce adversarial learning ideas by using a landmark - based boundary effectiveness discriminator .","As noticed in , facial landmarks of different annotation schemes can be derived from boundary heatmaps with the same definition .","Experiments have shown that the better quality estimated boundaries have , the more accurate landmarks will be .",0.1228070175438596,0.5675675675675675,0.5675675675675675,approach,ablation-analysis
face_alignment,5,introduction,introduction,37,23,23,"The boundary heatmap estimator , landmark regressor , and boundary effectiveness discriminator can be jointly learned in an end - to - end manner .","Experiments have shown that the better quality estimated boundaries have , the more accurate landmarks will be .",We used stacked hourglass structure to estimate facial boundary heatmap and model the structure between facial boundaries through message passing to increase its robustness to occlusion .,0.1298245614035087,0.6216216216216216,0.6216216216216216,approach,experimental-setup
face_alignment,5,introduction,introduction,38,24,24,We used stacked hourglass structure to estimate facial boundary heatmap and model the structure between facial boundaries through message passing to increase its robustness to occlusion .,"The boundary heatmap estimator , landmark regressor , and boundary effectiveness discriminator can be jointly learned in an end - to - end manner .","After generating facial boundary heatmaps , the next step is deriving facial landmarks using boundaries .",0.1333333333333333,0.6486486486486487,0.6486486486486487,approach,experimental-setup
face_alignment,5,introduction,introduction,40,26,26,The boundary heatmaps serve as structure cue to guide feature learning for the landmark regressor .,"After generating facial boundary heatmaps , the next step is deriving facial landmarks using boundaries .",We observe that a model guided by ground truth boundary heatmaps can achieve 76. 26 % AUC on 300 W test while the state - of - the - art method can only achieve 54.85 % .,0.1403508771929824,0.7027027027027027,0.7027027027027027,approach,model
face_alignment,5,experiments,experiments,224,28,28,All our models are trained with Caffe [ 24 ] on 4 Titan X GPUs .,"For ablation study , the estimator is stacked two times due to the consideration of time and computation cost .",Note that all testing images are cropped and resized according to provided bounding boxes without any spatial transformation for fair comparison with other methods .,0.7859649122807018,0.3414634146341464,0.509090909090909,experimental-setup,baselines
face_alignment,5,experiments,experiments,227,31,31,Comparison with existing approaches 4.1.1 Evaluation on 300W,"For the limited space of paper , we report all of the training details and experiment settings in our supplementary material .",We compare our approach against the state - of - the - art methods on 300W Fullset .,0.7964912280701755,0.3780487804878049,0.5636363636363636,results,experimental-setup
face_alignment,5,experiments,experiments,231,35,35,Our method performs best among all of the state - of - the - art methods .,"Apart from 300W Fullset , we also show our results on 300W Testset in .","To verify the effectiveness and potential of boundary maps , we use ground truth boundary in the proposed method and report results named "" LAB + oracle "" which significantly outperform all the methods .",0.8105263157894737,0.4268292682926829,0.6363636363636364,results,baselines
face_alignment,5,experiments,experiments,234,38,38,Evaluation on WFLW,The results demonstrate the effectiveness of boundary information and show great potential performance gain if the boundary information can be well captured .,"For comprehensively evaluating the robustness of our method , we report mean error , failure rate and AUC on the Testset and six typical subsets of WFLW on 3 .",0.8210526315789474,0.4634146341463415,0.6909090909090909,results,approach
face_alignment,5,experiments,experiments,237,41,41,"Though reasonable performance is obtained , there is illustrated to be still a lot of room for improvement for the extreme diversity of samples on WFLW , e.g. , large pose , exaggerated expressions and heavy occlusion .",These six subsets were split from Testset by the provided attribute annotations .,Cross - dataset evaluation on COFW and AFLW,0.8315789473684211,0.5,0.7454545454545455,results,approach
face_alignment,5,experiments,experiments,238,42,42,Cross - dataset evaluation on COFW and AFLW,"Though reasonable performance is obtained , there is illustrated to be still a lot of room for improvement for the extreme diversity of samples on WFLW , e.g. , large pose , exaggerated expressions and heavy occlusion .",COFW - 68 is produced by re-annotating COFW dataset with 68 landmarks annotation scheme to perform cross - dataset experiments by .,0.8350877192982457,0.5121951219512195,0.7636363636363637,results,approach
face_alignment,5,experiments,experiments,241,45,45,Our model outperforms previous results with a large margin .,shows the CED curves of our method against state - of - the - art methods on the COFW - 68 dataset .,We achieve 4.62 % mean error with 2.17 % failure rate .,0.8456140350877193,0.5487804878048781,0.8181818181818182,results,approach
face_alignment,5,experiments,experiments,242,46,46,We achieve 4.62 % mean error with 2.17 % failure rate .,Our model outperforms previous results with a large margin .,"The failure rate is significantly reduced by 3.75 % , which indicates the robustness of our method to handle occlusions .",0.8491228070175438,0.5609756097560976,0.8363636363636363,results,approach
face_alignment,5,experiments,experiments,243,47,47,"The failure rate is significantly reduced by 3.75 % , which indicates the robustness of our method to handle occlusions .",We achieve 4.62 % mean error with 2.17 % failure rate .,"In order to verify the capacity of handling cross - dataset face alignment of our method , we use boundary heatmaps estimator trained on 300W Fullset which has no overlap with COFW and AFLW dataset and compare the performance with and without using boundary information fusion ( "" LAB w/o boundary "" ) .",0.8526315789473684,0.573170731707317,0.8545454545454545,results,approach
face_alignment,5,experiments,experiments,249,53,53,"Moreover , our method uses boundary information achieves 29 % , 32 % and 29 % relative performance improve- ment over the baseline method ( "" LAB without boundary "" ) on COFW - 29 , AFLW - Full and AFLW - Frontal respectively .","Thanks to the generalization of facial boundaries , the estimator learned on 300 W can be conveniently used to supply boundary information for coordinate regression on COFW - 29 and AFLW dataset , even though these datasets have different annotation protocols .","Since COFW covers different level of occlusion and AFLW has significant view changes and challenging shape variations , the results emphasise the robustness brought by boundary information to occlusion , pose and shape variations .",0.8736842105263158,0.6463414634146342,0.9636363636363636,results,approach
face_alignment,5,experiments,ablation study,260,64,9,"It can be observed easily that boundary map ( "" BM "" ) is the most effective one .",We report the landmarks accuracy with oracle results in using different structure information .,Boundary information fusion is one of the key steps in our algorithm .,0.912280701754386,0.7804878048780488,0.3333333333333333,ablation-analysis,approach
face_alignment,5,experiments,ablation study,261,65,10,Boundary information fusion is one of the key steps in our algorithm .,"It can be observed easily that boundary map ( "" BM "" ) is the most effective one .",We can fuse boundary information at different levels for the regression network .,0.9157894736842104,0.7926829268292683,0.3703703703703704,ablation-analysis,approach
face_alignment,5,experiments,ablation study,263,67,12,"As indicated in , our final model that fuses boundary information in all four levels improves mean error from 7.12 % to 6.13 % .",We can fuse boundary information at different levels for the regression network .,"To evaluate the relationship between the quantity of boundary information fusion and the final prediction accuracy , we vary the number of fusion levels from 1 to 4 and report the mean error results in .",0.9228070175438596,0.8170731707317073,0.4444444444444444,ablation-analysis,baselines
face_alignment,5,experiments,ablation study,265,69,14,It can be observed that performance is improved consistently by fusing boundary heatmaps at more levels .,"To evaluate the relationship between the quantity of boundary information fusion and the final prediction accuracy , we vary the number of fusion levels from 1 to 4 and report the mean error results in .",Method,0.9298245614035088,0.8414634146341463,0.5185185185185185,ablation-analysis,baselines
face_alignment,5,experiments,ablation study,268,72,17,"The comparison between "" BL + HG "" and "" BL + HG/ B "" indicates the effectiveness of boundary information fusion rather than network structure changes .","BL BL+ FPG BL + FP BL + BM Mean Error 7 . 5.25 4.16 3.28 To verify the effectiveness of the fusion scheme shown in , we report the results of mean error on several settings in , i.e. , the baseline res - 18 network ( "" BL "" ) , hourglass module without boundary feature ( "" HG / B "" ) , hourglass module with boundary feature ( "" HG "" ) and consecutive convolutional layers with boundary feature ( "" CL "" ) .","The comparison between "" BL + HG "" and "" BL + CL "" indicates the effectiveness of the using hourglass structure design .",0.9403508771929824,0.8780487804878049,0.6296296296296297,ablation-analysis,experimental-setup
face_alignment,5,experiments,ablation study,269,73,18,"The comparison between "" BL + HG "" and "" BL + CL "" indicates the effectiveness of the using hourglass structure design .","The comparison between "" BL + HG "" and "" BL + HG/ B "" indicates the effectiveness of boundary information fusion rather than network structure changes .",Message passing plays a vital role for heatmap quality improvement when severe occlusions happen .,0.9438596491228072,0.8902439024390244,0.6666666666666666,ablation-analysis,baselines
face_alignment,5,experiments,ablation study,270,74,19,Message passing plays a vital role for heatmap quality improvement when severe occlusions happen .,"The comparison between "" BL + HG "" and "" BL + CL "" indicates the effectiveness of the using hourglass structure design .","As illustrated in on Occlusion Subset of WFLW , message passing , which combines information from visible boundaries and occluded ones , reduce the mean error over 11 % relatively .",0.9473684210526316,0.902439024390244,0.7037037037037037,ablation-analysis,approach
face_alignment,5,experiments,ablation study,272,76,21,Adversarial learning further improves the quality and effectiveness of boundary heatmaps .,"As illustrated in on Occlusion Subset of WFLW , message passing , which combines information from visible boundaries and occluded ones , reduce the mean error over 11 % relatively .","As illustrated in , heatmaps can be observed to be more focused and salience when adversarial loss is added .",0.9543859649122808,0.9268292682926828,0.7777777777777778,ablation-analysis,baselines
face_alignment,6,title,title,2,2,2,Face Alignment using a 3D Deeply - initialized Ensemble of Regression Trees, , ,0.0056980056980056,1.0,1.0,research-problem,approach
face_alignment,6,introduction,introduction,23,11,11,"In this paper we present the 3 DDE ( 3D Deeply - initialized Ensemble ) regressor , a robust and efficient face alignment algorithm based on a coarse - to - fine cascade of ERTs .","They are also much more efficient than deep models and , as we demonstrate in our experiments , with a good initialization they are also very accurate .","It is a hybrid approach that inherits good properties of ERT , such as the ability to impose a face shape prior , and the robustness of deep models .",0.0655270655270655,0.4583333333333333,0.4583333333333333,model,ablation-analysis
face_alignment,6,introduction,introduction,25,13,13,It is initialized by robustly fitting a 3 D face model to the probability maps produced by a CNN .,"It is a hybrid approach that inherits good properties of ERT , such as the ability to impose a face shape prior , and the robustness of deep models .","With this initialization we tackle one of the main drawbacks of ERT , namely the difficulty in initializing the regressor in the presence of occlusions and large face rotations .",0.0712250712250712,0.5416666666666666,0.5416666666666666,model,ablation-analysis
face_alignment,6,introduction,introduction,27,15,15,"On the other hand , the ERT implicitly imposes a prior face shape on the solution , addressing the shortcomings of deep models when occlusions and ambiguous face configurations are present .","With this initialization we tackle one of the main drawbacks of ERT , namely the difficulty in initializing the regressor in the presence of occlusions and large face rotations .","Finally , its coarse - to - fine structure tackles the combinatorial explosion of parts deformation , which is also a key limitation of approaches using shape constraints .",0.0769230769230769,0.625,0.625,model,ablation-analysis
face_alignment,6,introduction,introduction,28,16,16,"Finally , its coarse - to - fine structure tackles the combinatorial explosion of parts deformation , which is also a key limitation of approaches using shape constraints .","On the other hand , the ERT implicitly imposes a prior face shape on the solution , addressing the shortcomings of deep models when occlusions and ambiguous face configurations are present .",A preliminary version of our work appeared in .,0.0797720797720797,0.6666666666666666,0.6666666666666666,model,baselines
face_alignment,6,introduction,introduction,31,19,19,First we improve the initialization by using a RANSAC - like procedure that increases its robustness in the presence of occlusions .,Here we refine and extend it in several ways .,We have also introduced early stopping and better data augmentation techniques for increasing the regularization when training both the ERT and the CNN .,0.0883190883190883,0.7916666666666666,0.7916666666666666,model,experimental-setup
face_alignment,6,introduction,introduction,32,20,20,We have also introduced early stopping and better data augmentation techniques for increasing the regularization when training both the ERT and the CNN .,First we improve the initialization by using a RANSAC - like procedure that increases its robustness in the presence of occlusions .,We also extend the evaluation including the newly released WFLW database and a detailed ablation study .,0.0911680911680911,0.8333333333333334,0.8333333333333334,model,approach
face_alignment,6,experiments,implementation,210,27,6,"We use Adam stochastic optimization with ? 1 = 0.9 , ? 2 = 0.999 and = 1 e ? 8 parameters .","We generate different training samples in each epoch by applying random in plane rotations between 45 , scale changes by 15 % and translations by 5 % of bounding box size , randomly mirroring images horizontally and generating random rectangular occlusions .",We train until convergence with an initial learning rate ? = 0.001 .,0.5982905982905983,0.184931506849315,0.25,hyperparameters,approach
face_alignment,6,experiments,implementation,211,28,7,We train until convergence with an initial learning rate ? = 0.001 .,"We use Adam stochastic optimization with ? 1 = 0.9 , ? 2 = 0.999 and = 1 e ? 8 parameters .","When validation error levels out for 10 epochs , we multiply the learning rate by decay = 0.05 .",0.6011396011396012,0.1917808219178082,0.2916666666666667,hyperparameters,approach
face_alignment,6,experiments,implementation,212,29,8,"When validation error levels out for 10 epochs , we multiply the learning rate by decay = 0.05 .",We train until convergence with an initial learning rate ? = 0.001 .,In the CNN the cropped input face is reduced from 160160 to 11 pixels gradually dividing by half their size across B = 8 branches applying astride 2 convolution with kernel size 22 1 .,0.603988603988604,0.1986301369863013,0.3333333333333333,hyperparameters,approach
face_alignment,6,experiments,implementation,214,31,10,We apply batch normalization after each convolution .,In the CNN the cropped input face is reduced from 160160 to 11 pixels gradually dividing by half their size across B = 8 branches applying astride 2 convolution with kernel size 22 1 .,All layers contain 68 filters to describe the required landmark features .,0.6096866096866097,0.2123287671232877,0.4166666666666667,hyperparameters,approach
face_alignment,6,experiments,implementation,215,32,11,All layers contain 68 filters to describe the required landmark features .,We apply batch normalization after each convolution .,"We apply a Gaussian filter with ? = 33 to the output probability maps to stabilize the initialization , g 0 .",0.6125356125356125,0.2191780821917808,0.4583333333333333,hyperparameters,approach
face_alignment,6,experiments,implementation,216,33,12,"We apply a Gaussian filter with ? = 33 to the output probability maps to stabilize the initialization , g 0 .",All layers contain 68 filters to describe the required landmark features .,We train the coarse - to - fine ERT with the Gradient Boosting algorithm ) .,0.6153846153846154,0.226027397260274,0.5,hyperparameters,baselines
face_alignment,6,experiments,implementation,217,34,13,We train the coarse - to - fine ERT with the Gradient Boosting algorithm ) .,"We apply a Gaussian filter with ? = 33 to the output probability maps to stabilize the initialization , g 0 .",It requires a maximum of T = 20 stages of K = 50 regression trees per stage .,0.6182336182336182,0.2328767123287671,0.5416666666666666,hyperparameters,approach
face_alignment,6,experiments,implementation,218,35,14,It requires a maximum of T = 20 stages of K = 50 regression trees per stage .,We train the coarse - to - fine ERT with the Gradient Boosting algorithm ) .,The depth of trees is set to 4 .,0.6210826210826211,0.2397260273972602,0.5833333333333334,hyperparameters,approach
face_alignment,6,experiments,implementation,219,36,15,The depth of trees is set to 4 .,It requires a maximum of T = 20 stages of K = 50 regression trees per stage .,"The number of tests to choose the best split parameters , ? , is set to 200 .",0.6239316239316239,0.2465753424657534,0.625,hyperparameters,approach
face_alignment,6,experiments,implementation,220,37,16,"The number of tests to choose the best split parameters , ? , is set to 200 .",The depth of trees is set to 4 .,We resize each image to set the face size to 160160 pixels .,0.6267806267806267,0.2534246575342466,0.6666666666666666,hyperparameters,approach
face_alignment,6,experiments,implementation,221,38,17,We resize each image to set the face size to 160160 pixels .,"The number of tests to choose the best split parameters , ? , is set to 200 .","For feature extraction , the FREAK pattern diameter is reduced gradually in each stage ( i.e. , in the last stages the pixel pairs for each feature are closer ) .",0.6296296296296297,0.2602739726027397,0.7083333333333334,hyperparameters,experimental-setup
face_alignment,6,experiments,implementation,223,40,19,We generate Z = 25 initializations in the robust soft POSIT scheme of g 0 .,"For feature extraction , the FREAK pattern diameter is reduced gradually in each stage ( i.e. , in the last stages the pixel pairs for each feature are closer ) .","We augment the shapes of each face training image to create a set , SA , of at least N A = 60000 samples to train the cascade .",0.6353276353276354,0.273972602739726,0.7916666666666666,hyperparameters,baselines
face_alignment,6,experiments,implementation,224,41,20,"We augment the shapes of each face training image to create a set , SA , of at least N A = 60000 samples to train the cascade .",We generate Z = 25 initializations in the robust soft POSIT scheme of g 0 .,To avoid overfitting we use a shrinkage factor ? = 0.1 and subsampling factor ? = 0.5 in the ERT .,0.6381766381766382,0.2808219178082192,0.8333333333333334,hyperparameters,experimental-setup
face_alignment,6,experiments,implementation,225,42,21,To avoid overfitting we use a shrinkage factor ? = 0.1 and subsampling factor ? = 0.5 in the ERT .,"We augment the shapes of each face training image to create a set , SA , of at least N A = 60000 samples to train the cascade .","Our regressor triggers the coarse - to - fine strategy once the training error is below the validation error , e.g. , t = 5 in .",0.6410256410256411,0.2876712328767123,0.875,hyperparameters,approach
face_alignment,6,experiments,implementation,227,44,23,"Training the CNN and the coarse - to - fine ensemble of trees takes 48 hours using a NVidia GeForce GTX 1080 Ti ( 11 GB ) GPU and an dual Intel Xeon Silver 4114 CPU at 2.20 GHz ( 210 cores / 20 threads , 128 GB of RAM ) with a batch size of 32 images .","Our regressor triggers the coarse - to - fine strategy once the training error is below the validation error , e.g. , t = 5 in .","At runtime our method process test images on average at a rate of 12.5 FPS , where the CNN takes 75 ms and the ERT 5 ms per face image using C ++ , Tensorflow and OpenCV libraries .",0.6467236467236467,0.3013698630136986,0.9583333333333334,hyperparameters,baselines
face_alignment,6,experiments,experiments using public code,237,54,9,"Overall , 3 DDE is better than any other providing a public implementation in the literature .","The selected algorithms are representative of the three main families of solutions : a ) ensembles of regression trees ( c GPRT , RCPR , ERT ) , b) CNN - based approaches ( LAB , DAN , RCN ) and c ) mixed approaches with deep nets and ensembles of regression trees ( 3DDE , DCFE ) .","We improve over our preliminary algorithm , , because of the better 3D initialization and regularization ( see a complete analysis in section 4.5 ) .",0.6752136752136753,0.3698630136986301,0.6428571428571429,results,approach
face_alignment,6,experiments,experiments using public code,239,56,11,"In general we are able to improve by a large margin other ERT methods as RCPR , ERT or c GPRT because of the better initialization and the robust features provided by the CNN .","We improve over our preliminary algorithm , , because of the better 3D initialization and regularization ( see a complete analysis in section 4.5 ) .","We also outperform RCN ( without any denoising model ) , a CNN architecture like the one used in 3DDE .",0.6809116809116809,0.3835616438356164,0.7857142857142857,results,baselines
face_alignment,6,experiments,experiments using public code,240,57,12,"We also outperform RCN ( without any denoising model ) , a CNN architecture like the one used in 3DDE .","In general we are able to improve by a large margin other ERT methods as RCPR , ERT or c GPRT because of the better initialization and the robust features provided by the CNN .","Even DAN and LAB , that implement a cascade of CNN regressors , can not compete with the regularization obtained by using the cascade of ERT in 3 DDE ( see ) .",0.6837606837606838,0.3904109589041096,0.8571428571428571,results,approach
face_alignment,6,experiments,experiments using public code,241,58,13,"Even DAN and LAB , that implement a cascade of CNN regressors , can not compete with the regularization obtained by using the cascade of ERT in 3 DDE ( see ) .","We also outperform RCN ( without any denoising model ) , a CNN architecture like the one used in 3DDE .",The fact that the largest margin is in COFW reflects the importance of the implicit shape model in our cascade to address occlusions .,0.6866096866096866,0.3972602739726027,0.9285714285714286,results,baselines
face_alignment,6,experiments,experiments using published results,247,64,5,Our approach obtains the best overall performance in the indoor and outdoor subsets of the private competition ( see ) and in the full subset of the 300W public test set ( see ) .,First we test our method against the 300W benchmark .,This is due to the excellent accuracy achieved by the coarse - to - fine ERT scheme enforcing valid face shapes and the deep robust features extracted from the CNN .,0.7037037037037037,0.4383561643835616,0.119047619047619,results,baselines
face_alignment,6,experiments,experiments using published results,249,66,7,"In the challenging subset of the 300W public competition , SHN gets better results than 3DDE .",This is due to the excellent accuracy achieved by the coarse - to - fine ERT scheme enforcing valid face shapes and the deep robust features extracted from the CNN .,This is due to 3 DDE failing to estimate good landmark probability maps for images with large scale variations .,0.7094017094017094,0.4520547945205479,0.1666666666666666,results,approach
face_alignment,6,experiments,ablation study,296,113,12,"When combined with the cascaded ERT , the 3D initialization is key to achieve top overall performance , see CNN + MS + DE vs CNN + 3D + DE in the full subset .",Finally CF stands for using the coarseto - fine scheme .,"The reason for this is that , in the 3D case , the initialization takes care of the rigid component of face pose so that the ERT cascade only models non-rigid deformations .",0.8433048433048433,0.773972602739726,0.4137931034482759,ablation-analysis,baselines
face_alignment,6,experiments,ablation study,299,116,15,"Of course , the 3D initialization is fundamental to achieve good performance in presence of large face rotations .","Moreover , the projection of the 3D face model is a correct 2 D shape , a requirement for the ERT to converge to a valid face shape .","So , it provides the largest improvement in the pose subset .",0.8518518518518519,0.7945205479452054,0.5172413793103449,ablation-analysis,approach
face_alignment,6,experiments,ablation study,302,119,18,"The large receptive fields of CNNs are specially helpful in challenging situations , specifically those in the pose and occlusion subsets .",The use of CNN probability maps improves the NME in the full data set in about 20 % ( see CNN+ 3D + SE vs CNN + 3D + DE ) .,"The coarse - to - fine strategy in our cascaded ERT provides significative local improvements in difficult cases , with rare facial part combinations ( see ) .",0.8603988603988604,0.8150684931506851,0.6206896551724138,ablation-analysis,approach
face_alignment,6,experiments,ablation study,303,120,19,"The coarse - to - fine strategy in our cascaded ERT provides significative local improvements in difficult cases , with rare facial part combinations ( see ) .","The large receptive fields of CNNs are specially helpful in challenging situations , specifically those in the pose and occlusion subsets .","For this reason , the largest gain of CNN+3D+ DE + CF vs CNN+ 3D + DE occurs in the expressions subset .",0.8632478632478633,0.821917808219178,0.6551724137931034,ablation-analysis,approach
face_alignment,7,title,title,2,2,2,Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network, , ,0.0093457943925233,1.0,1.0,research-problem,experimental-setup
face_alignment,7,abstract,abstract,10,8,8,Code is available at https://github.com/YadiraF/PRNet.,Experiments on multiple challenging datasets show that our method surpasses other state - of - the - art methods on both reconstruction and alignment tasks by a large margin ., ,0.0467289719626168,1.0,1.0,code,baselines
face_alignment,7,introduction,introduction,12,2,2,3 D face reconstruction and face alignment are two fundamental and highly related topics in computer vision ., ,"In the last decades , researches in these two fields benefit each other .",0.0560747663551401,0.0666666666666666,0.0666666666666666,research-problem,experimental-setup
face_alignment,7,introduction,introduction,27,17,17,"In this paper , we propose an end - to - end method called Position map Regression Network ( PRN ) to jointly predict dense alignment and reconstruct 3 D face shape .","We find that these limitations do not exist in previous model - based methods , it motivates us to find anew approach to obtaining the 3D reconstruction and dense alignment simultaneously in a model - free manner .",Our method surpasses all other previous works on both 3 D face alignment and reconstruction on multiple datasets .,0.1261682242990654,0.5666666666666667,0.5666666666666667,model,experimental-setup
face_alignment,7,introduction,introduction,31,21,21,"Specifically , we design a UV position map , which is a 2D image recording the 3D coordinates of a complete facial point cloud , and at the same time keeping the semantic meaning at each UV place .",All of these are achieved by the elaborate design of the 2D representation of 3 D facial structure and the corresponding loss function .,We then train a simple encoder - decoder network with a weighted loss that focuses more on discriminative region to regress the UV position map from a single 2 D facial image .,0.1448598130841121,0.7,0.7,model,baselines
face_alignment,7,introduction,introduction,32,22,22,We then train a simple encoder - decoder network with a weighted loss that focuses more on discriminative region to regress the UV position map from a single 2 D facial image .,"Specifically , we design a UV position map , which is a 2D image recording the 3D coordinates of a complete facial point cloud , and at the same time keeping the semantic meaning at each UV place .","Figure1 shows our method is robust to poses , illuminations and occlusions .",0.1495327102803738,0.7333333333333333,0.7333333333333333,model,ablation-analysis
face_alignment,7,proposed method,training details,146,68,12,"For optimization , we use Adam optimizer with a learning rate begins at 0.0001 and decays half after each 5 epochs .",We use the network described 3 to train our transfer model .,The batch size is set as 16 .,0.6822429906542056,0.9714285714285714,0.8571428571428571,hyperparameters,experimental-setup
face_alignment,7,proposed method,training details,147,69,13,The batch size is set as 16 .,"For optimization , we use Adam optimizer with a learning rate begins at 0.0001 and decays half after each 5 epochs .",All of our training codes are implemented with TensorFlow .,0.6869158878504673,0.9857142857142858,0.9285714285714286,hyperparameters,experimental-setup
face_alignment,7,proposed method,training details,148,70,14,All of our training codes are implemented with TensorFlow .,The batch size is set as 16 ., ,0.6915887850467289,1.0,1.0,hyperparameters,experimental-setup
face_alignment,7,experimental results,3d face alignment,168,20,1,3D Face Alignment, , ,0.7850467289719626,0.3333333333333333,0.04,results,approach
face_alignment,7,experimental results,3d face alignment,172,24,5,"As shown in figure 5 , our result slightly outperforms the state - of - the - art method 3D - FAN when calculating per distance with 2D coordinates .","Firstly , we evaluate our method on a sparse set of 68 facial landmarks , and compare our result with 3 DDFA , DeFA and 3D - FAN on dataset AFLW2000 - 3D .","When considering the depth value , the performance discrepancy between our method and 3D - FAN increases .",0.8037383177570093,0.4,0.2,results,experimental-setup
face_alignment,7,experimental results,3d face alignment,173,25,6,"When considering the depth value , the performance discrepancy between our method and 3D - FAN increases .","As shown in figure 5 , our result slightly outperforms the state - of - the - art method 3D - FAN when calculating per distance with 2D coordinates .","Notice that , the 3D - FAN needs another network to predict the z coordinate of landmarks , while the depth value can be obtained directly in our method .",0.8084112149532711,0.4166666666666667,0.24,results,experimental-setup
face_alignment,7,experimental results,3d face alignment,178,30,11,The result shows that our method is robust to the change of pose and datasets .,"Follow the work , we also randomly select 696 faces from AFLW2000 to balance the distribution .","Although all the state - of - the - art methods of face alignment conduct evaluation on AFLW2000 - 3D dataset , the ground truth of this dataset is still controversial due to its annotation pipeline which is based on Landmarks Marching method in .",0.8317757009345794,0.5,0.44,results,approach
face_alignment,7,experimental results,3d face alignment,184,36,17,Examples from AFLW2000 - 3 D dataset show that our predictions are more accurate than ground truth in some cases .,"The first best result in each category is highlighted in bold , the lower is the better . :",Green : predicted landmarks by our method .,0.8598130841121495,0.6,0.68,results,approach
face_alignment,7,experimental results,3d face alignment,189,41,22,"As shown in figure 7 , our method outperforms the best methods with a large margin of more than 27 % on both 2 D and 3D coordinates . : CED curves on AFLW2000 - 3D .","In order to compare different methods with the same set of points , we select the points from the largest common face region provided by all methods , and finally around 45 K points were used for the evaluation .",Evaluation is performed on all points with both the 2D ( left ) and 3D ( right ) coordinates .,0.883177570093458,0.6833333333333333,0.88,results,experimental-setup
face_alignment,7,experimental results,ablation study,207,59,5,Network trained without using weight mask has worst performance compared with other two settings .,The results are shown in .,"By adding weights to specific regions such as 68 facial landmarks or central face region , weight ratio 3 shows considerable improvement on 68 points datasets over weight ratio 2 .",0.9672897196261684,0.9833333333333332,0.8333333333333334,ablation-analysis,model
face_alignment,7,experimental results,ablation study,208,60,6,"By adding weights to specific regions such as 68 facial landmarks or central face region , weight ratio 3 shows considerable improvement on 68 points datasets over weight ratio 2 .",Network trained without using weight mask has worst performance compared with other two settings ., ,0.97196261682243,1.0,1.0,ablation-analysis,experimental-setup
face_alignment,8,title,title,2,2,2,Joint 3D Face Reconstruction and Dense Face Alignment from A Single Image with 2D - Assisted Self - Supervised Learning, , ,0.0070422535211267,1.0,1.0,research-problem,experimental-setup
face_alignment,8,abstract,abstract,9,7,7,3 D face reconstruction from a single 2D image is a challenging problem with broad applications .,Best viewed in color .,Recent methods typically aim to learn a CNN - based 3 D face model that regresses coefficients of 3D Morphable Model ( 3 DMM ) from 2D images to render 3 D face reconstruction or dense face alignment .,0.0316901408450704,0.3684210526315789,0.3684210526315789,research-problem,baselines
face_alignment,8,abstract,abstract,23,2,2,3 D face reconstruction from a single 2D image is a challenging problem with broad applications ., ,Recent methods typically aim to learn a CNN - based 3 D face model that regresses coefficients of 3D Morphable Model ( 3 DMM ) from 2D images to render 3 D face reconstruction or dense face alignment .,0.0809859154929577,0.25,0.25,research-problem,baselines
face_alignment,8,introduction,introduction,31,2,2,3 D face reconstruction is an important task in the field of computer vision and graphics ., ,"For instance , the recovery of 3D face geometry from a single image can help address many challenges ( e.g. , large pose and occlusion ) for 2 D face alignment through dense face alignment .",0.1091549295774647,0.0606060606060606,0.0606060606060606,research-problem,experimental-setup
face_alignment,8,introduction,introduction,41,12,12,"In order to overcome the intrinsic limitation of existing 3 D face recovery models , we propose a novel learning method that leverages 2D "" in - the - wild "" face images to effectively supervise and facilitate the 3D face model learning .","Although some recent works bypass the 3 DMM parameter regression and use image - to - volume or image - to - image strategy instead , the ground truths are all still needed and generated from 3 DMM using 300W - LP , still lacking diversity .","With the method , the trained 3 D face model can perform 3 D face reconstruction and dense face alignment well .",0.1443661971830986,0.3636363636363637,0.3636363636363637,model,ablation-analysis
face_alignment,8,introduction,introduction,45,16,16,We design a novel self - supervised learning method that is able to train a 3 D face model with weak supervision from 2D images .,"Since these 2D images do not have any 3D annotations , it is not straightforward to exploit them in 3 D face model learning .","In particular , the proposed method takes the sparse annotated 2D landmarks as input and fully leverage the consistency within the 2 Dto - 2D and 3D - to - 3D self - mapping procedure as supervi-sion .",0.1584507042253521,0.4848484848484849,0.4848484848484849,model,experimental-setup
face_alignment,8,introduction,introduction,47,18,18,The model should be able to recover 2D landmarks from predicted 3D ones via direct 3D - to - 2D projection .,"In particular , the proposed method takes the sparse annotated 2D landmarks as input and fully leverage the consistency within the 2 Dto - 2D and 3D - to - 3D self - mapping procedure as supervi-sion .","Meanwhile , the 3D landmarks predicted from the annotated and recovered 2D landmarks via the model should be the same .",0.1654929577464788,0.5454545454545454,0.5454545454545454,model,model
face_alignment,8,introduction,introduction,49,20,20,"Additionally , our proposed method also exploits cycle - consistency over the 2D landmark predictions , i.e. , taking the recovered 2D landmarks as input , the model should be able to generate 2D landmarks ( by projecting its predicted 3D landmarks ) that have small difference with the annotated ones .","Meanwhile , the 3D landmarks predicted from the annotated and recovered 2D landmarks via the model should be the same .","By leveraging these self - supervision derived from 2 D face images without 3D annotations , our method could substantially improve the quality of learned 3 D face regression model , even though there is lack of 3D samples and no 3D annotations for the 2D samples .",0.1725352112676056,0.6060606060606061,0.6060606060606061,model,model
face_alignment,8,introduction,introduction,51,22,22,"To facilitate the overall learning procedure , our method also exploits self - critic learning .","By leveraging these self - supervision derived from 2 D face images without 3D annotations , our method could substantially improve the quality of learned 3 D face regression model , even though there is lack of 3D samples and no 3D annotations for the 2D samples .","It takes as input both the latent representation and 3 DMM coefficients of an face image and learns a critic model to evaluate the intrinsic consistency between the predicted 3 DMM coefficients and the corresponding face image , offering another supervision for 3 D face model learning .",0.1795774647887324,0.6666666666666666,0.6666666666666666,model,experimental-setup
face_alignment,8,experiments,training details and datasets,205,4,2,Our proposed 2 DASL is implemented with Pytorch ., ,"We use SGD optimizer for the CNN regressor with a learning rate beginning at 5 10 ?5 and decays exponentially , the discriminator uses the Adam as optimizer with the fixed learning rate 1 10 ?4 .",0.7218309859154929,0.0519480519480519,0.0869565217391304,experimental-setup,approach
face_alignment,8,experiments,training details and datasets,206,5,3,"We use SGD optimizer for the CNN regressor with a learning rate beginning at 5 10 ?5 and decays exponentially , the discriminator uses the Adam as optimizer with the fixed learning rate 1 10 ?4 .",Our proposed 2 DASL is implemented with Pytorch .,"The batch size is set as 32 . ? 1 , ? 2 , ? 3 and ?",0.7253521126760564,0.0649350649350649,0.1304347826086956,experimental-setup,experimental-setup
face_alignment,8,experiments,training details and datasets,209,8,6,We use a two - stage strategy to train our model .,"4 are set as 0.005 , 0.005 , 1 and 0.005 respectively .","In the first stage , we train the model using the overall loss L.",0.7359154929577465,0.1038961038961039,0.2608695652173913,experimental-setup,approach
face_alignment,8,experiments,training details and datasets,210,9,7,"In the first stage , we train the model using the overall loss L.",We use a two - stage strategy to train our model .,"In the second stage , we fine - tune our model using the Vertex Distance Cost , following .",0.7394366197183099,0.1168831168831168,0.3043478260869565,experimental-setup,ablation-analysis
face_alignment,8,experiments,training details and datasets,211,10,8,"In the second stage , we fine - tune our model using the Vertex Distance Cost , following .","In the first stage , we train the model using the overall loss L.",The dataset 300W - LP is used to train our model .,0.7429577464788732,0.1298701298701298,0.3478260869565217,experimental-setup,approach
face_alignment,8,experiments,dense face alignment,227,26,1,Dense face alignment, , ,0.7992957746478874,0.3376623376623377,0.032258064516129,results,approach
face_alignment,8,experiments,dense face alignment,239,38,13,"The results are shown in , where we can see our 2 DASL achieves the lowest NME ( % ) on the evaluation of both 2 D and 3D coordinates among all the methods .","Since PRNet and VRN - Guided are not 3 DMM based , and the point cloud of these two methods are not corresponding to 3 DMM , we only compare with them on the sparse 68 landmarks .","For 3 DMM - based methods : 3 DDFA and DeFA , our method outperforms them by a large margin on both the 68 spare landmarks and the dense coordinates .",0.8415492957746479,0.4935064935064935,0.4193548387096774,results,baselines
face_alignment,8,experiments,dense face alignment,240,39,14,"For 3 DMM - based methods : 3 DDFA and DeFA , our method outperforms them by a large margin on both the 68 spare landmarks and the dense coordinates .","The results are shown in , where we can see our 2 DASL achieves the lowest NME ( % ) on the evaluation of both 2 D and 3D coordinates among all the methods .","To further investigate performance of our 2 DASL across poses and datasets , we report the NME of faces with small , medium and large yaw angles on AFLW2000 - 3D dataset and the mean NME on both AFLW2000 - 3 D and AFLW - LPFA datasets .",0.8450704225352113,0.5064935064935064,0.4516129032258064,results,baselines
face_alignment,8,experiments,dense face alignment,245,44,19,"As can be observed , our method achieves the lowest mean NME on both of the two datasets , and the lowest NME across all poses on AFLW2000 - 3D .",The results of the compared method are directly from their published papers .,"Our 2DASL even performs better than PRNet , reducing NME by 0.09 and 0.08 on AFLW2000 - 3D and AFLW - LFPA , respectively .",0.8626760563380281,0.5714285714285714,0.6129032258064516,results,baselines
face_alignment,8,experiments,dense face alignment,246,45,20,"Our 2DASL even performs better than PRNet , reducing NME by 0.09 and 0.08 on AFLW2000 - 3D and AFLW - LFPA , respectively .","As can be observed , our method achieves the lowest mean NME on both of the two datasets , and the lowest NME across all poses on AFLW2000 - 3D .","Es - pecially on large poses ( from 60 to 90 ) , 2 DASL achieves 0.2 lower NME than PRNet .",0.8661971830985915,0.5844155844155844,0.6451612903225806,results,baselines
face_alignment,8,experiments,dense face alignment,247,46,21,"Es - pecially on large poses ( from 60 to 90 ) , 2 DASL achieves 0.2 lower NME than PRNet .","Our 2DASL even performs better than PRNet , reducing NME by 0.09 and 0.08 on AFLW2000 - 3D and AFLW - LFPA , respectively .","We believe more "" in - the -wild "" face images used for training ensures better performance of 2DASL .",0.8697183098591549,0.5974025974025974,0.6774193548387096,results,baselines
face_alignment,8,experiments,dense face alignment,249,48,23,3 D face reconstruction,"We believe more "" in - the -wild "" face images used for training ensures better performance of 2DASL .","In this section , we evaluate our 2 DASL on the task of 3D face reconstruction on AFLW2000 - 3D by comparing with 3 DDFA and DeFA .",0.8767605633802817,0.6233766233766234,0.7419354838709677,results,experimental-setup
face_alignment,8,experiments,dense face alignment,255,54,29,"As can be seen , the 3D reconstruction results of 2 DASL outperforms 3 DDFA by 0.39 , and 2.29 for DeFA , which are significant improvements .",shows the comparison results on AFLW2000 - 3D .,We show some visual results of our 2 DASL and compare with PRNet and VRN - Guided in .,0.897887323943662,0.7012987012987013,0.935483870967742,results,baselines
face_alignment,8,experiments,ablation study,265,64,8,"2 . Adding weights to central points of the facial landmarks reduces the NME by 0.09 to 0.23 on the two stages , respectively .",The ablation study results are shown in Tab .,Both self - critic and the self - supervision are effective to improve the performance .,0.9330985915492958,0.8311688311688312,0.3809523809523809,ablation-analysis,approach
face_alignment,8,experiments,ablation study,267,66,10,"If the self - critic learning is not used , the NME increases by 0.04/0.18 for with / without weight mask , respectively .",Both self - critic and the self - supervision are effective to improve the performance .,"While the self - supervision scheme reduce NME by 0.1 when the weight mask is used , and 0.23 if the weight mask is removed , no significant improvement is observed .",0.9401408450704224,0.8571428571428571,0.4761904761904762,ablation-analysis,approach
face_alignment,8,experiments,ablation study,268,67,11,"While the self - supervision scheme reduce NME by 0.1 when the weight mask is used , and 0.23 if the weight mask is removed , no significant improvement is observed .","If the self - critic learning is not used , the NME increases by 0.04/0.18 for with / without weight mask , respectively .",The best result is achieved when both these two modules are used .,0.943661971830986,0.8701298701298701,0.5238095238095238,ablation-analysis,baselines
face_alignment,8,experiments,ablation study,270,69,13,"Moreover , in our experiments , we found taking the FLMs as input can accelerate the convergence of training process .",The best result is achieved when both these two modules are used .,"Therefore , the first training stage just takes one or two epochs to reach a good model .",0.9507042253521126,0.8961038961038961,0.6190476190476191,ablation-analysis,approach
face_alignment,9,title,title,2,2,2,Semantic Alignment : Finding Semantically Consistent Ground - truth for Facial Landmark Detection, , ,0.0067796610169491,1.0,1.0,research-problem,experimental-setup
face_alignment,9,introduction,introduction,30,17,17,"In this paper , we propose a novel Semantic Alignment method which reduces the ' semantic ambiguity ' intrinsi-cally .","Clearly these noises can make the network training trapped into local minima , leading to degraded results .",We assume that there exist ' real ' ground - truths which are semantically consistent and more accurate than human annotations provided by databases .,0.1016949152542373,0.68,0.68,model,experimental-setup
face_alignment,9,introduction,introduction,32,19,19,"We model the ' real ' ground - truth as a latent variable to optimize , and the optimized ' real ' ground - truth then supervises the landmark detection network training .",We assume that there exist ' real ' ground - truths which are semantically consistent and more accurate than human annotations provided by databases .,"Accordingly , we propose a probabilistic model which can simultaneously search the ' real ' ground - truth and train the landmark detection network in an end - to - end way .",0.1084745762711864,0.76,0.76,model,approach
face_alignment,9,introduction,introduction,33,20,20,"Accordingly , we propose a probabilistic model which can simultaneously search the ' real ' ground - truth and train the landmark detection network in an end - to - end way .","We model the ' real ' ground - truth as a latent variable to optimize , and the optimized ' real ' ground - truth then supervises the landmark detection network training .","In this probabilistic model , the prior model is to constrain the latent variable to be close to the observations of the ' real ' ground truth , one of which is the human annotation .",0.111864406779661,0.8,0.8,model,ablation-analysis
face_alignment,9,introduction,introduction,34,21,21,"In this probabilistic model , the prior model is to constrain the latent variable to be close to the observations of the ' real ' ground truth , one of which is the human annotation .","Accordingly , we propose a probabilistic model which can simultaneously search the ' real ' ground - truth and train the landmark detection network in an end - to - end way .",The likelihood model is to reduce the Pearson Chi-square distance between the expected and the predicted distributions of ' real ' ground - truth .,0.1152542372881356,0.84,0.84,model,ablation-analysis
face_alignment,9,introduction,introduction,35,22,22,The likelihood model is to reduce the Pearson Chi-square distance between the expected and the predicted distributions of ' real ' ground - truth .,"In this probabilistic model , the prior model is to constrain the latent variable to be close to the observations of the ' real ' ground truth , one of which is the human annotation .",The heatmap generated by the hourglass architecture represents the confidence of each pixel and this confidence distribution is used to model the predicted distribution of likelihood .,0.1186440677966101,0.88,0.88,model,model
face_alignment,9,introduction,introduction,36,23,23,The heatmap generated by the hourglass architecture represents the confidence of each pixel and this confidence distribution is used to model the predicted distribution of likelihood .,The likelihood model is to reduce the Pearson Chi-square distance between the expected and the predicted distributions of ' real ' ground - truth .,"Apart from the proposed probabilistic framework , we further propose a global heatmap correction unit ( GHCU ) which maintains the global face shape constraint and recovers the unconfidently predicted landmarks caused by challenging factors such as occlusions and low resolution of images .",0.1220338983050847,0.92,0.92,model,model
face_alignment,9,introduction,introduction,37,24,24,"Apart from the proposed probabilistic framework , we further propose a global heatmap correction unit ( GHCU ) which maintains the global face shape constraint and recovers the unconfidently predicted landmarks caused by challenging factors such as occlusions and low resolution of images .",The heatmap generated by the hourglass architecture represents the confidence of each pixel and this confidence distribution is used to model the predicted distribution of likelihood .,"We conduct experiments on 300W , AFLW and 300 - VW databases and achieve the state of the art performance .",0.1254237288135593,0.96,0.96,model,ablation-analysis
face_alignment,9,experiments,experiments,201,19,19,"To perform data augmentation , we randomly sample the angle of rotation and the bounding box scale from Gaussian distribution .","In our experiments , all the training and testing images are cropped and resized to 256256 according to the provided bounding boxes .",We use a four - stage stacked hourglass network as our backbone which is trained by the optimizer RMSprop .,0.6813559322033899,0.1743119266055046,0.6129032258064516,experimental-setup,approach
face_alignment,9,experiments,experiments,202,20,20,We use a four - stage stacked hourglass network as our backbone which is trained by the optimizer RMSprop .,"To perform data augmentation , we randomly sample the angle of rotation and the bounding box scale from Gaussian distribution .","As described in Section 4 , our algorithm comprises two parts : network training and real groundtruth searching , which are alternatively optimized .",0.6847457627118644,0.1834862385321101,0.6451612903225806,experimental-setup,approach
face_alignment,9,experiments,experiments,207,25,25,"When training the roughly converged model with human annotations , the initial learning rate is 2.5 10 ?4 which is decayed to 2.5 10 ? 6 after 120 epochs .",to supervise the network training .,"When training with Semantic Alignment from the beginning of the aforementioned roughly converged model , the initial learning rate is 2.5 10 ? 6 and is divided by 5 , 2 and 2 at epoch 30 , 60 and 90 respectively .",0.7016949152542373,0.2293577981651376,0.8064516129032258,experimental-setup,approach
face_alignment,9,experiments,experiments,208,26,26,"When training with Semantic Alignment from the beginning of the aforementioned roughly converged model , the initial learning rate is 2.5 10 ? 6 and is divided by 5 , 2 and 2 at epoch 30 , 60 and 90 respectively .","When training the roughly converged model with human annotations , the initial learning rate is 2.5 10 ?4 which is decayed to 2.5 10 ? 6 after 120 epochs .","During semantic alignment , we search the latent variable ?",0.7050847457627119,0.2385321100917431,0.8387096774193549,experimental-setup,approach
face_alignment,9,experiments,experiments,211,29,29,We set batch size to 10 for network training .,"from a 1717 region centered at the current observation point o , and we crop a no larger than 2525 patch from the predicted heatmap around current position for Pearson Chi - square test in Eq. ( 4 ) .","For GHCU , the network architecture is shown in Tab .",0.7152542372881356,0.2660550458715596,0.935483870967742,experimental-setup,approach
face_alignment,9,experiments,experiments,213,31,31,1 . All our models are trained with PyTorch [ 18 ] on 2 Titan X GPUs .,"For GHCU , the network architecture is shown in Tab .", ,0.7220338983050848,0.2844036697247706,1.0,experimental-setup,experiments
face_alignment,9,experiments,comparison experiment,215,33,2,300 W ., ,We compare our approach against the state - of the - art methods on 300W in Tab .,0.7288135593220338,0.3027522935779817,0.0666666666666666,results,baselines
face_alignment,9,experiments,comparison experiment,220,38,7,"2 , we can see that HGs with our Semantic Alignment ( HGs + SA ) greatly outperform hourglass ( HGs ) only , 4.37 % vs 5.04 % in terms of NME on Full set , showing the great effectiveness of our Semantic Alignment ( SA ) .",From Tab.,"By adding GHCU , we can see that HGs + SA + GHCU slightly outperforms the HGs + SA .",0.7457627118644068,0.3486238532110092,0.2333333333333333,results,baselines
face_alignment,9,experiments,comparison experiment,221,39,8,"By adding GHCU , we can see that HGs + SA + GHCU slightly outperforms the HGs + SA .","2 , we can see that HGs with our Semantic Alignment ( HGs + SA ) greatly outperform hourglass ( HGs ) only , 4.37 % vs 5.04 % in terms of NME on Full set , showing the great effectiveness of our Semantic Alignment ( SA ) .","The improvement is not significant because the images of 300W are of high resolution , while GHCU works particularly well for images of low resolution and occlusions verified in the following evaluations .",0.7491525423728813,0.3577981651376147,0.2666666666666666,results,baselines
face_alignment,9,experiments,comparison experiment,223,41,10,"Following and which normalize the in - plane - rotation by training a preprocessing network , we conduct this normalization ( HGs + SA + GHCU + Norm ) and achieve state of the art performance on Challenge set and Full set : 6.38 % and 4.02 % .","The improvement is not significant because the images of 300W are of high resolution , while GHCU works particularly well for images of low resolution and occlusions verified in the following evaluations .","In particular , on Challenge set , we significantly outperform the state of the art method : 6.38 % ( HGs + SA +GHCU + Norm ) vs 6.98 % ( LAB ) , meaning that our method is particularly effective on challenging scenarios .",0.7559322033898305,0.3761467889908257,0.3333333333333333,results,baselines
face_alignment,9,experiments,comparison experiment,224,42,11,"In particular , on Challenge set , we significantly outperform the state of the art method : 6.38 % ( HGs + SA +GHCU + Norm ) vs 6.98 % ( LAB ) , meaning that our method is particularly effective on challenging scenarios .","Following and which normalize the in - plane - rotation by training a preprocessing network , we conduct this normalization ( HGs + SA + GHCU + Norm ) and achieve state of the art performance on Challenge set and Full set : 6.38 % and 4.02 % .",AFLW . 300W has 68 facial points which contain many weak semantic landmarks ( e.g. those on face contours ) .,0.7593220338983051,0.3853211009174312,0.3666666666666665,results,baselines
face_alignment,9,experiments,comparison experiment,225,43,12,AFLW . 300W has 68 facial points which contain many weak semantic landmarks ( e.g. those on face contours ) .,"In particular , on Challenge set , we significantly outperform the state of the art method : 6.38 % ( HGs + SA +GHCU + Norm ) vs 6.98 % ( LAB ) , meaning that our method is particularly effective on challenging scenarios .","In comparison , AFLW has only 19 points , most of which are strong semantic landmarks .",0.7627118644067796,0.3944954128440367,0.4,results,experimental-setup
face_alignment,9,experiments,comparison experiment,230,48,17,"As shown in Tab. 3 , HGs + SA outperforms",LAB used additional boundary information from outside database .,"HGs , 1.62 % vs 1.95 % .",0.7796610169491526,0.4403669724770641,0.5666666666666667,results,baselines
face_alignment,9,experiments,comparison experiment,231,49,18,"HGs , 1.62 % vs 1.95 % .","As shown in Tab. 3 , HGs + SA outperforms","It means that even though corner points are easily to be recognized , there is still random error in annotation , which can be corrected by SA .",0.7830508474576271,0.4495412844036697,0.6,results,model
face_alignment,9,experiments,comparison experiment,233,51,20,It is also observed that HGs + SA + GHCU works better than HGs + SA .,"It means that even though corner points are easily to be recognized , there is still random error in annotation , which can be corrected by SA .",300 - VW .,0.7898305084745763,0.4678899082568808,0.6666666666666666,results,baselines
face_alignment,9,experiments,comparison experiment,234,52,21,300 - VW .,It is also observed that HGs + SA + GHCU works better than HGs + SA .,"Unlike the image - based databases 300 W and AFLW , 300 - VW is video - based database , which is more challenging because the frame is of low resolution and with strong occlusions .",0.7932203389830509,0.4770642201834863,0.7,results,model
face_alignment,9,experiments,comparison experiment,238,56,25,"4 , we can see that HGs + SA greatly outperforms HGs in each of these three test sets .",From Tab.,"Furthermore , compared with HGs + SA , HGs + SA + GHCU reduce the error rate ( RMSE ) by 18 % on Category 3 test set , meaning that GHCU is very effective for video - based challenges such as low resolution and occlusions because .",0.8067796610169492,0.5137614678899083,0.8333333333333334,results,baselines
face_alignment,9,experiments,comparison experiment,239,57,26,"Furthermore , compared with HGs + SA , HGs + SA + GHCU reduce the error rate ( RMSE ) by 18 % on Category 3 test set , meaning that GHCU is very effective for video - based challenges such as low resolution and occlusions because .","4 , we can see that HGs + SA greatly outperforms HGs in each of these three test sets .",Comparison with state of the art on AFLW dataset .,0.8101694915254237,0.5229357798165137,0.8666666666666667,results,baselines
face_alignment,9,experiments,global heatmap correction unit,287,105,7,"To verify the effectiveness of different components in our framework , we conduct this ablation study on 300 - VW .",Ablation study .,"For a fair comparison , all the experiments use the same parameter settings .",0.9728813559322034,0.963302752293578,0.6363636363636364,ablation-analysis,baselines
face_alignment,9,experiments,global heatmap correction unit,290,108,10,"9 , Semantic alignment can consistently improve the performance on all subset sets , demonstrating the strong generalization capacity of SA .",As shown in Tab .,"GHCU is more effective on the challenge data set ( Category 3 ) : 8.15 % vs 9.91 % ; Combining SA and GHCU works better than single of them , showing the complementary of these two mechanisms .",0.9830508474576272,0.9908256880733946,0.9090909090909092,ablation-analysis,baselines
face_alignment,9,experiments,global heatmap correction unit,291,109,11,"GHCU is more effective on the challenge data set ( Category 3 ) : 8.15 % vs 9.91 % ; Combining SA and GHCU works better than single of them , showing the complementary of these two mechanisms .","9 , Semantic alignment can consistently improve the performance on all subset sets , demonstrating the strong generalization capacity of SA .", ,0.9864406779661016,1.0,1.0,ablation-analysis,baselines
face_detection,0,title,title,2,2,2,Accurate Face Detection for High Performance, , ,0.0135135135135135,1.0,1.0,research-problem,experimental-setup
face_detection,0,introduction,introduction,23,14,14,"In this work , we first modify the popular one - stage RetinaNet method to perform face detection as our baseline model .",Some works introduce the attention mechanism on the feature maps to focus on face regions for better detection performance .,Then some recent tricks are applied on this baseline to develop a high performance face detector namely AInnoFace :,0.1554054054054054,0.8235294117647058,0.8235294117647058,model,ablation-analysis
face_detection,0,introduction,introduction,24,15,15,Then some recent tricks are applied on this baseline to develop a high performance face detector namely AInnoFace :,"In this work , we first modify the popular one - stage RetinaNet method to perform face detection as our baseline model .",( 1 ) Employing the two - step classification and regression for detection ; ( 2 ) Applying the Intersection over Union ( IoU ) loss function for regression ; ( 3 ) Revisiting the data augmentation based on data - anchor - sampling for training ; ( 4 ) Utilizing the max - out operation for robuster classification ; ( 5 ) Using the multi-scale testing strategy for inference .,0.1621621621621621,0.8823529411764706,0.8823529411764706,model,model
face_detection,0,introduction,introduction,25,16,16,( 1 ) Employing the two - step classification and regression for detection ; ( 2 ) Applying the Intersection over Union ( IoU ) loss function for regression ; ( 3 ) Revisiting the data augmentation based on data - anchor - sampling for training ; ( 4 ) Utilizing the max - out operation for robuster classification ; ( 5 ) Using the multi-scale testing strategy for inference .,Then some recent tricks are applied on this baseline to develop a high performance face detector namely AInnoFace :,"Consequently , we achieve some new state - of - the - art AP results on the challenging face detection benchmark WIDER FACE dataset .",0.1689189189189189,0.9411764705882352,0.9411764705882352,model,model
face_detection,0,experiment,optimization detail,134,21,2,The backbone network in the proposed AInnoFace detector is initialized by the pretrained model on the ImageNet dataset ., ,"We use the "" xavier "" method to randomly initialize the parameters in the newly added convolutional layers .",0.9054054054054054,0.7,0.2,experimental-setup,experimental-setup
face_detection,0,experiment,optimization detail,135,22,3,"We use the "" xavier "" method to randomly initialize the parameters in the newly added convolutional layers .",The backbone network in the proposed AInnoFace detector is initialized by the pretrained model on the ImageNet dataset .,"The stochastic gradient descent ( SGD ) algorithm is used to fine - tune the model with 0.9 momentum , 0.0001 weight decay and batch size 32 .",0.9121621621621622,0.7333333333333333,0.3,experimental-setup,experimental-setup
face_detection,0,experiment,optimization detail,136,23,4,"The stochastic gradient descent ( SGD ) algorithm is used to fine - tune the model with 0.9 momentum , 0.0001 weight decay and batch size 32 .","We use the "" xavier "" method to randomly initialize the parameters in the newly added convolutional layers .",The warmup strategy is applied to gradually ramp up the learning rate from 0.0003125 to 0.01 at the first 5 epochs .,0.9189189189189192,0.7666666666666667,0.4,experimental-setup,approach
face_detection,0,experiment,optimization detail,137,24,5,The warmup strategy is applied to gradually ramp up the learning rate from 0.0003125 to 0.01 at the first 5 epochs .,"The stochastic gradient descent ( SGD ) algorithm is used to fine - tune the model with 0.9 momentum , 0.0001 weight decay and batch size 32 .","After that , it switches to the regular learning rate schedule , i.e. , dividing by 10 at 100 and 120 epochs and ending at 130 epochs .",0.9256756756756755,0.8,0.5,experimental-setup,experimental-setup
face_detection,0,experiment,optimization detail,138,25,6,"After that , it switches to the regular learning rate schedule , i.e. , dividing by 10 at 100 and 120 epochs and ending at 130 epochs .",The warmup strategy is applied to gradually ramp up the learning rate from 0.0003125 to 0.01 at the first 5 epochs .,The full training and testing codes are built on the PyTorch library .,0.9324324324324323,0.8333333333333334,0.6,experimental-setup,experimental-setup
face_detection,0,experiment,optimization detail,139,26,7,The full training and testing codes are built on the PyTorch library .,"After that , it switches to the regular learning rate schedule , i.e. , dividing by 10 at 100 and 120 epochs and ending at 130 epochs .",shows the comparison of the proposed AInnoFace detector with twenty - seven state - of - the - art methods on both the validation and testing subsets based on the precision - recall curve and AP .,0.9391891891891893,0.8666666666666667,0.7,experimental-setup,baselines
face_detection,0,experiment,optimization detail,141,28,9,"As shown in , our face detector sets some new state - of - the - art results based on the AP score across the three subsets on both validation and testing subsets , i.e. , 97.0 % ( Easy ) , 96.1 % ( Medium ) and 91.8 % ( Hard ) for validation subset , and 96.5 % ( Easy ) , 95.7 % ( Medium ) and 91.2 % ( Hard ) for testing subset .",shows the comparison of the proposed AInnoFace detector with twenty - seven state - of - the - art methods on both the validation and testing subsets based on the precision - recall curve and AP .,These results outperform all the compared state - of - the - art methods and demonstrate the superiority of our AInnoFace detector .,0.9527027027027029,0.9333333333333332,0.9,results,baselines
face_detection,0,experiment,optimization detail,142,29,10,These results outperform all the compared state - of - the - art methods and demonstrate the superiority of our AInnoFace detector .,"As shown in , our face detector sets some new state - of - the - art results based on the AP score across the three subsets on both validation and testing subsets , i.e. , 97.0 % ( Easy ) , 96.1 % ( Medium ) and 91.8 % ( Hard ) for validation subset , and 96.5 % ( Easy ) , 95.7 % ( Medium ) and 91.2 % ( Hard ) for testing subset .", ,0.9594594594594594,0.9666666666666668,1.0,results,baselines
face_detection,1,introduction,introduction,10,2,2,"Detecting faces in an image is considered to be one of the most practical tasks in computer vision applications , and many studies are proposed from the beginning of the computer vision research .", ,"After the advent of deep neural networks , many face detection algorithms applying the deep network have reported significant performance improvement to the conventional face detectors .",0.0432900432900432,0.0588235294117647,0.0588235294117647,research-problem,experimental-setup
face_detection,1,introduction,introduction,11,3,3,"After the advent of deep neural networks , many face detection algorithms applying the deep network have reported significant performance improvement to the conventional face detectors .","Detecting faces in an image is considered to be one of the most practical tasks in computer vision applications , and many studies are proposed from the beginning of the computer vision research .",The state - of - the - art ( SOTA ) face detectors for in - the - wild images employ the framework of the recent object detectors .,0.0476190476190476,0.088235294117647,0.088235294117647,research-problem,experimental-setup
face_detection,1,introduction,introduction,28,20,20,"In this paper , we propose a new multi-scale face detector with extremely tiny size ( EXTD ) resolving the two mentioned problems .","To alleviate the problem , the variants of Feature pyramid network ( FPN ) architecture such as are used but requires additional parameters and memories for re-expanding the feature map .","The main discovery is that we can share the network in generating each feature - map , as shown in .",0.1212121212121212,0.5882352941176471,0.5882352941176471,model,experimental-setup
face_detection,1,introduction,introduction,29,21,21,"The main discovery is that we can share the network in generating each feature - map , as shown in .","In this paper , we propose a new multi-scale face detector with extremely tiny size ( EXTD ) resolving the two mentioned problems .","As in the figure , we design a backbone network such that reduces the size of the feature map by half , and we can get the other feature maps with recurrently passing the network .",0.1255411255411255,0.6176470588235294,0.6176470588235294,model,model
face_detection,1,introduction,introduction,30,22,22,"As in the figure , we design a backbone network such that reduces the size of the feature map by half , and we can get the other feature maps with recurrently passing the network .","The main discovery is that we can share the network in generating each feature - map , as shown in .","The sharing can significantly reduce the number of parameters , and this enables our model to use more layers to generate the low - level feature maps used for detecting small faces .",0.1298701298701298,0.6470588235294118,0.6470588235294118,model,approach
face_detection,1,introduction,introduction,31,23,23,"The sharing can significantly reduce the number of parameters , and this enables our model to use more layers to generate the low - level feature maps used for detecting small faces .","As in the figure , we design a backbone network such that reduces the size of the feature map by half , and we can get the other feature maps with recurrently passing the network .","Also , the proposed iterative architecture makes the network to observe the features from various scale of faces and from various layer locations , and hence offer abundant semantic information to the network , without adding additional parameters .",0.1341991341991342,0.6764705882352942,0.6764705882352942,model,model
face_detection,1,introduction,introduction,32,24,24,"Also , the proposed iterative architecture makes the network to observe the features from various scale of faces and from various layer locations , and hence offer abundant semantic information to the network , without adding additional parameters .","The sharing can significantly reduce the number of parameters , and this enables our model to use more layers to generate the low - level feature maps used for detecting small faces .","Our baseline framework follows FPN - like structures , but can also be applied to SSD - like architecture .",0.1385281385281385,0.7058823529411765,0.7058823529411765,model,baselines
face_detection,1,introduction,introduction,33,25,25,"Our baseline framework follows FPN - like structures , but can also be applied to SSD - like architecture .","Also , the proposed iterative architecture makes the network to observe the features from various scale of faces and from various layer locations , and hence offer abundant semantic information to the network , without adding additional parameters .","For SSD based architecture , we adopt the setting from .",0.1428571428571428,0.7352941176470589,0.7352941176470589,model,experimental-setup
face_detection,1,extd,training,142,67,21,Code will be available at https ://github.com/clovaai.,Please refer Appendix A to seethe detailed training and optimization settings for training the proposed network ., ,0.6147186147186147,1.0,1.0,code,baselines
face_detection,1,experiments,experimental setting,162,20,16,"Also , we tested different activation functions : ReLU , PReLU , and Leaky - ReLU for each model .",B for the detailed configuration of the backbone networks for each case .,"The negative slope of the Leaky - ReLU is set to 0.25 , which is identical to the initial negative slope of the PReLU .",0.7012987012987013,0.2597402597402597,0.7272727272727273,hyperparameters,approach
face_detection,1,experiments,experimental setting,163,21,17,"The negative slope of the Leaky - ReLU is set to 0.25 , which is identical to the initial negative slope of the PReLU .","Also , we tested different activation functions : ReLU , PReLU , and Leaky - ReLU for each model .","In the following section , we will term each variation by a combination of abbreviations ; EXTD - model - channel - activation .",0.7056277056277056,0.2727272727272727,0.7727272727272727,hyperparameters,baselines
face_detection,1,experiments,performance analysis,178,36,10,"The results in 138 times lighter in model size and are 28.3 , 19.2 , and 11 times lighter in Madds .",Comparison to the Existing Methods :,"When compared to SOTA face detectors such as Pyra - midBox and DSFD , our best model EXTD - FPN - 64 - PReLU achieved lower results .",0.7705627705627706,0.4675324675324675,0.3225806451612903,results,approach
face_detection,1,experiments,performance analysis,179,37,11,"When compared to SOTA face detectors such as Pyra - midBox and DSFD , our best model EXTD - FPN - 64 - PReLU achieved lower results .","The results in 138 times lighter in model size and are 28.3 , 19.2 , and 11 times lighter in Madds .",The margin between PyramidBox and the proposed model on WIDER FACE hard case was 3.4 % .,0.7748917748917749,0.4805194805194805,0.3548387096774194,results,approach
face_detection,1,experiments,performance analysis,180,38,12,The margin between PyramidBox and the proposed model on WIDER FACE hard case was 3.4 % .,"When compared to SOTA face detectors such as Pyra - midBox and DSFD , our best model EXTD - FPN - 64 - PReLU achieved lower results .","Considering that PyramidBox inherits from S3FD and our model follows the equivalent training and detection setting to S3FD , our model would have a possibility to further increase the detection performance by adding the schemes proposed in Pyramid Box .",0.7792207792207793,0.4935064935064935,0.3870967741935484,results,approach
face_detection,1,experiments,performance analysis,182,40,14,"The m AP gap to DSFD , which is tremendously heavier , is about 5.0 % , but it would be safe to suggest that the proposed method offers more decent trade - off in that DSFD uses about 2860 times more parameters than the proposed method .","Considering that PyramidBox inherits from S3FD and our model follows the equivalent training and detection setting to S3FD , our model would have a possibility to further increase the detection performance by adding the schemes proposed in Pyramid Box .",This is also meaningful result in that our method did not use any kind of pre-training of the backbone network using the other dataset such as Image Net.,0.7878787878787878,0.5194805194805194,0.4516129032258064,results,baselines
face_detection,1,experiments,performance analysis,186,44,18,"When it comes to our SSD - based variations , they got lower mAP results than FPN - based variants .","From the graphs , we can see that our method is included in the SOTA group of the detectors using heavyweight pre-trained backbone networks .","However , when compared with the S3FD version trained with Mo - bile FaceNet backbone network , the proposed SSD variants achieved comparable or better detection performance .",0.8051948051948052,0.5714285714285714,0.5806451612903226,results,approach
face_detection,1,experiments,performance analysis,187,45,19,"However , when compared with the S3FD version trained with Mo - bile FaceNet backbone network , the proposed SSD variants achieved comparable or better detection performance .","When it comes to our SSD - based variations , they got lower mAP results than FPN - based variants .","It is a meaningful result in that the proposed variations have smaller feature map width , S3FD - Mobile Face Net holds feature map size of , and use the smaller number of layer blocks ; inverted residual blocks same as MobileFaceNet , repeatedly .",0.8095238095238095,0.5844155844155844,0.6129032258064516,results,baselines
face_detection,1,experiments,performance analysis,196,54,28,"From the table , we can see that our method achieved higher performance in WIDER FACE hard dataset than other cases .",One notable characteristic of the proposed method captured from the evaluation is that our detector obtained better performance when dealing with a small size of faces .,"Since the Easy and Medium cases are subsets of the Hard dataset , this means that the proposed method is especially fitted to capture small sized faces .",0.8484848484848485,0.7012987012987013,0.9032258064516128,results,approach
face_detection,1,experiments,variation analysis,204,62,5,"First , for all the different channel width , FPN based architecture achieved better detection performance compared to SSD based architecture , especially for detecting small faces .","From the table , we can find two common observations among the proposed variations .","The idea of expanding the number of layers for reaching the largest sized feature - map , for detecting the smallest size of objects , is a common strategy for SSD variant methods .",0.8831168831168831,0.8051948051948052,0.25,results,baselines
face_detection,1,experiments,variation analysis,214,72,15,"When tested with SSD based architecture , PReLU outperformed Leaky - ReLU with larger margin than those using FPN structure .","In all the cases including FPN based and SSD based structures , PReLU was the most effective choice when it comes to mAP , but the gap between Leaky - ReLU was not that significant for the FPN variants .",It is worth noting that ReLU occurred notable performance decreases especially when the channel width was small for both SSD and FPN cases .,0.9264069264069263,0.935064935064935,0.75,results,baselines
face_detection,10,abstract,abstract,4,2,2,"In this paper , we propose a novel face detection network with three novel contributions that address three key aspects of face detection , including better feature learning , progressive loss design and anchor assign based data augmentation , respectively .", ,"First , we propose a Feature Enhance Module ( FEM ) for enhancing the original feature maps to extend the single shot detector to dual shot detector .",0.0206185567010309,0.4,0.4,research-problem,baselines
face_detection,10,introduction,introduction,30,23,23,"In this paper , we propose three novel techniques to address the above three issues , respectively .","However , such strategy ignores random sampling in data augmentation , which still causes imbalance between positive and negative anchors .","First , we introduce a Feature Enhance Module ( FEM ) to enhance the discriminability and robustness of the features , which combines the advantages of the FPN in PyramidBox and Receptive Field Block ( RFB ) in RFBNet .",0.1546391752577319,0.6571428571428571,0.6571428571428571,model,approach
face_detection,10,introduction,introduction,31,24,24,"First , we introduce a Feature Enhance Module ( FEM ) to enhance the discriminability and robustness of the features , which combines the advantages of the FPN in PyramidBox and Receptive Field Block ( RFB ) in RFBNet .","In this paper , we propose three novel techniques to address the above three issues , respectively .","Second , motivated by the hierarchical loss and pyramid anchor in PyramidBox , we design Progressive Anchor Loss ( PAL ) that uses progressive anchor sizes for not only different levels , but also different shots .",0.1597938144329896,0.6857142857142857,0.6857142857142857,model,ablation-analysis
face_detection,10,introduction,introduction,32,25,25,"Second , motivated by the hierarchical loss and pyramid anchor in PyramidBox , we design Progressive Anchor Loss ( PAL ) that uses progressive anchor sizes for not only different levels , but also different shots .","First , we introduce a Feature Enhance Module ( FEM ) to enhance the discriminability and robustness of the features , which combines the advantages of the FPN in PyramidBox and Receptive Field Block ( RFB ) in RFBNet .","Specifically , we assign smaller anchor sizes in the first shot , and use larger sizes in the second shot .",0.1649484536082474,0.7142857142857143,0.7142857142857143,model,ablation-analysis
face_detection,10,introduction,introduction,33,26,26,"Specifically , we assign smaller anchor sizes in the first shot , and use larger sizes in the second shot .","Second , motivated by the hierarchical loss and pyramid anchor in PyramidBox , we design Progressive Anchor Loss ( PAL ) that uses progressive anchor sizes for not only different levels , but also different shots .","Third , we propose Improved Anchor Matching ( IAM ) , which integrates anchor partition strategy and anchor-based data augmentation to better match anchors and ground truth faces , and thus provides better initialization for the regressor .",0.1701030927835051,0.7428571428571429,0.7428571428571429,model,model
face_detection,10,introduction,introduction,34,27,27,"Third , we propose Improved Anchor Matching ( IAM ) , which integrates anchor partition strategy and anchor-based data augmentation to better match anchors and ground truth faces , and thus provides better initialization for the regressor .","Specifically , we assign smaller anchor sizes in the first shot , and use larger sizes in the second shot .",The three aspects are complementary so that these techniques can work together to further improve the performance .,0.1752577319587628,0.7714285714285715,0.7714285714285715,model,model
face_detection,10,introduction,introduction,36,29,29,"Besides , since these techniques are all related to two - stream design , we name the proposed network as Dual Shot Face Detector ( DSFD ) .",The three aspects are complementary so that these techniques can work together to further improve the performance .,"shows the effectiveness of DSFD on various variations , especially on extreme small faces or heavily occluded faces .",0.1855670103092784,0.8285714285714286,0.8285714285714286,model,approach
face_detection,10,experiments,implementation details,127,4,3,The backbone networks are initialized by the pretrained VGG / ResNet on Image Net .,"First , we present the details in implementing our network .",All newly added convolution layers ' parameters are initialized by the ' xavier ' method .,0.6546391752577321,0.0615384615384615,0.2307692307692308,experimental-setup,experimental-setup
face_detection,10,experiments,implementation details,128,5,4,All newly added convolution layers ' parameters are initialized by the ' xavier ' method .,The backbone networks are initialized by the pretrained VGG / ResNet on Image Net .,"We use SGD with 0.9 momentum , 0.0005 weight decay to fine - tune our DSFD model .",0.6597938144329897,0.0769230769230769,0.3076923076923077,experimental-setup,experimental-setup
face_detection,10,experiments,implementation details,129,6,5,"We use SGD with 0.9 momentum , 0.0005 weight decay to fine - tune our DSFD model .",All newly added convolution layers ' parameters are initialized by the ' xavier ' method .,The batch size is set to 16 .,0.6649484536082474,0.0923076923076923,0.3846153846153846,experimental-setup,baselines
face_detection,10,experiments,implementation details,130,7,6,The batch size is set to 16 .,"We use SGD with 0.9 momentum , 0.0005 weight decay to fine - tune our DSFD model .","The learning rate is set to 10 ?3 for the first 40 k steps , and we decay it to 10 ? 4 and 10 ? 5 for two 10 k steps .",0.6701030927835051,0.1076923076923077,0.4615384615384616,experimental-setup,experiments
face_detection,10,experiments,implementation details,131,8,7,"The learning rate is set to 10 ?3 for the first 40 k steps , and we decay it to 10 ? 4 and 10 ? 5 for two 10 k steps .",The batch size is set to 16 .,"During inference , the first shot 's outputs are ignored and the second shot predicts top 5 k high confident detections .",0.6752577319587629,0.123076923076923,0.5384615384615384,experimental-setup,experimental-setup
face_detection,10,experiments,implementation details,132,9,8,"During inference , the first shot 's outputs are ignored and the second shot predicts top 5 k high confident detections .","The learning rate is set to 10 ?3 for the first 40 k steps , and we decay it to 10 ? 4 and 10 ? 5 for two 10 k steps .",Non-maximum suppression is applied with jaccard overlap of 0.3 to produce top 750 high confident bounding boxes per image .,0.6804123711340206,0.1384615384615384,0.6153846153846154,experimental-setup,approach
face_detection,10,experiments,implementation details,133,10,9,Non-maximum suppression is applied with jaccard overlap of 0.3 to produce top 750 high confident bounding boxes per image .,"During inference , the first shot 's outputs are ignored and the second shot predicts top 5 k high confident detections .","For 4 bounding box coordinates , we round down top left coordinates and roundup width and height to expand the detection bounding box .",0.6855670103092784,0.1538461538461538,0.6923076923076923,experimental-setup,approach
face_detection,10,experiments,implementation details,134,11,10,"For 4 bounding box coordinates , we round down top left coordinates and roundup width and height to expand the detection bounding box .",Non-maximum suppression is applied with jaccard overlap of 0.3 to produce top 750 high confident bounding boxes per image .,The official code has been released at : https://github.com/TencentYoutuResearch/FaceDetection-DSFD .,0.6907216494845361,0.1692307692307692,0.7692307692307693,experimental-setup,approach
face_detection,10,experiments,implementation details,135,12,11,The official code has been released at : https://github.com/TencentYoutuResearch/FaceDetection-DSFD .,"For 4 bounding box coordinates , we round down top left coordinates and roundup width and height to expand the detection bounding box .","That means even with a higher threshold ( i.e. , 0.4 ) , using our IAM , we can still achieve more matched anchors .",0.6958762886597938,0.1846153846153846,0.8461538461538461,code,baselines
face_detection,10,experiments,analysis on dsfd,139,16,2,"In this subsection , we conduct extensive experiments and ablation studies on the WIDER FACE dataset to evaluate the effectiveness of several contributions of our proposed framework , including feature enhance module , progressive anchor loss , and improved anchor matching .", ,"For fair comparisons , we use the same parameter settings for all the experiments , except for the specified changes to the components .",0.7164948453608248,0.2461538461538461,0.1538461538461538,ablation-analysis,experimental-setup
face_detection,10,experiments,analysis on dsfd,143,20,6,"Feature Enhance Module First ,","To better understand DSFD , we select different baselines to ablate each component on how this part affects the final performance .","We adopt anchor designed in S3FD , PyramidBox and six original feature maps generated by VGG16 to perform classification and regression , which is named Face SSD ( FSSD ) as the baseline .",0.7371134020618557,0.3076923076923077,0.4615384615384616,ablation-analysis,approach
face_detection,10,experiments,analysis on dsfd,146,23,9,"shows that our feature enhance module can improve VGG16 - based FSSD from 92.6 % , 90.2 % , 79.1 % to 93.0 % , 91.4 % , 84.6 % .",We then use VGG16 - based FSSD as the baseline to add feature enchance module for comparison .,"Progressive Anchor Loss Second , we use Res50 - based FSSD as the baseline to add progressive anchor loss for comparison .",0.7525773195876289,0.3538461538461539,0.6923076923076923,ablation-analysis,baselines
face_detection,10,experiments,analysis on dsfd,147,24,10,"Progressive Anchor Loss Second , we use Res50 - based FSSD as the baseline to add progressive anchor loss for comparison .","shows that our feature enhance module can improve VGG16 - based FSSD from 92.6 % , 90.2 % , 79.1 % to 93.0 % , 91.4 % , 84.6 % .","We use four residual blocks ' ouputs in ResNet to replace the outputs of conv3 3 , conv4 3 , conv5 3 , conv fc7 in VGG .",0.7577319587628866,0.3692307692307693,0.7692307692307693,ablation-analysis,experimental-setup
face_detection,10,experiments,analysis on dsfd,150,27,13,"shows our progressive anchor loss can improve Res50 - based FSSD using FEM from 95.0 % , 94.1 % , 88.0 % to 95.3 % , 94.4 % , 88.6 % .","Except for VGG16 , we do not perform layer normalization .", ,0.7731958762886598,0.4153846153846154,1.0,ablation-analysis,model
face_detection,10,experiments,improved anchor matching,151,28,1,Improved Anchor Matching, , ,0.7783505154639175,0.4307692307692308,0.0434782608695652,ablation-analysis,approach
face_detection,10,experiments,improved anchor matching,153,30,3,"shows that our improved anchor matching can improve Res101 based FSSD using FEM from 95.8 % , 95.1 % , 89.7 % to 96.1 % , 95.2 % , 90.0 % .","To evaluate our improved anchor matching strategy , we use Res101 - based FSSD without anchor compensation as the baseline .","Finally , we can improve our DSFD to 96.6 % , 95.7 % , 90.4 % with ResNet 152 as the backbone . Besides , shows that our improved anchor matching strategy greatly increases the number of ground truth faces that are closed to the anchor , which can reduce the contradiction between the discrete anchor scales and continuous face scales .",0.788659793814433,0.4615384615384616,0.1304347826086956,ablation-analysis,baselines
face_detection,10,experiments,comparisons with state of the art methods,178,55,5,WIDER FACE Dataset,We also follow the similar way used in to build the image pyramid for multi-scale testing and use more powerful backbone similar as .,"It contains 393 , 703 annotated faces with large variations in scale , pose and occlusion in total 32 , 203 images .",0.9175257731958762,0.8461538461538461,0.3333333333333333,experiments,experimental-setup
face_detection,10,experiments,comparisons with state of the art methods,182,59,9,"As shown in , our DSFD achieves the best performance among all of the state - of - the - art face detectors based on the average precision ( AP ) across the three subsets , i.e. , 96.6 % ( Easy ) , 95.7 % ( Medium ) and 90.4 % ( Hard ) on validation set , and 96.0 % ( Easy ) , 95.3 % ( Medium ) and 90.0 % ( Hard ) on test set .","Besides , each subset is further defined into three levels of difficulty : ' Easy ' , ' Medium ' , ' Hard ' based on the detection rate of a baseline detector .","shows more examples to demonstrate the effects of DSFD on handling faces with various variations , in which the blue bounding boxes indicate the detector confidence is above 0.8 .",0.9381443298969072,0.9076923076923076,0.6,experiments,baselines
face_detection,10,experiments,comparisons with state of the art methods,184,61,11,FDDB Dataset,"shows more examples to demonstrate the effects of DSFD on handling faces with various variations , in which the blue bounding boxes indicate the detector confidence is above 0.8 .","It contains 5 , 171 faces in 2 , 845 images taken from the faces in the wild data set .",0.9484536082474226,0.9384615384615383,0.7333333333333333,experiments,experimental-setup
face_detection,10,experiments,comparisons with state of the art methods,187,64,14,"As shown in , our DSFD achieves state - of - the - art performance on both discontinuous and continuous ROC curves , i.e. 99.1 % and 86.2 % when the number of false positives equals to 1 , 000 .","Since WIDER FACE has bounding box annotation while faces in FDDB are represented by ellipses , we learn a post - hoc ellipses regressor to transform the final prediction results .","After adding additional annotations to those unlabeled faces , the false positives of our model can be further reduced and outperform all other methods .",0.963917525773196,0.9846153846153848,0.9333333333333332,experiments,baselines
face_detection,10,experiments,comparisons with state of the art methods,188,65,15,"After adding additional annotations to those unlabeled faces , the false positives of our model can be further reduced and outperform all other methods .","As shown in , our DSFD achieves state - of - the - art performance on both discontinuous and continuous ROC curves , i.e. 99.1 % and 86.2 % when the number of false positives equals to 1 , 000 .", ,0.9690721649484536,1.0,1.0,experiments,approach
face_detection,11,title,title,2,2,2,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection, , ,0.0071174377224199,1.0,1.0,research-problem,experimental-setup
face_detection,11,abstract,abstract,4,2,2,"A unified deep neural network , denoted the multi -scale CNN ( MS - CNN ) , is proposed for fast multi-scale object detection .", ,The MS - CNN consists of a proposal sub-network and a detection sub-network .,0.0142348754448398,0.05,0.05,research-problem,experimental-setup
face_detection,11,abstract,abstract,10,8,8,"State - of - the - art object detection performance , at up to 15 fps , is reported on datasets , such as KITTI and Caltech , containing a substantial number of small objects .","Feature upsampling by deconvolution is also explored , as an alternative to input upsampling , to reduce the memory and computation costs .",Introduction,0.0355871886120996,0.2,0.2,research-problem,baselines
face_detection,11,abstract,abstract,29,27,27,"This work proposes a unified multi-scale deep CNN , denoted the multi -scale CNN ( MS - CNN ) , for fast object detection .",This increases the memory and computation costs of the detector .,"Similar to , this network consists of two sub-networks : an object proposal network and an accurate detection network .",0.1032028469750889,0.675,0.675,model,experimental-setup
face_detection,11,abstract,abstract,30,28,28,"Similar to , this network consists of two sub-networks : an object proposal network and an accurate detection network .","This work proposes a unified multi-scale deep CNN , denoted the multi -scale CNN ( MS - CNN ) , for fast object detection .",Both of them are learned end - to - end and share computations .,0.1067615658362989,0.7,0.7,model,model
face_detection,11,abstract,abstract,31,29,29,Both of them are learned end - to - end and share computations .,"Similar to , this network consists of two sub-networks : an object proposal network and an accurate detection network .","However , to ease the inconsistency between the sizes of objects and receptive fields , object detection is performed with multiple output layers , each focusing on objects within certain scale ranges ( see ) .",0.1103202846975089,0.725,0.725,model,model
face_detection,11,abstract,abstract,32,30,30,"However , to ease the inconsistency between the sizes of objects and receptive fields , object detection is performed with multiple output layers , each focusing on objects within certain scale ranges ( see ) .",Both of them are learned end - to - end and share computations .,"The intuition is that lower network layers , such as "" conv - 3 , "" have smaller receptive fields , better matched to detect small objects .",0.1138790035587188,0.75,0.75,model,model
face_detection,11,abstract,abstract,35,33,33,The complimentary detectors at different output layers are combined to form a strong multi-scale detector .,"Conversely , higher layers , such as "" conv - 5 , "" are best suited for the detection of large objects .","This is shown to produce accurate object proposals on detection benchmarks with large variation of scale , such as KITTI , achieving a recall of over 95 % for only 100 proposals .",0.1245551601423487,0.825,0.825,model,baselines
face_detection,11,experimental evaluation,experimental evaluation,191,2,2,The performance of the MS - CNN detector was evaluated on the KITTI and Caltech Pedestrian benchmarks ., ,"These were chosen because , unlike VOC and ImageNet , they contain many small objects .",0.6797153024911032,0.024390243902439,0.1333333333333333,experimental-setup,approach
face_detection,11,experimental evaluation,experimental evaluation,202,13,13,"The detector was implemented in C ++ within the Caffe toolbox , and source code is available at https://github.com/zhaoweicai/mscnn.",The model configurations for original input size are shown in .,All times are reported for implementation on a single CPU core ( 2.40 GHz ) of an Intel Xeon E5 - 2630 server with 64 GB of RAM .,0.7188612099644128,0.1585365853658536,0.8666666666666667,code,baselines
face_detection,11,experimental evaluation,experimental evaluation,203,14,14,All times are reported for implementation on a single CPU core ( 2.40 GHz ) of an Intel Xeon E5 - 2630 server with 64 GB of RAM .,"The detector was implemented in C ++ within the Caffe toolbox , and source code is available at https://github.com/zhaoweicai/mscnn.",An NVIDIA Titan GPU was used for CNN computations .,0.7224199288256228,0.1707317073170731,0.9333333333333332,experimental-setup,approach
face_detection,11,experimental evaluation,experimental evaluation,204,15,15,An NVIDIA Titan GPU was used for CNN computations .,All times are reported for implementation on a single CPU core ( 2.40 GHz ) of an Intel Xeon E5 - 2630 server with 64 GB of RAM ., ,0.7259786476868327,0.1829268292682926,1.0,experimental-setup,approach
face_detection,11,experimental evaluation,object detection evaluation,260,71,35,The MS - CNN set a new record for the detection of pedestrians and cyclists .,A comparison to previous approaches is given in and .,"The columns "" Pedestrians - Mod "" and "" Cyclists - Mod "" show substantial gains ( 6 and 7 points respectively ) over 3 DOP , and much better performance than the Faster - RCNN , Regionlets , etc .",0.9252669039145908,0.8658536585365854,0.7608695652173914,results,approach
face_detection,11,experimental evaluation,object detection evaluation,262,73,37,"We also led a nontrivial margin over the very recent SDP + RPN , which used scale depen - dent pooling .","The columns "" Pedestrians - Mod "" and "" Cyclists - Mod "" show substantial gains ( 6 and 7 points respectively ) over 3 DOP , and much better performance than the Faster - RCNN , Regionlets , etc .","In terms of speed , the network is fairly fast .",0.9323843416370108,0.8902439024390244,0.8043478260869565,results,experimental-setup
face_detection,11,experimental evaluation,object detection evaluation,266,77,41,Pedestrian detection on Caltech The MS - CNN detector was also evaluated on the Caltech pedestrian benchmark .,On the original images ( 1250375 ) detection speed reaches 10 fps .,"The model "" h720 - ctx "" was compared to methods such as DeepParts , Com p ACT - Deep , CheckerBoard , LDCF , ACF , and SpatialPooling on three tasks : reasonable , medium and partial occlusion .",0.9466192170818504,0.9390243902439024,0.8913043478260869,results,experimental-setup
face_detection,11,experimental evaluation,object detection evaluation,268,79,43,"As shown in , the MS - CNN has state - of the - art performance . and ( c ) show that it performs very well for small and occluded objects , outperforming DeepParts , which explicitly addresses occlusion .","The model "" h720 - ctx "" was compared to methods such as DeepParts , Com p ACT - Deep , CheckerBoard , LDCF , ACF , and SpatialPooling on three tasks : reasonable , medium and partial occlusion .","Moreover , it misses a very small number of pedestrians , due to the accuracy of the proposal network .",0.9537366548042704,0.9634146341463414,0.9347826086956522,results,baselines
face_detection,12,abstract,abstract,4,2,2,Region proposal mechanisms are essential for existing deep learning approaches to object detection in images ., ,"Although they can generally achieve a good detection performance under normal circumstances , their recall in a scene with extreme cases is unacceptably low .",0.0289855072463768,0.2857142857142857,0.2857142857142857,research-problem,baselines
face_detection,12,introduction,introduction,12,3,3,"Existing deep learning approaches to solve this task ( e.g. , R - CNN and its variants ) mainly rely on region proposal mechanisms ( e.g. , region proposal networks ( RPN s ) ) to generate potential bounding boxes in an image and then classify these bounding boxes to achieve object detection .",Object detection in images is one of the most widely explored tasks in computer vision .,"Although such mechanisms can generally achieve a good detection performance under normal circumstances , their recall in a scene with extreme cases ( e.g. , complex occlusion ( ) , poor illumination ( ) , and large - scale small objects ( ) ) is unacceptably low .",0.0869565217391304,0.1428571428571428,0.1428571428571428,research-problem,ablation-analysis
face_detection,12,introduction,introduction,19,10,10,"Motivated by this , in this work , we propose a weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach , which uses segmentation models to achieve an accurate and robust object detection without NMS .","However , it is very hard ( and sometimes even impossible ) to find an appropriate threshold to adapt to the very complex situations in extreme cases .","It consists of two phases , namely , a training and a testing phase .",0.1376811594202898,0.4761904761904762,0.4761904761904762,model,ablation-analysis
face_detection,12,introduction,introduction,21,12,12,"In the training phase , WSMA - Seg first converts weakly supervised bounding box annotations in detection tasks to multi-channel segmentation - like masks , called multimodal annotations ; then , a segmentation model is trained using multimodal annotations as labels to learn multimodal heatmaps for the training images .","It consists of two phases , namely , a training and a testing phase .","In the testing phase , the resulting heatmaps of a given test image are converted into an instance - aware segmentation map based on a pixel - level logic operation ; then , a contour tracing operation is conducted to generate contours for objects using the segmentation map ; finally , bounding boxes of objects are created as circumscribed quadrilaterals of their corresponding contours .",0.1521739130434782,0.5714285714285714,0.5714285714285714,model,ablation-analysis
face_detection,12,introduction,introduction,22,13,13,"In the testing phase , the resulting heatmaps of a given test image are converted into an instance - aware segmentation map based on a pixel - level logic operation ; then , a contour tracing operation is conducted to generate contours for objects using the segmentation map ; finally , bounding boxes of objects are created as circumscribed quadrilaterals of their corresponding contours .","In the training phase , WSMA - Seg first converts weakly supervised bounding box annotations in detection tasks to multi-channel segmentation - like masks , called multimodal annotations ; then , a segmentation model is trained using multimodal annotations as labels to learn multimodal heatmaps for the training images .","WSMA - Seg has the following advantages : ( i ) as an NMS - free solution , WSMA - Seg avoids all hyperparameters related to anchor boxes and NMS ; so , the above - mentioned threshold selection problem is also avoided ; ( ii ) the complex occlusion problem can be alleviated by utilizing the topological structure of segmentation - like multimodal annotations ; and ( iii ) multimodal annotations are pixel - level annotations ; so , they can describe the objects more accurately and overcome the above - mentioned environment noise problem .",0.1594202898550724,0.6190476190476191,0.6190476190476191,model,approach
face_detection,12,weakly supervised multimodal annotation segmentation,rebar head detection,90,60,1,Rebar Head Detection, , ,0.6521739130434783,0.5882352941176471,0.0909090909090909,experiments,approach
face_detection,12,weakly supervised multimodal annotation segmentation,rebar head detection,98,68,9,"As shown in , our proposed method with Stack = 2 , Base = 40 , Depth = 5 has achieved the best performance among all solutions in terms of F1 Score .",shows the detection performances of our proposed WSMA - Seg and baselines on this dataset .,"In addition , the number of parameters needed for WSMA - Seg is much less than the baselines ( only 1 / 7 of Cascade RCNN and 1 / 4 of Faster RCNN ) , while the number of training epochs for WSMA - Seg is also less than those of the baselines .",0.7101449275362319,0.6666666666666666,0.8181818181818182,experiments,baselines
face_detection,12,weakly supervised multimodal annotation segmentation,rebar head detection,100,70,11,"Therefore , we can conclude that , compared to the state - of - the - art baselines , WSMA - Seg is much simpler , more effective , and more efficient .","In addition , the number of parameters needed for WSMA - Seg is much less than the baselines ( only 1 / 7 of Cascade RCNN and 1 / 4 of Faster RCNN ) , while the number of training epochs for WSMA - Seg is also less than those of the baselines .", ,0.7246376811594203,0.6862745098039216,1.0,experiments,baselines
face_detection,12,weakly supervised multimodal annotation segmentation,wider face detection,101,71,1,WIDER Face Detection, , ,0.7318840579710145,0.696078431372549,0.0909090909090909,experiments,approach
face_detection,12,weakly supervised multimodal annotation segmentation,wider face detection,104,74,4,WIDER Face results in a much lower detection accuracy compared to other face detection datasets .,"Face detections in this dataset are extremely challenging due to a high degree of variability in scale , pose , and occlusion .","WIDER Face has defined three levels of difficulties ( i.e. , Easy , Medium , and Hard ) , based on the detection accuracies of EdgeBox .",0.7536231884057971,0.7254901960784313,0.3636363636363637,experiments,model
face_detection,12,weakly supervised multimodal annotation segmentation,wider face detection,111,81,11,"The results show that our proposed WSMA - Seg outperforms the state - of - the - art baselines in all three categories , reaching 94.70 , 93.41 , and 87.23 in Easy , Medium , and Hard categories , respectively .",The experimental results in terms of F1 score are shown in ., ,0.8043478260869565,0.7941176470588235,1.0,experiments,baselines
face_detection,12,weakly supervised multimodal annotation segmentation,ms coco detection,112,82,1,MS COCO Detection, , ,0.8115942028985508,0.8039215686274509,0.0476190476190476,experiments,baselines
face_detection,12,weakly supervised multimodal annotation segmentation,ms coco detection,130,100,19,"The results show that our WSMA - Seg approach outperforms all state - of - the - art baselines in terms of most metrics , including the most challenging metrics , AP , AP s , AR 1 , and AR s .","Seven state - of - the - art solutions are selected as baselines , and the experimental results for four types of metrics are shown in .","For the other metrics , the performance of our proposed approach is also close to those of the best baselines .",0.9420289855072465,0.9803921568627452,0.9047619047619048,experiments,baselines
face_detection,12,weakly supervised multimodal annotation segmentation,ms coco detection,131,101,20,"For the other metrics , the performance of our proposed approach is also close to those of the best baselines .","The results show that our WSMA - Seg approach outperforms all state - of - the - art baselines in terms of most metrics , including the most challenging metrics , AP , AP s , AR 1 , and AR s .",This proves that the proposed WSMA - Seg approach generally achieves more accurate and robust object detection than the state - of - the - art approaches without NMS .,0.9492753623188406,0.9901960784313726,0.9523809523809524,experiments,baselines
face_detection,12,weakly supervised multimodal annotation segmentation,ms coco detection,132,102,21,This proves that the proposed WSMA - Seg approach generally achieves more accurate and robust object detection than the state - of - the - art approaches without NMS .,"For the other metrics , the performance of our proposed approach is also close to those of the best baselines .", ,0.9565217391304348,1.0,1.0,experiments,baselines
face_detection,13,title,title,2,2,2,RetinaFace : Single - stage Dense Face Localisation in the Wild, , ,0.0083333333333333,1.0,1.0,research-problem,approach
face_detection,13,abstract,abstract,4,2,2,"Though tremendous strides have been made in uncontrolled face detection , accurate and efficient face localisation in the wild remains an open challenge .", ,"This paper presents a robust single - stage face detector , named RetinaFace , which performs pixel - wise face localisation on various scales of faces by taking advantages of joint extra-supervised and self - supervised multi-task learning .",0.0166666666666666,0.1818181818181818,0.1818181818181818,research-problem,baselines
face_detection,13,abstract,abstract,13,11,11,Extra annotations and code have been made available at : https://github.com/deepinsight/insightface/tree/master/RetinaFace.,"( 5 ) By employing light - weight backbone networks , Retina Face can run real - time on a single CPU core fora VGA - resolution image .", ,0.0541666666666666,1.0,1.0,code,baselines
face_detection,13,introduction,introduction,43,30,30,"Based on a single - stage design , we propose a novel pixel - wise face localisation method named Reti- naFace , which employs a multi-task learning strategy to simultaneously predict face score , face box , five facial landmarks , and 3D position and correspondence of each facial pixel .","To summarise , our key contributions are :","On the WIDER FACE hard subset , RetinaFace outperforms the AP of the state of the art two - stage method ( ISRN ) by 1.1 % ( AP equal to 91.4 % ) .",0.1791666666666666,0.8571428571428571,0.8571428571428571,model,ablation-analysis
face_detection,13,experiments,implementation details,149,41,28,Training Details .,"Besides random crop , we also augment training data by random horizontal flip with the probability of 0.5 and photo-metric colour distortion .","We train the RetinaFace using SGD optimiser ( momentum at 0.9 , weight decay at 0.0005 , batch size of 8 4 ) on four NVIDIA Tesla P40 ( 24GB ) GPUs .",0.6208333333333333,0.3333333333333333,0.8235294117647058,experimental-setup,approach
face_detection,13,experiments,implementation details,150,42,29,"We train the RetinaFace using SGD optimiser ( momentum at 0.9 , weight decay at 0.0005 , batch size of 8 4 ) on four NVIDIA Tesla P40 ( 24GB ) GPUs .",Training Details .,"The learning rate starts from 10 ? 3 , rising to 10 ? 2 after 5 epochs , then divided by 10 at 55 and 68 epochs .",0.625,0.3414634146341464,0.8529411764705882,experimental-setup,baselines
face_detection,13,experiments,implementation details,151,43,30,"The learning rate starts from 10 ? 3 , rising to 10 ? 2 after 5 epochs , then divided by 10 at 55 and 68 epochs .","We train the RetinaFace using SGD optimiser ( momentum at 0.9 , weight decay at 0.0005 , batch size of 8 4 ) on four NVIDIA Tesla P40 ( 24GB ) GPUs .",The training process terminates at 80 epochs .,0.6291666666666667,0.3495934959349593,0.8823529411764706,experimental-setup,experimental-setup
face_detection,13,experiments,implementation details,152,44,31,The training process terminates at 80 epochs .,"The learning rate starts from 10 ? 3 , rising to 10 ? 2 after 5 epochs , then divided by 10 at 55 and 68 epochs .",Testing Details .,0.6333333333333333,0.3577235772357724,0.9117647058823528,experimental-setup,approach
face_detection,13,experiments,implementation details,153,45,32,Testing Details .,The training process terminates at 80 epochs .,"For testing on WIDER FACE , we follow the standard practices of and employ flip as well as multi-scale ( the short edge of image at [ 500 , 800 , 1100 , 1400 , 1700 ] ) strategies .",0.6375,0.3658536585365853,0.9411764705882352,experimental-setup,approach
face_detection,13,experiments,implementation details,154,46,33,"For testing on WIDER FACE , we follow the standard practices of and employ flip as well as multi-scale ( the short edge of image at [ 500 , 800 , 1100 , 1400 , 1700 ] ) strategies .",Testing Details .,Box voting [ 15 ] is applied on the union set of predicted face boxes using an IoU threshold at 0.4 .,0.6416666666666667,0.3739837398373984,0.9705882352941176,experimental-setup,model
face_detection,13,experiments,implementation details,155,47,34,Box voting [ 15 ] is applied on the union set of predicted face boxes using an IoU threshold at 0.4 .,"For testing on WIDER FACE , we follow the standard practices of and employ flip as well as multi-scale ( the short edge of image at [ 500 , 800 , 1100 , 1400 , 1700 ] ) strategies .", ,0.6458333333333334,0.3821138211382114,1.0,experimental-setup,model
face_detection,13,experiments,ablation study,161,53,6,"Adding the branch of five facial landmark regression significantly improves the face box AP ( 0.408 % ) and mAP ( 0.775 % ) on the Hard subset , suggesting that landmark localisation is crucial for improving the accuracy of face detection .","By applying the practices of state - of - the - art techniques ( i.e. FPN , context module , and deformable convolution ) , we setup a strong baseline ( 91.286 % ) , which is slightly better than ISRN ( 90.9 % ) .","By contrast , adding the dense regression branch increases the face box AP on Easy and Medium subsets but slightly deteriorates the results on the Hard subset , indicating the difficulty of dense regression under challenging scenarios .",0.6708333333333333,0.4308943089430894,0.5454545454545454,ablation-analysis,approach
face_detection,13,experiments,ablation study,162,54,7,"By contrast , adding the dense regression branch increases the face box AP on Easy and Medium subsets but slightly deteriorates the results on the Hard subset , indicating the difficulty of dense regression under challenging scenarios .","Adding the branch of five facial landmark regression significantly improves the face box AP ( 0.408 % ) and mAP ( 0.775 % ) on the Hard subset , suggesting that landmark localisation is crucial for improving the accuracy of face detection .","Nevertheless , learning landmark and dense regression jointly enables a further improvement compared to adding landmark regression only .",0.675,0.4390243902439024,0.6363636363636364,ablation-analysis,baselines
face_detection,13,experiments,ablation study,163,55,8,"Nevertheless , learning landmark and dense regression jointly enables a further improvement compared to adding landmark regression only .","By contrast , adding the dense regression branch increases the face box AP on Easy and Medium subsets but slightly deteriorates the results on the Hard subset , indicating the difficulty of dense regression under challenging scenarios .","This demonstrates that landmark regression does help dense regression , which in turn boosts face detection performance even further .",0.6791666666666667,0.4471544715447153,0.7272727272727273,ablation-analysis,approach
face_detection,13,experiments,face box accuracy,167,59,1,Face box Accuracy, , ,0.6958333333333333,0.4796747967479675,0.0909090909090909,experiments,approach
face_detection,13,experiments,face box accuracy,172,64,6,"More specifically , RetinaFace produces the best AP in all subsets of both validation and test sets , i.e. , 96.9 % ( Easy ) , 96.1 % ( Medium ) and 91.8 % ( Hard ) for validation set , and 96.3 % ( Easy ) , 95.6 % ( Medium ) and 91.4 % ( Hard ) for test set .",Our approach outper - forms these state - of - the - art methods in terms of AP .,"Compared to the recent best performed method , Reti - na Face sets up a new impressive record ( 91.4 % v.s. 90.3 % ) on the Hard subset which contains a large number of tiny faces .",0.7166666666666667,0.5203252032520326,0.5454545454545454,experiments,baselines
face_detection,13,experiments,face box accuracy,173,65,7,"Compared to the recent best performed method , Reti - na Face sets up a new impressive record ( 91.4 % v.s. 90.3 % ) on the Hard subset which contains a large number of tiny faces .","More specifically , RetinaFace produces the best AP in all subsets of both validation and test sets , i.e. , 96.9 % ( Easy ) , 96.1 % ( Medium ) and 91.8 % ( Hard ) for validation set , and 96.3 % ( Easy ) , 95.6 % ( Medium ) and 91.4 % ( Hard ) for test set .","In , we illustrate qualitative results on a selfie with dense faces .",0.7208333333333333,0.5284552845528455,0.6363636363636364,experiments,baselines
face_detection,13,experiments,face box accuracy,175,67,9,"RetinaFace successfully finds about 900 faces ( threshold at 0.5 ) out of the reported 1 , 151 faces .","In , we illustrate qualitative results on a selfie with dense faces .","Besides accurate bounding boxes , the five facial landmarks predicted by Retina Face are also very robust under the variations of pose , occlusion and resolution .",0.7291666666666666,0.5447154471544715,0.8181818181818182,experiments,baselines
face_detection,13,experiments,five facial landmark accuracy,178,70,1,Five Facial Landmark Accuracy, , ,0.7416666666666667,0.5691056910569106,0.125,experiments,approach
face_detection,13,experiments,five facial landmark accuracy,183,75,6,RetinaFace significantly decreases the normalised mean errors ( NME ) from 2.72 % to 2.21 % when compared to MTCNN .,"As shown in , we give the mean error of each facial landmark on the AFLW dataset .","In , we show the cumulative error distribution ( CED ) curves on the WIDER FACE validation set .",0.7625,0.6097560975609756,0.75,experiments,baselines
face_detection,13,experiments,five facial landmark accuracy,185,77,8,"Compared to MTCNN , RetinaFace significantly decreases the failure rate from 26.31 % to 9.37 % ( the NME threshold at 10 % ) .","In , we show the cumulative error distribution ( CED ) curves on the WIDER FACE validation set .", ,0.7708333333333334,0.6260162601626016,1.0,experiments,baselines
face_detection,13,experiments,dense facial landmark accuracy,186,78,1,Dense Facial Landmark Accuracy, , ,0.775,0.6341463414634146,0.0909090909090909,experiments,approach
face_detection,13,experiments,dense facial landmark accuracy,191,83,6,"Even though the performance gap exists between supervised and self - supervised methods , the dense regression results of RetinaFace are comparable with these state - of - the - art methods .","In , we give the CED curves of state - of - the - art methods as well as RetinaFace .","More specifically , we observe that ( 1 ) five facial landmarks regression can alleviate the training difficulty of dense regression branch and significantly improve the dense regression results .",0.7958333333333333,0.6747967479674797,0.5454545454545454,experiments,baselines
face_detection,13,experiments,dense facial landmark accuracy,192,84,7,"More specifically , we observe that ( 1 ) five facial landmarks regression can alleviate the training difficulty of dense regression branch and significantly improve the dense regression results .","Even though the performance gap exists between supervised and self - supervised methods , the dense regression results of RetinaFace are comparable with these state - of - the - art methods .",( 2 ) using single - stage features ( as in RetinaFace ) to predict dense correspondence parameters is much harder than employing ( Region of Interest ) RoI features ( as in Mesh Decoder ) .,0.8,0.6829268292682927,0.6363636363636364,experiments,baselines
face_detection,13,experiments,dense facial landmark accuracy,193,85,8,( 2 ) using single - stage features ( as in RetinaFace ) to predict dense correspondence parameters is much harder than employing ( Region of Interest ) RoI features ( as in Mesh Decoder ) .,"More specifically , we observe that ( 1 ) five facial landmarks regression can alleviate the training difficulty of dense regression branch and significantly improve the dense regression results .","As illustrated in , RetinaFace can easily handle faces with pose variations but has difficulty under complex scenarios .",0.8041666666666667,0.6910569105691057,0.7272727272727273,experiments,model
face_detection,13,experiments,face recognition accuracy,197,89,1,Face Recognition Accuracy, , ,0.8208333333333333,0.7235772357723578,0.0555555555555555,experiments,approach
face_detection,13,experiments,face recognition accuracy,204,96,8,"The results on CFP - FP , demonstrate that Reti - na Face can boost ArcFace 's verification accuracy from 98.37 % to 99.49 % .","In Tab. 4 , we show the influence of face detection and alignment on deep face recognition ( i.e. ArcFace ) by comparing the widely used MTCNN and the proposed Reti-naFace .",This result shows that the performance of frontal - profile face verification is now approaching that of frontal - frontal face verification ( e.g. 99.86 % on LFW ) .,0.85,0.7804878048780488,0.4444444444444444,experiments,baselines
face_detection,14,title,title,2,2,2,WIDER FACE : A Face Detection Benchmark, , ,0.0074349442379182,1.0,1.0,research-problem,experimental-setup
face_detection,14,introduction,introduction,32,19,19,We introduce a large - scale face detection dataset called WIDER FACE .,"In this work , we make three contributions .","It consists of 32 , 203 images with 393 , 703 labeled faces , which is 10 times larger than the current largest face detection dataset .",0.1189591078066914,0.7307692307692307,0.7307692307692307,dataset,experimental-setup
face_detection,14,introduction,introduction,33,20,20,"It consists of 32 , 203 images with 393 , 703 labeled faces , which is 10 times larger than the current largest face detection dataset .",We introduce a large - scale face detection dataset called WIDER FACE .,"The faces vary largely in appearance , pose , and scale , as shown in .",0.1226765799256505,0.7692307692307693,0.7692307692307693,dataset,baselines
face_detection,14,introduction,introduction,34,21,21,"The faces vary largely in appearance , pose , and scale , as shown in .","It consists of 32 , 203 images with 393 , 703 labeled faces , which is 10 times larger than the current largest face detection dataset .","In order to quantify different types of errors , we annotate multiple attributes : occlusion , pose , and event categories , which allows in depth analysis of existing algorithms .",0.1263940520446096,0.8076923076923077,0.8076923076923077,dataset,model
face_detection,14,introduction,introduction,35,22,22,"In order to quantify different types of errors , we annotate multiple attributes : occlusion , pose , and event categories , which allows in depth analysis of existing algorithms .","The faces vary largely in appearance , pose , and scale , as shown in .","We show an example of using WIDER FACE through proposing a multi-scale two - stage cascade framework , which uses divide and conquer strategy to deal with large scale variations .",0.1301115241635687,0.8461538461538461,0.8461538461538461,dataset,approach
face_detection,14,experimental results,benchmarks,149,5,4,"We select VJ , ACF , DPM , and Faceness as baselines .","For each class , we pick one algorithm as a baseline method .","The VJ , DPM , and Faceness detectors are either obtained from the authors or from open source library ( OpenCV ) .",0.5539033457249071,0.0520833333333333,0.0888888888888888,baselines,approach
face_detection,14,experimental results,benchmarks,155,11,10,Overall .,"Following previous work , we conduct linear transformation for each method to fit the annotation of WIDER FACE .","In this experiment , we employ the evaluation setting mentioned in Sec. 3.3 .",0.5762081784386617,0.1145833333333333,0.2222222222222222,results,baselines
face_detection,14,experimental results,benchmarks,158,14,13,"Faceness outperforms other methods on three subsets , with DPM and ACF as marginal second and third .",The results are shown in ( a.1 ) - ( a.3 ) .,"For the easy set , the average precision ( AP ) of most methods are over 60 % , but none of them surpasses 75 % .",0.5873605947955389,0.1458333333333333,0.2888888888888889,results,approach
face_detection,14,experimental results,benchmarks,159,15,14,"For the easy set , the average precision ( AP ) of most methods are over 60 % , but none of them surpasses 75 % .","Faceness outperforms other methods on three subsets , with DPM and ACF as marginal second and third .",The performance drops 10 % for all methods on the medium set .,0.5910780669144982,0.15625,0.3111111111111111,results,approach
face_detection,14,experimental results,benchmarks,160,16,15,The performance drops 10 % for all methods on the medium set .,"For the easy set , the average precision ( AP ) of most methods are over 60 % , but none of them surpasses 75 % .",The hard set is even more challenging .,0.5947955390334573,0.1666666666666666,0.3333333333333333,results,approach
face_detection,14,experimental results,benchmarks,161,17,16,The hard set is even more challenging .,The performance drops 10 % for all methods on the medium set .,"The performance quickly decreases , with a AP below 30 % for all methods .",0.5985130111524164,0.1770833333333333,0.3555555555555556,results,approach
face_detection,14,experimental results,benchmarks,164,20,19,Scale .,"To trace the reasons of failure , we examine performance on varying subsets of the data .","As described in Sec. 3.3 , we group faces according to the image height : small ( 10 - 50 pixels ) , medium ( 50 - 300 pixels ) , and large ( 300 or more pixels ) scales .",0.6096654275092936,0.2083333333333333,0.4222222222222222,ablation-analysis,baselines
face_detection,14,experimental results,benchmarks,166,22,21,The results of small scale are abysmal : none of the algorithms is able to achieve more than 12 % AP .,"As described in Sec. 3.3 , we group faces according to the image height : small ( 10 - 50 pixels ) , medium ( 50 - 300 pixels ) , and large ( 300 or more pixels ) scales .",This shows that current face detectors are incapable to deal with faces of small scale .,0.6171003717472119,0.2291666666666666,0.4666666666666667,ablation-analysis,baselines
face_detection,14,experimental results,benchmarks,168,24,23,Occlusion .,This shows that current face detectors are incapable to deal with faces of small scale .,Occlusion handling is a key performance metric for any face detectors .,0.6245353159851301,0.25,0.5111111111111111,ablation-analysis,baselines
face_detection,14,experimental results,benchmarks,172,28,27,"With partial occlusion , the performance drops significantly .","As mentioned in Sec. 3.3 , we classify faces into three categories : un - occluded , partially occluded ( 1 % - 30 % area occluded ) and heavily occluded ( over 30 % area occluded ) .",The maximum AP is only 26.5 % achieved by Faceness .,0.6394052044609665,0.2916666666666667,0.6,ablation-analysis,approach
face_detection,14,experimental results,benchmarks,173,29,28,The maximum AP is only 26.5 % achieved by Faceness .,"With partial occlusion , the performance drops significantly .",The performance further decreases in the heavy occlusion setting .,0.6431226765799256,0.3020833333333333,0.6222222222222222,ablation-analysis,approach
face_detection,14,experimental results,benchmarks,177,33,32,Pose .,"It is worth noting that Faceness and DPM , which are part based models , already perform relatively better than other methods on occlusion handling .","As discussed in Sec. 3.3 , we assign a face pose as atypical if either the roll or pitch degree is larger than 30 degree ; or the yaw is larger than 90 - degree .",0.6579925650557621,0.34375,0.7111111111111111,ablation-analysis,baselines
face_detection,14,experimental results,benchmarks,183,39,38,"The best performance is achieved by Faceness , with a recall below 20 % .",The performance clearly degrades for atypical pose .,The results suggest that current face detectors are only capable of dealing with faces with out - of - plane rotation and a small range of in - plane rotation .,0.6802973977695167,0.40625,0.8444444444444444,ablation-analysis,approach
face_detection,15,abstract,abstract,4,2,2,"Although tremendous strides have been made in face detection , one of the remaining open challenges is to achieve real - time speed on the CPU as well as maintain high performance , since effective models for face detection tend to be computationally prohibitive .", ,"To address this challenge , we propose a novel face detector , named FaceBoxes , with superior performance on both speed and accuracy .",0.0169491525423728,0.1818181818181818,0.1818181818181818,research-problem,baselines
face_detection,15,abstract,abstract,13,11,11,Code is available at https://github.com/sfzhang15/FaceBoxes .,"We comprehensively evaluate this method and present stateof - the - art detection performance on several face detection benchmark datasets , including the AFW , PASCAL face , and FDDB .", ,0.0550847457627118,1.0,1.0,code,baselines
face_detection,15,introduction,introduction,41,28,28,"In this paper , inspired by the RPN in Faster R - CNN and the multi-scale mechanism in SSD , we develop a state - of - the - art face detector with real - time speed on the CPU .","The speed would dramatically degrade as the number of faces increases ; 2 ) The cascade based detectors optimize each component separately , making the training process extremely complicated and the final model sub-optimal ; 3 ) For the VGA - resolution images , their runtime efficiency on the CPU is about 14 FPS , which is not fast enough to reach the real - time speed .","Specifically , we propose a novel face detector named FaceBoxes , which only contains a single fully convolutional neural network and can be trained end - to - end .",0.173728813559322,0.6666666666666666,0.6666666666666666,model,experimental-setup
face_detection,15,introduction,introduction,42,29,29,"Specifically , we propose a novel face detector named FaceBoxes , which only contains a single fully convolutional neural network and can be trained end - to - end .","In this paper , inspired by the RPN in Faster R - CNN and the multi-scale mechanism in SSD , we develop a state - of - the - art face detector with real - time speed on the CPU .",The proposed method has a lightweight yet powerful network structure ( as shown in ) that consists of the Rapidly Digested Convolutional Layers ( RDCL ) and the Multiple Scale Convolutional Layers ( MSCL ) .,0.1779661016949152,0.6904761904761905,0.6904761904761905,model,ablation-analysis
face_detection,15,introduction,introduction,43,30,30,The proposed method has a lightweight yet powerful network structure ( as shown in ) that consists of the Rapidly Digested Convolutional Layers ( RDCL ) and the Multiple Scale Convolutional Layers ( MSCL ) .,"Specifically , we propose a novel face detector named FaceBoxes , which only contains a single fully convolutional neural network and can be trained end - to - end .","The RDCL is designed to enable FaceBoxes to achieve real - time speed on the CPU , and the MSCL aims at enriching the receptive fields and discretizing anchors over different layers to handle various scales of faces .",0.1822033898305085,0.7142857142857143,0.7142857142857143,model,model
face_detection,15,introduction,introduction,44,31,31,"The RDCL is designed to enable FaceBoxes to achieve real - time speed on the CPU , and the MSCL aims at enriching the receptive fields and discretizing anchors over different layers to handle various scales of faces .",The proposed method has a lightweight yet powerful network structure ( as shown in ) that consists of the Rapidly Digested Convolutional Layers ( RDCL ) and the Multiple Scale Convolutional Layers ( MSCL ) .,"Besides , we propose a new anchor densification strategy to make different types of anchors have the same density on the input image , which significantly improves the recall rate of small faces .",0.1864406779661017,0.7380952380952381,0.7380952380952381,model,ablation-analysis
face_detection,15,introduction,introduction,45,32,32,"Besides , we propose a new anchor densification strategy to make different types of anchors have the same density on the input image , which significantly improves the recall rate of small faces .","The RDCL is designed to enable FaceBoxes to achieve real - time speed on the CPU , and the MSCL aims at enriching the receptive fields and discretizing anchors over different layers to handle various scales of faces .","Consequently , for VGA - resolution images , our face detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU .",0.1906779661016949,0.7619047619047619,0.7619047619047619,model,baselines
face_detection,15,experiments,runtime efficiency,176,8,6,"We first filter out most boxes by a confidence threshold of 0.05 and keep the top 400 boxes before applying NMS , then we perform NMS with jaccard overlap of 0.3 and keep the top 200 boxes .","During inference , our method outputs a large number of boxes ( e.g. , 8 , 525 boxes fora VGA-resolution image ) .",We measure the speed using Titan X ( Pascal ) and cuDNN v 5.1 with Intel Xeon E5-2660v3@2.60 GHz .,0.7457627118644068,0.1333333333333333,0.6,experimental-setup,approach
face_detection,15,experiments,runtime efficiency,177,9,7,We measure the speed using Titan X ( Pascal ) and cuDNN v 5.1 with Intel Xeon E5-2660v3@2.60 GHz .,"We first filter out most boxes by a confidence threshold of 0.05 and keep the top 400 boxes before applying NMS , then we perform NMS with jaccard overlap of 0.3 and keep the top 200 boxes .","As listed in Tab. 1 , comparing with recent CNN - based methods , our FaceBoxes can run at 20 FPS on the CPU with state - of - the - art accuracy .",0.75,0.15,0.7,experimental-setup,approach
face_detection,15,experiments,model analysis,194,26,14,FaceBoxes Anchor densification strategy is crucial .,Contribution,"Our anchor densification strategy is used to increase the density of small anchors ( i.e. , 32 32 and 64 64 ) in order to improve the recall rate of small faces .",0.8220338983050848,0.4333333333333333,0.5384615384615384,ablation-analysis,model
face_detection,15,experiments,model analysis,195,27,15,"Our anchor densification strategy is used to increase the density of small anchors ( i.e. , 32 32 and 64 64 ) in order to improve the recall rate of small faces .",FaceBoxes Anchor densification strategy is crucial .,From the results listed in Tab .,0.826271186440678,0.45,0.5769230769230769,ablation-analysis,approach
face_detection,15,experiments,model analysis,197,29,17,"2 , we can see that the m AP on FDDB is reduced from 96.0 % to 94.9 % after ablating the anchor densification strategy .",From the results listed in Tab .,"The sharp decline ( i.e. , 1.1 % ) demonstrates the effectiveness of the proposed anchor densification strategy .",0.8347457627118644,0.4833333333333333,0.6538461538461539,ablation-analysis,baselines
face_detection,15,experiments,model analysis,202,34,22,RDCL is efficient and accuracy - preserving .,"2 indicates that MSCL effectively increases the m AP by 1.0 % , owning to the diverse receptive fields and the multi -scale anchor tiling mechanism .",The design of RDCL enables our FaceBoxes to achieve real - time speed on the CPU .,0.8559322033898306,0.5666666666666667,0.8461538461538461,ablation-analysis,baselines
face_detection,15,experiments,evaluation on benchmark,209,41,3,AFW dataset .,"We evaluate the FaceBoxes on the common face detection benchmark datasets , including Annotated Faces in the Wild ( AFW ) , PASCAL Face , and Face Detection Data Set and Benchmark ( FDDB ) .",It has 205 images with 473 faces .,0.8855932203389829,0.6833333333333333,0.1363636363636363,experiments,baselines
face_detection,15,experiments,evaluation on benchmark,212,44,6,"As illustrated in , our FaceBoxes outperforms all others by a large margin .","We evaluate FaceBoxes against the well - known works and commercial face detectors ( e.g. , Face.com , Face + + and Picasa ) .",shows some qualitative results on the AFW dataset .,0.8983050847457628,0.7333333333333333,0.2727272727272727,experiments,experimental-setup
face_detection,15,experiments,evaluation on benchmark,214,46,8,PASCAL face dataset .,shows some qualitative results on the AFW dataset .,"It is collected from the test set of PASCAL person layout dataset , consisting of 1335 faces with large face appearance and pose variations from 851 images .",0.9067796610169492,0.7666666666666667,0.3636363636363637,experiments,experiments
face_detection,15,experiments,evaluation on benchmark,217,49,11,"Our method significantly outperforms all other methods and commercial face detectors ( e.g. , SkyBiometry , Face + + and Picasa ) .",shows the precision - recall curves on this dataset .,shows some qualitative results on the PASCAL face dataset .,0.9194915254237288,0.8166666666666667,0.5,experiments,baselines
face_detection,15,experiments,evaluation on benchmark,219,51,13,FDDB dataset .,shows some qualitative results on the PASCAL face dataset .,"It has 5 , 171 faces in 2 , 845 images taken from news articles on Yahoo websites .",0.9279661016949152,0.85,0.5909090909090909,experiments,experiments
face_detection,15,experiments,evaluation on benchmark,226,58,20,Our FaceBoxes achieves the state - of - the - art performance and outperforms all others by a large margin on discontinuous and continuous ROC curves .,The results are shown in and .,These results indicate that our FaceBoxes can robustly detect unconstrained faces .,0.9576271186440678,0.9666666666666668,0.9090909090909092,experiments,approach
face_detection,16,title,title,2,2,2,"HyperFace : A Deep Multi-task Learning Framework for Face Detection , Landmark Localization , Pose Estimation , and Gender Recognition", , ,0.0042918454935622,1.0,1.0,research-problem,experimental-setup
face_detection,16,introduction,introduction,15,6,6,"In this paper , we present a novel framework based on CNNs for simultaneous face detection , facial landmarks localization , head pose estimation and gender recognition from a given image ( see ) .","Recently , it has been shown that learning correlated tasks simultaneously can boost the performance of individual tasks , , .",We design a CNN architecture to learn common features for these tasks and exploit the synergy among them .,0.0321888412017167,0.1578947368421052,0.1578947368421052,model,experimental-setup
face_detection,16,introduction,introduction,16,7,7,We design a CNN architecture to learn common features for these tasks and exploit the synergy among them .,"In this paper , we present a novel framework based on CNNs for simultaneous face detection , facial landmarks localization , head pose estimation and gender recognition from a given image ( see ) .",We exploit the fact that information contained in features is hierarchically distributed throughout the network as demonstrated in .,0.0343347639484978,0.1842105263157894,0.1842105263157894,model,approach
face_detection,16,introduction,introduction,22,13,13,We refer the set of intermediate layer features as hyperfeatures .,It is evident that we need to make use of all the intermediate layers of a deep CNN in order to train different tasks under consideration .,We borrow this term from which uses it to denote a stack of local histograms for multilevel image coding .,0.0472103004291845,0.3421052631578947,0.3421052631578947,model,approach
face_detection,16,introduction,introduction,29,20,20,"Hence , we construct a separate fusion - CNN to fuse the hyperfeatures .",Recent advances in deep learning have shown that CNNs are capable of estimating an arbitrary complex function .,"In order to learn the tasks , we train them simultaneously using multiple loss functions .",0.0622317596566523,0.5263157894736842,0.5263157894736842,model,model
face_detection,16,introduction,introduction,30,21,21,"In order to learn the tasks , we train them simultaneously using multiple loss functions .","Hence , we construct a separate fusion - CNN to fuse the hyperfeatures .","In this way , the features get better at understanding faces , which leads to improvements in the performances of individual tasks .",0.0643776824034334,0.5526315789473685,0.5526315789473685,model,approach
face_detection,16,introduction,introduction,32,23,23,The deep CNN combined with the fusion - CNN can be learned together in an end -toend fashion .,"In this way , the features get better at understanding faces , which leads to improvements in the performances of individual tasks .","We also study the performance of face detection , landmarks localization , pose estimation and gender recognition tasks using off - the - shelf Region - based CNN ( R - CNN ) approach .",0.0686695278969957,0.6052631578947368,0.6052631578947368,model,approach
face_detection,16,experimental results,face detection,321,7,1,Face Detection, , ,0.6888412017167382,0.0526315789473684,0.0344827586206896,experiments,approach
face_detection,16,experimental results,face detection,333,19,13,"As can be seen from these figures , both HyperFace and HF - ResNet outperform all the reported academic and commercial detectors on the AFW and PASCAL datasets .",compares the performance of different detectors using the Receiver Operating Characteristic ( ROC ) curves on the FDDB dataset .,"HyperFace achieves a high mean average precision ( m AP ) of 97.9 % and 92.46 % , for AFW and PASCAL datasets respectively .",0.7145922746781116,0.1428571428571428,0.4482758620689655,experiments,baselines
face_detection,16,experimental results,face detection,334,20,14,"HyperFace achieves a high mean average precision ( m AP ) of 97.9 % and 92.46 % , for AFW and PASCAL datasets respectively .","As can be seen from these figures , both HyperFace and HF - ResNet outperform all the reported academic and commercial detectors on the AFW and PASCAL datasets .",HF - ResNet further improves the m AP to 99.4 % and 96.2 %,0.7167381974248928,0.150375939849624,0.4827586206896552,experiments,approach
face_detection,16,experimental results,face detection,335,21,15,HF - ResNet further improves the m AP to 99.4 % and 96.2 %,"HyperFace achieves a high mean average precision ( m AP ) of 97.9 % and 92.46 % , for AFW and PASCAL datasets respectively .",respectively .,0.7188841201716738,0.1578947368421052,0.5172413793103449,experiments,approach
face_detection,16,experimental results,face detection,340,26,20,"In spite of these issues , HyperFace performance is comparable to recently published deep learning - based face detection methods such as DP2MFD and Faceness on the FDDB dataset 1 with m AP of 90.1 % .","Second , re-sizing small faces to the input size of 227 227 adds distortion to the face resulting in low detection score .","It is interesting to note the performance differences between R - CNN Face , Multitask Face and HyperFace for the face detection tasks .",0.7296137339055794,0.1954887218045112,0.6896551724137931,experiments,baselines
face_detection,16,experimental results,face detection,342,28,22,clearly show that multitask CNNs ( Multitask Face and HyperFace ) outperform R - CNN Face by a wide margin .,"It is interesting to note the performance differences between R - CNN Face , Multitask Face and HyperFace for the face detection tasks .",The boost in the performance gain is mainly due to the following two reasons .,0.7339055793991416,0.2105263157894736,0.7586206896551724,experiments,baselines
face_detection,16,experimental results,landmarks localization,350,36,1,Landmarks Localization, , ,0.7510729613733905,0.2706766917293233,0.0232558139534883,experiments,experimental-setup
face_detection,16,experimental results,landmarks localization,370,56,21,shows that HyperFace performs consistently accurate overall pose angles .,"As can be seen from the figures , R - CNN Fiducial , Multitask Face , HyperFace and HF - ResNet outperform many recent state - of - the - art landmark localization methods including FaceDPL , 3DDFA and SDM .","This clearly suggests that while most of the methods work well on frontal faces , HyperFace is able to predict landmarks for faces with full pose variations .",0.7939914163090128,0.4210526315789473,0.4883720930232558,experiments,baselines
face_detection,16,experimental results,landmarks localization,372,58,23,"Moreover , we find that R - CNN Fiducial and Multitask Face attain similar performance .","This clearly suggests that while most of the methods work well on frontal faces , HyperFace is able to predict landmarks for faces with full pose variations .",The HyperFace has an advantage over them as it uses the intermediate layers for fusion .,0.7982832618025751,0.4360902255639097,0.5348837209302325,experiments,baselines
face_detection,16,experimental results,landmarks localization,376,62,27,"Additionally , we observe that HF - ResNet significantly improves the performance over HyperFace for both AFW and AFLW datasets .",Fusing the layers brings out that hidden information which boosts the performance for the landmark localization task .,The large margin in performance can be attributed to the larger depth for the HF - ResNet model .,0.8068669527896996,0.4661654135338346,0.627906976744186,experiments,baselines
face_detection,16,experimental results,landmarks localization,389,75,40,"We observe that HyperFace achieves a comparable NME of 10.88 , while HF - ResNet achieves the state - of - theart result on IBUG with NME of 8.18 .",compares the Normalized Mean Error ( NME ) obtained by HyperFace and HF - ResNet with other recently published methods .,This shows the effectiveness of the proposed models for 68 - point landmarks localization .,0.8347639484978541,0.5639097744360902,0.9302325581395348,experiments,baselines
face_detection,16,experimental results,pose estimation,393,79,1,Pose Estimation, , ,0.8433476394849786,0.5939849624060151,0.0588235294117647,experiments,approach
face_detection,16,experimental results,pose estimation,401,87,9,"As can be seen from the figure , both HyperFace and HF - ResNet outperform existing methods by a large margin .",The curve provides the fraction of faces for which the estimated pose is within some error tolerance .,"For the AFLW dataset , we do not have pose estimation evaluation for any previous method .",0.8605150214592274,0.6541353383458647,0.5294117647058824,experiments,baselines
face_detection,16,experimental results,pose estimation,409,95,17,"HF - ResNet further improves the performance for roll , pitch as well as yaw .",It shows that tasks which rely on the structure and orientation of the face work well with features from lower layers of the CNN ., ,0.8776824034334764,0.7142857142857143,1.0,experiments,approach
face_detection,16,experimental results,gender recognition,410,96,1,Gender Recognition, , ,0.8798283261802575,0.7218045112781954,0.0454545454545454,experiments,approach
face_detection,16,experimental results,gender recognition,418,104,9,"On the LFWA dataset , our method outperforms PANDA and FaceTracer , and is equal to .",The gender recognition performance of different methods is reported in .,"On the Celeb A dataset , our method performs comparably to .",0.8969957081545065,0.7819548872180451,0.4090909090909091,experiments,baselines
face_detection,16,experimental results,gender recognition,423,109,14,HF - ResNet achieves state - of - the - art results on both CelebA and LFWA datasets .,"Again , we do not see much difference in the performance of Multitask Face and HyperFace suggesting intermediate layers do not contribute much for the gender recognition task .","provides an experimental analysis of the postprocessing methods : IRP and L - NMS , for face detection task on the AFW dataset .",0.907725321888412,0.8195488721804511,0.6363636363636364,experiments,baselines
face_detection,16,experimental results,gender recognition,427,113,18,The HyperFace with a linear bounding box regression and traditional NMS achieves a m AP of 94 % .,"On the other hand , Quality SS refers to its slow version which outputs more than 10 , 000 region proposals consuming more than 10s for one image .",Just by replacing them with L - NMS provides a boost of 1.2 % .,0.9163090128755365,0.849624060150376,0.8181818181818182,experiments,baselines
face_detection,17,abstract,abstract,4,2,2,"Face detection has been well studied for many years and one of remaining challenges is to detect small , blurred and partially occluded faces in uncontrolled environment .", ,"This paper proposes a novel contextassisted single shot face detector , named PyramidBox to handle the hard face detection problem .",0.0166666666666666,0.1818181818181818,0.1818181818181818,research-problem,baselines
face_detection,17,abstract,abstract,12,10,10,Our code is available in Pad - dlePaddle : https://github.com/PaddlePaddle/models/tree/develop/,"By exploiting the value of context , PyramidBox achieves superior performance among the state - of - the - art over the two common face detection benchmarks , FDDB and WIDER FACE .",fluid/face_detection .,0.05,0.9090909090909092,0.9090909090909092,code,baselines
face_detection,17,introduction,introduction,31,18,18,"In this work , we use a semi-supervised solution to generate approximate labels for contextual parts related to faces and a series of anchors called PyramidAnchors are invented to be easily added to general anchor - based architectures .","To achieve this goal , extra labels are needed and the anchors matched to these parts should be designed .","Secondly , high - level contextual features should be adequately combined with the low - level ones .",0.1291666666666666,0.4736842105263158,0.4736842105263158,model,experimental-setup
face_detection,17,introduction,introduction,34,21,21,We investigate the performance of Feature Pyramid Networks ( FPN ) and modify it into a Low - level Feature Pyramid Network ( LFPN ) to join mutually helpful features together .,"The appearances of hard and easy faces can be quite differ -ent , which implies that not all high - level semantic features are really helpful to smaller targets .","Thirdly , the predict branch network should make full use of the joint feature .",0.1416666666666666,0.5526315789473685,0.5526315789473685,model,ablation-analysis
face_detection,17,introduction,introduction,36,23,23,We introduce the Context - sensitive prediction module ( CPM ) to incorporate context information around the target face with a wider and deeper network .,"Thirdly , the predict branch network should make full use of the joint feature .","Meanwhile , we propose a max - in - out layer for the prediction module to further improve the capability of classification network .",0.15,0.6052631578947368,0.6052631578947368,model,experimental-setup
face_detection,17,introduction,introduction,37,24,24,"Meanwhile , we propose a max - in - out layer for the prediction module to further improve the capability of classification network .",We introduce the Context - sensitive prediction module ( CPM ) to incorporate context information around the target face with a wider and deeper network .,"In addition , we propose a training strategy named as Data - anchor - sampling to make an adjustment on the distribution of the training dataset .",0.1541666666666666,0.631578947368421,0.631578947368421,model,baselines
face_detection,17,introduction,introduction,38,25,25,"In addition , we propose a training strategy named as Data - anchor - sampling to make an adjustment on the distribution of the training dataset .","Meanwhile , we propose a max - in - out layer for the prediction module to further improve the capability of classification network .","In order to learn more representable features , the diversity of hard - set samples is important and can be gained by data augmentation across samples .",0.1583333333333333,0.6578947368421053,0.6578947368421053,model,approach
face_detection,17,experiments,evaluation on benchmark,219,29,3,FDDB Dataset .,"We evaluate our PyramidBox on the most popular face detection benchmarks , including Face Detection Data Set and Benchmark ( FDDB ) and WIDER FACE .","It has 5 , 171 faces in 2 , 845 images collected from the Yahoo !",0.9125,0.7631578947368421,0.25,experiments,experiments
face_detection,17,experiments,evaluation on benchmark,223,33,7,The PyramidBox achieves state - ofart performance and the result is shown in and .,We evaluate our face detector on FDDB against the other state - of - art methods .,WIDER FACE Dataset .,0.9291666666666668,0.8684210526315791,0.5833333333333334,experiments,baselines
face_detection,17,experiments,evaluation on benchmark,224,34,8,WIDER FACE Dataset .,The PyramidBox achieves state - ofart performance and the result is shown in and .,"It contains 32 , 203 images and 393 , 703 annotated faces with a high degree of variability in scale , pose and occlusion .",0.9333333333333332,0.8947368421052632,0.6666666666666666,experiments,experiments
face_detection,17,experiments,evaluation on benchmark,228,38,12,"Our PyramidBox outperforms others across all three subsets , i.e. 0.961 ( easy ) , 0.950 ( medium ) , 0.889 ( hard ) for validation set , and 0.956 ( easy ) , 0.946 ( medium ) , 0.887 ( hard ) for testing set .","Our PyramidBox is trained only on the training set and evaluated on both validation set and testing set comparing with the state - of - the - art face detectors , such as. presents the precision - recall curves and mAP values .", ,0.95,1.0,1.0,experiments,baselines
face_detection,18,title,title,2,2,2,CMS- RCNN : Contextual Multi- Scale Region - based CNN for Unconstrained Face Detection, , ,0.006269592476489,1.0,1.0,research-problem,experimental-setup
face_detection,18,abstract,abstract,4,2,2,"Robust face detection in the wild is one of the ultimate components to support various facial related problems , i.e. unconstrained face recognition , facial periocular recognition , facial landmarking and pose estimation , facial expression recognition , 3 D facial model construction , etc .", ,"Although the face detection problem has been intensely studied for decades with various commercial applications , it still meets problems in some real - world scenarios due to numerous challenges , e.g. heavy facial occlusions , extremely low resolutions , strong illumination , exceptionally pose variations , image or video compression artifacts , etc .",0.012539184952978,0.0512820512820512,0.0512820512820512,research-problem,model
face_detection,18,abstract,abstract,5,3,3,"Although the face detection problem has been intensely studied for decades with various commercial applications , it still meets problems in some real - world scenarios due to numerous challenges , e.g. heavy facial occlusions , extremely low resolutions , strong illumination , exceptionally pose variations , image or video compression artifacts , etc .","Robust face detection in the wild is one of the ultimate components to support various facial related problems , i.e. unconstrained face recognition , facial periocular recognition , facial landmarking and pose estimation , facial expression recognition , 3 D facial model construction , etc .","In this paper , we present a face detection approach named Contextual Multi - Scale Region - based Convolution Neural Network ( CMS - RCNN ) to robustly solve the problems mentioned above .",0.0156739811912225,0.0769230769230769,0.0769230769230769,research-problem,ablation-analysis
face_detection,18,abstract,abstract,21,19,19,"This paper presents an advanced CNN based approach named Contextual Multi - Scale Region - based CNN ( CMS - RCNN ) to handle the problem of face detection in digital face images collected under numerous challenging conditions , e.g. heavy facial occlusion , illumination , extreme offangle , low - resolution , scale difference , etc .","Although the detection accuracy in recent face detection algorithms , , , , , has been highly improved due to the advancement of deep Convolutional Neural Networks ( CNN ) , they are still far from achieving the same detection capabilities as a human due to a number of challenges , are always the important factors that need to be considered .","Our designed region - based CNN architecture allows the network to simultaneously look at multi-scale features , as well as to explicitly look outside facial regions as the potential body regions .",0.0658307210031347,0.4871794871794872,0.4871794871794872,model,baselines
face_detection,18,abstract,abstract,22,20,20,"Our designed region - based CNN architecture allows the network to simultaneously look at multi-scale features , as well as to explicitly look outside facial regions as the potential body regions .","This paper presents an advanced CNN based approach named Contextual Multi - Scale Region - based CNN ( CMS - RCNN ) to handle the problem of face detection in digital face images collected under numerous challenging conditions , e.g. heavy facial occlusion , illumination , extreme offangle , low - resolution , scale difference , etc .","In other words , this process tries to mimic the way of face detection by human in a sense that when humans are not sure about a face , seeing the body will increase our confidence .",0.0689655172413793,0.5128205128205128,0.5128205128205128,model,experimental-setup
face_detection,18,abstract,abstract,24,22,22,Additionally this architecture also helps to synchronize both the global semantic features in high level layers and the localization features in low level layers for facial representation .,"In other words , this process tries to mimic the way of face detection by human in a sense that when humans are not sure about a face , seeing the body will increase our confidence .","Therefore , it is able to robustly deal with the challenges in the problem of unconstrained face detection .",0.0752351097178683,0.5641025641025641,0.5641025641025641,model,baselines
face_detection,18,abstract,abstract,26,24,24,Our CMS - RCNN method introduces the Multi - Scale Region Proposal Network ( MS - RPN ) to generate a set of region candidates and the Contextual Multi - Scale Convolution Neural Network ( CMS - CNN ) to do inference on the region candidates of facial regions .,"Therefore , it is able to robustly deal with the challenges in the problem of unconstrained face detection .",A confidence score and bounding box regression are computed for every candidate .,0.0815047021943573,0.6153846153846154,0.6153846153846154,model,baselines
face_detection,18,contextual multi scale r cnn,implementation details,228,94,2,Our CMS - RCNN is implemented in the Caffe deep learning framework ., ,"The first 5 sets of convolution layers have the same architecture as the deep VGG - 16 model , and during training their parameters are initialized from the pre-trained VGG - 16 .",0.7147335423197492,0.8703703703703703,0.125,experimental-setup,experimental-setup
face_detection,18,contextual multi scale r cnn,implementation details,229,95,3,"The first 5 sets of convolution layers have the same architecture as the deep VGG - 16 model , and during training their parameters are initialized from the pre-trained VGG - 16 .",Our CMS - RCNN is implemented in the Caffe deep learning framework .,"For simplicity we refer to the last convolution layers in set 3 , 4 and 5 as ' conv3 ' , ' conv4 ' , and ' conv5 ' respectively .",0.7178683385579937,0.8796296296296297,0.1875,experimental-setup,experimental-setup
face_detection,18,contextual multi scale r cnn,implementation details,232,98,6,"In the MS - RPN , we want ' conv3 ' , ' conv4 ' , and ' conv5 ' to be synchronized to the same size so that concatenation can be applied .",All the following layers are connected exclusively to these three layers .,So ' conv3 ' is followed by pooling layer to perform down - sampling .,0.7272727272727273,0.9074074074074074,0.375,experimental-setup,model
face_detection,18,contextual multi scale r cnn,implementation details,233,99,7,So ' conv3 ' is followed by pooling layer to perform down - sampling .,"In the MS - RPN , we want ' conv3 ' , ' conv4 ' , and ' conv5 ' to be synchronized to the same size so that concatenation can be applied .","Then ' conv3 ' , ' conv4 ' , and ' conv5 ' are normalized along the channel axis to a learnable re-weighting scale and concatenated together .",0.7304075235109718,0.9166666666666666,0.4375,experimental-setup,experimental-setup
face_detection,18,contextual multi scale r cnn,implementation details,234,100,8,"Then ' conv3 ' , ' conv4 ' , and ' conv5 ' are normalized along the channel axis to a learnable re-weighting scale and concatenated together .",So ' conv3 ' is followed by pooling layer to perform down - sampling .,"To ensure training convergence , the initial re-weighting scale needs to be carefully set .",0.7335423197492164,0.925925925925926,0.5,experimental-setup,experimental-setup
face_detection,18,contextual multi scale r cnn,implementation details,235,101,9,"To ensure training convergence , the initial re-weighting scale needs to be carefully set .","Then ' conv3 ' , ' conv4 ' , and ' conv5 ' are normalized along the channel axis to a learnable re-weighting scale and concatenated together .","Here we set the initial scale of ' conv3 ' , ' conv4 ' , and ' conv5 ' to be 66.84 , 94.52 , and 94.52 respectively .",0.7366771159874608,0.9351851851851852,0.5625,experimental-setup,experimental-setup
face_detection,18,contextual multi scale r cnn,implementation details,236,102,10,"Here we set the initial scale of ' conv3 ' , ' conv4 ' , and ' conv5 ' to be 66.84 , 94.52 , and 94.52 respectively .","To ensure training convergence , the initial re-weighting scale needs to be carefully set .","In the CMS - CNN , the RoI pooling layer already ensure that the pooled feature maps have the same size .",0.7398119122257053,0.9444444444444444,0.625,experimental-setup,experimental-setup
face_detection,18,contextual multi scale r cnn,implementation details,237,103,11,"In the CMS - CNN , the RoI pooling layer already ensure that the pooled feature maps have the same size .","Here we set the initial scale of ' conv3 ' , ' conv4 ' , and ' conv5 ' to be 66.84 , 94.52 , and 94.52 respectively .",Again we normalize the pooled features to make sure the downstream values are at reasonable scales when training is initialized .,0.7429467084639498,0.9537037037037036,0.6875,experimental-setup,experimental-setup
face_detection,18,contextual multi scale r cnn,implementation details,239,105,13,"Specifically , features pooled from ' conv3 ' , ' conv4 ' , and ' conv5 ' are initialized with scale to be 57.75 , 81.67 , and 81.67 respectively , for both face and body pipelines .",Again we normalize the pooled features to make sure the downstream values are at reasonable scales when training is initialized .,"The MS - RPN and the CMS - CNN share the same parameters for all convolution layers so that computation can be done once , resulting in higher efficiency .",0.7492163009404389,0.9722222222222222,0.8125,experimental-setup,experimental-setup
face_detection,18,contextual multi scale r cnn,implementation details,240,106,14,"The MS - RPN and the CMS - CNN share the same parameters for all convolution layers so that computation can be done once , resulting in higher efficiency .","Specifically , features pooled from ' conv3 ' , ' conv4 ' , and ' conv5 ' are initialized with scale to be 57.75 , 81.67 , and 81.67 respectively , for both face and body pipelines .","Additionally , in order to shrink the channel size of the concatenated feature map , a 11 convolution layer is then employed .",0.7523510971786834,0.9814814814814816,0.875,experimental-setup,experimental-setup
face_detection,18,contextual multi scale r cnn,implementation details,241,107,15,"Additionally , in order to shrink the channel size of the concatenated feature map , a 11 convolution layer is then employed .","The MS - RPN and the CMS - CNN share the same parameters for all convolution layers so that computation can be done once , resulting in higher efficiency .",Therefore the channel size of final feature map is at the same size as the original fifth convolution layer in Faster R - CNN .,0.7554858934169278,0.9907407407407408,0.9375,experimental-setup,experimental-setup
face_detection,18,experiments,experiments on wider face dataset,249,7,1,Experiments on WIDER FACE Dataset, , ,0.780564263322884,0.1044776119402985,0.0217391304347826,experiments,approach
face_detection,18,experiments,experiments on wider face dataset,268,26,20,"It achieves the best average precision in all level faces , i.e. AP = 0.902 ( Easy ) , 0.874 ( Medium ) and 0.643 ( Hard ) , and outperforms the second best baseline by 26.0 % ( Easy ) , 37.4 % ( Medium ) and 60.8 % ( Hard ) .",Our method outperforms those strong baselines by a large margin .,"These results suggest that as the difficulty level goes up , CMS - RCNN can detect challenging faces better .",0.8401253918495298,0.3880597014925373,0.4347826086956521,experiments,baselines
face_detection,18,experiments,experiments on fddb face database,295,53,1,Experiments on FDDB Face Database, , ,0.9247648902821316,0.7910447761194029,0.0666666666666666,experiments,experimental-setup
face_detection,18,experiments,experiments on fddb face database,304,62,10,Our method achieves the best recall rate on this database .,The evaluation is proceeded following the FDDB evaluation protocol and .,Numbers in the legend show the average precision scores .,0.9529780564263324,0.9253731343283582,0.6666666666666666,experiments,baselines
face_detection,18,experiments,experiments on fddb face database,307,65,13,The proposed CMS - RCNN approach outperforms most of the published face detection methods and achieves a very high recall rate comparing against all other methods ( as shown ) .,"compared against the published methods provided in the protocol , i.e. HyperFace , DP2 MFD , CCF , Faceness , NPDFace , MultiresHPM , DDFD , CascadeCNN , ACF - multiscale , Pico , HeadHunter , Joint Cascade , Boosted Exemplar , and PEP - Adapt .",This is concrete evidence to demonstrate that CMS - RCNN robustly detects unconstrained faces .,0.962382445141066,0.9701492537313432,0.8666666666666667,experiments,baselines
face_detection,19,title,title,2,2,2,Face Detection Using Improved Faster RCNN, , ,0.0181818181818181,1.0,1.0,research-problem,experimental-setup
face_detection,19,abstract,abstract,18,16,16,"In this report , we propose a detailed design Faster RCNN method named FDNet1.0 for face detection , which achieves more decent performance than previous methods .","However , there are still some issues with these methods that can be improved with elaborate design of the details .","A deformable layer with fewer channels is attached to the backbone network to produce a "" thin "" feature map , which is subsequently fed to a full connected layer , building an efficient yet accurate two - stage detector .",0.1636363636363636,0.3809523809523809,0.3809523809523809,model,baselines
face_detection,19,abstract,abstract,19,17,17,"A deformable layer with fewer channels is attached to the backbone network to produce a "" thin "" feature map , which is subsequently fed to a full connected layer , building an efficient yet accurate two - stage detector .","In this report , we propose a detailed design Faster RCNN method named FDNet1.0 for face detection , which achieves more decent performance than previous methods .","At testing time , we also find a comparable mean average precision ( m AP ) be achieved when the top - ranked proposals ( e.g. , 6000 ) are directly selected without NMS in the RPN stage over WIDER FACE dataset .",0.1727272727272727,0.4047619047619048,0.4047619047619048,model,baselines
face_detection,19,abstract,abstract,20,18,18,"At testing time , we also find a comparable mean average precision ( m AP ) be achieved when the top - ranked proposals ( e.g. , 6000 ) are directly selected without NMS in the RPN stage over WIDER FACE dataset .","A deformable layer with fewer channels is attached to the backbone network to produce a "" thin "" feature map , which is subsequently fed to a full connected layer , building an efficient yet accurate two - stage detector .",It is also beneficial for hard set to keep the small proposals ( < 16 pixels width / height ) at training and testing stage as there are many tinny faces of WIDER FACE dataset .,0.1818181818181818,0.4285714285714285,0.4285714285714285,model,baselines
face_detection,19,abstract,abstract,22,20,20,"Furthermore , the multi-scale training and testing strategy are also applied in our work .",It is also beneficial for hard set to keep the small proposals ( < 16 pixels width / height ) at training and testing stage as there are many tinny faces of WIDER FACE dataset .,Our key contributions are summarized as follows : ( 1 ) A light head based two - stage framework named FDNet1.0 is developed for face detection .,0.2,0.4761904761904762,0.4761904761904762,model,baselines
face_detection,19,experiments,implementation details,84,9,2,Single NVIDIA Tesla K80 is used for training and testing ., ,Mini batch size is set to 1 considering memory consumption .,0.7636363636363637,0.3103448275862069,0.1111111111111111,experimental-setup,approach
face_detection,19,experiments,implementation details,85,10,3,Mini batch size is set to 1 considering memory consumption .,Single NVIDIA Tesla K80 is used for training and testing .,"Specifically , ResNet_v1_101 trained on ImageNet - 128w is used for Faster RCNN feature extraction .",0.7727272727272727,0.3448275862068966,0.1666666666666666,experimental-setup,approach
face_detection,19,experiments,implementation details,86,11,4,"Specifically , ResNet_v1_101 trained on ImageNet - 128w is used for Faster RCNN feature extraction .",Mini batch size is set to 1 considering memory consumption .,It is helpful to freeze the first two blocks in the training stage as data size of WIDER FACE is not so large .,0.7818181818181819,0.3793103448275862,0.2222222222222222,experimental-setup,experimental-setup
face_detection,19,experiments,implementation details,89,14,7,"Aspect ratios ( 1 , 1.5 , 2 ) and scales ( 16 2 , 32 2 , 64 2 , 128 2 , 256 2 , 512 2 ) are carefully designed to capture better locations of faces in the RPN stage , and the number of filters for the RPN layer is set as 512 .","A deformable layer is used to output a "" thin "" feature map with exploiting image context .",The anchors with highest IoU score or IoU score with the ground truth above 0.7 are defined as positive .,0.8090909090909091,0.4827586206896552,0.3888888888888889,experimental-setup,baselines
face_detection,19,experiments,implementation details,94,19,12,"By the way , the batch size of RPN and R - CNN is respectively assigned as 256 and 128 .","The anchors with IoU score with the ground truth above 0.5 are assigned as positive , IoU score that is lower than 0.3 is defined as negative , IoU score above 0.3 but lower than 0.5 will be ignored .","The initial learning rate is set to 1e - 3 , and decrease to 1e - 4 after 20w iterations .",0.8545454545454545,0.6551724137931034,0.6666666666666666,experimental-setup,experimental-setup
face_detection,19,experiments,implementation details,95,20,13,"The initial learning rate is set to 1e - 3 , and decrease to 1e - 4 after 20w iterations .","By the way , the batch size of RPN and R - CNN is respectively assigned as 256 and 128 .",Weight decay is and momentum is set to 1e - 4 and 0.9 respectively .,0.8636363636363636,0.6896551724137931,0.7222222222222222,experimental-setup,experimental-setup
face_detection,19,experiments,implementation details,96,21,14,Weight decay is and momentum is set to 1e - 4 and 0.9 respectively .,"The initial learning rate is set to 1e - 3 , and decrease to 1e - 4 after 20w iterations .","In testing stage , multi-scale testing strategy is adapted to be robust to different scale faces .",0.8727272727272727,0.7241379310344828,0.7777777777777778,experimental-setup,approach
face_detection,19,experiments,comparison on benchmarks,103,28,3,"Compared with the recently published top approaches , FDNet1.0 wins two 1st places ( easy set = 95.9 % , medium set = 94.5 % ) and one 2nd place ( hard set = 87.9 % ) on the validation set , as illustrated in .",Our model is trained on the train set and evaluated on WIDER FACE validation set .,We believe that more kinds of data augmentation and hard example mining would further boost detection performance .,0.9363636363636364,0.9655172413793104,0.75,results,baselines
face_detection,2,title,title,2,2,2,Selective Refinement Network for High Performance Face Detection, , ,0.0084388185654008,1.0,1.0,research-problem,approach
face_detection,2,abstract,abstract,4,2,2,"High performance face detection remains a very challenging problem , especially when there exists many tiny faces .", ,"This paper presents a novel single - shot face detector , named Selective Refinement Network ( SRN ) , which introduces novel twostep classification and regression operations selectively into an anchor- based face detector to reduce false positives and improve location accuracy simultaneously .",0.0168776371308016,0.25,0.25,research-problem,baselines
face_detection,2,introduction,introduction,43,33,33,"In this paper , we investigate the effects of two - step classification and regression on different levels of detection layers and propose a novel face detection framework , named Selective Refinement Network ( SRN ) , which selectively applies two - step classification and regression to specific levels of detection layers .","However , blindly adding multi-step regression to the specific task ( i.e. , face detection ) is often counterproductive .","The network structure of SRN is shown in , which consists of two key modules , named as the Selective Two - step Classification ( STC ) module and the Selective Two - step Regression ( STR ) module .",0.1814345991561181,0.7333333333333333,0.7333333333333333,model,ablation-analysis
face_detection,2,introduction,introduction,44,34,34,"The network structure of SRN is shown in , which consists of two key modules , named as the Selective Two - step Classification ( STC ) module and the Selective Two - step Regression ( STR ) module .","In this paper , we investigate the effects of two - step classification and regression on different levels of detection layers and propose a novel face detection framework , named Selective Refinement Network ( SRN ) , which selectively applies two - step classification and regression to specific levels of detection layers .","Specifically , the STC is applied to filter out most simple negative samples ( illustrated in ( a ) ) from the low levels of detection layers , which contains 88.9 % samples .",0.1856540084388185,0.7555555555555555,0.7555555555555555,model,model
face_detection,2,introduction,introduction,45,35,35,"Specifically , the STC is applied to filter out most simple negative samples ( illustrated in ( a ) ) from the low levels of detection layers , which contains 88.9 % samples .","The network structure of SRN is shown in , which consists of two key modules , named as the Selective Two - step Classification ( STC ) module and the Selective Two - step Regression ( STR ) module .","As shown in , RetinaNet with STC improves the recall efficiency to a certain extent .",0.1898734177215189,0.7777777777777778,0.7777777777777778,model,model
face_detection,2,introduction,introduction,48,38,38,"In addition , we design a Receptive Field Enhancement ( RFE ) to provide more diverse receptive fields to better capture the extreme - pose faces .","On the other hand , the design of STR draws on the cascade idea to coarsely adjust the locations and sizes of anchors ( illustrated in ( c ) ) from high levels of detection layers to provide better initialization for the subsequent regressor .","Extensive experiments have been conducted on AFW , PASCAL face , FDDB , and WIDER FACE benchmarks and we set a new state - of - the - art performance .",0.2025316455696202,0.8444444444444444,0.8444444444444444,model,approach
face_detection,2,training and inference,training and inference,175,20,20,"The loss function for SRN is just the sum of the STC loss and the STR loss , i.e. , L = L STC + L STR .",Optimization .,"The backbone network is initialized by the pretrained ResNet - 50 model and all the parameters in the newly added convolution layers are initialized by the "" xavier "" method .",0.7383966244725738,0.7142857142857143,0.7142857142857143,experimental-setup,model
face_detection,2,training and inference,training and inference,176,21,21,"The backbone network is initialized by the pretrained ResNet - 50 model and all the parameters in the newly added convolution layers are initialized by the "" xavier "" method .","The loss function for SRN is just the sum of the STC loss and the STR loss , i.e. , L = L STC + L STR .","We fine - tune the SRN model using SGD with 0.9 momentum , 0.0001 weight decay , and batch size 32 .",0.7426160337552743,0.75,0.75,experimental-setup,experimental-setup
face_detection,2,training and inference,training and inference,177,22,22,"We fine - tune the SRN model using SGD with 0.9 momentum , 0.0001 weight decay , and batch size 32 .","The backbone network is initialized by the pretrained ResNet - 50 model and all the parameters in the newly added convolution layers are initialized by the "" xavier "" method .","We set the learning rate to 10 ?2 for the first 100 epochs , and decay it to 10 ? 3 and 10 ? 4 for another 20 and 10 epochs , respectively .",0.7468354430379747,0.7857142857142857,0.7857142857142857,experimental-setup,baselines
face_detection,2,training and inference,training and inference,178,23,23,"We set the learning rate to 10 ?2 for the first 100 epochs , and decay it to 10 ? 3 and 10 ? 4 for another 20 and 10 epochs , respectively .","We fine - tune the SRN model using SGD with 0.9 momentum , 0.0001 weight decay , and batch size 32 .",We implement SRN using the Py - Torch library .,0.7510548523206751,0.8214285714285714,0.8214285714285714,experimental-setup,baselines
face_detection,2,training and inference,training and inference,179,24,24,We implement SRN using the Py - Torch library .,"We set the learning rate to 10 ?2 for the first 100 epochs , and decay it to 10 ? 3 and 10 ? 4 for another 20 and 10 epochs , respectively .",Inference .,0.7552742616033755,0.8571428571428571,0.8571428571428571,experimental-setup,baselines
face_detection,2,experiments,evaluation on benchmark,213,30,2,AFW Dataset ., ,It consists of 205 images with 473 labeled faces .,0.8987341772151899,0.625,0.1,results,baselines
face_detection,2,experiments,evaluation on benchmark,217,34,6,"As shown in , SRN outperforms these state - of - the - art methods with the top AP score ( 99.87 % ) .","We compare SRN against seven state - of the - art methods and three commercial face detectors ( i.e. , Face.com , Face + + and Picasa ) .",PASCAL Face Dataset .,0.9156118143459916,0.7083333333333334,0.3,results,baselines
face_detection,2,experiments,evaluation on benchmark,218,35,7,PASCAL Face Dataset .,"As shown in , SRN outperforms these state - of - the - art methods with the top AP score ( 99.87 % ) .","It has 1 , 335 labeled faces in 851 images with large face appearance and pose variations .",0.919831223628692,0.7291666666666666,0.35,results,experiments
face_detection,2,experiments,evaluation on benchmark,221,38,10,SRN achieves the state - of - the - art results by improving 4.99 % AP score compared to the second best method STN .,"We present the precision - recall curves of the proposed SRN method and six state - of - the - art methods and three commercial face detectors ( i.e. , SkyBiometry , Face + + and Picasa ) in .",FDDB Dataset .,0.9324894514767932,0.7916666666666666,0.5,results,baselines
face_detection,2,experiments,evaluation on benchmark,222,39,11,FDDB Dataset .,SRN achieves the state - of - the - art results by improving 4.99 % AP score compared to the second best method STN .,"It contains 5 , 171 faces annotated in 2 , 845 images with a wide range of difficulties , such as occlusions , difficult poses , and low image resolutions .",0.9367088607594936,0.8125,0.55,results,experiments
face_detection,2,experiments,evaluation on benchmark,225,42,14,"As shown in ( c ) , our SRN sets a new state - of - the - art performance , i.e. , 98.8 % true positive rate when the number of false positives is equal to 1000 .",We evaluate the proposed SRN detector on the FDDB dataset and compare it with several state - of - the - art methods .,"These results indicate that SRN is robust to varying scales , large appearance changes , heavy occlusions , and severe blur degradations that are prevalent in detecting face in unconstrained real - life scenarios .",0.9493670886075948,0.875,0.7,results,baselines
face_detection,2,experiments,evaluation on benchmark,227,44,16,WIDER FACE Dataset .,"These results indicate that SRN is robust to varying scales , large appearance changes , heavy occlusions , and severe blur degradations that are prevalent in detecting face in unconstrained real - life scenarios .",We compare SRN with eighteen state - of - the - art face detection methods on both the validation and testing sets .,0.9578059071729956,0.9166666666666666,0.8,results,experiments
face_detection,2,experiments,evaluation on benchmark,230,47,19,"As shown in , we find that SRN performs favourably against the state - of - the - art based on the average precision ( AP ) across the three subsets , especially on the Hard subset which contains a large amount of small faces .","To obtain the evaluation results on the testing set , we submit the detection results of SRN to the authors for evaluation .","Specifically , it produces the best AP scores in all subsets of both validation and testing sets , i.e. , 96.4 % ( Easy ) , 95.3 % ( Medium ) and 90.2 % ( Hard ) for validation set , and 95.9 % ( Easy ) , 94.9 % ( Medium ) and 89.7 % ( Hard ) for testing set , surpassing all approaches , which demonstrates the superiority of the proposed detector .",0.9704641350210972,0.9791666666666666,0.95,results,baselines
face_detection,20,title,title,2,2,2,Aggregate Channel Features for Multi-view Face Detection, , ,0.0073529411764705,1.0,1.0,research-problem,experimental-setup
face_detection,20,abstract,abstract,4,2,2,Face detection has drawn much attention in recent decades since the seminal work by Viola and Jones ., ,"While many subsequences have improved the work with more powerful learning algorithms , the feature representation used for face detection still ca n't meet the demand for effectively and efficiently handling faces with large appearance variance in the wild .",0.0147058823529411,0.2857142857142857,0.2857142857142857,research-problem,experimental-setup
face_detection,20,introduction,introduction,11,2,2,Human face detection have long been one of the most fundamental problems in computer vision and humancomputer interaction ., ,"In the past decade , the most influential work should be the face detection framework proposed by Viola and Jones .",0.0404411764705882,0.0666666666666666,0.0666666666666666,research-problem,experimental-setup
face_detection,20,introduction,introduction,25,16,16,"In this paper , we adopt a variant of channel features called aggregate channel features , which are extracted directly as pixel values on subsampled channels .",The classifier learning process follows the VJ framework pipeline .,"Channel extension offers rich representation capacity , while simple feature form guarantees fast computation .",0.0919117647058823,0.5333333333333333,0.5333333333333333,model,experimental-setup
face_detection,20,introduction,introduction,27,18,18,"With these two superiorities , the aggregate channel features breakthrough the bottleneck in VJ framework and have the potential to make great advance in face detection .","Channel extension offers rich representation capacity , while simple feature form guarantees fast computation .","As we mainly concentrate our efforts to the feature representation rather than learning algorithms in this paper , we not only just adopt the aggregate channel features in face detection , but also try to explore the full potential of this novel representation .",0.0992647058823529,0.6,0.6,model,approach
face_detection,20,introduction,introduction,29,20,20,"To do so , we make a deep and all - round investigation into the specific feature parameters concerning channel types , feature pool size , subsampling method , feature scale and soon , which gives insights into the feature design and hopefully provides helpful guidelines for practitioners .","As we mainly concentrate our efforts to the feature representation rather than learning algorithms in this paper , we not only just adopt the aggregate channel features in face detection , but also try to explore the full potential of this novel representation .","Through the deep exploration , we find that : 1 ) multi-scaling the feature representation further enriches the representation capacity since original aggregate channel features have uniform feature scale ; 2 ) different combinations of channel types impact the performance greatly , while for face detection the color channel in LUV space , plus gradient magnitude channel and gradient histograms channels in RGB space show best result ; 3 ) multi-view detection is proven to be a good match with aggregate channel features as the representation naturally encodes the facial structure ( ) .",0.1066176470588235,0.6666666666666666,0.6666666666666666,model,approach
face_detection,20,introduction,introduction,30,21,21,"Through the deep exploration , we find that : 1 ) multi-scaling the feature representation further enriches the representation capacity since original aggregate channel features have uniform feature scale ; 2 ) different combinations of channel types impact the performance greatly , while for face detection the color channel in LUV space , plus gradient magnitude channel and gradient histograms channels in RGB space show best result ; 3 ) multi-view detection is proven to be a good match with aggregate channel features as the representation naturally encodes the facial structure ( ) .","To do so , we make a deep and all - round investigation into the specific feature parameters concerning channel types , feature pool size , subsampling method , feature scale and soon , which gives insights into the feature design and hopefully provides helpful guidelines for practitioners .","Although multi-view detection could effectively deal with diverse poses , additional issues come up as how to merge detections output by separately trained subview detectors , and how to deal with the offsets of location and scale between output detections and ground - truth .",0.1102941176470588,0.7,0.7,model,approach
face_detection,20,experiments,evaluation on benchmark face database,243,6,2,"As shown in , in AFW , our multi-scale detector achieves an ap value of 96.8 % , outperforming other academic methods by a large margin .", ,"When it comes to commercial systems , ours is better than Face.com and almost equal to Face ++ and Google Picasa .",0.8933823529411765,0.2,0.2,results,baselines
face_detection,20,experiments,evaluation on benchmark face database,244,7,3,"When it comes to commercial systems , ours is better than Face.com and almost equal to Face ++ and Google Picasa .","As shown in , in AFW , our multi-scale detector achieves an ap value of 96.8 % , outperforming other academic methods by a large margin .","Note that most of our false positives on AFW database are faces that have n't been annotated ( small , seriously occluded or artificial faces like mask and cartoon character ) .",0.8970588235294118,0.2333333333333333,0.3,results,baselines
face_detection,20,experiments,evaluation on benchmark face database,248,11,7,"In discrete score where evaluation metric is the same as in AFW , our detector achieves 83.7 % , which is a little better than Yan et al ..","For equality , we fix the number of false positives to 284 ( equivalent to an average of 1 False Positive Per Image ) and compare the true positive rate .","Note that the groundtruth in FDDB are elliptical faces , therefore the evaluation metric of an overlap ratio bigger than 0.5 can not reveal the true performance of the proposed detector well .",0.9117647058823528,0.3666666666666665,0.7,results,baselines
face_detection,20,experiments,evaluation on benchmark face database,250,13,9,"When using continuous score which takes the overlap ratio as the score , our method gets 61.9 % true positive rate at 1 FPPI for multiscale version , surpassing other methods which output rectangular detections by a notable margin ( the Yan et al . detector outputs the same elliptical detections as the groundtruth , therefore having advantages with this metric ) .","Note that the groundtruth in FDDB are elliptical faces , therefore the evaluation metric of an overlap ratio bigger than 0.5 can not reveal the true performance of the proposed detector well .",Our detector using single - scale features performs a little worse with the benefit of faster detection speed .,0.9191176470588236,0.4333333333333333,0.9,results,baselines
face_detection,20,experiments,evaluation on benchmark face database,251,14,10,Our detector using single - scale features performs a little worse with the benefit of faster detection speed .,"When using continuous score which takes the overlap ratio as the score , our method gets 61.9 % true positive rate at 1 FPPI for multiscale version , surpassing other methods which output rectangular detections by a notable margin ( the Yan et al . detector outputs the same elliptical detections as the groundtruth , therefore having advantages with this metric ) .", ,0.9227941176470588,0.4666666666666667,1.0,results,baselines
face_detection,21,title,title,2,2,2,Supervised Transformer Network for Efficient Face Detection, , ,0.0067796610169491,1.0,1.0,research-problem,approach
face_detection,21,abstract,abstract,4,2,2,Large pose variations remain to be a challenge that confronts real - word face detection ., ,"We propose a new cascaded Convolutional Neural Network , dubbed the name Supervised Transformer Network , to address this challenge .",0.0135593220338983,0.1818181818181818,0.1818181818181818,research-problem,baselines
face_detection,21,introduction,introduction,33,20,20,"In contrast , we propose a new cascade Convolutional Neural Network that is trained end - to - end .","However , the set of DNNs in Li et al. are trained sequentially , instead of end - to - end , which may not be desirable .","The first stage is a multi-task Region Proposal Network ( RPN ) , which simultaneously proposes candidate face regions along with associated facial landmarks .",0.111864406779661,0.4444444444444444,0.4444444444444444,model,ablation-analysis
face_detection,21,introduction,introduction,34,21,21,"The first stage is a multi-task Region Proposal Network ( RPN ) , which simultaneously proposes candidate face regions along with associated facial landmarks .","In contrast , we propose a new cascade Convolutional Neural Network that is trained end - to - end .","Inspired by Chen et al. , we jointly conduct face detection and face alignment , since face alignment is helpful to distinguish faces / non - faces patterns .",0.1152542372881356,0.4666666666666667,0.4666666666666667,model,model
face_detection,21,introduction,introduction,35,22,22,"Inspired by Chen et al. , we jointly conduct face detection and face alignment , since face alignment is helpful to distinguish faces / non - faces patterns .","The first stage is a multi-task Region Proposal Network ( RPN ) , which simultaneously proposes candidate face regions along with associated facial landmarks .","Different from Li et al. , this network is calculated on the original resolution to better leverage more discriminative information .",0.1186440677966101,0.4888888888888889,0.4888888888888889,model,experimental-setup
face_detection,21,introduction,introduction,36,23,23,"Different from Li et al. , this network is calculated on the original resolution to better leverage more discriminative information .","Inspired by Chen et al. , we jointly conduct face detection and face alignment , since face alignment is helpful to distinguish faces / non - faces patterns .","The alignment step warps each candidate face region to a canonical pose , which maps the facial landmarks into a set of canonical positions .",0.1220338983050847,0.5111111111111111,0.5111111111111111,model,ablation-analysis
face_detection,21,introduction,introduction,38,25,25,"The aligned candidate face region is then fed into the second - stage network , a RCNN , for further verification .","The alignment step warps each candidate face region to a canonical pose , which maps the facial landmarks into a set of canonical positions .",Note we only keep the K face candidate regions with top responses in a local neighborhood from the RPN .,0.1288135593220339,0.5555555555555556,0.5555555555555556,model,model
face_detection,21,introduction,introduction,39,26,26,Note we only keep the K face candidate regions with top responses in a local neighborhood from the RPN .,"The aligned candidate face region is then fed into the second - stage network , a RCNN , for further verification .","In other words , those Non-top K regions are suppressed .",0.1322033898305084,0.5777777777777777,0.5777777777777777,model,model
face_detection,21,introduction,introduction,43,30,30,"We concatenate the feature maps from the two cascaded networks together to form an architecture that is trained end - to - end , as shown in .","Inspired by previous work , which revealed that joint features from different spatial resolutions or scales will improve accuracy .","Note in the learning process , we treat the set of canonical positions also as parameters , which are learnt in the end - to - end learning process .",0.1457627118644068,0.6666666666666666,0.6666666666666666,model,experimental-setup
face_detection,21,introduction,introduction,45,32,32,Note that the canonical positions of the facial landmarks in the aligned face image and the predicted facial landmarks in the candidate face region jointly defines the transform from the candidate face region .,"Note in the learning process , we treat the set of canonical positions also as parameters , which are learnt in the end - to - end learning process .","In the end - to - end training , the training of the first - stage RPN to predict facial landmarks is also supervised by annotated facial landmarks in each true face regions .",0.1525423728813559,0.7111111111111111,0.7111111111111111,model,model
face_detection,21,introduction,introduction,46,33,33,"In the end - to - end training , the training of the first - stage RPN to predict facial landmarks is also supervised by annotated facial landmarks in each true face regions .",Note that the canonical positions of the facial landmarks in the aligned face image and the predicted facial landmarks in the candidate face region jointly defines the transform from the candidate face region .,We hence call our network a Supervised Transformer Network .,0.1559322033898305,0.7333333333333333,0.7333333333333333,model,model
face_detection,21,introduction,introduction,47,34,34,We hence call our network a Supervised Transformer Network .,"In the end - to - end training , the training of the first - stage RPN to predict facial landmarks is also supervised by annotated facial landmarks in each true face regions .","These two characteristics differentiate our model from the Spatial Transformer Network Transformer Network conducts regression on the transformation parameters directly , and b ) it is only supervised by the final recognition objective .",0.159322033898305,0.7555555555555555,0.7555555555555555,model,approach
face_detection,21,introduction,introduction,51,38,38,"Therefore , we propose a region - of - interest ( ROI ) convolution scheme to make the run-time of the Supervised Transformer Network to be more efficient .","However , in practice , the CPU is still the only choice inmost situations .",It first uses a conventional boosting cascade to obtain a set of face candidate areas .,0.1728813559322033,0.8444444444444444,0.8444444444444444,model,ablation-analysis
face_detection,21,introduction,introduction,52,39,39,It first uses a conventional boosting cascade to obtain a set of face candidate areas .,"Therefore , we propose a region - of - interest ( ROI ) convolution scheme to make the run-time of the Supervised Transformer Network to be more efficient .","Then , we combine these regions into irregular binary ROI mask .",0.1762711864406779,0.8666666666666667,0.8666666666666667,model,model
face_detection,21,introduction,introduction,53,40,40,"Then , we combine these regions into irregular binary ROI mask .",It first uses a conventional boosting cascade to obtain a set of face candidate areas .,"All DNN operations ( including convolution , ReLU , pooling , and concatenation ) are all processed inside the ROI mask , and hence significantly reduce the computation .",0.1796610169491525,0.8888888888888888,0.8888888888888888,model,model
face_detection,21,introduction,introduction,54,41,41,"All DNN operations ( including convolution , ReLU , pooling , and concatenation ) are all processed inside the ROI mask , and hence significantly reduce the computation .","Then , we combine these regions into irregular binary ROI mask .",Our contributions are :,0.1830508474576271,0.9111111111111112,0.9111111111111112,model,experimental-setup
face_detection,21,experiments,ablative evaluation of various network components,255,42,13,"As shown in , multi-task RPN , Supervised Transformer , and feature combination will bring about 1 % , 1 % , and 2 % recall improvement respectively .",We manually review the face detection results and add 67 unlabeled faces in the FDDB dataset to make sure all the false alarms are true .,"Besides , these three parts are complementary , remove anyone part will cause a recall drop .",0.8644067796610171,0.5454545454545454,0.5909090909090909,ablation-analysis,approach
face_detection,21,experiments,ablative evaluation of various network components,256,43,14,"Besides , these three parts are complementary , remove anyone part will cause a recall drop .","As shown in , multi-task RPN , Supervised Transformer , and feature combination will bring about 1 % , 1 % , and 2 % recall improvement respectively .","In the training phase , in order to increase the variation of training samples , we randomly select K positive / negative samples from each image for the RCNN network .",0.8677966101694915,0.5584415584415584,0.6363636363636364,ablation-analysis,approach
face_detection,21,experiments,ablative evaluation of various network components,262,49,20,We found that NMS tend to include too much noisy low confidence candidates .,We keep the same number of candidates for both NMS and Non-top K suppression ( K = 3 in the visual result ) .,"We also compare the PR curves of using all candidates , NMS , and non-top K suppression .",0.8881355932203391,0.6363636363636364,0.9090909090909092,ablation-analysis,approach
face_detection,21,experiments,ablative evaluation of various network components,264,51,22,"Our non - top K suppression is very close to using all candidates , and achieved consistently better results than NMS under the same number of candidates .","We also compare the PR curves of using all candidates , NMS , and non-top K suppression .", ,0.8949152542372881,0.6623376623376623,1.0,ablation-analysis,baselines
face_detection,21,experiments,comparing with state of the art,285,72,3,"On the FDDB dataset , we compare with all public methods .",We conduct face detection experiments on three benchmark datasets .,We regress the annotation ellipses with 5 facial points and ignore 67 unlabeled faces to make sure all false alarms are true .,0.9661016949152542,0.935064935064935,0.375,baselines,approach
face_detection,21,experiments,comparing with state of the art,287,74,5,"On the AFW and PASCAL faces datasets , we compare with ( 1 ) deformable part based methods , e.g. structure model and Tree Parts Model ( TSM ) ; ( 2 ) cascade - based methods , e.g .",We regress the annotation ellipses with 5 facial points and ignore 67 unlabeled faces to make sure all false alarms are true .,"Headhunter ; ( 3 ) commercial system , e.g. face.com , Face ++ and Picasa .",0.9728813559322034,0.9610389610389608,0.625,baselines,model
face_detection,21,experiments,comparing with state of the art,288,75,6,"Headhunter ; ( 3 ) commercial system , e.g. face.com , Face ++ and Picasa .","On the AFW and PASCAL faces datasets , we compare with ( 1 ) deformable part based methods , e.g. structure model and Tree Parts Model ( TSM ) ; ( 2 ) cascade - based methods , e.g .","We learn a global regression from 5 facial points to face rectangles to match the annotation for each dataset , and use toolbox from for the evaluation .",0.976271186440678,0.974025974025974,0.75,baselines,model
face_detection,3,abstract,abstract,4,2,2,"We propose a method to address challenges in unconstrained face detection , such as arbitrary pose variations and occlusions .", ,"First , a new image feature called Normalized Pixel Difference ( NPD ) is proposed .",0.0097799511002445,0.2222222222222222,0.2222222222222222,research-problem,experimental-setup
face_detection,3,introduction,introduction,14,3,3,It is the first step in automatic face recognition applications .,The objective of face detection is to find and locate faces in an image .,Face detection has been well studied for frontal and near frontal faces .,0.0342298288508557,0.0638297872340425,0.0638297872340425,research-problem,model
face_detection,3,introduction,introduction,19,8,8,"In this paper , we refer to face detection with arbitrary facial variations as the unconstrained face detection problem .","For example , the Viola - Jones face detector fails to detect most of the face images in the Face Detection Data set and Benchmark ( FDDB ) database ( examples shown in ) due to the difficulties mentioned above .",We are interested in face detection in unconstrained scenarios such as video surveillance or images captured by hand - held devices .,0.0464547677261613,0.1702127659574468,0.1702127659574468,research-problem,experimental-setup
face_detection,3,introduction,introduction,34,23,23,"First , we propose a simple pixel - level feature , called the Normalized Pixel Difference ( NPD ) .","In this paper , we are interested in developing effective features and robust classifiers for unconstrained face detection with arbitrary facial variation .","An NPD is computed as the ratio of the difference between any two pixel intensity values to the sum of their values , in the same form as the Weber Fraction in experimental psychology .",0.0831295843520782,0.4893617021276596,0.4893617021276596,model,ablation-analysis
face_detection,3,introduction,introduction,35,24,24,"An NPD is computed as the ratio of the difference between any two pixel intensity values to the sum of their values , in the same form as the Weber Fraction in experimental psychology .","First , we propose a simple pixel - level feature , called the Normalized Pixel Difference ( NPD ) .","The NPD feature has several desirable properties , such as scale invariance , boundedness , and ability to reconstruct the original image .",0.0855745721271393,0.5106382978723404,0.5106382978723404,model,experimental-setup
face_detection,3,introduction,introduction,36,25,25,"The NPD feature has several desirable properties , such as scale invariance , boundedness , and ability to reconstruct the original image .","An NPD is computed as the ratio of the difference between any two pixel intensity values to the sum of their values , in the same form as the Weber Fraction in experimental psychology .","we further show that NPD features can be obtained from a lookup table , and the resulting face detection template can be easily scaled for multiscale face detection .",0.0880195599022004,0.5319148936170213,0.5319148936170213,model,experimental-setup
face_detection,3,introduction,introduction,37,26,26,"we further show that NPD features can be obtained from a lookup table , and the resulting face detection template can be easily scaled for multiscale face detection .","The NPD feature has several desirable properties , such as scale invariance , boundedness , and ability to reconstruct the original image .","Secondly , we propose a deep quadratic tree learning method and construct a single soft - cascade AdaBoost classifier to handle complex face manifolds and arbitrary pose and occlusion conditions .",0.0904645476772616,0.5531914893617021,0.5531914893617021,model,approach
face_detection,3,introduction,introduction,38,27,27,"Secondly , we propose a deep quadratic tree learning method and construct a single soft - cascade AdaBoost classifier to handle complex face manifolds and arbitrary pose and occlusion conditions .","we further show that NPD features can be obtained from a lookup table , and the resulting face detection template can be easily scaled for multiscale face detection .","While individual NPD features may have "" weak "" discriminative ability , our work indicates that a subset of NPD features can be optimally learned and combined to construct more discriminative features in a deep quadratic tree .",0.0929095354523227,0.5744680851063829,0.5744680851063829,model,baselines
face_detection,3,introduction,introduction,40,29,29,"In this way , different types of faces can be automatically divided into different leaves of a tree classifier , and the complex face manifold in a high dimensional space can be partitioned in the learning process .","While individual NPD features may have "" weak "" discriminative ability , our work indicates that a subset of NPD features can be optimally learned and combined to construct more discriminative features in a deep quadratic tree .","This is the "" divide and conquer "" strategy to tackle unconstrained face detection in a single classifier , without pre-labeling of views in the training set of face images .",0.097799511002445,0.6170212765957447,0.6170212765957447,model,approach
face_detection,3,introduction,introduction,41,30,30,"This is the "" divide and conquer "" strategy to tackle unconstrained face detection in a single classifier , without pre-labeling of views in the training set of face images .","In this way , different types of faces can be automatically divided into different leaves of a tree classifier , and the complex face manifold in a high dimensional space can be partitioned in the learning process .","The resulting face detector is robust to variations in pose , occlusion , and illumination , as well as to blur and low image resolution .",0.1002444987775061,0.6382978723404256,0.6382978723404256,model,experimental-setup
face_detection,3,introduction,introduction,42,31,31,"The resulting face detector is robust to variations in pose , occlusion , and illumination , as well as to blur and low image resolution .","This is the "" divide and conquer "" strategy to tackle unconstrained face detection in a single classifier , without pre-labeling of views in the training set of face images .",The novelty of this work is summarized as follows :,0.1026894865525672,0.6595744680851063,0.6595744680851063,model,baselines
face_detection,3,introduction,introduction,52,41,41,The source code of the proposed method is available in http://www.cbsr.ia.ac.cn/users/scliao/ projects / npdface / .,"The face detector is able to handle illumination variations , pose variations , occlusions , outof - focus blur , and low resolution face images in unconstrained scenarios .",The remainder of this paper is organized as follows .,0.1271393643031785,0.8723404255319149,0.8723404255319149,code,experimental-setup
face_detection,3,npd for face detection,face detector,232,58,28,We used a detection template of 24 24 pixels .,"For bootstrapping nonface images , we also used the AFLW images , but masked the facial regions with random images containing no faces , as shown in .","We set the maximum depth of the tree classifiers to be learned as eight , so that at most eight NPD features need to be evaluated for each tree classifier .",0.5672371638141809,0.7945205479452054,0.6511627906976745,experimental-setup,baselines
face_detection,3,npd for face detection,face detector,233,59,29,"We set the maximum depth of the tree classifiers to be learned as eight , so that at most eight NPD features need to be evaluated for each tree classifier .",We used a detection template of 24 24 pixels .,"In the soft cascade training , we set the threshold of each exit as the minimal score of positive samples , i.e. we did not reject positive samples during training .",0.5696821515892421,0.8082191780821918,0.6744186046511628,experimental-setup,baselines
face_detection,3,npd for face detection,face detector,234,60,30,"In the soft cascade training , we set the threshold of each exit as the minimal score of positive samples , i.e. we did not reject positive samples during training .","We set the maximum depth of the tree classifiers to be learned as eight , so that at most eight NPD features need to be evaluated for each tree classifier .","Our final detector contains 1,226 deep quadratic trees , and 46,401 NPD features .",0.5721271393643031,0.821917808219178,0.6976744186046512,experimental-setup,model
face_detection,3,npd for face detection,face detector,235,61,31,"Our final detector contains 1,226 deep quadratic trees , and 46,401 NPD features .","In the soft cascade training , we set the threshold of each exit as the minimal score of positive samples , i.e. we did not reject positive samples during training .","Nevertheless , the average number of feature evaluations per detection window is only 114.5 considering stagewise nonface rejection , which is quite reasonable .",0.5745721271393643,0.8356164383561644,0.7209302325581395,experimental-setup,baselines
face_detection,3,npd for face detection,face detector,237,63,33,"For an analysis , we also trained a near frontal face detector using the proposed NPD features and the classic cascade of regression trees ( CART ) with depth of four .","Nevertheless , the average number of feature evaluations per detection window is only 114.5 considering stagewise nonface rejection , which is quite reasonable .","A subset of the training data 2 in was used , including 12,102 face images and 12,315 nonface images .",0.5794621026894865,0.8630136986301371,0.7674418604651163,experimental-setup,baselines
face_detection,3,npd for face detection,face detector,238,64,34,"A subset of the training data 2 in was used , including 12,102 face images and 12,315 nonface images .","For an analysis , we also trained a near frontal face detector using the proposed NPD features and the classic cascade of regression trees ( CART ) with depth of four .",The detection template is 20 20 pixels .,0.5819070904645477,0.8767123287671232,0.7906976744186046,experimental-setup,baselines
face_detection,3,npd for face detection,face detector,239,65,35,The detection template is 20 20 pixels .,"A subset of the training data 2 in was used , including 12,102 face images and 12,315 nonface images .","The detector cascade contains 15 stages , and for each stage , the target false accept rate was 0.5 , with a detection rate of 0.999 .",0.5843520782396088,0.8904109589041096,0.813953488372093,experimental-setup,baselines
face_detection,3,npd for face detection,face detector,240,66,36,"The detector cascade contains 15 stages , and for each stage , the target false accept rate was 0.5 , with a detection rate of 0.999 .",The detection template is 20 20 pixels .,Detector Speed Up,0.5867970660146701,0.9041095890410958,0.8372093023255814,experimental-setup,baselines
face_detection,3,experiments,experiments,254,7,7,Evaluation on FDDB Database,"For each detected face , we summarized the scores of AdaBoost classifiers in all stages of the cascade to be the final score ; this score was used to generate the Receiver Operating Characteristic ( ROC ) curves .",The FDDB dataset covers challenging scenarios for face detection .,0.6210268948655256,0.0454545454545454,0.1489361702127659,experiments,approach
face_detection,3,experiments,experiments,265,18,18,"It can be observed that the proposed method outperforms most of the baseline methods except four methods , , , published recently .","shows a comprehensive comparison of detection rates of various algorithms on the FDDB database at FP = 0 , 10 , and 100 , where methods marked with a 9 were trained on the same AFLW database as ours .",The proposed NPD face detector is the second best one at FP = 0 for the discrete metric and the third best one for the continuous metric .,0.6479217603911981,0.1168831168831168,0.3829787234042553,experiments,approach
face_detection,3,experiments,experiments,266,19,19,The proposed NPD face detector is the second best one at FP = 0 for the discrete metric and the third best one for the continuous metric .,"It can be observed that the proposed method outperforms most of the baseline methods except four methods , , , published recently .","Specifically , the NPD detector detects about 3 .",0.6503667481662592,0.1233766233766233,0.4042553191489361,experiments,approach
face_detection,3,experiments,experiments,274,27,27,"It can be observed that the proposed NPD detector is among the top performers for the discrete metric , though it is not as good as the four recent methods for the continuous metric .","In both Figs . 5 and 6 , the curve labels in the legend are sorted in descending order of the detection rates at zero false positives ( FP = 0 ) .","However , note that the FDDB database uses ellipses for groundtruth of face annotations , and several methods ( e.g. Yan - DPM , and HeadHunter ) output similar elliptical detections to improve the performance especially with the continuous metric .",0.6699266503667481,0.1753246753246753,0.5744680851063829,experiments,approach
face_detection,3,experiments,experiments,278,31,31,"Compared to recent methods , the Joint Cascade algorithm is the most competitive one to us in terms of accuracy and speed ( see Sec. 5.6 ) .","This processing is not as good as making elliptical detections , but is still better than the original square detections .","However , the Joint Cascade method used a sophisticated postprocessing classifier to remove hard negatives and hence improved the results .",0.6797066014669927,0.2012987012987013,0.6595744680851063,experiments,approach
face_detection,3,experiments,evaluation on genki database,295,48,1,Evaluation on GENKI Database, , ,0.7212713936430318,0.3116883116883117,0.0769230769230769,experiments,approach
face_detection,3,experiments,evaluation on genki database,307,60,13,The results show that the proposed NPD face detector significantly outperforms both the Viola - Jones and PittPatt face detectors .,The ROC curves of the three methods are shown in for both the discrete and continuous score metrics ., ,0.7506112469437652,0.3896103896103896,1.0,experiments,baselines
face_detection,3,experiments,evaluation on cmu mit database,308,61,1,Evaluation on CMU - MIT Database, , ,0.7530562347188264,0.3961038961038961,0.0625,experiments,approach
face_detection,3,experiments,evaluation on cmu mit database,314,67,7,"The results show that , compared to the Viola - Jones frontal face detector , the NPD detector performs better when the number of false positives , FP < 50 , while it is slightly worse than Viola - Jones at higher FPs .","shows the ROC curves for the proposed NPD face detector , the Soft cascade method , the SURF cascade method , and the Viola - Jones detector .","Compared to the SURF cascade detector , the NPD .",0.7677261613691931,0.435064935064935,0.4375,experiments,baselines
face_detection,4,abstract,abstract,4,2,2,"Face detection , as a fundamental technology for various applications , is always deployed on edge devices which have limited memory storage and low computing power .", ,This paper introduces a Light and Fast Face Detector ( LFFD ) for edge devices .,0.0117994100294985,0.1176470588235294,0.1176470588235294,research-problem,model
face_detection,4,introduction,introduction,42,23,23,"In this paper , we propose a Light and Fast Face Detector ( LFFD ) for edge devices , considerably balancing both accuracy and running efficiency .","Although the above methods can achieve state of the art results , they may not properly balance accuracy and latency .",The proposed method is inspired by the one - stage and multi-scale object detection method SSD which also enlightens some other face detectors .,0.1238938053097345,0.5,0.5,model,experimental-setup
face_detection,4,introduction,introduction,43,24,24,The proposed method is inspired by the one - stage and multi-scale object detection method SSD which also enlightens some other face detectors .,"In this paper , we propose a Light and Fast Face Detector ( LFFD ) for edge devices , considerably balancing both accuracy and running efficiency .",One of the characteristics of SSD is that pre-defined anchor boxes are manually designed for each detection branch .,0.1268436578171091,0.5217391304347826,0.5217391304347826,model,experimental-setup
face_detection,4,introduction,introduction,44,25,25,One of the characteristics of SSD is that pre-defined anchor boxes are manually designed for each detection branch .,The proposed method is inspired by the one - stage and multi-scale object detection method SSD which also enlightens some other face detectors .,These boxes always have different sizes and aspect ratios to cover objects with different scales and shapes .,0.1297935103244837,0.5434782608695652,0.5434782608695652,model,model
face_detection,4,light and fast face detector,training details,227,132,32,We initialize all parameters with xavier method and train the network from scratch .,Training parameters .,"The inputs first minus 127.5 , and then divided by 127.5 .",0.6696165191740413,0.9428571428571428,0.8,experimental-setup,experimental-setup
face_detection,4,light and fast face detector,training details,229,134,34,"The optimization method is SGD with 0.9 momentum , zero weight decay and batch size 32 .","The inputs first minus 127.5 , and then divided by 127.5 .",The reason for zero weight decay is that the number of parameters in the proposed network is much less than that of VGG16 .,0.6755162241887905,0.9571428571428572,0.85,experimental-setup,baselines
face_detection,4,light and fast face detector,training details,232,137,37,The initial learning rate is 0.1 .,"Thus , there is no need to punish .","We train 1,500,000 iterations and reduce the learning rate by multiplying 0.1 at iteration 600,000 , 1,000,000 , 1,200,000 and 1,400,000 .",0.6843657817109144,0.9785714285714284,0.925,experimental-setup,experimental-setup
face_detection,4,light and fast face detector,training details,233,138,38,"We train 1,500,000 iterations and reduce the learning rate by multiplying 0.1 at iteration 600,000 , 1,000,000 , 1,200,000 and 1,400,000 .",The initial learning rate is 0.1 .,The training time is about 5 days with two NVIDIA GTX 1080 TI .,0.6873156342182891,0.9857142857142858,0.95,experimental-setup,experimental-setup
face_detection,4,light and fast face detector,training details,234,139,39,The training time is about 5 days with two NVIDIA GTX 1080 TI .,"We train 1,500,000 iterations and reduce the learning rate by multiplying 0.1 at iteration 600,000 , 1,000,000 , 1,200,000 and 1,400,000 .",Our method is implemented using MXNet and the source code is released .,0.6902654867256637,0.9928571428571428,0.975,experimental-setup,baselines
face_detection,4,light and fast face detector,training details,235,140,40,Our method is implemented using MXNet and the source code is released .,The training time is about 5 days with two NVIDIA GTX 1080 TI ., ,0.6932153392330384,1.0,1.0,experimental-setup,experimental-setup
face_detection,4,experiments,evaluation on benchmarks,253,18,13,"Finally , the following methods are taken for comparison : DSFD ( Resnet152 backbone ) , Pyramid Box ( VGG16 backbone ) , S3 FD ( VGG16 backbone ) , SSH ( VGG16 backbone ) and FaceBoxes .","Therefore , we collect the compared methods which have released codes and models .",DSFD and Pyramid Box are state of the art methods .,0.7463126843657817,0.1875,0.2452830188679245,baselines,model
face_detection,4,experiments,evaluation on benchmarks,261,26,21,FDDB dataset .,We evaluate all methods on two benchmarks : FDDB and WDIER FACE .,FDDB contains 2845 images with 5171 unconstrained faces .,0.7699115044247787,0.2708333333333333,0.3962264150943397,experiments,experiments
face_detection,4,experiments,evaluation on benchmarks,268,33,28,"DSFD , Pyramid Box , S3FD and SSH can achieve high accuracy with marginal gaps .",The overall performance on both scoring types shows the similar trends .,"The proposed LFFD gains slightly lower accuracy than the first four methods , but outperforms FaceBoxes evidently .",0.7905604719764012,0.34375,0.5283018867924528,experiments,approach
face_detection,4,experiments,evaluation on benchmarks,269,34,29,"The proposed LFFD gains slightly lower accuracy than the first four methods , but outperforms FaceBoxes evidently .","DSFD , Pyramid Box , S3FD and SSH can achieve high accuracy with marginal gaps .",The results indicate that LFFD is superior for detecting unconstrained faces .,0.7935103244837758,0.3541666666666667,0.5471698113207547,experiments,baselines
face_detection,4,experiments,evaluation on benchmarks,270,35,30,The results indicate that LFFD is superior for detecting unconstrained faces .,"The proposed LFFD gains slightly lower accuracy than the first four methods , but outperforms FaceBoxes evidently .",WIDER FACE dataset .,0.7964601769911505,0.3645833333333333,0.5660377358490566,experiments,approach
face_detection,4,experiments,evaluation on benchmarks,271,36,31,WIDER FACE dataset .,The results indicate that LFFD is superior for detecting unconstrained faces .,"In WIDER FACE , there are 32,203 images and 393,703 labelled faces .",0.7994100294985249,0.375,0.5849056603773585,experiments,experiments
face_detection,4,experiments,evaluation on benchmarks,282,47,42,"Firstly , performance drop is evident for DSFD , PyramidBox , S3FD and SSH compared to their original results .",Some observations can be made .,"On the one hand , achieving high accuracy through only one inference is relatively difficult .",0.831858407079646,0.4895833333333333,0.7924528301886793,experiments,baselines
face_detection,4,experiments,evaluation on benchmarks,285,50,45,"Secondly , Pyramid Box obtains the best results on Hard parts , whereas the performance of SSH on Hard parts is decreased dramatically mainly due to the neglect of some tiny faces .","On the other hand , the tricks can indeed improve the accuracy impressively .","Thirdly , FaceBoxes does not get desirable results on Medium and Hard parts .",0.8407079646017699,0.5208333333333334,0.8490566037735849,experiments,baselines
face_detection,4,experiments,evaluation on benchmarks,286,51,46,"Thirdly , FaceBoxes does not get desirable results on Medium and Hard parts .","Secondly , Pyramid Box obtains the best results on Hard parts , whereas the performance of SSH on Hard parts is decreased dramatically mainly due to the neglect of some tiny faces .","Since Face - Boxes produces large stride 32 rapidly , which means that faces smaller than 32 pixels are hardly detected .",0.8436578171091446,0.53125,0.8679245283018868,experiments,approach
face_detection,4,experiments,evaluation on benchmarks,292,57,52,"Fourthly , the proposed method LFFD consistently outperforms Face - Boxes , although having gaps with state of the art methods .","To some extent , the results of FaceBoxes and FaceBoxes 3. 2 indicate that FaceBoxes can not cover faces with large range .","Additionally , LFFD is better than SSH that uses VGG16 as the backbone on Hard parts .",0.8613569321533924,0.59375,0.981132075471698,experiments,approach
face_detection,4,experiments,evaluation on benchmarks,293,58,53,"Additionally , LFFD is better than SSH that uses VGG16 as the backbone on Hard parts .","Fourthly , the proposed method LFFD consistently outperforms Face - Boxes , although having gaps with state of the art methods .", ,0.8643067846607669,0.6041666666666666,1.0,experiments,baselines
face_detection,5,abstract,abstract,5,4,4,"Abstract - Face detection and alignment in unconstrained environment are challenging due to various poses , illuminations and occlusions .",?,Recent studies show that deep learning approaches can achieve impressive performance on these two tasks .,0.0328947368421052,0.0869565217391304,0.0869565217391304,research-problem,experimental-setup
face_detection,5,abstract,abstract,30,29,29,"However , most of the available face detection and face alignment methods ignore the inherent correlation between these two tasks .",Zhang et al. proposed to use facial attribute recognition as an auxiliary task to enhance face alignment performance using deep convolutional neural network .,"Though there exist several works attempt to jointly solve them , there are still limitations in these works .",0.1973684210526316,0.6304347826086957,0.6304347826086957,research-problem,ablation-analysis
face_detection,5,abstract,abstract,40,39,39,"In this paper , we propose a new framework to integrate these two tasks using unified cascaded CNNs by multi-task learning .","It is desirable to design an online hard sample mining method for face detection and alignment , which is adaptive to the current training process automatically .",The proposed CNNs consist of three stages .,0.2631578947368421,0.8478260869565217,0.8478260869565217,model,baselines
face_detection,5,abstract,abstract,41,40,40,The proposed CNNs consist of three stages .,"In this paper , we propose a new framework to integrate these two tasks using unified cascaded CNNs by multi-task learning .","In the first stage , it produces candidate windows quickly through a shallow CNN .",0.2697368421052632,0.8695652173913043,0.8695652173913043,model,model
face_detection,5,abstract,abstract,42,41,41,"In the first stage , it produces candidate windows quickly through a shallow CNN .",The proposed CNNs consist of three stages .,"Then , it refines the windows to reject a large number of non-faces windows through a more complex CNN .",0.2763157894736842,0.8913043478260869,0.8913043478260869,model,model
face_detection,5,abstract,abstract,43,42,42,"Then , it refines the windows to reject a large number of non-faces windows through a more complex CNN .","In the first stage , it produces candidate windows quickly through a shallow CNN .","Finally , it uses a more powerful CNN to refine the result and output facial landmarks positions .",0.2828947368421052,0.9130434782608696,0.9130434782608696,model,model
face_detection,5,abstract,abstract,44,43,43,"Finally , it uses a more powerful CNN to refine the result and output facial landmarks positions .","Then , it refines the windows to reject a large number of non-faces windows through a more complex CNN .","Thanks to this multi-task learning framework , the performance of the algorithm can be notably improved .",0.2894736842105263,0.9347826086956522,0.9347826086956522,model,baselines
face_detection,5,experiments,experiments,108,3,3,"Then we compare our face detector and alignment against the state - of - the - art methods in Face Detection Data Set and Benchmark ( FDDB ) , WIDER FACE , and Annotated Facial Landmarks in the Wild ( AFLW ) benchmark .","In this section , we first evaluate the effectiveness of the proposed hard sample mining strategy .","FDDB dataset contains the annotations for 5,171 faces in a set of 2,845 images .",0.7105263157894737,0.0697674418604651,0.1875,baselines,approach
face_detection,5,experiments,the effectiveness of online hard sample mining,122,17,1,The effectiveness of online hard sample mining, , ,0.8026315789473685,0.3953488372093023,0.125,experiments,approach
face_detection,5,experiments,the effectiveness of online hard sample mining,128,23,7,It is very clear that the hard sample mining is beneficial to performance improvement .,shows the loss curves from two different training ways .,C.,0.8421052631578947,0.5348837209302325,0.875,experiments,approach
face_detection,5,experiments,the effectiveness of joint detection and alignment,130,25,1,The effectiveness of joint detection and alignment, , ,0.8552631578947368,0.5813953488372093,0.125,experiments,approach
face_detection,5,experiments,the effectiveness of joint detection and alignment,132,27,3,We also compare the performance of bounding box regression in these two O - Nets. suggests that joint landmarks localization task learning is beneficial for both face classification and bounding box regression tasks .,"To evaluate the contribution of joint detection and alignment , we evaluate the performances of two different O - Nets ( joint facial landmarks regression task and do not joint it ) on FDDB ( with the same P - Net and R - Net for fair comparison ) .",D. Evaluation on face detection,0.8684210526315791,0.627906976744186,0.375,experiments,approach
face_detection,5,experiments,the effectiveness of joint detection and alignment,133,28,4,D. Evaluation on face detection,We also compare the performance of bounding box regression in these two O - Nets. suggests that joint landmarks localization task learning is beneficial for both face classification and bounding box regression tasks .,"To evaluate the performance of our face detection method , we compare our method against the state - of - the - art methods in FDDB , and the state - of - the - art methods in WIDER FACE .",0.875,0.6511627906976745,0.5,experiments,experimental-setup
face_detection,5,experiments,the effectiveness of joint detection and alignment,135,30,6,( a ) - ( d ) shows that our method consistently outperforms all the previous approaches by a large margin in both the benchmarks .,"To evaluate the performance of our face detection method , we compare our method against the state - of - the - art methods in FDDB , and the state - of - the - art methods in WIDER FACE .",We also evaluate our approach on some challenge photos 1 .,0.8881578947368421,0.6976744186046512,0.75,experiments,baselines
face_detection,5,experiments,evaluation on face alignment,138,33,1,Evaluation on face alignment, , ,0.9078947368421052,0.7674418604651163,0.0909090909090909,experiments,approach
face_detection,5,experiments,evaluation on face alignment,143,38,6,( e ) shows that our method outperforms all the state - of - the - art methods with a margin .,"The mean error is measured by the distances between the estimated Examples are showed in http://kpzhang93.github.io/SPL/index.html landmarks and the ground truths , and normalized with respect to the inter-ocular distance .",F. Runtime efficiency,0.9407894736842104,0.8837209302325582,0.5454545454545454,experiments,baselines
face_detection,6,title,title,2,2,2,Robust Face Detection via Learning Small Faces on Hard Images, , ,0.0080645161290322,1.0,1.0,research-problem,approach
face_detection,6,introduction,introduction,13,2,2,"Face detection is a fundamental and important computer vision problem , which is critical for many face - related tasks , such as face alignment , tracking and recognition .", ,"Stem from the recent successful development of deep neural networks , massive CNN - based face detection approaches have been proposed and achieved the state - of - the - art performance .",0.0524193548387096,0.054054054054054,0.054054054054054,research-problem,model
face_detection,6,introduction,introduction,27,16,16,"To address this issue , we propose to mine hard examples at image level in parallel with anchor level .","However , due to the sparsity of ground - truth faces and positive anchors , traditional anchor - level hard example mining mainly focuses on mining hard negative anchors , and mining hard anchors on well - detected images exhibits less effectiveness since there is no useful information that can be further exploited in these easy images .","More specifically , we propose to dynamically assign difficulty scores to training images during the learning process , which can determine whether an image is already well - detected or still useful for further training .",0.1088709677419354,0.4324324324324325,0.4324324324324325,model,approach
face_detection,6,introduction,introduction,28,17,17,"More specifically , we propose to dynamically assign difficulty scores to training images during the learning process , which can determine whether an image is already well - detected or still useful for further training .","To address this issue , we propose to mine hard examples at image level in parallel with anchor level .",This allows us to fully utilize the images which were not perfectly detected to better facilitate the following learning process .,0.1129032258064516,0.4594594594594595,0.4594594594594595,model,ablation-analysis
face_detection,6,introduction,introduction,29,18,18,This allows us to fully utilize the images which were not perfectly detected to better facilitate the following learning process .,"More specifically , we propose to dynamically assign difficulty scores to training images during the learning process , which can determine whether an image is already well - detected or still useful for further training .","We show this strategy can make our detector more robust towards hard faces , without involving more complex network architecture and computation overhead .",0.1169354838709677,0.4864864864864865,0.4864864864864865,model,approach
face_detection,6,introduction,introduction,31,20,20,"Apart from mining the hard images , we also propose to improve the detection quality by exclusively exploiting small faces .","We show this strategy can make our detector more robust towards hard faces , without involving more complex network architecture and computation overhead .",Small faces are typically hard and have attracted extensive research attention .,0.125,0.5405405405405406,0.5405405405405406,model,experimental-setup
face_detection,6,introduction,introduction,34,23,23,"Compared with these methods , our detector is more efficient since it is specially designed to aggressively leveraging the small faces during training .","Existing methods aim at building a scale - invariant face detector to learn and infer on both small and big faces , with multiple levels of detection features and anchors of different sizes .","More specifically , large faces are automatically ignored during training due to our anchor design , so that the model can fully focus on the small hard faces .",0.1370967741935483,0.6216216216216216,0.6216216216216216,model,baselines
face_detection,6,experiments,experimental settings,165,8,5,"We use an ImageNet pretrained VGG16 model to initialize our network backbone , and our newly introduced layers are randomly initialized with Gaussian initialization .","For each training image , we first randomly resize it , and then we use the cropping and photometric distortion data augmentation methods discussed in Section 3.3 to pre-process the resized image .","We train the model with the itersize to be 2 , for 46 k iterations , with a learning rate of 0.004 , and then for another 14 k iterations with a smaller learning rate of 0.0004 .",0.6653225806451613,0.0963855421686747,0.4545454545454545,experimental-setup,experimental-setup
face_detection,6,experiments,experimental settings,166,9,6,"We train the model with the itersize to be 2 , for 46 k iterations , with a learning rate of 0.004 , and then for another 14 k iterations with a smaller learning rate of 0.0004 .","We use an ImageNet pretrained VGG16 model to initialize our network backbone , and our newly introduced layers are randomly initialized with Gaussian initialization .","During training , we use 4 GPUs to simultaneously to compute the gradient and update the weight by synchronized SGD with Momentum .",0.6693548387096774,0.108433734939759,0.5454545454545454,experimental-setup,approach
face_detection,6,experiments,experimental settings,167,10,7,"During training , we use 4 GPUs to simultaneously to compute the gradient and update the weight by synchronized SGD with Momentum .","We train the model with the itersize to be 2 , for 46 k iterations , with a learning rate of 0.004 , and then for another 14 k iterations with a smaller learning rate of 0.0004 .","The first two blocks of VGG16 are frozen during the training , and the rest layers of VGG16 are set to have a double learning rate .",0.6733870967741935,0.1204819277108433,0.6363636363636364,experimental-setup,experimental-setup
face_detection,6,experiments,experimental settings,168,11,8,"The first two blocks of VGG16 are frozen during the training , and the rest layers of VGG16 are set to have a double learning rate .","During training , we use 4 GPUs to simultaneously to compute the gradient and update the weight by synchronized SGD with Momentum .","Since our model is designed and trained on only small faces , we use a multiscale image pyramid for testing to deal with faces larger than our anchors .",0.6774193548387096,0.1325301204819277,0.7272727272727273,experimental-setup,experimental-setup
face_detection,6,experiments,experimental settings,170,13,10,"Specifically , we resize the testing image so that the short side contains 100 , 300 , 600 , 1000 and 1400 pixels for evaluation on WIDER FACE dataset .","Since our model is designed and trained on only small faces , we use a multiscale image pyramid for testing to deal with faces larger than our anchors .",We also follow the testing strategies used in Pyra - midBox 2 such as horizontal flip and bounding - box voting .,0.6854838709677419,0.1566265060240964,0.9090909090909092,experimental-setup,experimental-setup
face_detection,6,experiments,experimental settings,171,14,11,We also follow the testing strategies used in Pyra - midBox 2 such as horizontal flip and bounding - box voting .,"Specifically , we resize the testing image so that the short side contains 100 , 300 , 600 , 1000 and 1400 pixels for evaluation on WIDER FACE dataset .", ,0.6895161290322581,0.1686746987951807,1.0,experimental-setup,approach
face_detection,6,experiments,experiment results,174,17,3,"In , we show the precision - recall ( PR ) curve and average precision ( AP ) for our model compared with many other state - of - the - arts on these three subsets .","WIDER FACE dataset includes 3226 images and 39708 faces labelled in the val dataset , with three subsetseasy , medium and hard .","As we can see , our method achieves the best performance on the hard subset , and outperforms the current state - of - the - art by a large margin .",0.7016129032258065,0.2048192771084337,0.0434782608695652,results,approach
face_detection,6,experiments,experiment results,175,18,4,"As we can see , our method achieves the best performance on the hard subset , and outperforms the current state - of - the - art by a large margin .","In , we show the precision - recall ( PR ) curve and average precision ( AP ) for our model compared with many other state - of - the - arts on these three subsets .","Since the hard set is a super set of small and medium , which contains all faces taller than 10 pixels , the performance on hard set can represent the performance on the full testing dataset more accurately .",0.7056451612903226,0.216867469879518,0.0579710144927536,results,approach
face_detection,6,experiments,experiment results,177,20,6,"Our performance on the medium subset is comparable to the most recent state - of - the - art and the performance on the easy subset is a bit worse since our method focuses on learning hard faces , and the architecture of our model is simpler compared with other state - of - thearts .","Since the hard set is a super set of small and medium , which contains all faces taller than 10 pixels , the performance on hard set can represent the performance on the full testing dataset more accurately .",There is also a WIDER FACE test dataset with no annotations provided publicly .,0.7137096774193549,0.2409638554216867,0.0869565217391304,results,approach
face_detection,6,experiments,experiment results,183,26,12,"We show the discontinuous ROC curve at compared with , and our method achieves the state - of - the - art performance of TPR = 98.7 % given 1000 false positives .",We use the raw boundingbox result without fitting it into ellipse to compute ROC .,Pascal Faces dataset includes 1335 labeled faces on a set of 851 images extracted for the Pascal VOC dataset .,0.7379032258064516,0.3132530120481928,0.1739130434782608,results,baselines
face_detection,6,experiments,experiment results,185,28,14,"We show the PR curve at compared with , and our method achieves a new the state - of - the - art performance of AP = 99.0 .",Pascal Faces dataset includes 1335 labeled faces on a set of 851 images extracted for the Pascal VOC dataset .,AFW dataset includes 473 faces labelled in a set of 205 images .,0.7459677419354839,0.3373493975903614,0.2028985507246377,results,baselines
face_detection,6,experiments,experiment results,187,30,16,"As shown in compared with , our method achieves state - of - the - art and almost perfect performance , with an AP of 99.60 .",AFW dataset includes 473 faces labelled in a set of 205 images .,Ablation study and diagnosis Ablation experiments,0.7540322580645161,0.3614457831325301,0.2318840579710145,results,baselines
face_detection,6,experiments,experiment results,196,39,25,"Our model with single detection feature map performs better than the one with three detection feature maps , despite its shallower structure , fewer parameters and anchors .","state - of - the - art face detector , especially on the hard subset .",This confirms the effectiveness of our simple face detector with single detection feature map focusing on small faces .,0.7903225806451613,0.4698795180722892,0.3623188405797101,ablation-analysis,baselines
face_detection,6,experiments,experiment results,199,42,28,HIM can improve the performance on hard subset significantly without involving more complex network architecture nor computation overhead .,We also separately verify our newly proposed hard image mining ( HIM ) and dilated head architecture ( DH ) described in Subsection 3.2 and respectively .,"DH itself can also boost the performance , which shows the effectiveness of designing larger convolution for larger anchors .",0.8024193548387096,0.5060240963855421,0.4057971014492754,ablation-analysis,approach
face_detection,6,experiments,experiment results,200,43,29,"DH itself can also boost the performance , which shows the effectiveness of designing larger convolution for larger anchors .",HIM can improve the performance on hard subset significantly without involving more complex network architecture nor computation overhead .,Combining HIM and DH together can improve further towards the state - of - the - art performance .,0.8064516129032258,0.5180722891566265,0.4202898550724637,ablation-analysis,approach
face_detection,6,experiments,experiment results,201,44,30,Combining HIM and DH together can improve further towards the state - of - the - art performance .,"DH itself can also boost the performance , which shows the effectiveness of designing larger convolution for larger anchors .",Diagnosis of hard image mining,0.8104838709677419,0.5301204819277109,0.4347826086956521,ablation-analysis,approach
face_detection,6,experiments,experiment results,211,54,40,Both photometric distortion and cropping can contribute to a more robust face detector .,The ablation results evaluated on WIDER FACE val dataset are shown in .,Diagnosis of multi-scale testing,0.8508064516129032,0.6506024096385542,0.5797101449275363,ablation-analysis,approach
face_detection,6,experiments,experiment results,219,62,48,"Diagnosis of multi-scale testing. , the extra small scales are crucial to detect easy faces .",We diagnose the impact of the extra small scales ( i.e. 100 and 300 ) by removing them from the image pyramid . :,"Without resizing the short side to contain 100 and 300 pixels , the performance on easy subset is only 78.2 , which is even lower than the performance on medium and hard which contain much harder faces .",0.8830645161290323,0.7469879518072289,0.6956521739130435,ablation-analysis,approach
face_detection,7,title,title,2,2,2,Recurrent Scale Approximation for Object Detection in CNN, , ,0.0071942446043165,1.0,1.0,research-problem,experimental-setup
face_detection,7,abstract,abstract,4,2,2,"Since convolutional neural network ( CNN ) lacks an inherent mechanism to handle large scale variations , we always need to compute feature maps multiple times for multiscale object detection , which has the bottleneck of computational cost in practice .", ,"To address this , we devise a recurrent scale approximation ( RSA ) to compute feature map once only , and only through this map can we approximate the rest maps on other levels .",0.014388489208633,0.2,0.2,research-problem,baselines
face_detection,7,introduction,introduction,17,5,5,"Our codes and annotations mentioned in Sec.4.1 can be accessed at github.com/sciencefans/RSA-for-object-detection dle the variations caused by appearance , location and scale .","To localize objects at arbitrary scales and locations in an image , we need to han -","Most of the appearance variations can now be handled in CNN , benefiting from the invariance property of convolution and pooling operations .",0.0611510791366906,0.1282051282051282,0.1282051282051282,code,model
face_detection,7,introduction,introduction,35,23,23,"In this work , we propose a recurrent scale approximation ( RSA , see ) unit to achieve the goal aforementioned .",The intuition is illustrated in .,The RSA unit is designed to be plugged at some specific depths in a network and to be fed with an initial feature map at the largest scale .,0.1258992805755395,0.5897435897435898,0.5897435897435898,model,ablation-analysis
face_detection,7,introduction,introduction,36,24,24,The RSA unit is designed to be plugged at some specific depths in a network and to be fed with an initial feature map at the largest scale .,"In this work , we propose a recurrent scale approximation ( RSA , see ) unit to achieve the goal aforementioned .",The unit convolves the input in a recurrent manner to generate the prediction of the feature map that is half the size of the input .,0.1294964028776978,0.6153846153846154,0.6153846153846154,model,model
face_detection,7,introduction,introduction,37,25,25,The unit convolves the input in a recurrent manner to generate the prediction of the feature map that is half the size of the input .,The RSA unit is designed to be plugged at some specific depths in a network and to be fed with an initial feature map at the largest scale .,Such a scheme could feed the network with input at one scale only and approximate the rest features at smaller scales through a learnable RSA unit - a balance considering both efficiency and accuracy .,0.1330935251798561,0.6410256410256411,0.6410256410256411,model,model
face_detection,7,introduction,introduction,38,26,26,Such a scheme could feed the network with input at one scale only and approximate the rest features at smaller scales through a learnable RSA unit - a balance considering both efficiency and accuracy .,The unit convolves the input in a recurrent manner to generate the prediction of the feature map that is half the size of the input .,We propose two more schemes to further save the computational budget and improve the detection performance under the RSA framework .,0.1366906474820144,0.6666666666666666,0.6666666666666666,model,model
face_detection,7,introduction,introduction,40,28,28,The first is a scale - forecast network to globally predict potential scales for a novel image and we compute feature pyramids for just a certain set of scales based on the prediction .,We propose two more schemes to further save the computational budget and improve the detection performance under the RSA framework .,"There are only a few scales of objects appearing in the image and hence most of the feature pyramids correspond to the background , indicating a redundancy if maps on all levels are computed .",0.1438848920863309,0.7179487179487181,0.7179487179487181,model,ablation-analysis
face_detection,7,introduction,introduction,42,30,30,The second is a landmark retracing network that retraces the location of the regressed landmarks in the preceding layers and generates a confidence score for each landmark based on the landmark feature set .,"There are only a few scales of objects appearing in the image and hence most of the feature pyramids correspond to the background , indicating a redundancy if maps on all levels are computed .",The final score of identifying a face within an anchor is thereby revised by the LRN network .,0.1510791366906475,0.7692307692307693,0.7692307692307693,model,model
face_detection,7,introduction,introduction,43,31,31,The final score of identifying a face within an anchor is thereby revised by the LRN network .,The second is a landmark retracing network that retraces the location of the regressed landmarks in the preceding layers and generates a confidence score for each landmark based on the landmark feature set .,Such a design alleviates false positives caused by the accumulated error in the RSA unit .,0.1546762589928057,0.7948717948717948,0.7948717948717948,model,model
face_detection,7,introduction,introduction,46,34,34,The three components can be incorporated into a unified CNN framework and trained end - to - end .,The pipeline of our proposed algorithm is shown in .,Experiments show that our approach is superior to other state - of the - art methods in face detection and achieves reasonable results for object detection .,0.1654676258992805,0.8717948717948718,0.8717948717948718,model,approach
face_detection,7,our algorithm,face detection,178,106,8,"The structure of our model is a shallow version of the ResNet where the first seven ResNet blocks are used , i.e. , from conv1 to res3c .",All faces are labelled with bounding boxes and five landmarks .,We use this model in scale - forecast network and LRN .,0.6402877697841727,0.5326633165829145,0.2857142857142857,experimental-setup,experimental-setup
face_detection,7,our algorithm,face detection,179,107,9,We use this model in scale - forecast network and LRN .,"The structure of our model is a shallow version of the ResNet where the first seven ResNet blocks are used , i.e. , from conv1 to res3c .","All numbers of channels are set to half of the original ResNet model , for the consideration of time efficiency .",0.6438848920863309,0.5376884422110553,0.3214285714285714,experimental-setup,model
face_detection,7,our algorithm,face detection,180,108,10,"All numbers of channels are set to half of the original ResNet model , for the consideration of time efficiency .",We use this model in scale - forecast network and LRN .,We first train the scale - forecast network and then use the output of predicted scales to launch the RSA unit and LRN .,0.6474820143884892,0.542713567839196,0.3571428571428571,experimental-setup,experimental-setup
face_detection,7,our algorithm,face detection,181,109,11,We first train the scale - forecast network and then use the output of predicted scales to launch the RSA unit and LRN .,"All numbers of channels are set to half of the original ResNet model , for the consideration of time efficiency .",Note that the whole system ( RSA + LRN ) is trained end - to - end and the model is trained from scratch without resorting to a pretrained model since the number of channels is halved .,0.6510791366906474,0.5477386934673367,0.3928571428571429,experimental-setup,model
face_detection,7,our algorithm,face detection,183,111,13,The ratio of the positive and the negative is 1 : 1 in all experiments .,Note that the whole system ( RSA + LRN ) is trained end - to - end and the model is trained from scratch without resorting to a pretrained model since the number of channels is halved .,"The batch size is 4 ; base learning rate is set to 0.001 with a decrease of 6 % every 10,000 iterations .",0.6582733812949639,0.5577889447236181,0.4642857142857143,experimental-setup,approach
face_detection,7,our algorithm,face detection,184,112,14,"The batch size is 4 ; base learning rate is set to 0.001 with a decrease of 6 % every 10,000 iterations .",The ratio of the positive and the negative is 1 : 1 in all experiments .,"The maximum training iteration is 1,000,000 .",0.6618705035971223,0.5628140703517588,0.5,experimental-setup,approach
face_detection,7,our algorithm,face detection,185,113,15,"The maximum training iteration is 1,000,000 .","The batch size is 4 ; base learning rate is set to 0.001 with a decrease of 6 % every 10,000 iterations .",We use stochastic gradient descent as the optimizer .,0.6654676258992805,0.5678391959798995,0.5357142857142857,experimental-setup,baselines
face_detection,7,our algorithm,face detection,186,114,16,We use stochastic gradient descent as the optimizer .,"The maximum training iteration is 1,000,000 .",The scale - forecast network is of vital importance to the computational cost and accuracy in the networks afterwards .,0.6690647482014388,0.5728643216080402,0.5714285714285714,experimental-setup,approach
face_detection,7,our algorithm,face detection,187,115,17,The scale - forecast network is of vital importance to the computational cost and accuracy in the networks afterwards .,We use stochastic gradient descent as the optimizer .,reports the overall recall with different numbers of predicted scales on three benchmarks .,0.6726618705035972,0.5778894472361809,0.6071428571428571,experimental-setup,model
face_detection,7,our algorithm,face detection,190,118,20,"We can observe from the results that our trained scale network recalls almost 99 % at x = 1 , indicating that on average we only need to generate less than two predictions per image and that we can retrieve all face scales .","Since the number of faces and the number of potential scales in the image vary across datasets , we use the number of predicted scales per face ( x , total predicted scales over total number of faces ) and a global recall ( y , correct predicted scales overall ground truth scales ) as the evaluation metric .",Based on this prior final feature .,0.6834532374100719,0.592964824120603,0.7142857142857143,results,baselines
face_detection,7,our algorithm,face detection,194,122,24,"We can conclude that the deeper the RSA is branched out , the worse the feature approximation at smaller scales will be .","For each case , we report the error rate v.s. the level of down - sampling ratio in the unit .","knowledge , during inference , we set the threshold for predicting potential scales of the input so that it has approximately two predictions .",0.6978417266187049,0.6130653266331658,0.8571428571428571,results,approach
face_detection,7,our algorithm,ablative evaluation on rsa unit,206,134,7,Theoretically RSA can handle all scales of features in a deep CNN model and therefore can be branched out at any depth of the network .,"There are two remarks regarding the result : First , feature depth matters .","However , results from the figure indicate that as we plug RSA at deeper layers , its performance decades .",0.7410071942446043,0.6733668341708543,0.28,ablation-analysis,baselines
face_detection,7,our algorithm,ablative evaluation on rsa unit,214,142,15,The minimum operation in each component means only the scaleforecast network is used where no face appears in the image ; and the maximum operation indicates the amount when faces appear at all scales .,"Theoretical operations of each component are provided , denoted as ' Opts. ( VGA input ) ' below .",The actual runtime comparison between ours and baseline RPN is reported in smaller and the input map of RSA is thus larger .,0.7697841726618705,0.7135678391959799,0.6,ablation-analysis,model
face_detection,7,our algorithm,ablative evaluation on rsa unit,219,147,20,Most of the computation happens before layer res2 b and it has an acceptable error rate of 3.44 % .,In practice we choose case res2b to be the location where RSA is branched out .,We use this setting throughout the following experiments .,0.7877697841726619,0.7386934673366834,0.8,ablation-analysis,model
face_detection,7,our algorithm,ablative evaluation on rsa unit,222,150,23,"For a particular case , as the times of the recurrent operation increase , the error rate goes up due to the cumulative effect of rolling out the predictions .","Second , butterfly effect exists .","For example , in case res2b , the error rate is 3.44 % at level m = 1 and drops to 5.9 % after rolling out five times .",0.7985611510791367,0.7537688442211056,0.92,ablation-analysis,model
face_detection,8,title,title,2,2,2,Detecting Faces Using Region - based Fully Convolutional Networks, , ,0.0128205128205128,1.0,1.0,research-problem,experimental-setup
face_detection,8,abstract,abstract,4,2,2,Face detection has achieved great success using the region - based methods ., ,"In this report , we propose a region - based face detector applying deep networks in a fully convolutional fashion , named Face R - FCN .",0.0256410256410256,0.2857142857142857,0.2857142857142857,research-problem,experimental-setup
face_detection,8,introduction,introduction,22,13,13,"In this report , we develop a face detector on the top of R - FCN with elaborate design of the details , which achieves more decent performance than the R - CNN face detectors .",Previous works primarily focus on the R - CNN based methods and achieve promising results .,"According to the size of the general face , we carefully design size of anchors and RoIs .",0.141025641025641,0.5652173913043478,0.5652173913043478,model,experimental-setup
face_detection,8,introduction,introduction,23,14,14,"According to the size of the general face , we carefully design size of anchors and RoIs .","In this report , we develop a face detector on the top of R - FCN with elaborate design of the details , which achieves more decent performance than the R - CNN face detectors .","Since the contribution of facial parts maybe different for detection , we introduce a position - sensitive average pooling to generate embedding features for enhancing discrimination , and eliminate the effect of non-uniformed contribution in each facial part .",0.1474358974358974,0.6086956521739131,0.6086956521739131,model,approach
face_detection,8,introduction,introduction,24,15,15,"Since the contribution of facial parts maybe different for detection , we introduce a position - sensitive average pooling to generate embedding features for enhancing discrimination , and eliminate the effect of non-uniformed contribution in each facial part .","According to the size of the general face , we carefully design size of anchors and RoIs .","Furthermore , we also apply the multi-scale training and testing strategy in this work .",0.1538461538461538,0.6521739130434783,0.6521739130434783,model,ablation-analysis
face_detection,8,introduction,introduction,25,16,16,"Furthermore , we also apply the multi-scale training and testing strategy in this work .","Since the contribution of facial parts maybe different for detection , we introduce a position - sensitive average pooling to generate embedding features for enhancing discrimination , and eliminate the effect of non-uniformed contribution in each facial part .",The on - line hard example mining ( OHEM ) technique is integrated into our network as well for boosting the learning on hard examples .,0.1602564102564102,0.6956521739130435,0.6956521739130435,model,baselines
face_detection,8,introduction,introduction,26,17,17,The on - line hard example mining ( OHEM ) technique is integrated into our network as well for boosting the learning on hard examples .,"Furthermore , we also apply the multi-scale training and testing strategy in this work .",Our key contributions are summarized below :,0.1666666666666666,0.7391304347826086,0.7391304347826086,model,approach
face_detection,8,proposed approach,implementation details,107,55,2,Our training hyper - parameters are similar to Face R - CNN ., ,"Different from Face R - CNN , we initialize our network with the pre-trained weights of 101 - layer ResNet trained on Image Net .",0.6858974358974359,0.5789473684210527,0.125,experimental-setup,experimental-setup
face_detection,8,proposed approach,implementation details,108,56,3,"Different from Face R - CNN , we initialize our network with the pre-trained weights of 101 - layer ResNet trained on Image Net .",Our training hyper - parameters are similar to Face R - CNN .,"Specifically , we freeze the general kernels ( weights of few layers at the beginning ) of the pre-trained model throughout the entire training process in order to keep the essential feature extractor trained on ImageNet .",0.6923076923076923,0.5894736842105263,0.1875,experimental-setup,experimental-setup
face_detection,8,proposed approach,implementation details,109,57,4,"Specifically , we freeze the general kernels ( weights of few layers at the beginning ) of the pre-trained model throughout the entire training process in order to keep the essential feature extractor trained on ImageNet .","Different from Face R - CNN , we initialize our network with the pre-trained weights of 101 - layer ResNet trained on Image Net .","In terms of the RPN stage , Face R - FCN enumerates multiple configurations of the anchor in order to accurately search for faces .",0.6987179487179487,0.6,0.25,experimental-setup,experimental-setup
face_detection,8,proposed approach,implementation details,110,58,5,"In terms of the RPN stage , Face R - FCN enumerates multiple configurations of the anchor in order to accurately search for faces .","Specifically , we freeze the general kernels ( weights of few layers at the beginning ) of the pre-trained model throughout the entire training process in order to keep the essential feature extractor trained on ImageNet .",We combine a range of multiple scales and aspect ratios together to construct multi-scale anchors .,0.7051282051282052,0.6105263157894737,0.3125,experimental-setup,model
face_detection,8,proposed approach,implementation details,111,59,6,We combine a range of multiple scales and aspect ratios together to construct multi-scale anchors .,"In terms of the RPN stage , Face R - FCN enumerates multiple configurations of the anchor in order to accurately search for faces .",These anchors then map to the original image to calculate the IoU scores with the ground truth for further picking up with following rules :,0.7115384615384616,0.6210526315789474,0.375,experimental-setup,approach
face_detection,8,proposed approach,implementation details,115,63,10,The RPN and R - FCN are both learned jointly with the softmax loss and the smooth L1 loss .,The R - FCN is then trained on the processed anchors ( proposals ) where the positive samples and negative samples are defined as IoU greater than 0.5 and between 0.1 and 0.5 respectively .,Non- maximum suppression ( NMS ) is adopted for regularizing the anchors with certain IoU scores .,0.7371794871794872,0.6631578947368421,0.625,experimental-setup,experimental-setup
face_detection,8,proposed approach,implementation details,116,64,11,Non- maximum suppression ( NMS ) is adopted for regularizing the anchors with certain IoU scores .,The RPN and R - FCN are both learned jointly with the softmax loss and the smooth L1 loss .,The proposals are processed by OHEM to train with hard examples .,0.7435897435897436,0.6736842105263158,0.6875,experimental-setup,approach
face_detection,8,proposed approach,implementation details,118,66,13,We set the 256 for the size of RPN mini-batch and 128 for R - FCN respectively .,The proposals are processed by OHEM to train with hard examples .,Approximate joint training strategy is applied for training in the end - to - end fashion .,0.7564102564102564,0.6947368421052632,0.8125,experimental-setup,experimental-setup
face_detection,8,proposed approach,implementation details,120,68,15,"We utilize multi-scale training where the input image is resized with bilinear interpolation to various scales ( say , 1024 or 1200 ) .",Approximate joint training strategy is applied for training in the end - to - end fashion .,"In the testing stage , multi-scale testing is performed by scale image into an image pyramid for better detecting on both tiny and general faces .",0.7692307692307693,0.7157894736842105,0.9375,experimental-setup,experimental-setup
face_detection,8,proposed approach,implementation details,121,69,16,"In the testing stage , multi-scale testing is performed by scale image into an image pyramid for better detecting on both tiny and general faces .","We utilize multi-scale training where the input image is resized with bilinear interpolation to various scales ( say , 1024 or 1200 ) .", ,0.7756410256410257,0.7263157894736842,1.0,experimental-setup,approach
face_detection,8,proposed approach,comparison on benchmarks,125,73,4,"As illustrated in , our proposed approach consistently wins the 1st place across the three subsets on both the validation set and test set of WIDER FACE and significantly outperforms the existing results .",We train our model on the training set of WIDER FACE and perform evaluation on the validation set and test set following the Scenario - Int criterion .,"In particular , on WIDER FACE hard subset , our approach is superior to the prior best - performing one by a clear margin , which demonstrates the robustness of our algorithm .",0.8012820512820513,0.7684210526315789,0.1538461538461538,results,baselines
face_detection,8,proposed approach,comparison on benchmarks,126,74,5,"In particular , on WIDER FACE hard subset , our approach is superior to the prior best - performing one by a clear margin , which demonstrates the robustness of our algorithm .","As illustrated in , our proposed approach consistently wins the 1st place across the three subsets on both the validation set and test set of WIDER FACE and significantly outperforms the existing results .",FDDB,0.8076923076923077,0.7789473684210526,0.1923076923076923,results,approach
face_detection,8,proposed approach,comparison on benchmarks,127,75,6,FDDB,"In particular , on WIDER FACE hard subset , our approach is superior to the prior best - performing one by a clear margin , which demonstrates the robustness of our algorithm .",There are two evaluation protocols for evaluating the FDDB dataset : one is 10 - fold cross- validation and the other is unrestricted training ( using the data outside FDDB for training ) .,0.8141025641025641,0.7894736842105263,0.2307692307692308,results,baselines
face_detection,8,proposed approach,comparison on benchmarks,133,81,12,"From , it is clearly that Face R - FCN consistently achieves the impressive performance in terms of both the discrete ROC curve and continuous ROC curve .",The discrete ROC curves and continuous ROC curves of these approaches are plotted in .,Our discrete ROC curve is superior to the prior best - performing method .,0.8525641025641025,0.8526315789473684,0.4615384615384616,results,baselines
face_detection,8,proposed approach,comparison on benchmarks,134,82,13,Our discrete ROC curve is superior to the prior best - performing method .,"From , it is clearly that Face R - FCN consistently achieves the impressive performance in terms of both the discrete ROC curve and continuous ROC curve .",We also obtain the best true positive rate of the discrete ROC curve at 1000/2000 false positives ( 98.49%/99.07 % ) .,0.8589743589743589,0.8631578947368421,0.5,results,baselines
face_detection,8,proposed approach,comparison on benchmarks,135,83,14,We also obtain the best true positive rate of the discrete ROC curve at 1000/2000 false positives ( 98.49%/99.07 % ) .,Our discrete ROC curve is superior to the prior best - performing method .,"For the reason that we do not optimize our method to regress the elliptical ground truth in FDDB dataset , our continuous ROC curve is lower than the first place and slightly lower than .",0.8653846153846154,0.8736842105263158,0.5384615384615384,results,baselines
face_detection,8,proposed approach,comparison on benchmarks,141,89,20,"Face R - FCN shows the superior performance over the prior methods across the three subsets ( easy , medium and hard ) in both validation and test sets .",All of these methods use the same Scenario - Int criterion .,Best viewed in color .,0.903846153846154,0.936842105263158,0.7692307692307693,results,baselines
face_detection,8,proposed approach,comparison on benchmarks,147,95,26,"Finally , we obtain the true positive rate 98. 99 % of the discrete ROC curve at 1000 false positives and 99. 42 % at 2000 false positives , which are new state - of - the - art among all the published methods on FDDB .","As expected , the performance of Face R - FCN is further improved .", ,0.9423076923076924,1.0,1.0,results,baselines
face_detection,9,title,title,2,2,2,Finding Tiny Faces, , ,0.0072202166064981,1.0,1.0,research-problem,experimental-setup
face_detection,9,abstract,abstract,7,5,5,"Though tremendous strides have been made in object recognition , one of the remaining open challenges is detecting small objects .",Detector confidence is given by the colorbar on the right : can you confidently identify errors ?,"We explore three aspects of the problem in the context of finding small faces : the role of scale invariance , image resolution , and contextual reasoning .",0.0252707581227436,0.3571428571428571,0.3571428571428571,research-problem,baselines
face_detection,9,abstract,abstract,8,6,6,"We explore three aspects of the problem in the context of finding small faces : the role of scale invariance , image resolution , and contextual reasoning .","Though tremendous strides have been made in object recognition , one of the remaining open challenges is detecting small objects .","While most recognition approaches aim to be scale - invariant , the cues for recognizing a 3 px tall face are fundamentally different than those for recognizing a 300 px tall face .",0.0288808664259927,0.4285714285714285,0.4285714285714285,research-problem,baselines
face_detection,9,introduction,introduction,26,10,10,"lem in the context of face detection : the role of scale invariance , image resolution and contextual reasoning .","We define templates over features extracted from multiple layers of a deep model , which is analogous to foveal descriptors ( e ) .",Scaleinvariance is a fundamental property of almost all current recognition and object detection systems .,0.0938628158844765,0.2083333333333333,0.2083333333333333,research-problem,model
face_detection,9,introduction,introduction,33,17,17,"Instead of a "" one-size - fitsall "" approach , we train separate detectors tuned for different scales ( and aspect ratios ) .","On one hand , we want a small template that can detect small faces ; on the other hand , we want a large template that can exploit detailed features ( of say , facial parts ) to increase accuracy .",Training a large collection of scale - specific detectors may suffer from lack of training data for individual scales and inefficiency from running a large number of detectors attest time .,0.1191335740072202,0.3541666666666667,0.3541666666666667,approach,ablation-analysis
face_detection,9,introduction,introduction,35,19,19,"To address both concerns , we train and run scale - specific detectors in a multitask fashion : they make use of features defined over multiple layers of single ( deep ) feature hierarchy .",Training a large collection of scale - specific detectors may suffer from lack of training data for individual scales and inefficiency from running a large number of detectors attest time .,"While such a strategy results in detectors of high accuracy for large objects , finding small things is still challenging .",0.1263537906137184,0.3958333333333333,0.3958333333333333,approach,ablation-analysis
face_detection,9,introduction,introduction,41,25,25,"To extend features fine - tuned from these networks to objects of novel sizes , we employ a simply strategy : resize images at test - time by interpolation and decimation .",We demonstrate that existing networks are tuned for objects of a characteristic size ( encountered in pre-training datasets such as ImageNet ) .,"While many recognition systems are applied in a "" multi-resolution "" fashion by processing an image pyramid , we find that interpolating the lowest layer of the pyramid is particularly crucial for finding small objects .",0.148014440433213,0.5208333333333334,0.5208333333333334,approach,ablation-analysis
face_detection,9,introduction,introduction,42,26,26,"While many recognition systems are applied in a "" multi-resolution "" fashion by processing an image pyramid , we find that interpolating the lowest layer of the pyramid is particularly crucial for finding small objects .","To extend features fine - tuned from these networks to objects of novel sizes , we employ a simply strategy : resize images at test - time by interpolation and decimation .",Hence our final approach is a delicate mixture of scale - specific detectors that are used in a scale - invariant fashion ( by processing an image pyramid to capture large scale variations ) .,0.151624548736462,0.5416666666666666,0.5416666666666666,approach,model
face_detection,9,introduction,introduction,43,27,27,Hence our final approach is a delicate mixture of scale - specific detectors that are used in a scale - invariant fashion ( by processing an image pyramid to capture large scale variations ) .,"While many recognition systems are applied in a "" multi-resolution "" fashion by processing an image pyramid , we find that interpolating the lowest layer of the pyramid is particularly crucial for finding small objects .",How best to encode context ?,0.1552346570397112,0.5625,0.5625,approach,model
face_detection,9,introduction,introduction,52,36,36,"We demonstrate that convolutional deep features extracted from multiple layers ( also known as "" hypercolumn "" features ) are effective "" foveal "" descriptors that capture both high - resolution detail and coarse low - resolution cues across large receptive field ( ) .",One of the challenges appears to be how to effectively encode large image regions .,We show that highresolution components of our foveal descriptors ( extracted from lower convolutional layers ) are crucial for such accurate localization in .,0.187725631768953,0.75,0.75,approach,baselines
face_detection,9,introduction,introduction,53,37,37,We show that highresolution components of our foveal descriptors ( extracted from lower convolutional layers ) are crucial for such accurate localization in .,"We demonstrate that convolutional deep features extracted from multiple layers ( also known as "" hypercolumn "" features ) are effective "" foveal "" descriptors that capture both high - resolution detail and coarse low - resolution cues across large receptive field ( ) .",Our contribution :,0.1913357400722021,0.7708333333333334,0.7708333333333334,approach,baselines
face_detection,9,experiments,experiments,218,2,2,WIDER FACE :, ,We train a model with 25 templates on WIDER FACE 's training set and report the performance of our best model HR - ResNet101 ( A+B ) on the held - out test set .,0.7870036101083032,0.0327868852459016,0.0327868852459016,experiments,approach
face_detection,9,experiments,experiments,220,4,4,"As shows , our hybrid - resolution model ( HR ) achieves state - of - the - art performance on all difficulty levels , but most importantly , reduces error on the "" hard "" set by 2X .",We train a model with 25 templates on WIDER FACE 's training set and report the performance of our best model HR - ResNet101 ( A+B ) on the held - out test set .,"Note that "" hard "" set includes all faces taller than 10 px , hence more accurately represents performance on the full testset .",0.7942238267148014,0.0655737704918032,0.0655737704918032,experiments,baselines
face_detection,9,experiments,experiments,224,8,8,FDDB :,Please refer to the benchmark website for full evaluation and our Appendix A for more quantitative diagnosis .,We test our WIDER FACE - trained model on FDDB .,0.8086642599277978,0.1311475409836065,0.1311475409836065,experiments,approach
face_detection,9,experiments,experiments,226,10,10,"Our out - of - the - box detector ( HR ) outperforms all published results on the discrete score , which uses a standard 50 % intersection - over - union threshold to define correctness .",We test our WIDER FACE - trained model on FDDB .,"Because FDDB uses bounding ellipses while WIDER FACE using bounding boxes , we train a post -hoc linear regressor to transform bounding box predictions to ellipses .",0.8158844765342961,0.1639344262295081,0.1639344262295081,experiments,model
face_detection,9,experiments,experiments,228,12,12,"With the post - hoc regressor , our detector achieves state - of - the - art performance on the continuous score ( measuring average bounding - box overlap ) as well .","Because FDDB uses bounding ellipses while WIDER FACE using bounding boxes , we train a post -hoc linear regressor to transform bounding box predictions to ellipses .",Our regressor is trained with 10 - fold cross validation .,0.8231046931407943,0.1967213114754098,0.1967213114754098,experiments,approach
face_detection,9,experiments,experiments,229,13,13,Our regressor is trained with 10 - fold cross validation .,"With the post - hoc regressor , our detector achieves state - of - the - art performance on the continuous score ( measuring average bounding - box overlap ) as well .",plots the performance of our detector both with and without the elliptical regressor ( ER ) .,0.8267148014440433,0.2131147540983606,0.2131147540983606,experiments,approach
hypernym_discovery,0,title,title,2,2,2,ADAPT at SemEval- 2018 Task 9 : Skip - Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora, , ,0.025,1.0,1.0,research-problem,approach
hypernym_discovery,0,abstract,abstract,4,2,2,This paper describes a simple but competitive unsupervised system for hypernym discovery ., ,"The system uses skip - gram word embeddings with negative sampling , trained on specialised corpora .",0.05,0.2857142857142857,0.2857142857142857,research-problem,approach
hypernym_discovery,0,introduction,introduction,20,11,11,The ADAPT team focused on the two specialised domain English subtasks by developing an unsupervised system that builds word embeddings from the supplied reference corpora for these domains .,"In addition to these English vocabularies , general language domain vocabularies for Spanish and Italian were also provided .","Word embeddings trained on large corpora have been shown to capture semantic relations between words , including hypernym -hyponym relations .",0.25,0.34375,0.34375,approach,approach
hypernym_discovery,0,introduction,introduction,25,16,16,"Even though unsupervised systems tend to rank behind supervised systems in NLP tasks in general , our motivation to focus on an unsupervised approach is derived from the fact that they do not require explicit hand - annotated data , and from the expectation that they are able to generalise more easily to unseen hypernym - hyponym pairs .","Indeed , we show that for the medical domain subtask , our system beats the other unsupervised systems , although it still ranks behind the supervised systems .",The rest of this system description paper is organised as follows :,0.3125,0.5,0.5,approach,ablation-analysis
hypernym_discovery,0,results,results,60,1,1,Results, , ,0.75,0.1111111111111111,0.1111111111111111,results,baselines
hypernym_discovery,0,results,results,61,2,2,Our official submission ranked at eleven out of eighteen on the medical domain subtask with a Mean Average Precision ( MAP ) of 8.13 ., ,"However , it ranked first place among all the unsupervised systems on this subtask .",0.7625,0.2222222222222222,0.2222222222222222,results,baselines
hypernym_discovery,0,results,results,62,3,3,"However , it ranked first place among all the unsupervised systems on this subtask .",Our official submission ranked at eleven out of eighteen on the medical domain subtask with a Mean Average Precision ( MAP ) of 8.13 .,"On the music industry domain subtask , our system ranked 13th out of 16 places with a MAP of 1.88 , ranking 4th among the unsupervised systems .",0.775,0.3333333333333333,0.3333333333333333,results,approach
hypernym_discovery,0,results,results,63,4,4,"On the music industry domain subtask , our system ranked 13th out of 16 places with a MAP of 1.88 , ranking 4th among the unsupervised systems .","However , it ranked first place among all the unsupervised systems on this subtask .","We believe that one reason why the music industry scores are so much lower than the medical results is due to our system not producing an output for 233 of the music industry input words ( 45 % of the total ) , compared to the 128 medical input words ( 26 % ) it failed to predict .",0.7875,0.4444444444444444,0.4444444444444444,results,approach
hypernym_discovery,1,title,title,2,2,2,SJTU- NLP at SemEval-2018 Task 9 : Neural Hypernym Discovery with Term Embeddings, , ,0.0185185185185185,1.0,1.0,research-problem,experimental-setup
hypernym_discovery,1,abstract,abstract,4,2,2,"This paper describes a hypernym discovery system for our participation in the SemEval - 2018 Task 9 , which aims to discover the best ( set of ) candidate hypernyms for input concepts or entities , given the search space of a pre-defined vocabulary .", ,We introduce a neural network architecture for the concerned task and empirically study various neural network models to build the representations in latent space for words and phrases .,0.037037037037037,0.3333333333333333,0.3333333333333333,research-problem,baselines
hypernym_discovery,1,introduction,introduction,14,6,6,"A relevant well - known scenario is hypernym detection , which is a binary task to decide whether a hypernymic relationship holds between a pair of words or not .",The hypernym discovery task aims to discover the most appropriate hypernym ( s ) for input concepts or entities from a pre-defined corpus .,"A hypernym detection system should be capable of learning taxonomy and lexical semantics , including pattern - based methods and graph - based approaches .",0.1296296296296296,0.3157894736842105,0.3157894736842105,research-problem,ablation-analysis
hypernym_discovery,1,introduction,introduction,23,15,15,"In this work , we introduce a neural network architecture for the concerned task and empirically study various neural networks to model the distributed representations for words and phrases .",tures .,"In our system , we leverage an unambiguous vector representation via term embedding , and we take advantage of deep neural networks to discover the hypernym relationships between terms .",0.2129629629629629,0.7894736842105263,0.7894736842105263,model,experimental-setup
hypernym_discovery,1,introduction,introduction,24,16,16,"In our system , we leverage an unambiguous vector representation via term embedding , and we take advantage of deep neural networks to discover the hypernym relationships between terms .","In this work , we introduce a neural network architecture for the concerned task and empirically study various neural networks to model the distributed representations for words and phrases .",The rest of the paper is organized as follows :,0.2222222222222222,0.8421052631578947,0.8421052631578947,model,approach
hypernym_discovery,1,experiment,setting,83,8,6,Our model was implemented using the Theano 1 .,"For the sake of computational efficiency , we simply average the sense embedding if a word has more than one sense embedding ( among various domains ) .",The diagonal variant of Ada - Grad is used for neural network training .,0.7685185185185185,0.2857142857142857,0.4,experimental-setup,approach
hypernym_discovery,1,experiment,setting,84,9,7,The diagonal variant of Ada - Grad is used for neural network training .,Our model was implemented using the Theano 1 .,We tune the hyper - parameters with the following range of values : learning rate ?,0.7777777777777778,0.3214285714285714,0.4666666666666667,experimental-setup,approach
hypernym_discovery,1,experiment,setting,85,10,8,We tune the hyper - parameters with the following range of values : learning rate ?,The diagonal variant of Ada - Grad is used for neural network training .,"{ 1 e ? 3 , 1 e ? 2 } , dropout probability ? { 0.1 , 0.2 } , CNN filter width ? { 2 , 3 , 4 }.",0.7870370370370371,0.3571428571428571,0.5333333333333333,experimental-setup,experimental-setup
hypernym_discovery,1,experiment,setting,86,11,9,"{ 1 e ? 3 , 1 e ? 2 } , dropout probability ? { 0.1 , 0.2 } , CNN filter width ? { 2 , 3 , 4 }.",We tune the hyper - parameters with the following range of values : learning rate ?,The hidden dimension of all neural models are 200 .,0.7962962962962963,0.3928571428571429,0.6,experimental-setup,baselines
hypernym_discovery,1,experiment,setting,87,12,10,The hidden dimension of all neural models are 200 .,"{ 1 e ? 3 , 1 e ? 2 } , dropout probability ? { 0.1 , 0.2 } , CNN filter width ? { 2 , 3 , 4 }.",The batch size is set to 20 and the word embedding and sense embedding sizes are set to 300 .,0.8055555555555556,0.4285714285714285,0.6666666666666666,experimental-setup,approach
hypernym_discovery,1,experiment,setting,88,13,11,The batch size is set to 20 and the word embedding and sense embedding sizes are set to 300 .,The hidden dimension of all neural models are 200 .,"All of our models are trained on a single GPU ( NVIDIA GTX 980 Ti ) , with roughly 1.5h for general - purpose subtask for English and 0.5h domain - specific domain - specific ones for medical and music .",0.8148148148148148,0.4642857142857143,0.7333333333333333,experimental-setup,experiments
hypernym_discovery,1,experiment,setting,89,14,12,"All of our models are trained on a single GPU ( NVIDIA GTX 980 Ti ) , with roughly 1.5h for general - purpose subtask for English and 0.5h domain - specific domain - specific ones for medical and music .",The batch size is set to 20 and the word embedding and sense embedding sizes are set to 300 .,We run all our models up to 50 epoch and select the best result in validation . :,0.8240740740740741,0.5,0.8,experimental-setup,baselines
hypernym_discovery,1,experiment,result and analysis,96,21,4,"Convolution or recurrent gated mechanisms in either CNN - based ( CNN , RCNN ) or RNN ( GRU , LSTM ) based neural networks could essentially be helpful of modeling the semantic connections between words in a phrase , and guide the networks to discover the hypernym relationships .",This result indicates simply averaging the embedding of words in a phrase is not an appropriate solution to represent a phrase .,"We also observe CNN - based network performance is better than RNN - based , which indicates local features between words could be more important than long - term dependency in this task where the term length is up to trigrams .",0.8888888888888888,0.75,0.3636363636363637,results,baselines
hypernym_discovery,1,experiment,result and analysis,97,22,5,"We also observe CNN - based network performance is better than RNN - based , which indicates local features between words could be more important than long - term dependency in this task where the term length is up to trigrams .","Convolution or recurrent gated mechanisms in either CNN - based ( CNN , RCNN ) or RNN ( GRU , LSTM ) based neural networks could essentially be helpful of modeling the semantic connections between words in a phrase , and guide the networks to discover the hypernym relationships .","To investigate the performance of neural models on specific domains , we conduct experiments on medical and medicine subtask .",0.8981481481481481,0.7857142857142857,0.4545454545454545,results,approach
hypernym_discovery,1,experiment,result and analysis,98,23,6,"To investigate the performance of neural models on specific domains , we conduct experiments on medical and medicine subtask .","We also observe CNN - based network performance is better than RNN - based , which indicates local features between words could be more important than long - term dependency in this task where the term length is up to trigrams .",shows the result .,0.9074074074074074,0.8214285714285714,0.5454545454545454,results,approach
hypernym_discovery,1,experiment,result and analysis,100,25,8,"All the neural models outperform term embedding averaging in terms of all the metrics and CNN - based network also performs better than RNN - based ones in most of the metrics using word embedding , which verifies our hypothesis in the general - purpose task .",shows the result .,"Compared with word embedding , the sense embedding shows a much poorer result though they work closely in generalpurpose subtask .",0.925925925925926,0.8928571428571429,0.7272727272727273,results,approach
hypernym_discovery,1,experiment,result and analysis,101,26,9,"Compared with word embedding , the sense embedding shows a much poorer result though they work closely in generalpurpose subtask .","All the neural models outperform term embedding averaging in terms of all the metrics and CNN - based network also performs better than RNN - based ones in most of the metrics using word embedding , which verifies our hypothesis in the general - purpose task .","The reason might be the simple averaging of sense embedding of various domains for a word , which may introduce too much noise and bias the over all sense representation .",0.9351851851851852,0.9285714285714286,0.8181818181818182,results,approach
hypernym_discovery,2,title,title,2,2,2,Hypernyms under Siege : Linguistically - motivated Artillery for Hypernymy Detection, , ,0.008130081300813,1.0,1.0,research-problem,approach
hypernym_discovery,2,introduction,introduction,10,2,2,"In the last two decades , the NLP community has invested a consistent effort in developing automated methods to recognize hypernymy .", ,"Such effort is motivated by the role this semantic relation plays in a large number of tasks , such as taxonomy creation and recognizing textual entailment .",0.040650406504065,0.0952380952380952,0.0952380952380952,research-problem,experimental-setup
hypernym_discovery,2,introduction,introduction,21,13,13,"In this paper we perform an extensive evaluation of various unsupervised distributional measures for hypernymy detection , using several distributional semantic models that differ by context type and feature weighting .","Additional recent hypernymy detection methods include a multimodal perspective , a supervised method using unsupervised measure scores as features , and a neural method integrating path - based and distributional information .",Some measure vari - ants and context - types are tested for the first time .,0.0853658536585365,0.6190476190476191,0.6190476190476191,approach,experimental-setup
hypernym_discovery,2,introduction,introduction,24,16,16,"We analyze the performance of the measures in different settings and suggest a principled way to select the suitable measure , context type and feature weighting according to the task setting , yielding consistent performance across datasets .","We demonstrate that since each of these measures captures a different aspect of the hypernymy relation , there is no single measure that consistently performs well in discriminating hypernymy from different semantic relations .",We also compare the unsupervised measures to the state - of - the - art supervised methods .,0.0975609756097561,0.7619047619047619,0.7619047619047619,approach,approach
hypernym_discovery,2,introduction,introduction,25,17,17,We also compare the unsupervised measures to the state - of - the - art supervised methods .,"We analyze the performance of the measures in different settings and suggest a principled way to select the suitable measure , context type and feature weighting according to the task setting , yielding consistent performance across datasets .","We show that supervised methods outperform the unsupervised ones , while also being more efficient , computed on top of low - dimensional vectors .",0.1016260162601626,0.8095238095238095,0.8095238095238095,approach,approach
hypernym_discovery,2,experiments,experiments,117,1,1,Experiments, , ,0.475609756097561,0.0093457943925233,1.0,experiments,experimental-setup
hypernym_discovery,2,experiments,comparing unsupervised measures,118,2,1,Comparing Unsupervised Measures, , ,0.4796747967479675,0.0186915887850467,0.05,experiments,approach
hypernym_discovery,2,experiments,comparing unsupervised measures,131,15,14,"The results show preference to the syntactic context - types ( dep and joint ) , which might be explained by the fact that these contexts are richer ( as they contain both proximity and syntactic information ) and therefore more discriminative .","In order to better understand the results , we focus on the second type of evaluation , in which we discriminate hypernyms from each other relation .","In feature weighting there is no consistency , but interestingly , raw frequency appears to be successful in hypernymy detection , contrary to previously reported results for word similarity tasks , where PPMI was shown to outperform it .",0.532520325203252,0.1401869158878504,0.7,experiments,approach
hypernym_discovery,2,experiments,comparing unsupervised measures,132,16,15,"In feature weighting there is no consistency , but interestingly , raw frequency appears to be successful in hypernymy detection , contrary to previously reported results for word similarity tasks , where PPMI was shown to outperform it .","The results show preference to the syntactic context - types ( dep and joint ) , which might be explained by the fact that these contexts are richer ( as they contain both proximity and syntactic information ) and therefore more discriminative .",The new SLQS variants are on top of the list in many settings .,0.5365853658536586,0.1495327102803738,0.75,experiments,approach
hypernym_discovery,2,experiments,comparing unsupervised measures,133,17,16,The new SLQS variants are on top of the list in many settings .,"In feature weighting there is no consistency , but interestingly , raw frequency appears to be successful in hypernymy detection , contrary to previously reported results for word similarity tasks , where PPMI was shown to outperform it .","In particular they perform well in discriminating hypernyms from symmetric relations ( antonymy , synonymy , coordination ) .",0.540650406504065,0.1588785046728972,0.8,experiments,approach
hypernym_discovery,2,experiments,comparing unsupervised measures,134,18,17,"In particular they perform well in discriminating hypernyms from symmetric relations ( antonymy , synonymy , coordination ) .",The new SLQS variants are on top of the list in many settings .,"The measures based on the reversed inclusion hypothesis performed inconsistently , achieving perfect score in the discrimination of hypernyms from unrelated words , and performing well in few other cases , always in combination with syntactic contexts .",0.5447154471544715,0.1682242990654205,0.85,experiments,approach
hypernym_discovery,2,experiments,best measure per classification task,181,65,44,Comparison to State - of - the - art Supervised Methods,"For this reason , generality - based measures perform well on BLESS , and struggle with Weeds , which is handled better using inclusion - based measures .","For comparison with the state - of - the - art , we evaluated several supervised hypernymy detection methods , based on the word embeddings of x and y: concatenation v x ?",0.7357723577235772,0.6074766355140186,0.5116279069767442,experiments,experimental-setup
hypernym_discovery,2,experiments,best measure per classification task,191,75,54,"The over all performance of the embeddingbased classifiers is almost perfect , and in particular the best performance is achieved using the concatenation method with either GloVe or the dependency - based embeddings .","We also re-evaluated the unsupervised measures , this time reporting the results on the validation set ( 10 % ) for comparison .","As expected , the unsupervised measures perform worse than the embedding - based classifiers , though generally not bad on their own .",0.7764227642276422,0.7009345794392523,0.627906976744186,experiments,baselines
hypernym_discovery,2,experiments,best measure per classification task,192,76,55,"As expected , the unsupervised measures perform worse than the embedding - based classifiers , though generally not bad on their own .","The over all performance of the embeddingbased classifiers is almost perfect , and in particular the best performance is achieved using the concatenation method with either GloVe or the dependency - based embeddings .","These results may suggest that unsupervised methods should be preferred only when no training data is available , leaving all the other cases to supervised methods .",0.7804878048780488,0.7102803738317757,0.6395348837209303,experiments,approach
hypernym_discovery,3,title,title,2,2,2,Supervised Distributional Hypernym Discovery via Domain Adaptation, , ,0.0118343195266272,1.0,1.0,research-problem,approach
hypernym_discovery,3,introduction,introduction,18,9,9,"In addition , while not being taxonomy learning systems per se , semi-supervised systems for Information Extraction such as NELL rely crucially on taxonomized concepts and their relations within their learning process .","Notable approaches of this kind include Yago , WikiTaxonomy , and the Wikipedia Bitaxonomy .","Taxonomy learning is roughly based on a twostep process , namely is -a ( hypernymic ) relation de-tection , and graph induction .",0.1065088757396449,0.375,0.375,research-problem,ablation-analysis
hypernym_discovery,3,introduction,introduction,25,16,16,"In this paper we propose TAXOEMBED 2 , a hypernym detection algorithm based on sense embeddings , which can be easily applied to the construction of lexical taxonomies .","These pairs maybe represented either as a concatenation of both vectors , difference , dot -product , or including additional linguistic information for LSTMbased learning .","It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces and , unlike previous approaches , leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain of knowledge .",0.1479289940828402,0.6666666666666666,0.6666666666666666,model,experimental-setup
hypernym_discovery,3,introduction,introduction,26,17,17,"It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces and , unlike previous approaches , leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain of knowledge .","In this paper we propose TAXOEMBED 2 , a hypernym detection algorithm based on sense embeddings , which can be easily applied to the construction of lexical taxonomies .",Our best configuration ( ranking first in two thirds of the experiments conducted ) considers two training sources :,0.1538461538461538,0.7083333333333334,0.7083333333333334,model,ablation-analysis
hypernym_discovery,3,introduction,introduction,27,18,18,Our best configuration ( ranking first in two thirds of the experiments conducted ) considers two training sources :,"It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces and , unlike previous approaches , leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain of knowledge .",( 1 ) Manually curated pairs from Wikidata ; and ( 2 ) Hypernymy relations from a KB which integrates several Open Information Extraction ( OIE ) systems .,0.1597633136094674,0.75,0.75,model,model
hypernym_discovery,3,introduction,introduction,28,19,19,( 1 ) Manually curated pairs from Wikidata ; and ( 2 ) Hypernymy relations from a KB which integrates several Open Information Extraction ( OIE ) systems .,Our best configuration ( ranking first in two thirds of the experiments conducted ) considers two training sources :,"Since our method uses a very large semantic network as reference sense inventory , we are able to perform jointly hypernym extraction and dis ambiguation , from which 1 The terminology is not entirely unified in this respect .",0.165680473372781,0.7916666666666666,0.7916666666666666,model,experimental-setup
hypernym_discovery,3,introduction,introduction,29,20,20,"Since our method uses a very large semantic network as reference sense inventory , we are able to perform jointly hypernym extraction and dis ambiguation , from which 1 The terminology is not entirely unified in this respect .",( 1 ) Manually curated pairs from Wikidata ; and ( 2 ) Hypernymy relations from a KB which integrates several Open Information Extraction ( OIE ) systems .,"In addition to pattern - based , other terms like path - based or rule - based are also used .",0.1715976331360946,0.8333333333333334,0.8333333333333334,model,model
hypernym_discovery,3,introduction,introduction,31,22,22,2 Data and source code available from the following link : www.taln.upf.edu/taxoembed . expanding existing ontologies becomes a trivial task .,"In addition to pattern - based , other terms like path - based or rule - based are also used .","Compared to word - level taxonomy learning , TAXO - EMBED results in more refined and unambiguous hypernymic relations at the sense level , with a direct application in tasks such as semantic search .",0.1834319526627219,0.9166666666666666,0.9166666666666666,code,experimental-setup
hypernym_discovery,3,evaluation,evaluation,131,9,9,"We compare against a number of taxonomy learning and Information Extraction systems , namely , WiBi and DefIE .",We randomly extract 200 test BabelNet synsets ( 20 per domain ) whose hypernyms are missing in Wikidata .,Yago and WiBi are used as upper bounds due to the nature of their hypernymic relations .,0.7751479289940828,0.3103448275862069,0.3103448275862069,baselines,approach
hypernym_discovery,3,evaluation,evaluation,136,14,14,"Finally , DefIE is an automaic OIE system relying on the syntactic structure of pre-dis ambiguated definitions 13 .","WiBi , on the other hand , exploits , among a number of different Wikipedia - specific heuristics , categories and the syntactic structure of the introductory sentence of Wikipedia pages .",Three annotators manually evaluated the validity of the hypernyms extracted by each system ( one per test instance ) .,0.8047337278106509,0.4827586206896552,0.4827586206896552,baselines,model
hypernym_discovery,3,evaluation,evaluation,138,16,16,shows the results of TAXOEMBED and all comparison systems .,Three annotators manually evaluated the validity of the hypernyms extracted by each system ( one per test instance ) .,"As expected , Yago and WiBi achieve the best over all results .",0.8165680473372781,0.5517241379310345,0.5517241379310345,results,approach
hypernym_discovery,3,evaluation,evaluation,139,17,17,"As expected , Yago and WiBi achieve the best over all results .",shows the results of TAXOEMBED and all comparison systems .,"However , TAXOEM - BED , based solely on distributional information , performed competitively in detecting new hypernyms when compared to DefIE , improving its recall in most domains , and even surpassing Yago in technical areas like biology or health .",0.8224852071005917,0.5862068965517241,0.5862068965517241,results,approach
hypernym_discovery,3,evaluation,evaluation,140,18,18,"However , TAXOEM - BED , based solely on distributional information , performed competitively in detecting new hypernyms when compared to DefIE , improving its recall in most domains , and even surpassing Yago in technical areas like biology or health .","As expected , Yago and WiBi achieve the best over all results .","However , our model does not perform particularly well on media and physics .",0.8284023668639053,0.6206896551724138,0.6206896551724138,results,baselines
hypernym_discovery,3,evaluation,evaluation,141,19,19,"However , our model does not perform particularly well on media and physics .","However , TAXOEM - BED , based solely on distributional information , performed competitively in detecting new hypernyms when compared to DefIE , improving its recall in most domains , and even surpassing Yago in technical areas like biology or health .",In most domains our model is able to discover novel hypernym relations thatare not captured by any other system ( e.g. therapy for radiation treatment planning in the health domain or decoration for molding in the art domain ) .,0.8343195266272192,0.6551724137931034,0.6551724137931034,results,approach
hypernym_discovery,4,abstract,abstract,3,2,2,Hypernym discovery aims to discover the hypernym word sets given a hyponym word and proper corpus ., ,"This paper proposes a simple but effective method for the discovery of hypernym sets based on word embedding , which can be used to measure the contextual similarities between words .",0.0258620689655172,0.4,0.4,research-problem,ablation-analysis
hypernym_discovery,4,introduction,introduction,11,5,5,"In the past SemEval contest ( Sem Eval - 2015 task 17 1 , SemEval - 2016 task 13 2 ) , the "" Hypernym Detection "" task was treated as a classfication task , i.e. , given a ( hyponym , hypernym ) pair , deciding whether the pair is a true hypernymic relation or not .","To date , the hypernymy relation also plays an important role in Knowledge Base Construction task .",This has led to criticisms regarding its oversimplification .,0.0948275862068965,0.5,0.5,research-problem,ablation-analysis
hypernym_discovery,4,evaluation,experimental setup,82,3,2,Word2vec is used to produce the word embeddings ., ,The skip - gram model ( - cbow 0 ) is used with the embedding dimension set to 300 ( - size 300 ) .,0.7068965517241379,0.0967741935483871,0.3333333333333333,hyperparameters,experiments
hypernym_discovery,4,evaluation,experimental setup,83,4,3,The skip - gram model ( - cbow 0 ) is used with the embedding dimension set to 300 ( - size 300 ) .,Word2vec is used to produce the word embeddings .,The other options are by default .,0.7155172413793104,0.1290322580645161,0.5,hyperparameters,experiments
hypernym_discovery,4,evaluation,results based on projection learning,87,8,1,Results Based on Projection Learning, , ,0.75,0.2580645161290322,0.0833333333333333,results,approach
hypernym_discovery,4,evaluation,results based on projection learning,91,12,5,"By using the same evaluating metrics as PRF in the cited paper , our best F - value on the validation set is 0.68 ( the paper result is 0.73 ) when the best cluster number is 2 and the threshold is ( 17.7 , 17.3 ) .",The training set consists of the negative pairs and the positive pairs in 3:1 ratio .,"We apply the learned projection matrices and thresholds on the validation data , extract out the candidate hypernyms from the given vocabulary and truncate the top 15 candidates by sorting them according to the d ( ?",0.7844827586206896,0.3870967741935484,0.4166666666666667,results,baselines
hypernym_discovery,4,evaluation,results based on projection learning,96,17,10,"This projection learning method performs not very well on task9 , we think the most probable reason is that in , the problem is formalized as a classification problem , in which the ( hyponym , hypernym ) pairs are given .","The generated results are not very promising , see","However , our task is formalized as a hypernym discovery problem given only hyponmys .",0.8275862068965517,0.5483870967741935,0.8333333333333334,results,model
hypernym_discovery,4,evaluation,results based on nn,99,20,1,Results Based on NN, , ,0.8534482758620692,0.6451612903225806,0.0833333333333333,results,approach
hypernym_discovery,4,evaluation,results based on nn,102,23,4,The performance evaluated using either cross validation or the test data is much worse than that of a typical hypernym prediction task reported by .,shows the results evaluated on the test data .,This illustrates that hypernym discovery is indeed a much harder task than the hypernym prediction task .,0.8793103448275862,0.7419354838709677,0.3333333333333333,results,baselines
hypernym_discovery,4,evaluation,results based on nn,104,25,6,"Although the method proposed by us is quite simple , our submissions are the 1st on Spanish , the 2nd on Italian , the 6th on English , ranked by the metric of MAP .",This illustrates that hypernym discovery is indeed a much harder task than the hypernym prediction task .,This proves the effectiveness of the method .,0.8965517241379308,0.8064516129032258,0.5,results,approach
hypernym_discovery,4,evaluation,results based on nn,106,27,8,"Compared with the results got by cross validation , the performance evaluated on the test data ) dropped significantly on English ( MAP dropped by 4 % ) and Italian ( MAP dropped by 8 % ) , but increased by a margin on Spanish ( MAP increased by 3.6 % ) .",This proves the effectiveness of the method .,"We consider that it is due to the properties of provided data , i.e. , the hypernyms in the test set are similar to those in the training set for Spanish , but dissimilar for English or Italian .",0.913793103448276,0.8709677419354839,0.6666666666666666,results,baselines
hypernym_discovery,5,title,title,2,2,2,CRIM at SemEval-2018 Task 9 : A Hybrid Approach to Hypernym Discovery, , ,0.0106382978723404,1.0,1.0,research-problem,experimental-setup
hypernym_discovery,5,introduction,introduction,12,6,6,"The system developed by the CRIM team for the task of hypernym discovery exploits a combination of two approaches : an unsupervised , pattern - based approach and a supervised , projection learning approach .",More details on this task maybe found in the task description paper .,"These two approaches are described in Sections 2 and 3 , then Section 4 describes our hybrid system and Section 5 presents our results .",0.0638297872340425,0.8571428571428571,0.8571428571428571,model,model
hypernym_discovery,5,experiments and results,experiments and results,134,13,13,Our hybrid system was ranked 1st on all three sub - tasks for which we submitted runs .,"For more details , see ) .","As shown in , the scores obtained using this system are much higher than the strongest baselines for this task .",0.7127659574468085,0.1940298507462686,0.5652173913043478,results,approach
hypernym_discovery,5,experiments and results,experiments and results,135,14,14,"As shown in , the scores obtained using this system are much higher than the strongest baselines for this task .",Our hybrid system was ranked 1st on all three sub - tasks for which we submitted runs .,"Furthermore , it is likely that we could improve our scores on 2A and 2B , since we only tuned the system on 1A .",0.7180851063829787,0.2089552238805969,0.6086956521739131,results,approach
hypernym_discovery,5,experiments and results,experiments and results,136,15,15,"Furthermore , it is likely that we could improve our scores on 2A and 2B , since we only tuned the system on 1A .","As shown in , the scores obtained using this system are much higher than the strongest baselines for this task .","If we compare runs 1 and 2 of our hybrid system , we see that data augmentation improved our scores slightly on 1A and 2B , and increased them by several points on 2A .",0.7234042553191491,0.2238805970149253,0.6521739130434783,results,baselines
hypernym_discovery,5,experiments and results,experiments and results,137,16,16,"If we compare runs 1 and 2 of our hybrid system , we see that data augmentation improved our scores slightly on 1A and 2B , and increased them by several points on 2A .","Furthermore , it is likely that we could improve our scores on 2A and 2B , since we only tuned the system on 1A .","Our cross-evaluation results are better than the supervised baseline computed using the normal evaluation setup , so training our system on general - purpose data produced better results on a domain - specific test set than a strong , supervised baseline trained on the domain - specific data .",0.7287234042553191,0.2388059701492537,0.6956521739130435,results,baselines
hypernym_discovery,5,experiments and results,experiments and results,138,17,17,"Our cross-evaluation results are better than the supervised baseline computed using the normal evaluation setup , so training our system on general - purpose data produced better results on a domain - specific test set than a strong , supervised baseline trained on the domain - specific data .","If we compare runs 1 and 2 of our hybrid system , we see that data augmentation improved our scores slightly on 1A and 2B , and increased them by several points on 2A .",also shows the scores we would have obtained on the test set if we had used only the unsupervised ( pattern - based ) or supervised ( projection learning ) parts of our system .,0.7340425531914894,0.2537313432835821,0.7391304347826086,results,baselines
hypernym_discovery,5,experiments and results,experiments and results,140,19,19,"Note that the unsupervised system outperformed all other unsupervised systems evaluated on this task , and even outperformed the supervised baseline on 2A .",also shows the scores we would have obtained on the test set if we had used only the unsupervised ( pattern - based ) or supervised ( projection learning ) parts of our system .,"Combining the outputs of the 2 systems improves the best score of either system on all test sets , sometimes by as much as 10 points .",0.7446808510638298,0.2835820895522388,0.8260869565217391,results,baselines
hypernym_discovery,5,experiments and results,experiments and results,143,22,22,"Given this observation , we find it somewhat surprising that run 1 is the best on all 3 test sets when we use the hybrid system .","Notice also that the results obtained using only the supervised system indicate that data augmentation had a positive effect on our 2A scores only ( compare runs 1 and 2 ) , although our tests on the trial set suggested it would also have a positive effect on our 1A scores .","One possible explanation is that adding the synthetic examples makes the errors of the supervised system more different from those of the unsupervised system , and that this in turn makes the ensemble method more beneficial , but we have n't looked into this .",0.7606382978723404,0.3283582089552239,0.9565217391304348,results,baselines
hypernym_discovery,5,experiments and results,ablation tests,146,25,2,"To assess the influence of different aspects of the supervised system and its training algorithm , we carried out a few simple ablation tests on subtask 1 A .", ,The baseline for these tests is our supervised projection learning system - we did not apply pattern - based hypernym discovery for any of these tests .,0.7765957446808509,0.373134328358209,0.0454545454545454,ablation-analysis,experimental-setup
hypernym_discovery,5,experiments and results,ablation tests,165,44,21,"These results show that 2 of the techniques we used , namely subsampling and multitask learning , actually harmed our system 's performance on test set 1 A , although our experiments on the trial set suggested that they would be beneficial .",The results obtained on test set 1A are shown in .,This maybe due to the small size of the trial set ( i.e. 50 queries ) or some difference in the underlying distributions of the trial and test sets .,0.8776595744680851,0.6567164179104478,0.4772727272727273,ablation-analysis,baselines
hypernym_discovery,5,experiments and results,ablation tests,167,46,23,"On the other hand , fine - tuning the word embeddings during training seems to be one of the keys to the success of this approach , as are the use of multiple projection matrices , and the sampling of multiple negative examples for each positive example .",This maybe due to the small size of the trial set ( i.e. 50 queries ) or some difference in the underlying distributions of the trial and test sets .,The way we initialize ?,0.8882978723404256,0.6865671641791045,0.5227272727272727,ablation-analysis,approach
hypernym_discovery,5,experiments and results,ablation tests,173,52,29,"We should also note that the supervised model is prone to overfitting , and we found early stopping to be particularly important .","It is worth noting that our supervised model outperforms the supervised baseline provided for this task ( see ) even when it exploits a single projection matrix , however the difference in scores between these 2 systems is only 2 or 3 points , depending on the evaluation metric .",Qualitative Analysis,0.9202127659574468,0.7761194029850746,0.6590909090909091,ablation-analysis,approach
hypernym_discovery,6,title,title,2,2,2,EXPR at SemEval- 2018 Task 9 : A Combined Approach for Hypernym Discovery, , ,0.015625,1.0,1.0,research-problem,experimental-setup
hypernym_discovery,6,introduction,introduction,32,21,21,"To tackle this task , we propose an approach that combines a path - based technique and distributional technique via concatenating two feature vectors : a feature vector constructed using dependency parser output and a feature vector obtained using term embeddings .",The term is either a concept or an entity .,"Then , by using the concatenated vector we create a binary supervised classifier model based on support vector machine ( SVM ) algorithm .",0.25,0.9130434782608696,0.9130434782608696,approach,experimental-setup
hypernym_discovery,6,introduction,introduction,33,22,22,"Then , by using the concatenated vector we create a binary supervised classifier model based on support vector machine ( SVM ) algorithm .","To tackle this task , we propose an approach that combines a path - based technique and distributional technique via concatenating two feature vectors : a feature vector constructed using dependency parser output and a feature vector obtained using term embeddings .",The model predicts if a term and its candidate hypernym are hypernym related or not .,0.2578125,0.9565217391304348,0.9565217391304348,approach,model
hypernym_discovery,6,introduction,introduction,34,23,23,The model predicts if a term and its candidate hypernym are hypernym related or not .,"Then , by using the concatenated vector we create a binary supervised classifier model based on support vector machine ( SVM ) algorithm .", ,0.265625,1.0,1.0,approach,approach
hypernym_discovery,6,results and analysis,results and analysis,98,1,1,Results and Analysis, , ,0.765625,0.0476190476190476,0.0476190476190476,results,baselines
hypernym_discovery,6,results and analysis,results and analysis,101,4,4,"For the three corpora , our system performs better than STJU system , and it performs better than the MFH system on the English corpora .","The table 1 ( a , b and c) below shows the result of our system and other supervised systems to discover hypernyms for Concept terms only .","In addition , the result shows that our system performs well in discovering new hypernyms not defined in the gold hypernyms where it yields good False Positive values in the three corpora and we achieve the best False Positive value in Medical corpus The evaluation results of our system and other supervised systems .",0.7890625,0.1904761904761904,0.1904761904761904,results,approach
hypernym_discovery,6,results and analysis,results and analysis,102,5,5,"In addition , the result shows that our system performs well in discovering new hypernyms not defined in the gold hypernyms where it yields good False Positive values in the three corpora and we achieve the best False Positive value in Medical corpus The evaluation results of our system and other supervised systems .","For the three corpora , our system performs better than STJU system , and it performs better than the MFH system on the English corpora .",Our system result was beneath the expectation .,0.796875,0.2380952380952381,0.2380952380952381,results,approach
hypernym_discovery,6,results and analysis,results and analysis,114,17,17,"As shown in the table 2 , the candidate hypernym extraction ( CHE ) coverage for English testing terms is 950 ( 63 % ) , that means our system is unable to extract any candidate hypernym for 550 ( 37 % ) terms ( 398 entities and 152 concepts ) .","This condition leads to failure to extract candidate hypernyms for some entity terms that ca n't be identified as noun phrases in the corpus such as "" Up All Night "" , "" Someday Came Suddenly "" , "" Now What "" , etc .","Furthermore , our system suffers from a major computational issue when applied to a large corpus .",0.890625,0.8095238095238095,0.8095238095238095,results,approach
hypernym_discovery,7,abstract,abstract,3,2,2,"This paper describes 300 - sparsans ' participation in SemEval - 2018 Task 9 : Hypernym Discovery , with a system based on sparse coding and a formal concept hierarchy obtained from word embeddings .", ,"Our system took first place in subtasks ( 1B ) Italian ( all and entities ) , ( 1C ) Spanish entities , and ( 2B ) music entities .",0.024,0.6666666666666666,0.6666666666666666,research-problem,approach
hypernym_discovery,7,introduction,introduction,14,10,10,Here we apply sparse feature pairs to hypernym extraction .,utilizes non-negative sparse coding for word translation by training sparse word vectors for the two languages such that coding bases correspond to each other .,"The role of an attribute pair i , j ? ? ( q ) ? ( h ) ( where q is the query word , h is the hypernym candidate , and ?( w ) is the index of a non -zero component in the sparse representations of w ) is similar to interaction terms in regression , see section 2 for details .",0.1119999999999999,0.131578947368421,0.3846153846153846,model,experimental-setup
hypernym_discovery,7,introduction,introduction,16,12,12,Sparse representation is related to hypernymy in various natural ways .,"The role of an attribute pair i , j ? ? ( q ) ? ( h ) ( where q is the query word , h is the hypernym candidate , and ?( w ) is the index of a non -zero component in the sparse representations of w ) is similar to interaction terms in regression , see section 2 for details .",One of them is through Formal concept Analysis ( FCA ) .,0.128,0.1578947368421052,0.4615384615384616,model,experimental-setup
hypernym_discovery,7,introduction,introduction,17,13,13,One of them is through Formal concept Analysis ( FCA ) .,Sparse representation is related to hypernymy in various natural ways .,The idea of acquiring concept hierarchies from a text corpus with the tools of Formal concept Analysis ( FCA ) is relatively new .,0.136,0.1710526315789473,0.5,model,model
hypernym_discovery,7,introduction,introduction,21,17,17,"Another natural formulation is related to hierarchical sparse coding , where trees describe the order in which variables "" enter the model "" ( i.e. , take non - zero values ) .","See the next section for a description of formal concept lattices , and how hypernyms can be found in them .","A node may take a non-zero value only if it s ancestors also do : the dimensions that correspond to top level nodes should focus on "" general "" meaning components thatare present in most words .",0.168,0.2236842105263158,0.6538461538461539,model,ablation-analysis
hypernym_discovery,7,introduction,introduction,24,20,20,Exploiting the correspondence between the variable tree and the hypernym hierarchy offers itself as a natural choice .,offer an implementation that is efficient for gigaword corpora .,"The task evaluated systems on their ability to extract hypernyms for query words in five subtasks ( three languages , English , Italian , and Spanish , and two domains , medical and music ) .",0.192,0.2631578947368421,0.7692307692307693,model,approach
hypernym_discovery,7,results,query type sensitive baselining,98,18,2,"Our submission with attribute pairs achieved first place in categories ( 1B ) Italian ( all and entities ) , ( 1C ) Spanish entities , and ( 2B ) music entities .", ,This is in part due to our good choice of a fallback solution in the case of OOV queries : we applied a category - sensitive baseline returning the most frequent train hypernym in the corresponding query type ( concept or entity ) .,0.784,0.4390243902439024,0.5,results,model
hypernym_discovery,8,title,title,2,2,2,Apollo at SemEval-2018 Task 9 : Detecting Hypernymy Relations Using Syntactic Dependencies, , ,0.0303030303030303,1.0,1.0,research-problem,approach
hypernym_discovery,8,introduction,introduction,8,2,2,This paper presents the Apollo team 's system for hypernym discovery which participated in task 9 of Semeval 2018 based on unsupervised machine learning ., ,It is a rule - based system that exploits syntactic dependency paths that generalize Hearst - style lexical patterns .,0.1212121212121212,0.1176470588235294,0.1176470588235294,model,experimental-setup
hypernym_discovery,8,introduction,introduction,9,3,3,It is a rule - based system that exploits syntactic dependency paths that generalize Hearst - style lexical patterns .,This paper presents the Apollo team 's system for hypernym discovery which participated in task 9 of Semeval 2018 based on unsupervised machine learning .,"The paper is structured in 4 sections : this section presents existing approaches for automatic extraction of hypernymy relations , Section 2 contains the current system architecture .",0.1363636363636363,0.1764705882352941,0.1764705882352941,model,approach
hypernym_discovery,8,results,results,60,1,1,Results, , ,0.9090909090909092,0.1428571428571428,0.1428571428571428,results,baselines
hypernym_discovery,8,results,results,63,4,4,"While some relations have not been very fruitful ( such as X "" obj "" Y , for insance ) , others , instead , have been very productive , generating tens of thousands relations .",presents the percentages of the most representative syntactic relations which we have identified .,"The project 's results show that we have managed to accomplish the main objective of this project , to outperform the random strategy .",0.9545454545454546,0.5714285714285714,0.5714285714285714,results,model
hypernym_discovery,8,results,results,64,5,5,"The project 's results show that we have managed to accomplish the main objective of this project , to outperform the random strategy .","While some relations have not been very fruitful ( such as X "" obj "" Y , for insance ) , others , instead , have been very productive , generating tens of thousands relations .","The lower scores have been obtained for multiword expressions , for which we plan to add dedicated modules .",0.9696969696969696,0.7142857142857143,0.7142857142857143,results,approach
hypernym_discovery,8,results,results,65,6,6,"The lower scores have been obtained for multiword expressions , for which we plan to add dedicated modules .","The project 's results show that we have managed to accomplish the main objective of this project , to outperform the random strategy .","An issue that we have noticed was that the given vocabulary was quite restrictive , for instance , it contains words like "" above - water "" , "" artesian water "" , "" bath water "" etc. , but it does n't contain the word "" water "" ( we had a case when our system identified the word "" water "" as a hypernym and it was a correct hypernym , but due to the fact that the vocabulary does n't contain the word "" water "" , it can not be evaluated ) and many other examples like this .",0.9848484848484848,0.8571428571428571,0.8571428571428571,results,approach
natural_language_inference,0,title,title,2,2,2,Neural Models for Reasoning over Multiple Mentions using Coreference, , ,0.0114285714285714,1.0,1.0,research-problem,approach
natural_language_inference,0,introduction,introduction,12,4,4,"We call this coreference - based reasoning since multiple pieces of information , which may lie across sentence , paragraph or document boundaries , are tied together with the help of referring expressions which denote the same real - world entity .",One important form of reasoning for Question Answering ( QA ) models is the ability to aggregate information from multiple mentions of entities .,shows examples .,0.0685714285714285,0.2105263157894736,0.2105263157894736,research-problem,ablation-analysis
natural_language_inference,0,introduction,introduction,20,12,12,"Specifically , given an input sequence and coreference clusters extracted from an external system , we introduce a term in the update equations for Gated Recurrent Units ( GRU ) which depends on the hidden state of the coreferent antecedent of the current token ( if it exists ) .","Hence , in this work we use the annotations produced by such systems to adapt a standard RNN layer by introducing a bias towards coreferent recency .",This way hidden states are propagated along coreference chains and the original sequence in parallel .,0.1142857142857142,0.631578947368421,0.631578947368421,approach,ablation-analysis
natural_language_inference,0,introduction,introduction,21,13,13,This way hidden states are propagated along coreference chains and the original sequence in parallel .,"Specifically , given an input sequence and coreference clusters extracted from an external system , we introduce a term in the update equations for Gated Recurrent Units ( GRU ) which depends on the hidden state of the coreferent antecedent of the current token ( if it exists ) .",We compare our Coref - GRU layer with the regular GRU layer by incorporating it in a recent model for reading comprehension .,0.12,0.6842105263157895,0.6842105263157895,approach,model
natural_language_inference,0,introduction,introduction,22,14,14,We compare our Coref - GRU layer with the regular GRU layer by incorporating it in a recent model for reading comprehension .,This way hidden states are propagated along coreference chains and the original sequence in parallel .,"On synthetic data specifically constructed to test coreferencebased reasoning , C - GRUs lead to a large improvement over regular GRUs .",0.1257142857142857,0.7368421052631579,0.7368421052631579,approach,ablation-analysis
natural_language_inference,0,experiments results,experiments results,94,7,7,BAbi AI tasks .,A task is considered failed if its Max performance is < 0.95 .,Our first set of experiments are on the 1 K training version of the synthetic b Abi AI tasks .,0.5371428571428571,0.125,0.125,results,baselines
natural_language_inference,0,experiments results,experiments results,100,13,13,In each case we see clear improvements of using C - GRU layers over GRU layers .,We also include the results for a single layer version of GA Reader ( which we denote simply as Bi - GRU or Bi - C - GRU when using coreference ) to enable fair comparison with EntNets .,"Interestingly , EntNets , which have > 99 % performance when trained with 10K examples only reach 70 % performance with 1 K training examples .",0.5714285714285714,0.2321428571428571,0.2321428571428571,results,approach
natural_language_inference,0,experiments results,experiments results,105,18,18,"Comparing to the QRN baseline , we found that C - GRU was significantly worse on task 15 ( basic deduction ) .",All these tasks require aggregation of information across sentences to derive the answer .,"On closer examination we found that this was because our simplistic coreference module which matches tokens exactly was notable to resolve "" mice "" to "" mouses "" and "" cats "" to "" cat "" .",0.6,0.3214285714285714,0.3214285714285714,results,baselines
natural_language_inference,0,experiments results,experiments results,107,20,20,"On the other hand , C - GRU was significantly better than QRN on task 16 ( basic induction ) .","On closer examination we found that this was because our simplistic coreference module which matches tokens exactly was notable to resolve "" mice "" to "" mouses "" and "" cats "" to "" cat "" .",We also include a baseline which uses coreference features as 1 - hot vectors appended to the input word vectors ( GA w/ GRU + 1 - hot ) .,0.6114285714285714,0.3571428571428571,0.3571428571428571,results,approach
natural_language_inference,0,experiments results,experiments results,112,25,25,Wikihop dataset .,"In both cases there is a sharp drop in performance , showing that specifically using coreference for connecting mentions is important .","Next we apply our model to the Wikihop dataset , which is specifically constructed to test multi-hop reading comprehension across documents .",0.64,0.4464285714285714,0.4464285714285714,results,baselines
natural_language_inference,0,experiments results,experiments results,120,33,33,"We see higher performance for the C - GRU model in the low data regime , and better generalization throughout the training curve for all three settings .",In we also show the training curves of exp ( ? loss ) on the validation set .,"This supports our conjecture that the GRU layer has difficulty learning the kind of coreference - based reasoning required in this dataset , and that the bias towards coreferent recency helps with that .",0.6857142857142857,0.5892857142857143,0.5892857142857143,results,approach
natural_language_inference,1,title,title,2,2,2,Cut to the Chase : A Context Zoom - in Network for Reading Comprehension, , ,0.0135135135135135,1.0,1.0,research-problem,approach
natural_language_inference,1,abstract,abstract,4,2,2,In recent years many deep neural networks have been proposed to solve Reading Comprehension ( RC ) tasks ., ,Most of these models suffer from reasoning overlong documents and do not trivially generalize to cases where the answer is not present as a span in a given document .,0.027027027027027,0.3333333333333333,0.3333333333333333,research-problem,experimental-setup
natural_language_inference,1,abstract,abstract,7,5,5,"To show the effectiveness of our architecture , we conducted several experiments on the recently proposed and challenging RC dataset ' Nar - rative QA ' .",We present a novel neural - based architecture that is capable of extracting relevant regions based on a given question - document pair and generating a well - formed answer .,"The proposed architecture outperforms state - of - the - art results ( Tay et al. , 2018 ) by 12.62 % ( ROUGE - L ) relative improvement .",0.0472972972972973,0.8333333333333334,0.8333333333333334,research-problem,approach
natural_language_inference,1,introduction,introduction,19,11,11,"To address the issues above we develop a novel context zoom - in network ( ConZNet ) for RC tasks , which can skip through irrelevant parts of a document and generate an answer using only the relevant regions of text .","In addition , unlike humans , they can not easily skip through irrelevant parts to comprehend long documents .",The ConZNet architecture consists of two phases .,0.1283783783783783,0.4782608695652174,0.4782608695652174,model,ablation-analysis
natural_language_inference,1,introduction,introduction,20,12,12,The ConZNet architecture consists of two phases .,"To address the issues above we develop a novel context zoom - in network ( ConZNet ) for RC tasks , which can skip through irrelevant parts of a document and generate an answer using only the relevant regions of text .",In the first phase we identify the relevant regions of text by employing a reinforcement learning algorithm .,0.1351351351351351,0.5217391304347826,0.5217391304347826,model,approach
natural_language_inference,1,introduction,introduction,21,13,13,In the first phase we identify the relevant regions of text by employing a reinforcement learning algorithm .,The ConZNet architecture consists of two phases .,"These relevant regions are not only useful to generate the answer , but can also be presented to the user as supporting information along with the answer .",0.1418918918918918,0.5652173913043478,0.5652173913043478,model,approach
natural_language_inference,1,introduction,introduction,23,15,15,"The second phase is based on an encoder - decoder architecture , which comprehends the identified regions of text and generates the answer by using a residual self - attention network as encoder and a RNNbased sequence generator along with a pointer network as the decoder .","These relevant regions are not only useful to generate the answer , but can also be presented to the user as supporting information along with the answer .",It has the ability to generate better well - formed answers not verbatim present in the document than span prediction models .,0.1554054054054054,0.6521739130434783,0.6521739130434783,model,model
natural_language_inference,1,introduction,introduction,27,19,19,"Unlike existing approaches , our method has the ability to select relevant regions of text not just based on the question but also on how well regions are related to each other .",retrieve a relevant paragraph based on the question and predict the answer span . select sentence ( s ) to make a summary of the entire document with a feed - forward network and generate an answer based on the summary .,"Moreover , our decoder combines span prediction and sequence generation .",0.1824324324324324,0.8260869565217391,0.8260869565217391,model,approach
natural_language_inference,1,introduction,introduction,28,20,20,"Moreover , our decoder combines span prediction and sequence generation .","Unlike existing approaches , our method has the ability to select relevant regions of text not just based on the question but also on how well regions are related to each other .",This allows the decoder to copy words from the relevant regions of text as well as to generate words from a fixed vocabulary .,0.1891891891891892,0.8695652173913043,0.8695652173913043,model,approach
natural_language_inference,1,introduction,introduction,29,21,21,This allows the decoder to copy words from the relevant regions of text as well as to generate words from a fixed vocabulary .,"Moreover , our decoder combines span prediction and sequence generation .","We evaluate our model using one of the challenging RC datasets , called ' NarrativeQA ' , which was released recently by .",0.1959459459459459,0.9130434782608696,0.9130434782608696,model,approach
natural_language_inference,1,experimental results,baselines,117,9,2,"We compare our model against reported models in ( Seq2Seq , ASR , BiDAF ) and the Multi-range Reasoning Unit ( MRU ) in .", ,"We implemented two baseline models ( Baseline 1 , Baseline 2 ) with Context Zoom layer similar to .",0.7905405405405406,0.25,0.3333333333333333,baselines,model
natural_language_inference,1,experimental results,baselines,118,10,3,"We implemented two baseline models ( Baseline 1 , Baseline 2 ) with Context Zoom layer similar to .","We compare our model against reported models in ( Seq2Seq , ASR , BiDAF ) and the Multi-range Reasoning Unit ( MRU ) in .",In both baselines we replace the span prediction layer with an answer generation layer .,0.7972972972972973,0.2777777777777778,0.5,baselines,baselines
natural_language_inference,1,experimental results,implementation details,125,17,4,The model is implemented using Python and Tensorflow .,"Similarly , we further tokenize each sentence , corresponding question and answer using the word tokenizer of NLTK .",All the weights of the model are initialized by Glorot Initialization and biases are initialized with zeros .,0.8445945945945946,0.4722222222222222,0.2857142857142857,experimental-setup,approach
natural_language_inference,1,experimental results,implementation details,126,18,5,All the weights of the model are initialized by Glorot Initialization and biases are initialized with zeros .,The model is implemented using Python and Tensorflow .,"We use a 300 dimensional word vectors from GloVe ( with 840 billion pre-trained vectors ) to initialize the word embeddings , which we kept constant during training .",0.8513513513513513,0.5,0.3571428571428571,experimental-setup,baselines
natural_language_inference,1,experimental results,implementation details,127,19,6,"We use a 300 dimensional word vectors from GloVe ( with 840 billion pre-trained vectors ) to initialize the word embeddings , which we kept constant during training .",All the weights of the model are initialized by Glorot Initialization and biases are initialized with zeros .,All the words that do not appear in Glove are initialized by sampling from a uniform random distribution between .,0.8581081081081081,0.5277777777777778,0.4285714285714285,experimental-setup,experimental-setup
natural_language_inference,1,experimental results,implementation details,128,20,7,All the words that do not appear in Glove are initialized by sampling from a uniform random distribution between .,"We use a 300 dimensional word vectors from GloVe ( with 840 billion pre-trained vectors ) to initialize the word embeddings , which we kept constant during training .",We apply dropout between the layers with keep probability of 0.8 ( i.e dropout = 0.2 ) .,0.8648648648648649,0.5555555555555556,0.5,experimental-setup,experimental-setup
natural_language_inference,1,experimental results,implementation details,129,21,8,We apply dropout between the layers with keep probability of 0.8 ( i.e dropout = 0.2 ) .,All the words that do not appear in Glove are initialized by sampling from a uniform random distribution between .,The number of hidden units are set to 100 .,0.8716216216216216,0.5833333333333334,0.5714285714285714,experimental-setup,approach
natural_language_inference,1,experimental results,implementation details,130,22,9,The number of hidden units are set to 100 .,We apply dropout between the layers with keep probability of 0.8 ( i.e dropout = 0.2 ) .,"We trained our model with the AdaDelta ( Zeiler , 2012 ) optimizer for 50 epochs , an initial learning rate of 0.1 , and a minibatch size of 32 .",0.8783783783783784,0.6111111111111112,0.6428571428571429,experimental-setup,experiments
natural_language_inference,1,experimental results,implementation details,131,23,10,"We trained our model with the AdaDelta ( Zeiler , 2012 ) optimizer for 50 epochs , an initial learning rate of 0.1 , and a minibatch size of 32 .",The number of hidden units are set to 100 .,The hyperparameter ' sample size ' ( number of relevant sentences ) is chosen based on the model performance on the devset .,0.8851351351351351,0.6388888888888888,0.7142857142857143,experimental-setup,baselines
natural_language_inference,1,experimental results,results,138,30,3,The performance of our model gradually dropped from sample size 7 onwards .,"To validate the importance of finding relevant sentences in contrast to using an entire document for answer generation , we experimented with sample sizes beyond 5 .",This result shows evidence that only a few relevant sentences are sufficient to answer a question .,0.9324324324324323,0.8333333333333334,0.3333333333333333,results,baselines
natural_language_inference,1,experimental results,results,139,31,4,This result shows evidence that only a few relevant sentences are sufficient to answer a question .,The performance of our model gradually dropped from sample size 7 onwards .,We also experimented with various sample sizes to seethe effect of intra sentence relations for an - swer generation .,0.9391891891891893,0.8611111111111112,0.4444444444444444,results,approach
natural_language_inference,1,experimental results,results,141,33,6,The performance of the model improved dramatically with sample sizes 3 and 5 compared to the sample size of 1 .,We also experimented with various sample sizes to seethe effect of intra sentence relations for an - swer generation .,These results show that the importance of selecting multiple relevant sentences for generating an answer .,0.9527027027027029,0.9166666666666666,0.6666666666666666,results,baselines
natural_language_inference,1,experimental results,results,144,36,9,This result points out that the self - attention mechanism in the Context zoom layer is an important component to identify related relevant sentences .,"In addition , the low performance of Baseline 2 indicates that just selecting multiple sentences is not enough , they should also be related to each other .", ,0.972972972972973,1.0,1.0,results,approach
natural_language_inference,10,title,title,2,2,2,A Simple and Effective Approach to the Story Cloze Test, , ,0.0185185185185185,1.0,1.0,research-problem,experimental-setup
natural_language_inference,10,abstract,abstract,26,24,24,"Where previous approaches rely on feature engineering or involved neural network architectures , we achieve high accuracy with a fully neural approach involving only a single feedforward network and pre-trained skip - thought embeddings .","First , we achieve near state - of - the - art performance ( within 1.1 % ) but with a much simpler , fullyneural approach .","Second , we find that considering only the last sentence of the context outperforms models that consider the full context .",0.2407407407407407,0.5217391304347826,0.5217391304347826,model,baselines
natural_language_inference,10,abstract,abstract,27,25,25,"Second , we find that considering only the last sentence of the context outperforms models that consider the full context .","Where previous approaches rely on feature engineering or involved neural network architectures , we achieve high accuracy with a fully neural approach involving only a single feedforward network and pre-trained skip - thought embeddings .",Previous approaches focused on the accuracy achieved by either considering the whole context or ignoring the whole context of the story .,0.25,0.5434782608695652,0.5434782608695652,model,model
natural_language_inference,10,abstract,abstract,29,27,27,"In sum , our approach differs from previous efforts in the joint use of three strategies : ( 1 ) using skip - thought embeddings for sentences in the story in a feed - forward neural network , ( 2 ) training the model on the provided validation set , and ( 3 ) considering the two endings with only the last sentence in the prompt .",Previous approaches focused on the accuracy achieved by either considering the whole context or ignoring the whole context of the story .,"This paper is structured as follows : we will discuss previous approaches to the problem and how they compare to our approach , describe our model and the experiments we ran in detail , and finally discuss reasons for our model 's superior performance and why ignoring the first three sentences of the story produces better accuracy .",0.2685185185185185,0.5869565217391305,0.5869565217391305,model,ablation-analysis
natural_language_inference,10,experiments,experimental method,83,11,4,We use cross-entropy loss and SGD with learning rate of 0.01 .,"When training on the validation set , we holdout 10 % of the validation set , and tune hyper - parameters to find a configuration that maximizes the accuracy on the held out data .","During training , we save the model every 3000 iterations , and calculate the validation accuracy .",0.7685185185185185,0.3548387096774194,0.5,hyperparameters,approach
natural_language_inference,10,experiments,experimental method,84,12,5,"During training , we save the model every 3000 iterations , and calculate the validation accuracy .",We use cross-entropy loss and SGD with learning rate of 0.01 .,"We train each model five times ( except the FC models , which we train once due to time considerations ) , and report the average test set accuracy of the model .",0.7777777777777778,0.3870967741935484,0.625,hyperparameters,approach
natural_language_inference,10,experiments,results and discussion,89,17,2,The 3 - layer feed - forward neural network trained on the validation set by summing the skip - thought embeddings of the last sentence ( LS ) of the story prompt and the ending gives the best accuracy ( 76.5 % ) ., ,"This approach is far simpler than previous approaches in the literature ; it requires no feature engineering , nor intricate neural network architecture , and achieves close to state - of - the - art accuracy .",0.8240740740740741,0.5483870967741935,0.125,results,approach
natural_language_inference,10,experiments,results and discussion,91,19,4,"Comparing ' val - LS- skip ' to ' val - LS - Glo Ve ' ( i.e. , using skip - thought embeddings for sentences vs. GloVe word embeddings ) , we confirm that the success of this approach lies in the sizable boost to accuracy from the use of pretrained skip - thought embeddings .","This approach is far simpler than previous approaches in the literature ; it requires no feature engineering , nor intricate neural network architecture , and achieves close to state - of - the - art accuracy .","This is perhaps unsurprising given the success of skip - thought embeddings in story - related tasks , ) , since the model was trained on a large corpus of fiction .",0.8425925925925926,0.6129032258064516,0.25,results,approach
natural_language_inference,10,experiments,results and discussion,95,23,8,"We note that the model trained using only the last sentence ( LS ) of the story context has higher accuracy compared to the model that uses a GRU to encode the full context ( FC ) , and even the model which encodes the entire context .","In the absence of such a large dataset to learn such asso-ciations from , the LSTM with Glo Ve embedding inputs is unable to encode the necessary information to do well on this task .",It is unclear from our experiments why this might be .,0.8796296296296297,0.7419354838709677,0.5,results,baselines
natural_language_inference,11,title,title,2,2,2,Published as a conference paper at ICLR 2017 DYNAMIC COATTENTION NETWORKS FOR QUESTION ANSWERING, , ,0.0080645161290322,1.0,1.0,research-problem,experimental-setup
natural_language_inference,11,introduction,introduction,12,2,2,Question answering ( QA ) is a crucial task in natural language processing that requires both natural language understanding and world knowledge ., ,"Previous QA datasets tend to be high in quality due to human annotation , but small in size .",0.0483870967741935,0.021505376344086,0.1,research-problem,experimental-setup
natural_language_inference,11,introduction,introduction,13,3,3,"Previous QA datasets tend to be high in quality due to human annotation , but small in size .",Question answering ( QA ) is a crucial task in natural language processing that requires both natural language understanding and world knowledge .,"Hence , they did not allow for training data-intensive , expressive models such as deep neural networks .",0.0524193548387096,0.032258064516129,0.15,research-problem,experimental-setup
natural_language_inference,11,introduction,introduction,22,12,12,"We introduce the Dynamic Coattention Network ( DCN ) , illustrated in , an end - to - end neural network for question answering .","However , show that the dataset retains a diverse set of answers and requires different forms of logical reasoning , including multi-sentence reasoning .","The model consists of a coattentive encoder that captures the interactions between the question and the document , as well as a dynamic pointing decoder that alternates between estimating the start and end of the answer span .",0.0887096774193548,0.1290322580645161,0.6,model,experimental-setup
natural_language_inference,11,introduction,introduction,23,13,13,"The model consists of a coattentive encoder that captures the interactions between the question and the document , as well as a dynamic pointing decoder that alternates between estimating the start and end of the answer span .","We introduce the Dynamic Coattention Network ( DCN ) , illustrated in , an end - to - end neural network for question answering .",Our single model obtains an F1 of 75.9 % compared to the best published result of 71.0 % .,0.0927419354838709,0.1397849462365591,0.65,model,model
natural_language_inference,11,experiments,implementation details,127,4,3,"To preprocess the corpus , we use the tokenizer from Stanford CoreNLP .",We train and evaluate our model on the SQuAD dataset .,We use as Glo Ve word vectors pretrained on the 840B Common Crawl corpus .,0.5120967741935484,0.0588235294117647,0.25,experimental-setup,approach
natural_language_inference,11,experiments,implementation details,128,5,4,We use as Glo Ve word vectors pretrained on the 840B Common Crawl corpus .,"To preprocess the corpus , we use the tokenizer from Stanford CoreNLP .",We limit the vocabulary to words that are present in the Common Crawl corpus and set embeddings for out - of - vocabulary words to zero .,0.5161290322580645,0.0735294117647058,0.3333333333333333,experimental-setup,experimental-setup
natural_language_inference,11,experiments,implementation details,129,6,5,We limit the vocabulary to words that are present in the Common Crawl corpus and set embeddings for out - of - vocabulary words to zero .,We use as Glo Ve word vectors pretrained on the 840B Common Crawl corpus .,"Empirically , we found that training the embeddings consistently led to overfitting and subpar performance , and hence only report results with fixed word embeddings .",0.5201612903225806,0.088235294117647,0.4166666666666667,experimental-setup,baselines
natural_language_inference,11,experiments,implementation details,131,8,7,"We use a max sequence length of 600 during training and a hidden state size of 200 for all recurrent units , maxout layers , and linear layers .","Empirically , we found that training the embeddings consistently led to overfitting and subpar performance , and hence only report results with fixed word embeddings .",All LSTMs have randomly initialized parameters and an initial state of zero .,0.5282258064516129,0.1176470588235294,0.5833333333333334,experimental-setup,baselines
natural_language_inference,11,experiments,implementation details,132,9,8,All LSTMs have randomly initialized parameters and an initial state of zero .,"We use a max sequence length of 600 during training and a hidden state size of 200 for all recurrent units , maxout layers , and linear layers .",Sentinel vectors are randomly initialized and optimized during training .,0.532258064516129,0.1323529411764706,0.6666666666666666,experimental-setup,baselines
natural_language_inference,11,experiments,implementation details,133,10,9,Sentinel vectors are randomly initialized and optimized during training .,All LSTMs have randomly initialized parameters and an initial state of zero .,"For the dynamic decoder , we set the maximum number of iterations to 4 and use a maxout pool size of 16 .",0.5362903225806451,0.1470588235294117,0.75,experimental-setup,approach
natural_language_inference,11,experiments,implementation details,134,11,10,"For the dynamic decoder , we set the maximum number of iterations to 4 and use a maxout pool size of 16 .",Sentinel vectors are randomly initialized and optimized during training .,"We use dropout to regularize our network during training , and optimize the model using ADAM .",0.5403225806451613,0.1617647058823529,0.8333333333333334,experimental-setup,baselines
natural_language_inference,11,experiments,implementation details,135,12,11,"We use dropout to regularize our network during training , and optimize the model using ADAM .","For the dynamic decoder , we set the maximum number of iterations to 4 and use a maxout pool size of 16 .",All models are implemented and trained with Chainer .,0.5443548387096774,0.1764705882352941,0.9166666666666666,experimental-setup,baselines
natural_language_inference,11,experiments,implementation details,136,13,12,All models are implemented and trained with Chainer .,"We use dropout to regularize our network during training , and optimize the model using ADAM .", ,0.5483870967741935,0.1911764705882352,1.0,experimental-setup,approach
natural_language_inference,11,experiments,results,145,22,9,"The performance of the Dynamic Coattention Network on the SQuAD dataset , compared to other submitted models on the leaderboard 3 , is shown in The DCN has the capability to estimate the start and end points of the answer span multiple times , each time conditioned on its previous estimates .",The training and development sets are publicly available while the test set is withheld .,"By doing so , the model is able to explore local maxima corresponding to multiple plausible answers , as is shown in .",0.5846774193548387,0.3235294117647059,0.1636363636363636,results,approach
natural_language_inference,11,experiments,results,146,23,10,"By doing so , the model is able to explore local maxima corresponding to multiple plausible answers , as is shown in .","The performance of the Dynamic Coattention Network on the SQuAD dataset , compared to other submitted models on the leaderboard 3 , is shown in The DCN has the capability to estimate the start and end points of the answer span multiple times , each time conditioned on its previous estimates .",Question 3 : What kind of weapons did Tesla 's treatise concern ?,0.5887096774193549,0.3382352941176471,0.1818181818181818,results,experimental-setup
natural_language_inference,12,title,title,2,2,2,FINDING REMO ( RELATED MEMORY OBJECT ) : A SIMPLE NEURAL ARCHITECTURE FOR TEXT BASED REASONING, , ,0.0097560975609756,1.0,1.0,research-problem,experimental-setup
natural_language_inference,12,abstract,abstract,4,2,2,"To solve the text - based question and answering task that requires relational reasoning , it is necessary to memorize a large amount of information and find out the question relevant information from the memory .", ,Most approaches were based on external memory and four components proposed by Memory Network .,0.0195121951219512,0.2222222222222222,0.2222222222222222,research-problem,baselines
natural_language_inference,12,abstract,abstract,11,9,9,* Code is publicly available at : https://github.com/juung/RMN,It shows new state - of - the - art results in jointly trained b AbI - 10 k story - based question answering tasks and bAbI dialog - based question answering tasks ., ,0.0536585365853658,1.0,1.0,code,baselines
natural_language_inference,12,introduction,introduction,42,31,31,"Our proposed model , "" Relation Memory Network "" ( RMN ) , is able to find complex relation even when a lot of information is given .","has suggested attention mechanisms as a solution to filter out unimportant relations ; however , since it interrupts the reasoning operation , it may not be the most optimal solution to the problem .",It uses MLP to find out relevant information with a new generalization which simply erase the information already used .,0.2048780487804878,0.8378378378378378,0.8378378378378378,model,baselines
natural_language_inference,12,introduction,introduction,43,32,32,It uses MLP to find out relevant information with a new generalization which simply erase the information already used .,"Our proposed model , "" Relation Memory Network "" ( RMN ) , is able to find complex relation even when a lot of information is given .","In other words , RMN inherits RN 's MLP - based output feature map on Memory Network architecture .",0.2097560975609756,0.8648648648648649,0.8648648648648649,model,model
natural_language_inference,12,introduction,introduction,44,33,33,"In other words , RMN inherits RN 's MLP - based output feature map on Memory Network architecture .",It uses MLP to find out relevant information with a new generalization which simply erase the information already used .,Experiments show its state - of the - art result on the text - based question answering tasks .,0.2146341463414634,0.8918918918918919,0.8918918918918919,model,ablation-analysis
natural_language_inference,12,relation memory network,training details,125,77,2,bAbI story - based QA dataset, ,We trained 2 hop RMN jointly on all tasks using 10 k dataset for model to infer the solution suited to each type of tasks .,0.6097560975609756,0.5238095238095238,0.0666666666666666,hyperparameters,experimental-setup
natural_language_inference,12,relation memory network,training details,129,81,6,"Embedding component is similar to , where story and question are embedded through different LSTMs ; 32 unit word - lookup embeddings ; 32 unit LSTM for story and question .","Then , we labeled each sentence with its relative position .","For attention component , as we use 2 hop RMN , there are g 1 ?",0.6292682926829268,0.5510204081632653,0.2,hyperparameters,baselines
natural_language_inference,12,relation memory network,training details,130,82,7,"For attention component , as we use 2 hop RMN , there are g 1 ?","Embedding component is similar to , where story and question are embedded through different LSTMs ; 32 unit word - lookup embeddings ; 32 unit LSTM for story and question .","and g 2 ? ; both are three - layer MLP consisting of 256 , 128 , 1 unit with ReLU activation function . f ? is composed of 512 , 512 , and 159 units ( the number of words appearing in bAbI dataset is 159 ) of three - layer MLP with ReLU non-linearities where the final layer was a linear that produced logits fora softmax over the answer vocabulary .",0.6341463414634146,0.5578231292517006,0.2333333333333333,hyperparameters,results
natural_language_inference,12,relation memory network,training details,132,84,9,"For regularization , we use batch normalization for all MLPs .","and g 2 ? ; both are three - layer MLP consisting of 256 , 128 , 1 unit with ReLU activation function . f ? is composed of 512 , 512 , and 159 units ( the number of words appearing in bAbI dataset is 159 ) of three - layer MLP with ReLU non-linearities where the final layer was a linear that produced logits fora softmax over the answer vocabulary .",The softmax output was optimized with a cross - entropy loss function using the Adam optimizer with a learning rate of 2 e ?4 .,0.6439024390243903,0.5714285714285714,0.3,hyperparameters,baselines
natural_language_inference,12,relation memory network,training details,133,85,10,The softmax output was optimized with a cross - entropy loss function using the Adam optimizer with a learning rate of 2 e ?4 .,"For regularization , we use batch normalization for all MLPs .",bAbI dialog dataset,0.6487804878048781,0.5782312925170068,0.3333333333333333,hyperparameters,model
natural_language_inference,12,relation memory network,training details,134,86,11,bAbI dialog dataset,The softmax output was optimized with a cross - entropy loss function using the Adam optimizer with a learning rate of 2 e ?4 .,"We trained on full dialog scripts with every model response as answer , all previous dialog history as sentences to be memorized , and the last user utterance as question .",0.6536585365853659,0.5850340136054422,0.3666666666666665,hyperparameters,experimental-setup
natural_language_inference,12,relation memory network,training details,135,87,12,"We trained on full dialog scripts with every model response as answer , all previous dialog history as sentences to be memorized , and the last user utterance as question .",bAbI dialog dataset,"Model selects the most probable response from 4,212 candidates which are ranked from a set of all bot utterances appearing in training , validation and test sets ( plain and OOV ) for all tasks combined .",0.6585365853658537,0.5918367346938775,0.4,hyperparameters,baselines
natural_language_inference,12,relation memory network,training details,136,88,13,"Model selects the most probable response from 4,212 candidates which are ranked from a set of all bot utterances appearing in training , validation and test sets ( plain and OOV ) for all tasks combined .","We trained on full dialog scripts with every model response as answer , all previous dialog history as sentences to be memorized , and the last user utterance as question .",We also report results when we use match type features for dialog .,0.6634146341463415,0.5986394557823129,0.4333333333333333,hyperparameters,model
natural_language_inference,12,relation memory network,babi dialog,154,106,1,BABI DIALOG, , ,0.7512195121951221,0.7210884353741497,0.04,results,approach
natural_language_inference,12,relation memory network,babi dialog,156,108,3,"Without any match type , RN and RMN outperform previous memory - augmented models on both normal and OOV tasks .",The results in the show that the RMN has the best results in any conditions .,This is mainly attributed to the impressive result on task 4 which can be interpreted as an effect of MLP based output feature map .,0.7609756097560976,0.7346938775510204,0.12,results,baselines
natural_language_inference,12,relation memory network,babi dialog,160,112,7,"We converted RMN 's attention component to inner product based attention , and the results revealed the error rate increased to 11.3 % .",We assumed that inner product was not sufficient to capture their implicit similarity and performed an supporting experiment .,"For the task 3 and task 5 where the maximum length is especially longer than the others , RN performs worse than MemN2N , GMe m N2N and RMN .",0.7804878048780488,0.7619047619047619,0.28,results,baselines
natural_language_inference,12,relation memory network,babi dialog,162,114,9,The number of unnecessary object pairs created by the RN not only increases the processing time but also decreases the accuracy .,"For the task 3 and task 5 where the maximum length is especially longer than the others , RN performs worse than MemN2N , GMe m N2N and RMN .","With the match type feature , all models other than RMN have significantly improved their performance except for task 3 compared to the plain condition .",0.7902439024390244,0.7755102040816326,0.36,results,model
natural_language_inference,12,relation memory network,babi dialog,163,115,10,"With the match type feature , all models other than RMN have significantly improved their performance except for task 3 compared to the plain condition .",The number of unnecessary object pairs created by the RN not only increases the processing time but also decreases the accuracy .,RMN was helped by the match type only on the OOV tasks and this implies RMN is able to find relation in the With Match condition for the normal tasks .,0.7951219512195122,0.782312925170068,0.4,results,baselines
natural_language_inference,12,relation memory network,babi dialog,171,123,18,"Different from other tasks , RMN yields the same error rate 25.1 % with MemN2N and GMe m N2N on the task 3 .","Supporting sentence of task 4 have one keyword out of three words , whereas supporting sentences of task 1 and 2 consist of four keywords ( cuisine , location , number and price ) out of sixteen words .",The main goal of task 3 is to recommend restaurant from knowledge base in the order of rating .,0.8341463414634146,0.8367346938775511,0.72,results,model
natural_language_inference,13,title,title,2,2,2,Natural Language Comprehension with the EpiReader, , ,0.0072992700729927,1.0,1.0,research-problem,approach
natural_language_inference,13,abstract,abstract,4,2,2,"We present the EpiReader , a novel model for machine comprehension of text .", ,"Machine comprehension of unstructured , real - world text is a major research goal for natural language processing .",0.0145985401459854,0.3333333333333333,0.3333333333333333,research-problem,experimental-setup
natural_language_inference,13,abstract,abstract,5,3,3,"Machine comprehension of unstructured , real - world text is a major research goal for natural language processing .","We present the EpiReader , a novel model for machine comprehension of text .","Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text , and evaluate a model 's response to the questions .",0.0182481751824817,0.5,0.5,research-problem,baselines
natural_language_inference,13,abstract,abstract,6,4,4,"Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text , and evaluate a model 's response to the questions .","Machine comprehension of unstructured , real - world text is a major research goal for natural language processing .","The EpiReader is an end - to - end neural model comprising two components : the first component proposes a small set of candidate answers after comparing a question to its supporting text , and the second component formulates hypotheses using the proposed candidates and the question , then reranks the hypotheses based on their estimated concordance with the supporting text .",0.0218978102189781,0.6666666666666666,0.6666666666666666,research-problem,ablation-analysis
natural_language_inference,13,introduction,introduction,13,5,5,"We propose a deep , end - to - end , neural comprehension model that we call the EpiReader .","In this paper , we argue that the same principle can be applied to machine comprehension of natural language .","Comprehension of natural language by machines , at a near- human level , is a prerequisite for an extremely broad class of useful applications of artificial intelligence .",0.0474452554744525,0.1282051282051282,0.1282051282051282,approach,ablation-analysis
natural_language_inference,13,introduction,introduction,26,18,18,The EpiReader factors into two components .,We compare the EpiReader to these earlier models through training and evaluation on the CNN and CBT datasets .,The first component extracts a small set of potential answers based on a shallow comparison of the question with its supporting text ; we call this the Extractor .,0.0948905109489051,0.4615384615384616,0.4615384615384616,approach,approach
natural_language_inference,13,introduction,introduction,27,19,19,The first component extracts a small set of potential answers based on a shallow comparison of the question with its supporting text ; we call this the Extractor .,The EpiReader factors into two components .,The second component reranks the proposed answers based on deeper semantic comparisons with the text ; we call this the Reasoner .,0.0985401459854014,0.4871794871794872,0.4871794871794872,approach,ablation-analysis
natural_language_inference,13,introduction,introduction,28,20,20,The second component reranks the proposed answers based on deeper semantic comparisons with the text ; we call this the Reasoner .,The first component extracts a small set of potential answers based on a shallow comparison of the question with its supporting text ; we call this the Extractor .,We can summarize this process as Extract ?,0.1021897810218978,0.5128205128205128,0.5128205128205128,approach,approach
natural_language_inference,13,introduction,introduction,32,24,24,"The semantic comparisons implemented by the Reasoner are based on the concept of recognizing textual entailment ( RTE ) , also known as natural language inference .",Test 2 .,This process is computationally demanding .,0.1167883211678832,0.6153846153846154,0.6153846153846154,approach,experimental-setup
natural_language_inference,13,introduction,introduction,34,26,26,"Thus , the Extractor serves the important function of filtering a large set of potential answers down to a small , tractable set of likely candidates for more thorough testing .",This process is computationally demanding .,"The Extractor follows the form of a pointer network , and uses a differentiable attention mechanism to indicate words in the text that potentially answer the question .",0.1240875912408759,0.6666666666666666,0.6666666666666666,approach,model
natural_language_inference,13,introduction,introduction,35,27,27,"The Extractor follows the form of a pointer network , and uses a differentiable attention mechanism to indicate words in the text that potentially answer the question .","Thus , the Extractor serves the important function of filtering a large set of potential answers down to a small , tractable set of likely candidates for more thorough testing .",This approach was used ( on it s own ) for question answering with the Attention Sum Reader .,0.1277372262773722,0.6923076923076923,0.6923076923076923,approach,model
natural_language_inference,13,introduction,introduction,36,28,28,This approach was used ( on it s own ) for question answering with the Attention Sum Reader .,"The Extractor follows the form of a pointer network , and uses a differentiable attention mechanism to indicate words in the text that potentially answer the question .",The Extractor outputs a small set of answer candidates along with their estimated probabilities of correctness .,0.1313868613138686,0.7179487179487181,0.7179487179487181,approach,approach
natural_language_inference,13,introduction,introduction,37,29,29,The Extractor outputs a small set of answer candidates along with their estimated probabilities of correctness .,This approach was used ( on it s own ) for question answering with the Attention Sum Reader .,"The Reasoner forms hypotheses by inserting the candidate answers into the question , then estimates the concordance of each hypothesis with each sentence in the supporting text .",0.1350364963503649,0.7435897435897436,0.7435897435897436,approach,model
natural_language_inference,13,introduction,introduction,38,30,30,"The Reasoner forms hypotheses by inserting the candidate answers into the question , then estimates the concordance of each hypothesis with each sentence in the supporting text .",The Extractor outputs a small set of answer candidates along with their estimated probabilities of correctness .,"We use these estimates as a measure of the evidence for a hypothesis , and aggregate evidence overall sentences .",0.1386861313868613,0.7692307692307693,0.7692307692307693,approach,model
natural_language_inference,13,introduction,introduction,39,31,31,"We use these estimates as a measure of the evidence for a hypothesis , and aggregate evidence overall sentences .","The Reasoner forms hypotheses by inserting the candidate answers into the question , then estimates the concordance of each hypothesis with each sentence in the supporting text .","In the end , we combine the Reasoner 's evidence with the Extractor 's probability estimates to produce a final ranking of the answer candidates .",0.1423357664233576,0.7948717948717948,0.7948717948717948,approach,model
natural_language_inference,13,introduction,introduction,40,32,32,"In the end , we combine the Reasoner 's evidence with the Extractor 's probability estimates to produce a final ranking of the answer candidates .","We use these estimates as a measure of the evidence for a hypothesis , and aggregate evidence overall sentences .",This paper is organized as follows .,0.145985401459854,0.8205128205128205,0.8205128205128205,approach,approach
natural_language_inference,13,evaluation,implementation and training details,221,3,2,"To train our model we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .", ,"The word embeddings were initialized randomly , drawing from the uniform distribution over .",0.8065693430656934,0.0535714285714285,0.1538461538461538,experimental-setup,approach
natural_language_inference,13,evaluation,implementation and training details,222,4,3,"The word embeddings were initialized randomly , drawing from the uniform distribution over .","To train our model we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .","We used batches of 32 examples , and early stopping with a patience of 2 epochs .",0.8102189781021898,0.0714285714285714,0.2307692307692308,experimental-setup,approach
natural_language_inference,13,evaluation,implementation and training details,223,5,4,"We used batches of 32 examples , and early stopping with a patience of 2 epochs .","The word embeddings were initialized randomly , drawing from the uniform distribution over .",Our model was implement in Theano using the Keras framework .,0.8138686131386861,0.0892857142857142,0.3076923076923077,experimental-setup,approach
natural_language_inference,13,evaluation,implementation and training details,224,6,5,Our model was implement in Theano using the Keras framework .,"We used batches of 32 examples , and early stopping with a patience of 2 epochs .",The results presented below for the EpiReader were obtained by searching over a small grid of hyperparameter settings .,0.8175182481751825,0.1071428571428571,0.3846153846153846,experimental-setup,approach
natural_language_inference,13,evaluation,implementation and training details,229,11,10,"All our models used 2 - regularization at 0.001 , ? = 50 , and ? = 0.04 .","As has been done previously , we train separate models on CBT 's named entity ( CBT - NE ) and common noun ( CBT - CN ) splits .",We did not use dropout but plan to investigate its effect in the future .,0.8357664233576643,0.1964285714285714,0.7692307692307693,experimental-setup,baselines
natural_language_inference,13,evaluation,results,236,18,4,The EpiReader achieves state - of - the - art performance across the board for both datasets .,We measure EpiReader performance at the output of both the Extractor and the Reasoner .,"On CNN , we score 2.2 % higher on test than the best previous model of .",0.8613138686131386,0.3214285714285714,0.3076923076923077,results,baselines
natural_language_inference,13,evaluation,results,237,19,5,"On CNN , we score 2.2 % higher on test than the best previous model of .",The EpiReader achieves state - of - the - art performance across the board for both datasets .,"Interestingly , an analysis of the CNN dataset by suggests that approximately 25 % of the test examples contain coreference errors or questions which are "" ambiguous / hard "" even for a human analyst .",0.864963503649635,0.3392857142857143,0.3846153846153846,results,baselines
natural_language_inference,13,evaluation,results,242,24,10,On CBT - CN our single model scores 4.0 % higher than the previous best of the AS Reader .,"If , on the 25 % of "" noisy "" questions , the model can shift it s hit rate from , e.g. , 1/10 to 1 / 3 , then there is still a fair amount of performance to gain .",The improvement on CBT - NE is more modest at 1.1 % .,0.8832116788321168,0.4285714285714285,0.7692307692307693,results,approach
natural_language_inference,13,evaluation,results,243,25,11,The improvement on CBT - NE is more modest at 1.1 % .,On CBT - CN our single model scores 4.0 % higher than the previous best of the AS Reader .,"Looking more closely at our CBT - NE results , we found that the validation and test accuracies had relatively high variance even in late epochs of training .",0.8868613138686131,0.4464285714285714,0.8461538461538461,results,approach
natural_language_inference,13,evaluation,results,244,26,12,"Looking more closely at our CBT - NE results , we found that the validation and test accuracies had relatively high variance even in late epochs of training .",The improvement on CBT - NE is more modest at 1.1 % .,"We discovered that many of the validation and test questions were asked about the same named entity , which may explain this issue .",0.8905109489051095,0.4642857142857143,0.9230769230769232,results,baselines
natural_language_inference,14,title,title,2,2,2,End - To - End Memory Networks, , ,0.0099502487562189,1.0,1.0,research-problem,approach
natural_language_inference,14,abstract,abstract,14,12,12,"In this work , we present a novel recurrent neural network ( RNN ) architecture where the recurrence reads from a possibly large external memory multiple times before outputting a symbol .","In [ 23 , 8 , 2 ] , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the actions of neural networks .",Our model can be considered a continuous form of the Memory Network implemented in [ 23 ] .,0.0696517412935323,0.1846153846153846,0.5217391304347826,model,baselines
natural_language_inference,14,abstract,abstract,15,13,13,Our model can be considered a continuous form of the Memory Network implemented in [ 23 ] .,"In this work , we present a novel recurrent neural network ( RNN ) architecture where the recurrence reads from a possibly large external memory multiple times before outputting a symbol .","The model in that work was not easy to train via backpropagation , and required supervision at each layer of the network .",0.0746268656716417,0.2,0.5652173913043478,model,baselines
natural_language_inference,14,abstract,abstract,17,15,15,"The continuity of the model we present here means that it can be trained end - to - end from input - output pairs , and so is applicable to more tasks , i.e. tasks where such supervision is not available , such as in language modeling or realistically supervised question answering tasks .","The model in that work was not easy to train via backpropagation , and required supervision at each layer of the network .","Our model can also be seen as a version of RNNsearch [ 2 ] with multiple computational steps ( which we term "" hops "" ) per output symbol .",0.0845771144278607,0.2307692307692308,0.6521739130434783,model,baselines
natural_language_inference,14,abstract,abstract,18,16,16,"Our model can also be seen as a version of RNNsearch [ 2 ] with multiple computational steps ( which we term "" hops "" ) per output symbol .","The continuity of the model we present here means that it can be trained end - to - end from input - output pairs , and so is applicable to more tasks , i.e. tasks where such supervision is not available , such as in language modeling or realistically supervised question answering tasks .","We will show experimentally that the multiple hops over the long - term memory are crucial to good performance of our model on these tasks , and that training the memory representation can be integrated in a scalable manner into our end - to - end neural network model .",0.0895522388059701,0.2461538461538461,0.6956521739130435,model,model
natural_language_inference,14,synthetic question and answering experiments,training details,133,37,3,"For each mini-batch update , the 2 norm of the whole gradient of all parameters is measured 5 and if larger than L = 50 , then it is scaled down to have norm L.","The training procedure we use is the same as the QA tasks , except for the following .",This was crucial for good performance .,0.6616915422885572,0.5522388059701493,0.15,experimental-setup,baselines
natural_language_inference,14,synthetic question and answering experiments,training details,135,39,5,"We use the learning rate annealing schedule from , namely , if the validation cost has not decreased after one epoch , then the learning rate is scaled down by a factor 1.5 .",This was crucial for good performance .,"Training terminates when the learning rate drops below 10 ? 5 , i.e. after 50 epochs or so .",0.6716417910447762,0.582089552238806,0.25,experimental-setup,experimental-setup
natural_language_inference,14,synthetic question and answering experiments,training details,137,41,7,"Weights are initialized using N ( 0 , 0.05 ) and batch size is set to 128 .","Training terminates when the learning rate drops below 10 ? 5 , i.e. after 50 epochs or so .","On the Penn tree dataset , we repeat each training 10 times with different random initializations and pick the one with smallest validation cost .",0.681592039800995,0.6119402985074627,0.35,experimental-setup,baselines
natural_language_inference,14,synthetic question and answering experiments,training details,138,42,8,"On the Penn tree dataset , we repeat each training 10 times with different random initializations and pick the one with smallest validation cost .","Weights are initialized using N ( 0 , 0.05 ) and batch size is set to 128 .","However , we have done only a single training run on Text8 dataset due to limited time constraints .",0.6865671641791045,0.6268656716417911,0.4,experimental-setup,approach
natural_language_inference,14,synthetic question and answering experiments,baselines,153,57,3,"MemNN : The strongly supervised AM + NG + NL Memory Networks approach , proposed in .",We compare our approach 2 ( abbreviated to MemN2N ) to a range of alternate models :,This is the best reported approach in that paper .,0.7611940298507462,0.8507462686567164,0.25,baselines,model
natural_language_inference,14,synthetic question and answering experiments,baselines,157,61,7,MemNN- WSH :,"It employs n-gram modeling , nonlinear layers and an adaptive number of hops per query .",A weakly supervised heuristic version of MemNN where the supporting sentence labels are not used in training .,0.7810945273631841,0.9104477611940298,0.5833333333333334,baselines,baselines
natural_language_inference,14,synthetic question and answering experiments,baselines,158,62,8,A weakly supervised heuristic version of MemNN where the supporting sentence labels are not used in training .,MemNN- WSH :,"Since we are unable to backpropagate through the max operations in each layer , we enforce that the first memory hop should share at least one word with the question , and that the second memory hop should share at least one word with the first hop and at least one word with the answer .",0.7860696517412935,0.9253731343283582,0.6666666666666666,baselines,baselines
natural_language_inference,14,synthetic question and answering experiments,baselines,161,65,11,"LSTM : A standard LSTM model , trained using question / answer pairs only ( i.e. also weakly supervised ) .","All those memories that conform are called valid memories , and the goal during training is to rank them higher than invalid memories using the same ranking criteria as during strongly supervised training .","For more detail , see .",0.8009950248756219,0.9701492537313432,0.9166666666666666,baselines,baselines
natural_language_inference,15,title,title,2,2,2,Neural Natural Language Inference Models Enhanced with External Knowledge, , ,0.0086206896551724,1.0,1.0,research-problem,approach
natural_language_inference,15,abstract,abstract,4,2,2,Modeling natural language inference is a very challenging task ., ,"With the availability of large annotated data , it has recently become feasible to train complex models such as neural - network - based inference models , which have shown to achieve the state - of - the - art performance .",0.0172413793103448,0.2857142857142857,0.2857142857142857,research-problem,experimental-setup
natural_language_inference,15,abstract,abstract,5,3,3,"With the availability of large annotated data , it has recently become feasible to train complex models such as neural - network - based inference models , which have shown to achieve the state - of - the - art performance .",Modeling natural language inference is a very challenging task .,"Although there exist relatively large annotated data , can machines learn all knowledge needed to perform natural language inference ( NLI ) from these data ?",0.021551724137931,0.4285714285714285,0.4285714285714285,research-problem,baselines
natural_language_inference,15,abstract,abstract,6,4,4,"Although there exist relatively large annotated data , can machines learn all knowledge needed to perform natural language inference ( NLI ) from these data ?","With the availability of large annotated data , it has recently become feasible to train complex models such as neural - network - based inference models , which have shown to achieve the state - of - the - art performance .","If not , how can neural - network - based NLI models benefit from external knowledge and how to build NLI models to leverage it ?",0.0258620689655172,0.5714285714285714,0.5714285714285714,research-problem,baselines
natural_language_inference,15,abstract,abstract,7,5,5,"If not , how can neural - network - based NLI models benefit from external knowledge and how to build NLI models to leverage it ?","Although there exist relatively large annotated data , can machines learn all knowledge needed to perform natural language inference ( NLI ) from these data ?","In this paper , we enrich the state - of - the - art neural natural language inference models with external knowledge .",0.0301724137931034,0.7142857142857143,0.7142857142857143,research-problem,baselines
natural_language_inference,15,introduction,introduction,12,3,3,"Natural language inference ( NLI ) , also known as recognizing textual entailment ( RTE ) , is an important NLP problem concerned with determining inferential relationship ( e.g. , entailment , contradiction , or neutral ) between a premise p and a hypothesis h.",Reasoning and inference are central to both human and artificial intelligence .,"In general , modeling informal inference in language is a very challenging and basic problem towards achieving true natural language understanding .",0.0517241379310344,0.1764705882352941,0.1764705882352941,research-problem,ablation-analysis
natural_language_inference,15,introduction,introduction,23,14,14,"In this paper we enrich neural - network - based NLI models with external knowledge in coattention , local inference collection , and inference composition components .","In general , although in many tasks learning tabula rasa achieved state - of - the - art performance , we believe complicated NLP problems such as NLI could benefit from leveraging knowledge accumulated by humans , particularly in a foreseeable future when machines are unable to learn it by themselves .",We show the proposed model improves the state - of - the - art NLI models to achieve better performances on the SNLI and MultiNLI datasets .,0.0991379310344827,0.8235294117647058,0.8235294117647058,model,ablation-analysis
natural_language_inference,15,introduction,introduction,24,15,15,We show the proposed model improves the state - of - the - art NLI models to achieve better performances on the SNLI and MultiNLI datasets .,"In this paper we enrich neural - network - based NLI models with external knowledge in coattention , local inference collection , and inference composition components .","The advantage of using external knowledge is more significant when the size of training data is restricted , suggesting that if more knowledge can be obtained , it may bring more benefit .",0.1034482758620689,0.8823529411764706,0.8823529411764706,model,approach
natural_language_inference,15,introduction,introduction,25,16,16,"The advantage of using external knowledge is more significant when the size of training data is restricted , suggesting that if more knowledge can be obtained , it may bring more benefit .",We show the proposed model improves the state - of - the - art NLI models to achieve better performances on the SNLI and MultiNLI datasets .,"In addition to attaining the state - of - theart performance , we are also interested in understanding how external knowledge contributes to the major components of typical neural - networkbased NLI models .",0.1077586206896551,0.9411764705882352,0.9411764705882352,model,baselines
natural_language_inference,15,experiment set up,training details,160,45,4,The main training details are as follows : the dimension of the hidden states of LSTMs and word embeddings are 300 .,All our models were strictly selected on the development set of the SNLI data and the in - domain development set of MultiNLI and were then tested on the corresponding test set .,"The word embeddings are initialized by 300D GloVe 840B , and out - of - vocabulary words among them are initialized randomly .",0.6896551724137931,0.4017857142857143,0.1666666666666666,hyperparameters,experimental-setup
natural_language_inference,15,experiment set up,training details,161,46,5,"The word embeddings are initialized by 300D GloVe 840B , and out - of - vocabulary words among them are initialized randomly .",The main training details are as follows : the dimension of the hidden states of LSTMs and word embeddings are 300 .,All word embeddings are updated during training .,0.6939655172413793,0.4107142857142857,0.2083333333333333,hyperparameters,experimental-setup
natural_language_inference,15,experiment set up,training details,163,48,7,"Adam ( Kingma and Ba , 2014 ) is used for optimization with an initial learning rate of 0.0004 .",All word embeddings are updated during training .,The mini - batch size is set to 32 .,0.7025862068965517,0.4285714285714285,0.2916666666666667,hyperparameters,approach
natural_language_inference,15,experiment set up,training details,164,49,8,The mini - batch size is set to 32 .,"Adam ( Kingma and Ba , 2014 ) is used for optimization with an initial learning rate of 0.0004 .",Note that the above hyperparameter settings are same as those used in the baseline ESIM model .,0.7068965517241379,0.4375,0.3333333333333333,hyperparameters,experimental-setup
natural_language_inference,15,experiment set up,training details,170,55,14,"The proposed model , namely Knowledge - based Inference Model ( KIM ) , which enriches ESIM with external knowledge , obtains an accuracy of 88.6 % , the best single - model performance reported on the SNLI dataset .","Among them , ESIM is one of the previous state - of - the - art systems with an 88.0 % test - set accuracy .",The difference between ESIM and KIM is statistically significant under the one - tailed paired t- test at the 99 % significance level .,0.7327586206896551,0.4910714285714286,0.5833333333333334,results,baselines
natural_language_inference,15,experiment set up,training details,171,56,15,The difference between ESIM and KIM is statistically significant under the one - tailed paired t- test at the 99 % significance level .,"The proposed model , namely Knowledge - based Inference Model ( KIM ) , which enriches ESIM with external knowledge , obtains an accuracy of 88.6 % , the best single - model performance reported on the SNLI dataset .",Note that the KIM model reported here uses five semantic relations described in Section 4 .,0.7370689655172413,0.5,0.625,results,baselines
natural_language_inference,15,experiment set up,training details,177,62,21,shows the performance of models on the MultiNLI dataset .,"We consider this is due to the fact that TransE embedding is not specifically sensitive to inference information ; e.g. , it does not model co-hyponyms features , and its potential benefit has already been covered by the semantic relation features used .","The baseline ESIM achieves 76.8 % and 75.8 % on in - domain and cross - domain test set , respectively .",0.7629310344827587,0.5535714285714286,0.875,results,approach
natural_language_inference,15,experiment set up,training details,178,63,22,"The baseline ESIM achieves 76.8 % and 75.8 % on in - domain and cross - domain test set , respectively .",shows the performance of models on the MultiNLI dataset .,"If we extend the ESIM with external knowledge , we achieve significant gains to 77.2 % and 76.4 % respectively .",0.7672413793103449,0.5625,0.9166666666666666,results,baselines
natural_language_inference,15,experiment set up,training details,179,64,23,"If we extend the ESIM with external knowledge , we achieve significant gains to 77.2 % and 76.4 % respectively .","The baseline ESIM achieves 76.8 % and 75.8 % on in - domain and cross - domain test set , respectively .","Again , the gains are consistent on SNLI and MultiNLI , and we expect they would be orthogonal to other factors when external knowledge is added into other stateof - the - art models .",0.7715517241379308,0.5714285714285714,0.9583333333333334,results,approach
natural_language_inference,16,title,title,2,2,2,Text Understanding with the Attention Sum Reader Network, , ,0.0083333333333333,1.0,1.0,research-problem,approach
natural_language_inference,16,abstract,abstract,5,3,3,"Thanks to the size of these datasets , the associated text comprehension task is well suited for deep - learning techniques that currently seem to outperform all alternative approaches .",Several large cloze - style context - questionanswer datasets have been introduced recently : the CNN and Daily Mail news data and the Children 's Book Test .,"We present a new , simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models .",0.0208333333333333,0.5,0.5,research-problem,baselines
natural_language_inference,16,task and datasets,datasets,65,30,18,Our model called the Attention Sum Reader ( AS Reader ) 4 is tailor - made to leverage the fact that the answer is a word from the context document .,Our Model - Attention Sum Reader,This is a double - edged sword .,0.2708333333333333,0.3658536585365853,0.6666666666666666,model,ablation-analysis
natural_language_inference,16,task and datasets,datasets,70,35,23,We compute a vector embedding of the query .,1 .,2 .,0.2916666666666667,0.4268292682926829,0.8518518518518519,model,experimental-setup
natural_language_inference,16,task and datasets,datasets,72,37,25,We compute a vector embedding of each individual word in the context of the whole document ( contextual embedding ) .,2 .,3 .,0.3,0.451219512195122,0.925925925925926,model,approach
natural_language_inference,16,task and datasets,datasets,74,39,27,"Using a dot product between the question embedding and the contextual embedding of each occurrence of a candidate answer in the document , we select the most likely answer .",3 ., ,0.3083333333333333,0.475609756097561,1.0,model,approach
natural_language_inference,16,evaluation,training details,172,5,2,To train the model we used stochastic gradient descent with the ADAM update rule and learning rate of 0.001 or 0.0005 ., ,During training we minimized the following negative log-likelihood with respect to ?:,0.7166666666666667,0.0925925925925926,0.1,hyperparameters,approach
natural_language_inference,16,evaluation,training details,178,11,8,Weights in the GRU networks were initialized by random orthogonal matrices and biases were initialized to zero .,"2 . The initial weights in the word embedding matrix were drawn randomly uniformly from the interval [ ? 0.1 , 0.1 ] .",We also used a gradient clipping threshold of 10 and batches of size 32 .,0.7416666666666667,0.2037037037037037,0.4,hyperparameters,experimental-setup
natural_language_inference,16,evaluation,training details,179,12,9,We also used a gradient clipping threshold of 10 and batches of size 32 .,Weights in the GRU networks were initialized by random orthogonal matrices and biases were initialized to zero .,During training we randomly shuffled all examples in each epoch .,0.7458333333333333,0.2222222222222222,0.45,hyperparameters,approach
natural_language_inference,16,evaluation,results,212,45,6,mance of our single model is a little bit worse than performance of simultaneously published models .,Ensembles of our models set new state - of - the - art results on all evaluated datasets .,Compared to our work these models were trained with Dropout regularization which might improve single model performance .,0.8833333333333333,0.8333333333333334,0.4,results,model
natural_language_inference,16,evaluation,results,214,47,8,"However , ensemble of our models outperforms these models even though they use pre-trained word embeddings .",Compared to our work these models were trained with Dropout regularization which might improve single model performance .,On the CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5 % .,0.8916666666666667,0.8703703703703703,0.5333333333333333,results,baselines
natural_language_inference,16,evaluation,results,215,48,9,On the CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5 % .,"However , ensemble of our models outperforms these models even though they use pre-trained word embeddings .",The average performance of the top 20 % models according to validation accuracy is 69.9 % which is even 0.5 % better than the single best - validation model .,0.8958333333333334,0.8888888888888888,0.6,results,baselines
natural_language_inference,16,evaluation,results,216,49,10,The average performance of the top 20 % models according to validation accuracy is 69.9 % which is even 0.5 % better than the single best - validation model .,On the CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5 % .,This shows that there were many models that performed better on test set than the best - validation model .,0.9,0.9074074074074074,0.6666666666666666,results,baselines
natural_language_inference,16,evaluation,results,218,51,12,Fusing multiple models then gives a significant further increase in accuracy on both CNN and Daily Mail datasets ..,This shows that there were many models that performed better on test set than the best - validation model .,CBT .,0.9083333333333332,0.9444444444444444,0.8,results,approach
natural_language_inference,16,evaluation,results,220,53,14,"In named entity prediction our best single model with accuracy of 68.6 % performs 2 % absolute better than the MemNN with self supervision , the averaging ensemble performs 4 % absolute better than the best previous result .",CBT .,In common noun prediction our single models is 0.4 % absolute better than Mem NN however the ensemble improves the performance to 69 % which is 6 % absolute better than MemNN .,0.9166666666666666,0.9814814814814816,0.9333333333333332,results,baselines
natural_language_inference,16,evaluation,results,221,54,15,In common noun prediction our single models is 0.4 % absolute better than Mem NN however the ensemble improves the performance to 69 % which is 6 % absolute better than MemNN .,"In named entity prediction our best single model with accuracy of 68.6 % performs 2 % absolute better than the MemNN with self supervision , the averaging ensemble performs 4 % absolute better than the best previous result .", ,0.9208333333333332,1.0,1.0,results,baselines
natural_language_inference,17,title,title,2,2,2,GLUE : A MULTI - TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTAND - ING, , ,0.0049504950495049,1.0,1.0,research-problem,approach
natural_language_inference,17,abstract,abstract,4,2,2,"For natural language understanding ( NLU ) technology to be maximally useful , it must be able to process language in away that is not exclusive to a single task , genre , or dataset .", ,"In pursuit of this objective , we introduce the General Language Understanding Evaluation ( GLUE ) benchmark , a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks .",0.0099009900990099,0.25,0.25,research-problem,ablation-analysis
natural_language_inference,17,abstract,abstract,9,7,7,"However , the low absolute performance of our best model indicates the need for improved general NLU systems .",We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task .,Published as a conference paper at ICLR 2019,0.0222772277227722,0.875,0.875,research-problem,baselines
natural_language_inference,17,introduction,introduction,15,5,5,"To facilitate research in this direction , we present the General Language Understanding Evaluation ( GLUE ) benchmark : a collection of NLU tasks including question answering , sentiment analysis , and textual entailment , and an associated online platform for model evaluation , comparison , and analysis .","If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs , then it is critical to develop a more unified model that can learn to execute a range of different linguistic tasks in different domains .",GLUE does not place any constraints on model architecture beyond the ability to process single - sentence and sentence - pair inputs and to make corresponding predictions .,0.0371287128712871,0.1851851851851852,0.1851851851851852,dataset,ablation-analysis
natural_language_inference,17,introduction,introduction,16,6,6,GLUE does not place any constraints on model architecture beyond the ability to process single - sentence and sentence - pair inputs and to make corresponding predictions .,"To facilitate research in this direction , we present the General Language Understanding Evaluation ( GLUE ) benchmark : a collection of NLU tasks including question answering , sentiment analysis , and textual entailment , and an associated online platform for model evaluation , comparison , and analysis .","For some GLUE tasks , training data is plentiful , but for others it is limited or fails to match the genre of the test set .",0.0396039603960396,0.2222222222222222,0.2222222222222222,dataset,model
natural_language_inference,17,introduction,introduction,17,7,7,"For some GLUE tasks , training data is plentiful , but for others it is limited or fails to match the genre of the test set .",GLUE does not place any constraints on model architecture beyond the ability to process single - sentence and sentence - pair inputs and to make corresponding predictions .,GLUE therefore favors models that can learn to represent linguistic knowledge in away that facilitates sample - efficient learning and effective knowledge - transfer across tasks .,0.042079207920792,0.2592592592592592,0.2592592592592592,dataset,ablation-analysis
natural_language_inference,17,introduction,introduction,20,10,10,"Four of the datasets feature privately - held test data , which will be used to ensure that the benchmark is used fairly .",None of the datasets in GLUE were created from scratch for the benchmark ; we rely on preexisting datasets because they have been implicitly agreed upon by the NLP community as challenging and interesting .,1,0.0495049504950495,0.3703703703703704,0.3703703703703704,dataset,approach
natural_language_inference,17,baselines,baselines,157,7,7,Original code for the baselines is available at https://github.com/nyu-mll/GLUE-baselines and a newer version is available at https://github.com/jsalt18-sentence-repl/jiant.,We implement our models in the AllenNLP library .,Architecture,0.3886138613861386,0.25,0.25,code,baselines
natural_language_inference,17,baselines,baselines,159,9,9,"Our simplest baseline architecture is based on sentence - to - vector encoders , and sets aside GLUE 's ability to evaluate models with more complex structures .",Architecture,"Taking inspiration from Conneau et al. , the model uses a two - layer , 1500D ( per direction ) BiLSTM with max pooling and 300D Glo Ve word embeddings ( 840B Common Crawl version ; .",0.3935643564356436,0.3214285714285714,0.3214285714285714,baselines,baselines
natural_language_inference,17,benchmark results,benchmark results,183,5,5,We find that multi-task training yields better overall scores over single - task training amongst models using attention or ELMo .,We present performance on the main benchmark tasks in .,"Attention generally has negligible or negative aggregate effect in single task training , but helps in multi-task training .",0.452970297029703,0.3125,0.3125,results,approach
natural_language_inference,17,benchmark results,benchmark results,185,7,7,"We see a consistent improvement in using ELMo embeddings in place of GloVe or CoVe embeddings , particularly for single - sentence tasks .","Attention generally has negligible or negative aggregate effect in single task training , but helps in multi-task training .",Using CoVe has mixed effects over using only Glo Ve .,0.457920792079208,0.4375,0.4375,results,baselines
natural_language_inference,17,benchmark results,benchmark results,187,9,9,"Among the pre-trained sentence representation models , we observe fairly consistent gains moving from CBoW to Skip - Thought to Infersent and GenSen .",Using CoVe has mixed effects over using only Glo Ve .,"Relative to the models trained directly on the GLUE tasks , InferSent is competitive and GenSen outperforms all but the two best .",0.4628712871287129,0.5625,0.5625,results,baselines
natural_language_inference,17,benchmark results,benchmark results,188,10,10,"Relative to the models trained directly on the GLUE tasks , InferSent is competitive and GenSen outperforms all but the two best .","Among the pre-trained sentence representation models , we observe fairly consistent gains moving from CBoW to Skip - Thought to Infersent and GenSen .","Looking at results per task , we find that the sentence representation models substantially underperform on CoLA compared to the models directly trained on the task .",0.4653465346534654,0.625,0.625,results,baselines
natural_language_inference,17,benchmark results,benchmark results,189,11,11,"Looking at results per task , we find that the sentence representation models substantially underperform on CoLA compared to the models directly trained on the task .","Relative to the models trained directly on the GLUE tasks , InferSent is competitive and GenSen outperforms all but the two best .","On the other hand , for STS - B , models trained directly on the task lag significantly behind the performance of the best sentence representation model .",0.4678217821782178,0.6875,0.6875,results,baselines
natural_language_inference,17,benchmark results,benchmark results,190,12,12,"On the other hand , for STS - B , models trained directly on the task lag significantly behind the performance of the best sentence representation model .","Looking at results per task , we find that the sentence representation models substantially underperform on CoLA compared to the models directly trained on the task .","Finally , there are tasks for which no model does particularly well .",0.4702970297029703,0.75,0.75,results,baselines
natural_language_inference,17,benchmark results,benchmark results,192,14,14,"On WNLI , no model exceeds most - frequent - class guessing ( 65.1 % ) and we substitute the model predictions for the most -frequent baseline .","Finally , there are tasks for which no model does particularly well .","On RTE and in aggregate , even our best baselines leave room for improvement .",0.4752475247524752,0.875,0.875,results,baselines
natural_language_inference,17,benchmark results,benchmark results,193,15,15,"On RTE and in aggregate , even our best baselines leave room for improvement .","On WNLI , no model exceeds most - frequent - class guessing ( 65.1 % ) and we substitute the model predictions for the most -frequent baseline .",These early results indicate that solving GLUE is beyond the capabilities of current models and methods .,0.4777227722772277,0.9375,0.9375,results,approach
natural_language_inference,18,title,title,2,2,2,Parameter Re-Initialization through Cyclical Batch Size Schedules, , ,0.0099502487562189,1.0,1.0,research-problem,baselines
natural_language_inference,18,abstract,abstract,4,2,2,Optimal parameter initialization remains a crucial problem for neural network training ., ,A poor weight initialization may take longer to train and / or converge to sub-optimal solutions .,0.0199004975124378,0.2857142857142857,0.2857142857142857,research-problem,baselines
natural_language_inference,18,introduction,introduction,19,10,10,Our work explores the idea of adapting the weight initialization to the optimization dynamics of the specific learning task at hand .,These are mostly agnostic to the model architecture and the specific learning task .,"From the Bayesian perspective , improved weight initialization can be viewed as starting with a better prior , which leads to a more accurate posterior and thus better generalization ability .",0.0945273631840796,0.2564102564102564,0.2564102564102564,model,experimental-setup
natural_language_inference,18,introduction,introduction,20,11,11,"From the Bayesian perspective , improved weight initialization can be viewed as starting with a better prior , which leads to a more accurate posterior and thus better generalization ability .",Our work explores the idea of adapting the weight initialization to the optimization dynamics of the specific learning task at hand .,This problem has been explored extensively in Bayesian optimization .,0.099502487562189,0.282051282051282,0.282051282051282,model,ablation-analysis
natural_language_inference,18,introduction,introduction,22,13,13,"For example , in the seminal works , an adaptive prior is implemented via Markov Chain Monte Carlo ( MCMC ) methods .",This problem has been explored extensively in Bayesian optimization .,"Motivated by these ideas , we incorporate an "" adaptive initialization "" for neural network training ( see section 2 for details ) , where we use cyclical batch size schedules to control the noise ( or temperature ) of SGD .",0.1094527363184079,0.3333333333333333,0.3333333333333333,model,model
natural_language_inference,18,introduction,introduction,23,14,14,"Motivated by these ideas , we incorporate an "" adaptive initialization "" for neural network training ( see section 2 for details ) , where we use cyclical batch size schedules to control the noise ( or temperature ) of SGD .","For example , in the seminal works , an adaptive prior is implemented via Markov Chain Monte Carlo ( MCMC ) methods .","As argued in , both learning rate and batch size can be used to control the noise of SGD but the latter has an advantage in that it allows more parallelization opportunity .",0.1144278606965174,0.358974358974359,0.358974358974359,model,ablation-analysis
natural_language_inference,18,introduction,introduction,26,17,17,"Here , we build upon this work by studying different cyclical annealing strategies for a wide range of problems .",The idea of using batch size to control the noise in a simple cyclical schedule was recently proposed in .,"Additionally , we discuss how this can be combined with anew adversarial regularization scheme recently proposed in , as well as prior work in order to obtain ensembles of models at no additional cost .",0.1293532338308457,0.4358974358974359,0.4358974358974359,model,approach
natural_language_inference,18,results,language results,82,7,1,Language Results, , ,0.4079601990049751,0.1458333333333333,0.05,results,baselines
natural_language_inference,18,results,language results,83,8,2,Language modeling is a challenging problem due to the complex and long - range interactions between distant words ., ,"One hope is that large / deep models might be able to capture these complex interactions , but large models easily overfit on these tasks and exhibit large gaps between training set and testing set performance .",0.4129353233830846,0.1666666666666666,0.1,results,model
natural_language_inference,18,results,language results,85,10,4,"CBS schedules effectively help us avoid overfitting , and in addition snapshot ensembling enables even greater performance .","One hope is that large / deep models might be able to capture these complex interactions , but large models easily overfit on these tasks and exhibit large gaps between training set and testing set performance .",We evaluate a large variety of CBS schedules to positive results as shown in .,0.4228855721393035,0.2083333333333333,0.2,results,baselines
natural_language_inference,18,results,language results,88,13,7,"As we can see , the best performing CBS schedules result in significant improvements in perplexity ( up to 7.91 ) over the baseline schedules and also offer reductions in the number of SGD training iterations ( up to 33 % ) .","Results are measured in perplexity , a standard figure of merit for evaluating the quality of language models by measuring its prediction of the empirical distribution of words ( lower perplexity value is better ) .","For example , CBS schedules achieve improvement of 7.91 perplexity improvement on WikiText 2 via CBS - 1 - T and reduce the SGD iterations from 164 k to 111 k via the CBS - 1 - A schedule .",0.4378109452736319,0.2708333333333333,0.35,results,baselines
natural_language_inference,18,results,language results,90,15,9,Notice that almost all CBS schedules outperform the baseline schedule .,"For example , CBS schedules achieve improvement of 7.91 perplexity improvement on WikiText 2 via CBS - 1 - T and reduce the SGD iterations from 164 k to 111 k via the CBS - 1 - A schedule .",shows the training and testing perplexity of the L2 model on PTB and WikiTest 2 as trained via the baseline schedule along with our best CBS schedule ( from ) .,0.4477611940298508,0.3125,0.45,results,model
natural_language_inference,18,results,language results,97,22,16,"In our experiments , CBS schedules do not yield large performance improvements on models like E1 which exhibit smaller disparities between training and testing performance .","As seen in To further explore the properties of cyclical batch size schedules , we also evaluate these schedules on natural language inference tasks , as shown in .",This is inline with our limitation in that CBS is more effective for models which tend to overfit .,0.482587064676617,0.4583333333333333,0.8,results,model
natural_language_inference,18,results,image classification results,102,27,1,Image Classification Results, , ,0.5074626865671642,0.5625,0.0909090909090909,results,baselines
natural_language_inference,18,results,image classification results,103,28,2,"As seen in , the training curves of CBS schedules also exhibit the aforementioned cyclical spikes both in training loss and testing accuracy .", ,"Similarly in the previously discussed language experiments , these spikes correspond to cycles in the CBS schedules and can bethought of as re-initializations of the neural network weights .",0.5124378109452736,0.5833333333333334,0.1818181818181818,results,baselines
natural_language_inference,18,results,image classification results,105,30,4,We observe that CBS achieves similar performance to the baseline .,"Similarly in the previously discussed language experiments , these spikes correspond to cycles in the CBS schedules and can bethought of as re-initializations of the neural network weights .","We offer further support for the hypothesis that CBS schedules are more effective for overfitting neural networks with experiments on model C4 , which achieves 94.35 % training accuracy and 55 . 55 % testing accuracy on Cifar - 10 .",0.5223880597014925,0.625,0.3636363636363637,results,baselines
natural_language_inference,18,results,image classification results,107,32,6,"With CBS - 15 , we see 90.71 % training accuracy and 56. 44 % testing accuracy , which is a larger improvement than that offered by CBS on convolutional models on Cifar - 10 .","We offer further support for the hypothesis that CBS schedules are more effective for overfitting neural networks with experiments on model C4 , which achieves 94.35 % training accuracy and 55 . 55 % testing accuracy on Cifar - 10 .",We also explore combining CBS with the recent adversarial regularization proposed by .,0.5323383084577115,0.6666666666666666,0.5454545454545454,results,baselines
natural_language_inference,18,results,image classification results,109,34,8,Combining CBS - 15 on C2 with this strategy improves accuracy to 94.82 % .,We also explore combining CBS with the recent adversarial regularization proposed by .,This outperforms other schedules shown in .,0.5422885572139303,0.7083333333333334,0.7272727272727273,results,baselines
natural_language_inference,18,results,image classification results,111,36,10,Applying snapshot ensembling on C3 trained with CBS - 15 - 2 leads to improved accuracy of 93. 56 % as compared to 92.58 % .,This outperforms other schedules shown in .,"After ensembling ResNet50 on Imagenet with snapshots from the last two cycles , the performance increases to 76.401 % from 75.336 % .",0.5522388059701493,0.75,0.9090909090909092,results,baselines
natural_language_inference,18,results,image classification results,112,37,11,"After ensembling ResNet50 on Imagenet with snapshots from the last two cycles , the performance increases to 76.401 % from 75.336 % .",Applying snapshot ensembling on C3 trained with CBS - 15 - 2 leads to improved accuracy of 93. 56 % as compared to 92.58 % ., ,0.5572139303482587,0.7708333333333334,1.0,results,baselines
natural_language_inference,19,title,title,2,2,2,Natural Language Inference by Tree - Based Convolution and Heuristic Matching, , ,0.0125,1.0,1.0,research-problem,approach
natural_language_inference,19,abstract,abstract,4,2,2,"In this paper , we propose the TBCNNpair model to recognize entailment and contradiction between two sentences .", ,"In our model , a tree - based convolutional neural network ( TBCNN ) captures sentencelevel semantics ; then heuristic matching layers like concatenation , element - wise product / difference combine the information in individual sentences .",0.025,0.5,0.5,research-problem,baselines
natural_language_inference,19,introduction,introduction,8,2,2,Recognizing entailment and contradiction between two sentences ( called a premise and a hypothesis ) is known as natural language inference ( NLI ) in ., ,"Provided with a premise sentence , the task is to judge whether the hypothesis can be inferred ( entailment ) , or the hypothesis can not be true .",0.05,0.0571428571428571,0.0571428571428571,research-problem,experimental-setup
natural_language_inference,19,introduction,introduction,10,4,4,"Several examples are illustrated in NLI is in the core of natural language understanding and has wide applications in NLP , e.g. , question answering and automatic summarization .","Provided with a premise sentence , the task is to judge whether the hypothesis can be inferred ( entailment ) , or the hypothesis can not be true .","Moreover , NLI is also related to other tasks of sentence pair modeling , including paraphrase detection , relation recognition of discourse units , etc .",0.0625,0.1142857142857142,0.1142857142857142,research-problem,experimental-setup
natural_language_inference,19,introduction,introduction,33,27,27,"In this paper , we propose the TBCNN - pair neural model to recognize entailment and contradiction between two sentences .","However , it is less clear whether TBCNN can be harnessed to model sentence pairs for implicit logical inference , as is in the NLI task .","We lever- age our newly proposed TBCNN model to capture structural information in sentences , which is important to NLI .",0.20625,0.7714285714285715,0.7714285714285715,model,experimental-setup
natural_language_inference,19,introduction,introduction,34,28,28,"We lever- age our newly proposed TBCNN model to capture structural information in sentences , which is important to NLI .","In this paper , we propose the TBCNN - pair neural model to recognize entailment and contradiction between two sentences .","For example , the phrase "" riding bicycles on the streets "" in can be well recognized by TBCNN via the dependency relations dobj ( riding , bicycles ) and prep on ( riding , street ) .",0.2125,0.8,0.8,model,ablation-analysis
natural_language_inference,19,introduction,introduction,36,30,30,"As we can see , TBCNN is more robust than sequential convolution in terms of word order distortion , which maybe introduced by determinators , modifiers , etc .","For example , the phrase "" riding bicycles on the streets "" in can be well recognized by TBCNN via the dependency relations dobj ( riding , bicycles ) and prep on ( riding , street ) .","A pooling layer then aggregates information along the tree , serving as away of semantic compositonality .",0.225,0.8571428571428571,0.8571428571428571,model,model
natural_language_inference,19,introduction,introduction,37,31,31,"A pooling layer then aggregates information along the tree , serving as away of semantic compositonality .","As we can see , TBCNN is more robust than sequential convolution in terms of word order distortion , which maybe introduced by determinators , modifiers , etc .","Finally , two sentences ' information is combined by several heuristic matching layers , including concatenation , element - wise product and difference ; they are effective in capturing relationships between two sentences , but remain low complexity .",0.23125,0.8857142857142857,0.8857142857142857,model,model
natural_language_inference,19,introduction,introduction,38,32,32,"Finally , two sentences ' information is combined by several heuristic matching layers , including concatenation , element - wise product and difference ; they are effective in capturing relationships between two sentences , but remain low complexity .","A pooling layer then aggregates information along the tree , serving as away of semantic compositonality .","To sum up , the main contributions of this paper are two - fold :",0.2375,0.9142857142857144,0.9142857142857144,model,model
natural_language_inference,19,evaluation,hyperparameter settings,128,10,2,"All our neural layers , including embeddings , were set to 300 dimensions .", ,"The model is mostly robust when the dimension is large , e.g. , several hundred .",0.8,0.2631578947368421,0.2222222222222222,hyperparameters,baselines
natural_language_inference,19,evaluation,hyperparameter settings,130,12,4,Word embeddings were pretrained ourselves by word2vec on the English Wikipedia corpus and fined tuned during training as apart of model parameters .,"The model is mostly robust when the dimension is large , e.g. , several hundred .",We applied 2 penalty of 310 ? 4 ; dropout was chosen by validation with a granularity of 0.1 .,0.8125,0.3157894736842105,0.4444444444444444,hyperparameters,baselines
natural_language_inference,19,evaluation,hyperparameter settings,131,13,5,We applied 2 penalty of 310 ? 4 ; dropout was chosen by validation with a granularity of 0.1 .,Word embeddings were pretrained ourselves by word2vec on the English Wikipedia corpus and fined tuned during training as apart of model parameters .,We see that a large dropout rate ( ?,0.81875,0.3421052631578947,0.5555555555555556,hyperparameters,approach
natural_language_inference,19,evaluation,hyperparameter settings,134,16,8,"Initial learning rate was set to 1 , and a power decay was applied .",0.3 ) hurts the performance ( and also makes training slow ) for such a large dataset as opposed to small datasets in other tasks .,We used stochastic gradient descent with a batch size of 50 .,0.8375,0.4210526315789473,0.8888888888888888,hyperparameters,approach
natural_language_inference,19,evaluation,hyperparameter settings,135,17,9,We used stochastic gradient descent with a batch size of 50 .,"Initial learning rate was set to 1 , and a power decay was applied .", ,0.84375,0.4473684210526316,1.0,hyperparameters,approach
natural_language_inference,19,evaluation,performance,138,20,3,"As seen , the TBCNN sentence pair model , followed by simple concatenation alone , outperforms existing sentence encoding - based approaches ( without pretraining ) , including a feature - rich method using 6 groups of humanengineered features , long short term memory .",Table 3 compares our model with previous results .,""" cat "" refers to concatenation ; "" - "" and "" "" denote element - wise difference and product , resp .",0.8625,0.5263157894736842,0.2,results,model
natural_language_inference,19,evaluation,performance,140,22,5,Model Variant,""" cat "" refers to concatenation ; "" - "" and "" "" denote element - wise difference and product , resp .","Valid Acc. Test Acc. ( LSTM ) - based RNNs , and traditional CNNs .",0.875,0.5789473684210527,0.3333333333333333,results,baselines
natural_language_inference,19,evaluation,performance,144,26,9,We first analyze each heuristic separately : using element - wise product alone is significantly worse than concatenation or element - wise difference ; the latter two are comparable to each other .,compares different heuristics of matching .,"Combining different matching heuristics improves the result : the TBCNN - pair model with concatenation , element - wise product and difference yields the highest performance of 82.1 % .",0.9,0.6842105263157895,0.6,results,baselines
natural_language_inference,19,evaluation,performance,145,27,10,"Combining different matching heuristics improves the result : the TBCNN - pair model with concatenation , element - wise product and difference yields the highest performance of 82.1 % .",We first analyze each heuristic separately : using element - wise product alone is significantly worse than concatenation or element - wise difference ; the latter two are comparable to each other .,"As analyzed in Section 3.2 , the element - wise difference matching layer does not add to model complexity and can be absorbed as a special case into simple concatenation .",0.90625,0.7105263157894737,0.6666666666666666,results,approach
natural_language_inference,19,evaluation,performance,148,30,13,Further applying element - wise product improves the accuracy by another 0.5 % .,"However , explicitly using such heuristic yields an accuracy boost of 1 - 2 % .","The full TBCNN - pair model outperforms all existing sentence encoding - based approaches , in - cluding a 1024d gated recurrent unit ( GRU ) - based RNN with "" skip - thought "" pretraining .",0.925,0.7894736842105263,0.8666666666666667,results,approach
natural_language_inference,19,evaluation,performance,149,31,14,"The full TBCNN - pair model outperforms all existing sentence encoding - based approaches , in - cluding a 1024d gated recurrent unit ( GRU ) - based RNN with "" skip - thought "" pretraining .",Further applying element - wise product improves the accuracy by another 0.5 % .,"The results obtained by our model are also comparable to several attention - based LSTMs , which are more computationally intensive than ours in terms of complexity order .",0.93125,0.8157894736842105,0.9333333333333332,results,baselines
natural_language_inference,2,title,title,2,2,2,Stochastic Answer Networks for Natural Language Inference, , ,0.0145985401459854,1.0,1.0,research-problem,approach
natural_language_inference,2,motivation,motivation,9,2,2,"The natural language inference task , also known as recognizing textual entailment ( RTE ) , is to infer the relation between a pair of sentences ( e.g. , premise and hypothesis ) .", ,"This task is challenging , since it requires a model to fully understand the sentence meaning , ( i.e. , lexical and compositional semantics ) .",0.0656934306569343,0.032258064516129,0.032258064516129,research-problem,ablation-analysis
natural_language_inference,2,motivation,motivation,17,10,10,"Inspired by the recent success of multi-step inference on Machine Reading Comprehension ( MRC ) , we explore the multi-step inference strategies on NLI .","To best of our knowledge , all of works on NLI use a single step inference .","Rather than directly predicting the results given the inputs , the model maintains a state and iteratively refines its predictions .",0.1240875912408759,0.1612903225806451,0.1612903225806451,model,ablation-analysis
natural_language_inference,2,motivation,motivation,18,11,11,"Rather than directly predicting the results given the inputs , the model maintains a state and iteratively refines its predictions .","Inspired by the recent success of multi-step inference on Machine Reading Comprehension ( MRC ) , we explore the multi-step inference strategies on NLI .","We show that our model outperforms single - step inference and further achieves the state - of - the - art on SNLI , MultiNLI , SciTail , and Quora Question Pairs datasets .",0.1313868613138686,0.1774193548387097,0.1774193548387097,model,model
natural_language_inference,2,experiments,implementation details,80,11,2,The spaCy tool 2 is used to tokenize all the dataset and PyTorch is used to implement our models ., ,We fix word embedding with 300 - dimensional GloVe word vectors .,0.583941605839416,0.1746031746031746,0.1538461538461538,experimental-setup,approach
natural_language_inference,2,experiments,implementation details,81,12,3,We fix word embedding with 300 - dimensional GloVe word vectors .,The spaCy tool 2 is used to tokenize all the dataset and PyTorch is used to implement our models .,"For the character encoding , we use a concatenation of the multi-filter Convolutional Neural Nets with windows 1 , 3 , 5 and the hidden size 50 , 100 , 150 .",0.5912408759124088,0.1904761904761904,0.2307692307692308,experimental-setup,baselines
natural_language_inference,2,experiments,implementation details,82,13,4,"For the character encoding , we use a concatenation of the multi-filter Convolutional Neural Nets with windows 1 , 3 , 5 and the hidden size 50 , 100 , 150 .",We fix word embedding with 300 - dimensional GloVe word vectors .,3,0.5985401459854015,0.2063492063492063,0.3076923076923077,experimental-setup,experimental-setup
natural_language_inference,2,experiments,implementation details,84,15,6,So lexicon embeddings are d =600 - dimensions .,3,The embedding for the out - of - vocabulary is zeroed .,0.6131386861313869,0.2380952380952381,0.4615384615384616,experimental-setup,approach
natural_language_inference,2,experiments,implementation details,85,16,7,The embedding for the out - of - vocabulary is zeroed .,So lexicon embeddings are d =600 - dimensions .,"The hidden size of LSTM in the contextual encoding layer , memory generation layer is set to 128 , thus the input size of output layer is 1024 ( 128 * 2 * 4 ) as Eq 2 .",0.6204379562043796,0.2539682539682539,0.5384615384615384,experimental-setup,approach
natural_language_inference,2,experiments,implementation details,86,17,8,"The hidden size of LSTM in the contextual encoding layer , memory generation layer is set to 128 , thus the input size of output layer is 1024 ( 128 * 2 * 4 ) as Eq 2 .",The embedding for the out - of - vocabulary is zeroed .,The projection size in the attention layer is set to 256 .,0.6277372262773723,0.2698412698412698,0.6153846153846154,experimental-setup,experimental-setup
natural_language_inference,2,experiments,implementation details,87,18,9,The projection size in the attention layer is set to 256 .,"The hidden size of LSTM in the contextual encoding layer , memory generation layer is set to 128 , thus the input size of output layer is 1024 ( 128 * 2 * 4 ) as Eq 2 .","To speedup training , we use weight normalization .",0.635036496350365,0.2857142857142857,0.6923076923076923,experimental-setup,experimental-setup
natural_language_inference,2,experiments,implementation details,88,19,10,"To speedup training , we use weight normalization .",The projection size in the attention layer is set to 256 .,"The dropout rate is 0.2 , and the dropout mask is fixed through time steps in LSTM .",0.6423357664233577,0.3015873015873016,0.7692307692307693,experimental-setup,approach
natural_language_inference,2,experiments,implementation details,89,20,11,"The dropout rate is 0.2 , and the dropout mask is fixed through time steps in LSTM .","To speedup training , we use weight normalization .",The mini - batch size is set to 32 .,0.6496350364963503,0.3174603174603174,0.8461538461538461,experimental-setup,baselines
natural_language_inference,2,experiments,implementation details,90,21,12,The mini - batch size is set to 32 .,"The dropout rate is 0.2 , and the dropout mask is fixed through time steps in LSTM .",Our optimizer is Adamax and its learning rate is initialized as 0.002 and decreased by 0.5 after each 10 epochs .,0.656934306569343,0.3333333333333333,0.9230769230769232,experimental-setup,experiments
natural_language_inference,2,experiments,implementation details,91,22,13,Our optimizer is Adamax and its learning rate is initialized as 0.002 and decreased by 0.5 after each 10 epochs .,The mini - batch size is set to 32 ., ,0.6642335766423357,0.3492063492063492,1.0,experimental-setup,baselines
natural_language_inference,2,experiments,results,101,32,10,shows that our multi-step model consistently outperforms the single - step model on the dev set of all four datasets in terms of accuracy .,We use 5 - steps with the prediction dropout rate 0.2 on the all experiments .,"For example , on SciTail dataset , SAN outperforms the single - step model by .",0.7372262773722628,0.5079365079365079,0.2439024390243902,results,approach
natural_language_inference,2,experiments,results,102,33,11,"For example , on SciTail dataset , SAN outperforms the single - step model by .",shows that our multi-step model consistently outperforms the single - step model on the dev set of all four datasets in terms of accuracy .,We compare our results with the state - of - the - art in and BERT use a large amount of external knowledge or a large scale pretrained contextual embeddings .,0.7445255474452555,0.5238095238095238,0.2682926829268293,results,baselines
natural_language_inference,2,experiments,results,105,36,14,"On SciTail dataset , SAN even outperforms GPT .","However , SAN is still competitive these models .","Due to the space limitation , we only list two top models .",0.7664233576642335,0.5714285714285714,0.3414634146341464,results,baselines
natural_language_inference,2,experiments,results,108,39,17,"Comparing with Single - step baseline , the proposed model obtains + 2.8 improvement on the Sc - iTail test set ( 94.0 vs 91.2 ) and + 2.1 improvement on the SciTail dev set ( 96.1 vs 93.9 ) .",We further utilize BERT as a feature extractor 6 and use the SAN answer module on top of it .,This shows the generalization of the proposed model which can be easily adapted on other models,0.7883211678832117,0.6190476190476191,0.4146341463414634,results,baselines
natural_language_inference,2,experiments,results,128,59,37,"Our model outperforms the best system in RepEval 2017 inmost cases , except on "" Conditional "" and "" Tense Difference "" categories .","It contains 1,000 examples , each tagged by categories shown in .","We also find that SAN works extremely well on "" Active / Passive "" and "" Paraphrase "" categories .",0.9343065693430656,0.9365079365079364,0.902439024390244,results,approach
natural_language_inference,2,experiments,results,129,60,38,"We also find that SAN works extremely well on "" Active / Passive "" and "" Paraphrase "" categories .","Our model outperforms the best system in RepEval 2017 inmost cases , except on "" Conditional "" and "" Tense Difference "" categories .","Comparing with Chen 's model , the biggest improvement of SAN ( 50 % vs 77 % and 58 % vs 85 % on Matched and Mismatched settings respectively ) is on the "" Antonym "" category .",0.9416058394160584,0.9523809523809524,0.9268292682926828,results,approach
natural_language_inference,2,experiments,results,130,61,39,"Comparing with Chen 's model , the biggest improvement of SAN ( 50 % vs 77 % and 58 % vs 85 % on Matched and Mismatched settings respectively ) is on the "" Antonym "" category .","We also find that SAN works extremely well on "" Active / Passive "" and "" Paraphrase "" categories .","In particular , on the most challenging "" Long Sentence "" and "" Quantity / Time "" categories , SAN 's result is substantially better than previous systems .",0.9489051094890508,0.9682539682539684,0.951219512195122,results,baselines
natural_language_inference,2,experiments,results,131,62,40,"In particular , on the most challenging "" Long Sentence "" and "" Quantity / Time "" categories , SAN 's result is substantially better than previous systems .","Comparing with Chen 's model , the biggest improvement of SAN ( 50 % vs 77 % and 58 % vs 85 % on Matched and Mismatched settings respectively ) is on the "" Antonym "" category .",This demonstrates the robustness of multi-step inference .,0.9562043795620438,0.984126984126984,0.975609756097561,results,approach
natural_language_inference,20,title,title,2,2,2,Neural Tree Indexers for Text Understanding, , ,0.0065359477124183,1.0,1.0,research-problem,approach
natural_language_inference,20,introduction,introduction,19,9,9,"In this study , we introduce Neural Tree Indexers ( NTI ) , a class of tree structured models for NLP tasks .",However its dependence on a syntactic tree architecture limits practical NLP applications .,NTI takes a sequence of tokens and produces its representation by constructing a full n-ary tree in a bottom - up fashion .,0.0620915032679738,0.4090909090909091,0.4090909090909091,model,approach
natural_language_inference,20,introduction,introduction,20,10,10,NTI takes a sequence of tokens and produces its representation by constructing a full n-ary tree in a bottom - up fashion .,"In this study , we introduce Neural Tree Indexers ( NTI ) , a class of tree structured models for NLP tasks .",Each node in NTI is associated with one of the node transformation functions : leaf node mapping and non-leaf node composition functions .,0.065359477124183,0.4545454545454545,0.4545454545454545,model,approach
natural_language_inference,20,introduction,introduction,21,11,11,Each node in NTI is associated with one of the node transformation functions : leaf node mapping and non-leaf node composition functions .,NTI takes a sequence of tokens and produces its representation by constructing a full n-ary tree in a bottom - up fashion .,"Unlike previous recursive models , the tree structure for NTI is relaxed , i.e. , NTI does not require the input sequences to be parsed syntactically ; and therefore it is flexible and can be directly applied to a wide range of NLP tasks beyond sentence modeling .",0.0686274509803921,0.5,0.5,model,approach
natural_language_inference,20,introduction,introduction,23,13,13,"Furthermore , we propose different variants of node composition function and attention over tree for our NTI models .","Unlike previous recursive models , the tree structure for NTI is relaxed , i.e. , NTI does not require the input sequences to be parsed syntactically ; and therefore it is flexible and can be directly applied to a wide range of NLP tasks beyond sentence modeling .","When a sequential leaf node transformer such as LSTM is chosen , the NTI network forms a sequence - tree hybrid model taking advantage of both conditional and compositional powers of sequential and recursive models . :",0.0751633986928104,0.5909090909090909,0.5909090909090909,model,approach
natural_language_inference,20,introduction,introduction,24,14,14,"When a sequential leaf node transformer such as LSTM is chosen , the NTI network forms a sequence - tree hybrid model taking advantage of both conditional and compositional powers of sequential and recursive models . :","Furthermore , we propose different variants of node composition function and attention over tree for our NTI models .",A binary tree form of Neural Tree Indexers ( NTI ) in the context of question answering and natural language inference .,0.0784313725490196,0.6363636363636364,0.6363636363636364,model,model
natural_language_inference,20,methods,natural language inference,147,89,1,Natural Language Inference, , ,0.4803921568627451,0.4517766497461929,0.0175438596491228,experiments,approach
natural_language_inference,20,methods,natural language inference,193,135,47,Our best score on this task is 87.3 % accuracy obtained with the full tree matching NTI model .,The last set of methods designs inter-sentence relation with soft attention .,The previous best performing model on the task performs phrase matching by using the attention mechanism .,0.630718954248366,0.6852791878172588,0.8245614035087719,experiments,baselines
natural_language_inference,20,methods,natural language inference,195,137,49,Our results show that NTI - SLSTM improved the performance of the sequential LSTM encoder by approximately 2 % .,The previous best performing model on the task performs phrase matching by using the attention mechanism .,"Not surprisingly , using LSTM as leaf node function helps in learning better representations .",0.6372549019607843,0.6954314720812182,0.8596491228070176,experiments,baselines
natural_language_inference,20,methods,natural language inference,198,140,52,"The node - by - node attention models improve the performance , indicating that modeling inter-sentence interaction is an important element in NLI .",Our NTI - SLSTM - LSTM is a hybrid model which encodes a sequence sequentially through its leaf node function and then hierarchically composes the output representations .,Aggregating matching vector between trees or sequences with a separate LSTM model is effective .,0.6470588235294118,0.7106598984771574,0.912280701754386,experiments,baselines
natural_language_inference,20,methods,answer sentence selection,204,146,1,Answer Sentence Selection, , ,0.6666666666666666,0.7411167512690355,0.0476190476190476,experiments,approach
natural_language_inference,20,methods,answer sentence selection,221,163,18,"The Deep LSTM and LSTM attention models outperform the previous best result by a large margin , nearly 5 - 6 % .",The Bigram - CNN model is a simple convolutional neural net .,NASM improves the result further and sets a strong baseline by combining variational autoencoder with the soft attention .,0.7222222222222222,0.8274111675126904,0.8571428571428571,experiments,baselines
natural_language_inference,20,methods,answer sentence selection,222,164,19,NASM improves the result further and sets a strong baseline by combining variational autoencoder with the soft attention .,"The Deep LSTM and LSTM attention models outperform the previous best result by a large margin , nearly 5 - 6 % .","In NASM , they adopt a deep three - layer LSTM and introduced a latent stochastic attention mechanism over the answer sentence .",0.7254901960784313,0.8324873096446701,0.9047619047619048,experiments,baselines
natural_language_inference,20,methods,answer sentence selection,224,166,21,Our NTI model exceeds NASM by approximately 0.4 % on MAP for this task .,"In NASM , they adopt a deep three - layer LSTM and introduced a latent stochastic attention mechanism over the answer sentence .", ,0.7320261437908496,0.8426395939086294,1.0,experiments,baselines
natural_language_inference,20,methods,sentence classification,225,167,1,Sentence Classification, , ,0.7352941176470589,0.8477157360406091,0.032258064516129,experiments,approach
natural_language_inference,20,methods,sentence classification,234,176,10,Our NTI - SLSTM model performed slightly worse A dog mouth holds a retrieved ball .,NTI - SLSTM-LSTM ( as shown in ) set the state - of - the - art results on both subtasks .,A cat nurses puppies .,0.7647058823529411,0.8934010152284264,0.3225806451612903,experiments,baselines
natural_language_inference,20,methods,sentence classification,255,197,31,"After we transformed the input with the LSTM leaf node function , we achieved the best performance on this task .",The CT - LSTM model composes phrases according to the output of a sentence parser and uses anode composition function similar to S - LSTM ., ,0.8333333333333334,1.0,1.0,experiments,approach
natural_language_inference,21,title,title,2,2,2,Attention - over - Attention Neural Networks for Reading Comprehension, , ,0.0090497737556561,1.0,1.0,research-problem,experimental-setup
natural_language_inference,21,abstract,abstract,4,2,2,Cloze - style reading comprehension is a representative problem in mining relationship between document and query ., ,"In this paper , we present a simple but novel model called attention - over - attention reader for better solving cloze - style reading comprehension task .",0.0180995475113122,0.2857142857142857,0.2857142857142857,research-problem,baselines
natural_language_inference,21,introduction,introduction,11,2,2,"To read and comprehend the human languages are challenging tasks for the machines , which requires that the understanding of natural languages and the ability to do reasoning over various clues .", ,"Reading comprehension is a general problem in the real world , which aims to read and comprehend a given article or context , and answer the questions based on it .",0.0497737556561085,0.0869565217391304,0.0869565217391304,research-problem,ablation-analysis
natural_language_inference,21,introduction,introduction,20,11,11,"In this paper , we present a novel neural network architecture , called attention - over - attention model .","Following these datasets , avast variety of neural network approaches have been proposed , and most of them stem from the attention - based neural network , which has become a stereotype inmost of the NLP tasks and is well - known by its capability of learning the "" importance "" distribution over the inputs .","As we can understand the meaning literally , our model aims to place another attention mechanism over the existing document - level attention .",0.090497737556561,0.4782608695652174,0.4782608695652174,model,experimental-setup
natural_language_inference,21,introduction,introduction,21,12,12,"As we can understand the meaning literally , our model aims to place another attention mechanism over the existing document - level attention .","In this paper , we present a novel neural network architecture , called attention - over - attention model .","Unlike the previous works , that are using heuristic merging functions , or setting various pre-defined non-trainable terms , our model could automatically generate an "" attended attention "" over various document - level attentions , and make a mutual look not only from query - to - document but also document - to - query , which will benefit from the interactive information .",0.0950226244343891,0.5217391304347826,0.5217391304347826,model,approach
natural_language_inference,21,introduction,introduction,22,13,13,"Unlike the previous works , that are using heuristic merging functions , or setting various pre-defined non-trainable terms , our model could automatically generate an "" attended attention "" over various document - level attentions , and make a mutual look not only from query - to - document but also document - to - query , which will benefit from the interactive information .","As we can understand the meaning literally , our model aims to place another attention mechanism over the existing document - level attention .","To sum up , the main contributions of our work are listed as follows .",0.0995475113122172,0.5652173913043478,0.5652173913043478,model,approach
natural_language_inference,21,experiments,experimental setups,141,4,3,Embedding Layer :,The general settings of our neural network model are listed below in detail .,"The embedding weights are randomly initialized with the uniformed distribution in the interval [ ? 0.05 , 0.05 ].",0.6380090497737556,0.0930232558139534,0.1428571428571428,experimental-setup,approach
natural_language_inference,21,experiments,experimental setups,142,5,4,"The embedding weights are randomly initialized with the uniformed distribution in the interval [ ? 0.05 , 0.05 ].",Embedding Layer :,CNN,0.6425339366515838,0.1162790697674418,0.1904761904761904,experimental-setup,experiments
natural_language_inference,21,experiments,experimental setups,148,11,10,Hidden Layer : Internal weights of GRUs are initialized with random orthogonal matrices .,"Also , it should be noted that we do not exploit any pretrained embedding models .",Optimization :,0.669683257918552,0.2558139534883721,0.4761904761904762,experimental-setup,approach
natural_language_inference,21,experiments,experimental setups,150,13,12,"We adopted ADAM optimizer for weight updating , with an initial learning rate of 0.001 .",Optimization :,"As the GRU units still suffer from the gradient exploding issues , we set the gradient clipping threshold to 5 .",0.6787330316742082,0.3023255813953488,0.5714285714285714,experimental-setup,approach
natural_language_inference,21,experiments,experimental setups,151,14,13,"As the GRU units still suffer from the gradient exploding issues , we set the gradient clipping threshold to 5 .","We adopted ADAM optimizer for weight updating , with an initial learning rate of 0.001 .",We used batched training strategy of 32 samples .,0.6832579185520362,0.3255813953488372,0.6190476190476191,experimental-setup,baselines
natural_language_inference,21,experiments,experimental setups,152,15,14,We used batched training strategy of 32 samples .,"As the GRU units still suffer from the gradient exploding issues , we set the gradient clipping threshold to 5 .",Dimensions of embedding and hidden layer for each task are listed in .,0.6877828054298643,0.3488372093023256,0.6666666666666666,experimental-setup,approach
natural_language_inference,21,experiments,experimental setups,154,17,16,"In re-ranking step , we generate 5 - best list from the baseline neural network model , as we did not observe a significant variance when changing the N - best list size .",Dimensions of embedding and hidden layer for each task are listed in .,"All language model features are trained on the training proportion of each dataset , with 8 - gram wordbased setting and Kneser - Ney smoothing trained by SRILM toolkit .",0.6968325791855203,0.3953488372093023,0.7619047619047619,experimental-setup,approach
natural_language_inference,21,experiments,experimental setups,155,18,17,"All language model features are trained on the training proportion of each dataset , with 8 - gram wordbased setting and Kneser - Ney smoothing trained by SRILM toolkit .","In re-ranking step , we generate 5 - best list from the baseline neural network model , as we did not observe a significant variance when changing the N - best list size .","The results are reported with the best model , which is selected by the performance of validation set .",0.7013574660633484,0.4186046511627907,0.8095238095238095,experimental-setup,baselines
natural_language_inference,21,experiments,experimental setups,158,21,20,"Implementation is done with Theano ( Theano Development Team , 2016 ) and Keras , and all models are trained on Tesla K40 GPU . :","The ensemble model is made up of four best models , which are trained using different random seed .",Embedding and hidden layer dimensions for each task .,0.7149321266968326,0.4883720930232558,0.9523809523809524,experimental-setup,model
natural_language_inference,21,experiments,overall results,163,26,4,"As we can see that , our AoA Reader outperforms state - of - the - art systems by a large margin , where 2.3 % and 2.0 % absolute improvements over EpiReader in CBTest NE and CN test sets , which demonstrate the effectiveness of our model .","The statistics of these datasets are listed in , and the experimental results are given in .","Also by adding additional features in the re-ranking step , there is another significant boost 2.0 % to 3.7 % over Ao A Reader in CBTest NE / CN test sets .",0.7375565610859729,0.6046511627906976,0.4444444444444444,results,baselines
natural_language_inference,21,experiments,overall results,164,27,5,"Also by adding additional features in the re-ranking step , there is another significant boost 2.0 % to 3.7 % over Ao A Reader in CBTest NE / CN test sets .","As we can see that , our AoA Reader outperforms state - of - the - art systems by a large margin , where 2.3 % and 2.0 % absolute improvements over EpiReader in CBTest NE and CN test sets , which demonstrate the effectiveness of our model .","We have also found that our single model could stay on par with the previous best ensemble system , and even we have an absolute improvement of 0.9 % beyond the best ensemble model ( Iterative Attention ) in the CBTest NE validation set .",0.7420814479638009,0.627906976744186,0.5555555555555556,results,approach
natural_language_inference,21,experiments,overall results,165,28,6,"We have also found that our single model could stay on par with the previous best ensemble system , and even we have an absolute improvement of 0.9 % beyond the best ensemble model ( Iterative Attention ) in the CBTest NE validation set .","Also by adding additional features in the re-ranking step , there is another significant boost 2.0 % to 3.7 % over Ao A Reader in CBTest NE / CN test sets .","When it comes to ensemble model , our AoA Reader also shows significant improvements over previous best ensemble models by a large margin and setup a new state - of - the - art system .",0.746606334841629,0.6511627906976745,0.6666666666666666,results,baselines
natural_language_inference,21,experiments,overall results,166,29,7,"When it comes to ensemble model , our AoA Reader also shows significant improvements over previous best ensemble models by a large margin and setup a new state - of - the - art system .","We have also found that our single model could stay on par with the previous best ensemble system , and even we have an absolute improvement of 0.9 % beyond the best ensemble model ( Iterative Attention ) in the CBTest NE validation set .","To investigate the effectiveness of employing attention - over - attention mechanism , we also compared our model to CAS Reader , which used predefined merging heuristics , such as sum or avg etc .",0.751131221719457,0.6744186046511628,0.7777777777777778,results,approach
natural_language_inference,21,experiments,overall results,168,31,9,"Instead of using pre-defined merging heuristics , and letting the model explicitly learn the weights between individual attentions results in a significant boost in the performance , where 4.1 % and 3.7 % improvements can be made in CNN validation and test set against CAS Reader .","To investigate the effectiveness of employing attention - over - attention mechanism , we also compared our model to CAS Reader , which used predefined merging heuristics , such as sum or avg etc .", ,0.7601809954751131,0.7209302325581395,1.0,results,baselines
natural_language_inference,21,experiments,effectiveness of re ranking strategy,172,35,4,"From the results in , we found that the NE and CN category both benefit a lot from the re-ranking features , but the proportions are quite different .","To have a thorough investigation in the re-ranking step , we listed the detailed improvements while adding each feature mentioned in Section 4 .","Generally speaking , in NE category , the performance is mainly boosted by the LM local feature .",0.7782805429864253,0.813953488372093,0.3333333333333333,ablation-analysis,approach
natural_language_inference,21,experiments,effectiveness of re ranking strategy,173,36,5,"Generally speaking , in NE category , the performance is mainly boosted by the LM local feature .","From the results in , we found that the NE and CN category both benefit a lot from the re-ranking features , but the proportions are quite different .","However , on the contrary , the CN category benefits from LM global and LM wc rather than the LM local .",0.7828054298642534,0.8372093023255814,0.4166666666666667,ablation-analysis,baselines
natural_language_inference,22,title,title,2,2,2,Multi- task Sentence Encoding Model for Semantic Retrieval in Question Answering Systems, , ,0.009090909090909,1.0,1.0,research-problem,approach
natural_language_inference,22,abstract,abstract,4,2,2,Question Answering ( QA ) systems are used to provide proper responses to users ' questions automatically ., ,Sentence matching is an essential task in the QA systems and is usually reformulated as a Paraphrase Identification ( PI ) problem .,0.0181818181818181,0.064516129032258,0.064516129032258,research-problem,experimental-setup
natural_language_inference,22,abstract,abstract,5,3,3,Sentence matching is an essential task in the QA systems and is usually reformulated as a Paraphrase Identification ( PI ) problem .,Question Answering ( QA ) systems are used to provide proper responses to users ' questions automatically .,"Given a question , the aim of the task is to find the most similar question from a QA knowledge base .",0.0227272727272727,0.0967741935483871,0.0967741935483871,research-problem,ablation-analysis
natural_language_inference,22,abstract,abstract,28,26,26,"We employ a connected graph to depict the paraphrase relation between sentences for the PI task , and propose a multi-task sentence - encoding model , which solves the paraphrase identification task and the sentence intent classification task simultaneously .",The main contributions of this work are summarized as follows :,"We propose a semantic retrieval framework that integrates the encoding - based sentence matching model with the approximate nearest neighbor search technology , which allows us to find the most similar question very quickly from all available questions , instead of within only a few candidates , in the QA knowledge base .",0.1272727272727272,0.8387096774193549,0.8387096774193549,model,baselines
natural_language_inference,22,abstract,abstract,29,27,27,"We propose a semantic retrieval framework that integrates the encoding - based sentence matching model with the approximate nearest neighbor search technology , which allows us to find the most similar question very quickly from all available questions , instead of within only a few candidates , in the QA knowledge base .","We employ a connected graph to depict the paraphrase relation between sentences for the PI task , and propose a multi-task sentence - encoding model , which solves the paraphrase identification task and the sentence intent classification task simultaneously .",We evaluated our proposed method on various QA datasets and the experimental results show the effectiveness and superiority of our method .,0.1318181818181818,0.8709677419354839,0.8709677419354839,model,baselines
natural_language_inference,22,experiments,experiments,153,14,14,"For Quora dataset , we use the Glove - 840B - 300D vector as the pre-trained word embedding .",B. Settings of Experiments,The character embedding is randomly initialized with 150 D and the hidden size of BiGRU is set to 300 .,0.6954545454545454,0.1818181818181818,0.1818181818181818,experimental-setup,approach
natural_language_inference,22,experiments,experiments,154,15,15,The character embedding is randomly initialized with 150 D and the hidden size of BiGRU is set to 300 .,"For Quora dataset , we use the Glove - 840B - 300D vector as the pre-trained word embedding .",We set = 0.8 in the multi - task loss function .,0.7,0.1948051948051948,0.1948051948051948,experimental-setup,approach
natural_language_inference,22,experiments,experiments,155,16,16,We set = 0.8 in the multi - task loss function .,The character embedding is randomly initialized with 150 D and the hidden size of BiGRU is set to 300 .,"For the sentence intent classification task , we only keep the sentence clusters with question number greater than 3 , and the remaining sentence clusters with question numberless than or equal to 3 are regarded as a special "" other "" cluster .",0.7045454545454546,0.2077922077922078,0.2077922077922078,experimental-setup,approach
natural_language_inference,22,experiments,experiments,157,18,18,"Dropout layer is also applied to the output of the attentive pooling layer , with a dropout rate of 0.1 .","For the sentence intent classification task , we only keep the sentence clusters with question number greater than 3 , and the remaining sentence clusters with question numberless than or equal to 3 are regarded as a special "" other "" cluster .",An Adam optimizer is used to optimize all the trainable weights .,0.7136363636363636,0.2337662337662337,0.2337662337662337,experimental-setup,experimental-setup
natural_language_inference,22,experiments,experiments,158,19,19,An Adam optimizer is used to optimize all the trainable weights .,"Dropout layer is also applied to the output of the attentive pooling layer , with a dropout rate of 0.1 .",The learning rate is set to 4e - 4 and the batch size is set to 200 .,0.7181818181818181,0.2467532467532467,0.2467532467532467,experimental-setup,approach
natural_language_inference,22,experiments,experiments,159,20,20,The learning rate is set to 4e - 4 and the batch size is set to 200 .,An Adam optimizer is used to optimize all the trainable weights .,"When the performance of the model is no longer improved , an SGD optimizer with a learning rate of 1e - 3 is used to find a better local optimum .",0.7227272727272728,0.2597402597402597,0.2597402597402597,experimental-setup,experiments
natural_language_inference,22,experiments,experiments,160,21,21,"When the performance of the model is no longer improved , an SGD optimizer with a learning rate of 1e - 3 is used to find a better local optimum .",The learning rate is set to 4e - 4 and the batch size is set to 200 .,C. Comparing with other methods,0.7272727272727273,0.2727272727272727,0.2727272727272727,experimental-setup,approach
natural_language_inference,22,experiments,experiments,163,24,24,ESIM : Enhanced Sequential Inference Model is an interaction - based model for natural language inference .,We compared our model with the following models :,It uses BiLSTM to encode sentence contexts and uses the attention mechanism to calculate the information between two sentences .,0.740909090909091,0.3116883116883117,0.3116883116883117,baselines,baselines
natural_language_inference,22,experiments,experiments,166,27,27,BiMPM : Bilateral Multi- Perspective Matching model is an interaction - based sentence matching model with superior performance .,ESIM has shown excellent performance on the SNLI dataset .,"The model uses a BiLSTM layer to learn the sentence representation , four different types of multiperspective matching layers to match two sentences , an additional BiLSTM layer to aggregate the matching results , and a two - layer feed - forward network for prediction .",0.7545454545454545,0.3506493506493506,0.3506493506493506,baselines,approach
natural_language_inference,22,experiments,experiments,168,29,29,"SSE : Shortcut - Stacked Sentence Encoder is an encodingbased sentence - matching model , which enhances multi - layer BiLSTM with short - cut connections .","The model uses a BiLSTM layer to learn the sentence representation , four different types of multiperspective matching layers to match two sentences , an additional BiLSTM layer to aggregate the matching results , and a two - layer feed - forward network for prediction .","SSE has been proved to be effective in improving the performance of sentence encoder , recording state - of - the - art performance of the sentence - encoding models on Quora dataset .",0.7636363636363637,0.3766233766233766,0.3766233766233766,baselines,model
natural_language_inference,22,experiments,experiments,170,31,31,DIIN : Densely Interactive Inference Network is an interaction - based model for natural language inference ( NLI ) .,"SSE has been proved to be effective in improving the performance of sentence encoder , recording state - of - the - art performance of the sentence - encoding models on Quora dataset .",It hierarchically extracts semantic features from interaction space to achieve a high - level understanding of the sentence pair .,0.7727272727272727,0.4025974025974026,0.4025974025974026,baselines,model
natural_language_inference,22,experiments,experiments,178,39,39,"Quora dataset : BiMPM and ESIM models without any sentence interaction information , and is very close to DIIN , the state - of - the - art interaction - based model , but we do n't any external knowledge in our method .",The results of experiments on four sentence matching datasets are summarized as follows :,LCQMC dataset : Experimental results of LCQMC dataset compared with the existing models are shown in .,0.8090909090909091,0.5064935064935064,0.5064935064935064,experiments,baselines
natural_language_inference,22,experiments,experiments,179,40,40,LCQMC dataset : Experimental results of LCQMC dataset compared with the existing models are shown in .,"Quora dataset : BiMPM and ESIM models without any sentence interaction information , and is very close to DIIN , the state - of - the - art interaction - based model , but we do n't any external knowledge in our method .",The experimental results show that our model outperforms state - of the - art models .,0.8136363636363636,0.5194805194805194,0.5194805194805194,experiments,model
natural_language_inference,22,experiments,experiments,180,41,41,The experimental results show that our model outperforms state - of the - art models .,LCQMC dataset : Experimental results of LCQMC dataset compared with the existing models are shown in .,BQ dataset : BQ dataset is a specific - domain dataset with a low average overlap rate .,0.8181818181818182,0.5324675324675324,0.5324675324675324,experiments,approach
natural_language_inference,22,experiments,experiments,181,42,42,BQ dataset : BQ dataset is a specific - domain dataset with a low average overlap rate .,The experimental results show that our model outperforms state - of the - art models .,"As shown in , our model outperforms state - of - the - art models by a large margin , reaching 83 . 62 % , recording the state - of - the - art performance .",0.8227272727272728,0.5454545454545454,0.5454545454545454,experiments,experimental-setup
natural_language_inference,22,experiments,experiments,182,43,43,"As shown in , our model outperforms state - of - the - art models by a large margin , reaching 83 . 62 % , recording the state - of - the - art performance .",BQ dataset : BQ dataset is a specific - domain dataset with a low average overlap rate .,TCS dataset :,0.8272727272727273,0.5584415584415584,0.5584415584415584,experiments,approach
natural_language_inference,22,experiments,experiments,183,44,44,TCS dataset :,"As shown in , our model outperforms state - of - the - art models by a large margin , reaching 83 . 62 % , recording the state - of - the - art performance .",As shown in show that our MSEM model achieves the best performance .,0.8318181818181818,0.5714285714285714,0.5714285714285714,experiments,baselines
natural_language_inference,22,experiments,experiments,184,45,45,As shown in show that our MSEM model achieves the best performance .,TCS dataset :,This indicates that our model is also very effective in the spoken question - answering scenario .,0.8363636363636363,0.5844155844155844,0.5844155844155844,experiments,approach
natural_language_inference,22,experiments,experiments,193,54,54,We first study the contribution of the ARU component .,Note that we do the significant test for each ablation experiment using the t-test ( p < 0.05 ) .,"The accuracy decreases , the accuracy will drop to 88.25 % .",0.8772727272727273,0.7012987012987013,0.7012987012987013,ablation-analysis,approach
natural_language_inference,22,experiments,experiments,194,55,55,"The accuracy decreases , the accuracy will drop to 88.25 % .",We first study the contribution of the ARU component .,Next we compare the effect of attentive pooling vs max pooling .,0.8818181818181818,0.7142857142857143,0.7142857142857143,ablation-analysis,approach
natural_language_inference,22,experiments,experiments,196,57,57,It turns out that the attentive pooling is better than max pooling .,Next we compare the effect of attentive pooling vs max pooling .,"Then if we remove the highway network , the accuracy will drop to 88.36 % .",0.8909090909090909,0.7402597402597403,0.7402597402597403,ablation-analysis,approach
natural_language_inference,22,experiments,experiments,197,58,58,"Then if we remove the highway network , the accuracy will drop to 88.36 % .",It turns out that the attentive pooling is better than max pooling .,"Finally when we remove the character - level embedding , the accuracy will drop to 88.26 % .",0.8954545454545455,0.7532467532467533,0.7532467532467533,ablation-analysis,approach
natural_language_inference,22,experiments,experiments,198,59,59,"Finally when we remove the character - level embedding , the accuracy will drop to 88.26 % .","Then if we remove the highway network , the accuracy will drop to 88.36 % .",A possible reason might be that the character - level embedding can better handle the out - of - vocab ( OOV ) words .,0.9,0.7662337662337663,0.7662337662337663,ablation-analysis,approach
natural_language_inference,23,title,title,2,2,2,Deep Fusion LSTMs for Text Semantic Matching, , ,0.0093896713615023,1.0,1.0,research-problem,experimental-setup
natural_language_inference,23,introduction,introduction,14,2,2,"Among many natural language processing ( NLP ) tasks , such as text classification , question answering and machine translation , a common problem is modelling the relevance / similarity of a pair of texts , which is also called text semantic matching .", ,"Due to the semantic gap problem , text semantic matching is still a challenging problem .",0.0657276995305164,0.074074074074074,0.074074074074074,research-problem,ablation-analysis
natural_language_inference,23,introduction,introduction,25,13,13,"In this paper , we adopt a deep fusion strategy to model the strong interactions of two sentences .",The final matching score depends on these different levels of interactions .,"Given two texts x 1 :m and y 1 :n , we define a matching vector h i , j to represent the interaction of the subsequences x 1:i and y 1:j .",0.1173708920187793,0.4814814814814815,0.4814814814814815,model,experimental-setup
natural_language_inference,23,introduction,introduction,28,16,16,"Thus , text matching can be regarded as modelling the interaction of two texts in a recursive matching way .","h i , j depends on the matching vectors h s ,t on previous interactions 1 ? s < i and 1 ? t < j.","Following this idea , we propose deep fusion long short - term memory neural networks ( DF - LSTMs ) to model the interactions recursively .",0.1314553990610328,0.5925925925925926,0.5925925925925926,model,approach
natural_language_inference,23,introduction,introduction,29,17,17,"Following this idea , we propose deep fusion long short - term memory neural networks ( DF - LSTMs ) to model the interactions recursively .","Thus , text matching can be regarded as modelling the interaction of two texts in a recursive matching way .","More concretely , DF - LSTMs consist of two interconnected conditional LSTMs , each of which models apiece of text under the influence of another .",0.136150234741784,0.6296296296296297,0.6296296296296297,model,ablation-analysis
natural_language_inference,23,introduction,introduction,30,18,18,"More concretely , DF - LSTMs consist of two interconnected conditional LSTMs , each of which models apiece of text under the influence of another .","Following this idea , we propose deep fusion long short - term memory neural networks ( DF - LSTMs ) to model the interactions recursively .",The output vector of DF - LSTMs is fed into a task - specific output layer to compute the match - ing score .,0.1408450704225352,0.6666666666666666,0.6666666666666666,model,model
natural_language_inference,23,introduction,introduction,31,19,19,The output vector of DF - LSTMs is fed into a task - specific output layer to compute the match - ing score .,"More concretely , DF - LSTMs consist of two interconnected conditional LSTMs , each of which models apiece of text under the influence of another .",The contributions of this paper can be summarized as follows .,0.1455399061032863,0.7037037037037037,0.7037037037037037,model,model
natural_language_inference,23,experiment,competitor methods,139,4,2,Neural bag - of - words ( NBOW ) :, ,"Each sequence is represented as the sum of the embeddings of the words it contains , then they are concatenated and fed to a MLP .",0.6525821596244131,0.0655737704918032,0.0338983050847457,baselines,approach
natural_language_inference,23,experiment,competitor methods,140,5,3,"Each sequence is represented as the sum of the embeddings of the words it contains , then they are concatenated and fed to a MLP .",Neural bag - of - words ( NBOW ) :,"Single LSTM : Two sequences are encoded by a single LSTM , proposed by .",0.6572769953051644,0.081967213114754,0.0508474576271186,baselines,approach
natural_language_inference,23,experiment,competitor methods,141,6,4,"Single LSTM : Two sequences are encoded by a single LSTM , proposed by .","Each sequence is represented as the sum of the embeddings of the words it contains , then they are concatenated and fed to a MLP .","Parallel LSTMs : Two sequences are first encoded by two LSTMs separately , then they are concatenated and fed to a MLP .",0.6619718309859155,0.0983606557377049,0.0677966101694915,baselines,baselines
natural_language_inference,23,experiment,competitor methods,142,7,5,"Parallel LSTMs : Two sequences are first encoded by two LSTMs separately , then they are concatenated and fed to a MLP .","Single LSTM : Two sequences are encoded by a single LSTM , proposed by .","Attention LSTMs : Two sequences are encoded by LSTMs with attention mechanism , proposed by .",0.6666666666666666,0.1147540983606557,0.0847457627118644,baselines,experimental-setup
natural_language_inference,23,experiment,competitor methods,143,8,6,"Attention LSTMs : Two sequences are encoded by LSTMs with attention mechanism , proposed by .","Parallel LSTMs : Two sequences are first encoded by two LSTMs separately , then they are concatenated and fed to a MLP .","Word - by - word Attention LSTMs : An improved strategy of attention LSTMs , which introduces word - by - word attention mechanism and is proposed by . :",0.6713615023474179,0.1311475409836065,0.1016949152542373,baselines,experimental-setup
natural_language_inference,23,experiment,competitor methods,144,9,7,"Word - by - word Attention LSTMs : An improved strategy of attention LSTMs , which introduces word - by - word attention mechanism and is proposed by . :","Attention LSTMs : Two sequences are encoded by LSTMs with attention mechanism , proposed by .",Accuracies of our proposed model against other neural models on SNLI corpus .,0.676056338028169,0.1475409836065573,0.1186440677966101,baselines,model
natural_language_inference,23,experiment,competitor methods,155,20,18,"we can see that the proposed model also shows its superiority on this task , which outperforms the stateof - the - arts methods on both metrics ( P@1 ( 5 ) and P@1 ( 10 ) ) with a large margin .",Results of MQA are shown in the .,"By analyzing the evaluation results of questionanswer matching in , we can see strong interaction models ( attention LSTMs , our DF - LSTMs ) consistently outperform the weak interaction models ( NBOW , parallel LSTMs ) with a large margin , which suggests the importance of modelling strong interaction of two sentences .",0.7276995305164319,0.3278688524590164,0.3050847457627119,results,baselines
natural_language_inference,23,experiment,competitor methods,156,21,19,"By analyzing the evaluation results of questionanswer matching in , we can see strong interaction models ( attention LSTMs , our DF - LSTMs ) consistently outperform the weak interaction models ( NBOW , parallel LSTMs ) with a large margin , which suggests the importance of modelling strong interaction of two sentences .","we can see that the proposed model also shows its superiority on this task , which outperforms the stateof - the - arts methods on both metrics ( P@1 ( 5 ) and P@1 ( 10 ) ) with a large margin .",Understanding Behaviors of Neurons in DF - LSTMs,0.7323943661971831,0.3442622950819672,0.3220338983050847,results,baselines
natural_language_inference,24,title,title,2,2,2,Reading Wikipedia to Answer Open-Domain Questions, , ,0.0089285714285714,1.0,1.0,research-problem,experimental-setup
natural_language_inference,24,introduction,introduction,9,2,2,"This paper considers the problem of answering factoid questions in an open - domain setting using Wikipedia as the unique knowledge source , such as one does when looking for answers in an encyclopedia .", ,Wikipedia is a constantly evolving source of detailed information that could facilitate intelligent machines - if they are able to leverage its power .,0.0401785714285714,0.0869565217391304,0.0869565217391304,research-problem,ablation-analysis
natural_language_inference,24,introduction,introduction,11,4,4,"Unlike knowledge bases ( KBs ) such as Freebase or DB - Pedia , which are easier for computers to process but too sparsely populated for open - domain question answering , Wikipedia contains up - to - date knowledge that humans are interested in .",Wikipedia is a constantly evolving source of detailed information that could facilitate intelligent machines - if they are able to leverage its power .,"It is designed , however , for humans - not machines - to read .",0.0491071428571428,0.1739130434782608,0.1739130434782608,research-problem,ablation-analysis
natural_language_inference,24,introduction,introduction,14,7,7,"In order to answer any question , one must first retrieve the few relevant articles among more than 5 million items , and then scan them carefully to identify the answer .",Using Wikipedia articles as the knowledge source causes the task of question answering ( QA ) to combine the challenges of both large - scale open - domain QA and of machine comprehension of text .,"We term this setting , machine reading at scale ( MRS ) .",0.0625,0.3043478260869565,0.3043478260869565,model,model
natural_language_inference,24,introduction,introduction,15,8,8,"We term this setting , machine reading at scale ( MRS ) .","In order to answer any question , one must first retrieve the few relevant articles among more than 5 million items , and then scan them carefully to identify the answer .",Our work treats Wikipedia as a collection of articles and does not rely on its internal graph structure .,0.0669642857142857,0.3478260869565217,0.3478260869565217,model,experimental-setup
natural_language_inference,24,introduction,introduction,16,9,9,Our work treats Wikipedia as a collection of articles and does not rely on its internal graph structure .,"We term this setting , machine reading at scale ( MRS ) .","As a result , our approach is generic and could be switched to other collections of documents , books , or even daily updated newspapers .",0.0714285714285714,0.391304347826087,0.391304347826087,model,experimental-setup
natural_language_inference,24,introduction,introduction,17,10,10,"As a result , our approach is generic and could be switched to other collections of documents , books , or even daily updated newspapers .",Our work treats Wikipedia as a collection of articles and does not rely on its internal graph structure .,"Large - scale QA systems like IBM 's Deep QA rely on multiple sources to answer : besides Wikipedia , it is also paired with KBs , dictionaries , and even news articles , books , etc .",0.0758928571428571,0.4347826086956521,0.4347826086956521,model,approach
natural_language_inference,24,introduction,introduction,20,13,13,Having a single knowledge source forces the model to be very precise while searching for an answer as the evidence might appear only once .,"As a result , such systems heavily rely on information redundancy among the sources to answer correctly .","This challenge thus encourages research in the ability of a machine to read , a key motivation for the machine comprehension subfield and the creation of datasets such as SQuAD , and CBT .",0.0892857142857142,0.5652173913043478,0.5652173913043478,model,model
natural_language_inference,24,experiments,reader evaluation on squad,174,11,3,We use 3 - layer bidirectional LSTMs with h = 128 hidden units for both paragraph and question encoding .,Implementation details,"We apply the Stanford CoreNLP toolkit for tokenization and also generating lemma , partof - speech , and named entity tags .",0.7767857142857143,0.2115384615384615,0.1875,hyperparameters,experimental-setup
natural_language_inference,24,experiments,reader evaluation on squad,175,12,4,"We apply the Stanford CoreNLP toolkit for tokenization and also generating lemma , partof - speech , and named entity tags .",We use 3 - layer bidirectional LSTMs with h = 128 hidden units for both paragraph and question encoding .,"Lastly , all the training examples are sorted by the length of paragraph and divided into minibatches of 32 examples each .",0.78125,0.2307692307692308,0.25,hyperparameters,approach
natural_language_inference,24,experiments,reader evaluation on squad,176,13,5,"Lastly , all the training examples are sorted by the length of paragraph and divided into minibatches of 32 examples each .","We apply the Stanford CoreNLP toolkit for tokenization and also generating lemma , partof - speech , and named entity tags .",We use Adamax for optimization as described in .,0.7857142857142857,0.25,0.3125,hyperparameters,approach
natural_language_inference,24,experiments,reader evaluation on squad,177,14,6,We use Adamax for optimization as described in .,"Lastly , all the training examples are sorted by the length of paragraph and divided into minibatches of 32 examples each .",Dropout with p = 0.3 is applied to word embeddings and all the hidden units of LSTMs .,0.7901785714285714,0.2692307692307692,0.375,hyperparameters,baselines
natural_language_inference,24,experiments,reader evaluation on squad,178,15,7,Dropout with p = 0.3 is applied to word embeddings and all the hidden units of LSTMs .,We use Adamax for optimization as described in .,presents our evaluation results on both development and test sets .,0.7946428571428571,0.2884615384615384,0.4375,hyperparameters,experimental-setup
natural_language_inference,24,experiments,reader evaluation on squad,182,19,11,"Our system ( single model ) can achieve 70.0 % exact match and 79.0 % F 1 scores on the test set , which surpasses all the published results and can match the top performance on the SQuAD leaderboard at the time of writing .",Result and analysis,"Additionally , we think that our model is conceptually simpler than most of the existing systems .",0.8125,0.3653846153846153,0.6875,results,baselines
natural_language_inference,24,experiments,reader evaluation on squad,186,23,15,"Without the aligned question embedding feature ( only word embedding and a few manual features ) , our system is still able to achieve F1 over 77 % .",As shown in all the features contribute to the performance of our final system .,"More interestingly , if we remove both f aligned and f exact match , the performance drops dramatically , so we conclude that both features play a similar but complementary role in the feature representation related to the paraphrased nature of a question vs. the context around an answer .",0.8303571428571429,0.4423076923076923,0.9375,results,baselines
natural_language_inference,24,experiments,reader evaluation on squad,187,24,16,"More interestingly , if we remove both f aligned and f exact match , the performance drops dramatically , so we conclude that both features play a similar but complementary role in the feature representation related to the paraphrased nature of a question vs. the context around an answer .","Without the aligned question embedding feature ( only word embedding and a few manual features ) , our system is still able to achieve F1 over 77 % .", ,0.8348214285714286,0.4615384615384616,1.0,results,approach
natural_language_inference,25,title,title,2,2,2,A Deep Cascade Model for Multi - Document Reading Comprehension, , ,0.0077821011673151,1.0,1.0,research-problem,approach
natural_language_inference,25,introduction,introduction,13,2,2,"Machine reading comprehension ( MRC ) , which empowers computers with the ability to read and comprehend knowledge and then answer questions from textual data , has made rapid progress in recent years .", ,"From the early cloze - style test to answer extraction from a single paragraph , and to the more complex open - domain question answering from web data , great efforts have been made to push the MRC technique to more practical applications .",0.0505836575875486,0.037037037037037,0.037037037037037,research-problem,experimental-setup
natural_language_inference,25,introduction,introduction,14,3,3,"From the early cloze - style test to answer extraction from a single paragraph , and to the more complex open - domain question answering from web data , great efforts have been made to push the MRC technique to more practical applications .","Machine reading comprehension ( MRC ) , which empowers computers with the ability to read and comprehend knowledge and then answer questions from textual data , has made rapid progress in recent years .","The rapid progress of MRC in recent years mostly owes to the release of the single - paragraph benchmark dataset SQuAD ) , on which various deep attention - based methods have been proposed to constantly push the state - of - the - art performance .",0.0544747081712062,0.0555555555555555,0.0555555555555555,research-problem,experimental-setup
natural_language_inference,25,introduction,introduction,29,18,18,"To address the above problems , we propose a deep cascade model which combines the advantages of both methods in a coarse - to - fine manner .","This computation cost limits its application to the operational online environment , such as Amazon 2 and Taobao 3 , where efficiency is a critical factor to be considered .",The deep cascade model is designed to properly keep the balance between the effectiveness and efficiency .,0.11284046692607,0.3333333333333333,0.3333333333333333,model,ablation-analysis
natural_language_inference,25,introduction,introduction,30,19,19,The deep cascade model is designed to properly keep the balance between the effectiveness and efficiency .,"To address the above problems , we propose a deep cascade model which combines the advantages of both methods in a coarse - to - fine manner .","At early stages of the model , simple features and ranking functions are used to select a candidate set of most relevant contents , filtering out the irrelevant documents and paragraphs as much as possible .",0.1167315175097276,0.3518518518518518,0.3518518518518518,model,experimental-setup
natural_language_inference,25,introduction,introduction,31,20,20,"At early stages of the model , simple features and ranking functions are used to select a candidate set of most relevant contents , filtering out the irrelevant documents and paragraphs as much as possible .",The deep cascade model is designed to properly keep the balance between the effectiveness and efficiency .,Then the selected paragraphs are passed to the attention - based deep MRC model for extracting the actual answer span at word level .,0.1206225680933852,0.3703703703703704,0.3703703703703704,model,approach
natural_language_inference,25,introduction,introduction,32,21,21,Then the selected paragraphs are passed to the attention - based deep MRC model for extracting the actual answer span at word level .,"At early stages of the model , simple features and ranking functions are used to select a candidate set of most relevant contents , filtering out the irrelevant documents and paragraphs as much as possible .","To better support the answer extraction , we also introduce the document extraction and paragraph extraction as two auxiliary tasks , which helps to quickly narrow down the entire search space .",0.1245136186770428,0.3888888888888889,0.3888888888888889,model,approach
natural_language_inference,25,introduction,introduction,33,22,22,"To better support the answer extraction , we also introduce the document extraction and paragraph extraction as two auxiliary tasks , which helps to quickly narrow down the entire search space .",Then the selected paragraphs are passed to the attention - based deep MRC model for extracting the actual answer span at word level .,"We jointly optimize all the three tasks in a unified deep MRC model , which shares some common bottom layers .",0.1284046692607003,0.4074074074074074,0.4074074074074074,model,approach
natural_language_inference,25,introduction,introduction,34,23,23,"We jointly optimize all the three tasks in a unified deep MRC model , which shares some common bottom layers .","To better support the answer extraction , we also introduce the document extraction and paragraph extraction as two auxiliary tasks , which helps to quickly narrow down the entire search space .","This cascaded structure enables the models to perform a coarse - to - fine pruning at different stages , better models can be learnt effectively and efficiently .",0.1322957198443579,0.4259259259259259,0.4259259259259259,model,baselines
natural_language_inference,25,introduction,introduction,35,24,24,"This cascaded structure enables the models to perform a coarse - to - fine pruning at different stages , better models can be learnt effectively and efficiently .","We jointly optimize all the three tasks in a unified deep MRC model , which shares some common bottom layers .","The overall framework of our model is demonstrated in , which consists of three modules : document retrieval , paragraph retrieval and answer extraction .",0.1361867704280155,0.4444444444444444,0.4444444444444444,model,approach
natural_language_inference,25,introduction,introduction,36,25,25,"The overall framework of our model is demonstrated in , which consists of three modules : document retrieval , paragraph retrieval and answer extraction .","This cascaded structure enables the models to perform a coarse - to - fine pruning at different stages , better models can be learnt effectively and efficiently .",The first module takes the question and a collection of raw documents as input .,0.1400778210116731,0.462962962962963,0.462962962962963,model,approach
natural_language_inference,25,introduction,introduction,37,26,26,The first module takes the question and a collection of raw documents as input .,"The overall framework of our model is demonstrated in , which consists of three modules : document retrieval , paragraph retrieval and answer extraction .","The module at each subsequent stage consumes the output from the previous stage , and further prunes the documents , paragraphs and answer spans given the question .",0.1439688715953307,0.4814814814814815,0.4814814814814815,model,model
natural_language_inference,25,introduction,introduction,38,27,27,"The module at each subsequent stage consumes the output from the previous stage , and further prunes the documents , paragraphs and answer spans given the question .",The first module takes the question and a collection of raw documents as input .,"For each of the first two modules , we define a ranking function and an extraction function .",0.1478599221789883,0.5,0.5,model,model
natural_language_inference,25,introduction,introduction,40,29,29,"The ranking function is first used as a preliminary filter to discard most of the irrelevant documents or paragraphs , so as to keep our framework efficient .","For each of the first two modules , we define a ranking function and an extraction function .","The extraction function is then designed to deal with the auxiliary document and paragraph extraction tasks , which is jointly optimized with the final answer extraction module for better extraction performance .",0.1556420233463035,0.5370370370370371,0.5370370370370371,model,experimental-setup
natural_language_inference,25,introduction,introduction,41,30,30,"The extraction function is then designed to deal with the auxiliary document and paragraph extraction tasks , which is jointly optimized with the final answer extraction module for better extraction performance .","The ranking function is first used as a preliminary filter to discard most of the irrelevant documents or paragraphs , so as to keep our framework efficient .","The local ranking functions in different modules gradually increase in cost and complexity , to properly keep the balance between the effectiveness and efficiency .",0.159533073929961,0.5555555555555556,0.5555555555555556,model,baselines
natural_language_inference,25,experiments,implementation details,208,24,3,We choose K = 4 and N = 2 for the good performance when evaluating on the dev set .,"For the cascade ranking functions , the number of selected documents K and paragraphs N are the key factors to balance the effectiveness and efficiency trade - off .","Since the Trivia QA documents often contain many small paragraphs , we also restructure the documents by merging consecutive paragraphs to a maximum size of 600 words for each paragraph as in ( Clark and Gardner 2017 ) .",0.8093385214007782,0.3529411764705882,0.25,experimental-setup,approach
natural_language_inference,25,experiments,implementation details,211,27,6,"For the multi-task deep attention framework , we adopt the Adam optimizer for training , with a mini-batch size of 32 and initial learning rate of 0.0005 .",The detailed analysis will be given and discussed in the next section .,We use the GloVe 300 dimensional word embeddings in TriviaQA and train a word2 vec word embeddings with the whole DuReader corpus for DuReader .,0.8210116731517509,0.3970588235294117,0.5,experimental-setup,baselines
natural_language_inference,25,experiments,implementation details,212,28,7,We use the GloVe 300 dimensional word embeddings in TriviaQA and train a word2 vec word embeddings with the whole DuReader corpus for DuReader .,"For the multi-task deep attention framework , we adopt the Adam optimizer for training , with a mini-batch size of 32 and initial learning rate of 0.0005 .",The word embeddings are fixed during training .,0.8249027237354085,0.4117647058823529,0.5833333333333334,experimental-setup,approach
natural_language_inference,25,experiments,implementation details,213,29,8,The word embeddings are fixed during training .,We use the GloVe 300 dimensional word embeddings in TriviaQA and train a word2 vec word embeddings with the whole DuReader corpus for DuReader .,The hidden size of LSTM is set as 150 for TriviaQA and 128 for DuReader .,0.8287937743190662,0.4264705882352941,0.6666666666666666,experimental-setup,experiments
natural_language_inference,25,experiments,implementation details,214,30,9,The hidden size of LSTM is set as 150 for TriviaQA and 128 for DuReader .,The word embeddings are fixed during training .,The task - specific hyper - parameters ? 1 and ? 2 in Equ. 15 are set as ? 1 = ? 2 = 0.5 . Regularization parameter ? in Equ.,0.8326848249027238,0.4411764705882353,0.75,experimental-setup,experimental-setup
natural_language_inference,25,experiments,implementation details,215,31,10,The task - specific hyper - parameters ? 1 and ? 2 in Equ. 15 are set as ? 1 = ? 2 = 0.5 . Regularization parameter ? in Equ.,The hidden size of LSTM is set as 150 for TriviaQA and 128 for DuReader .,16 is set as a small value of 0.01 .,0.8365758754863813,0.4558823529411765,0.8333333333333334,experimental-setup,baselines
natural_language_inference,25,experiments,implementation details,216,32,11,16 is set as a small value of 0.01 .,The task - specific hyper - parameters ? 1 and ? 2 in Equ. 15 are set as ? 1 = ? 2 = 0.5 . Regularization parameter ? in Equ.,All models are trained on Nvidia Tesla M40 GPU with Cudnn LSTM cell in Tensorflow 1.3 .,0.8404669260700389,0.4705882352941176,0.9166666666666666,experimental-setup,approach
natural_language_inference,25,experiments,implementation details,217,33,12,All models are trained on Nvidia Tesla M40 GPU with Cudnn LSTM cell in Tensorflow 1.3 .,16 is set as a small value of 0.01 ., ,0.8443579766536965,0.4852941176470588,1.0,experimental-setup,baselines
natural_language_inference,25,experiments,off line evaluation,221,37,4,"We can see that by adopting the deep cascade learning framework , the proposed model outperforms the previous state - of - the - art methods by an evident margin on both datasets , which validates the effectiveness of the proposed method in addressing the challenging multi-document MRC task .","The results of our single deep cascade model 4 on TriviaQA Web and DuReader 1.0 are summarized in , respectively .",Ablation Study,0.8599221789883269,0.5441176470588235,0.1666666666666666,results,baselines
natural_language_inference,25,experiments,off line evaluation,226,42,9,"From the results , we can see that : 1 ) the shared LSTM plays an important role in answer extraction among multiple documents , the benefit lies in two parts : a ) it helps to normalize the content probability score from multiple documents so that the answers extracted from different documents can be directly compared ; b ) it can keep the ranking order from document ranking component in mind , which may serve as an additional signal when predicting the best answer .",We only submit the single model without any model ensemble . 31.9 39.2 PR + BiDAF 37.55 41.81 Cross - Passage Verify 40.97 44.18 R- net 44,"By incorporating the manual features , the performance can be further improved slightly .",0.8793774319066148,0.6176470588235294,0.375,ablation-analysis,approach
natural_language_inference,25,experiments,off line evaluation,227,43,10,"By incorporating the manual features , the performance can be further improved slightly .","From the results , we can see that : 1 ) the shared LSTM plays an important role in answer extraction among multiple documents , the benefit lies in two parts : a ) it helps to normalize the content probability score from multiple documents so that the answers extracted from different documents can be directly compared ; b ) it can keep the ranking order from document ranking component in mind , which may serve as an additional signal when predicting the best answer .","2 ) Both the preliminary cascade ranking and multi-task answer extraction strategy are vital for the final performance , which serve as a good trade - off between the pure pipeline method and fully joint learning method .",0.8832684824902723,0.6323529411764706,0.4166666666666667,ablation-analysis,approach
natural_language_inference,25,experiments,off line evaluation,228,44,11,"2 ) Both the preliminary cascade ranking and multi-task answer extraction strategy are vital for the final performance , which serve as a good trade - off between the pure pipeline method and fully joint learning method .","By incorporating the manual features , the performance can be further improved slightly .","By removing the rich irrelevant noisy data in the cascade document and paragraph ranking stage , the downside MRC model can better extract the answer from the more relevant content data .",0.8871595330739299,0.6470588235294118,0.4583333333333333,ablation-analysis,approach
natural_language_inference,25,experiments,off line evaluation,230,46,13,"Jointly training the three extraction tasks can provide great benefits , which shows that the three tasks are actually closely related and can boost each other with shared representations at bottom layers .","By removing the rich irrelevant noisy data in the cascade document and paragraph ranking stage , the downside MRC model can better extract the answer from the more relevant content data .",Effectiveness v.s. Efficiency Trade - off,0.8949416342412452,0.6764705882352942,0.5416666666666666,ablation-analysis,approach
natural_language_inference,26,title,title,2,2,2,U - Net : Machine Reading Comprehension with Unanswerable Questions, , ,0.0070175438596491,1.0,1.0,research-problem,approach
natural_language_inference,26,introduction,introduction,12,2,2,"Machine reading comprehension ( MRC ) is a challenging task in natural language processing , which requires that machine can read , understand , and answer questions about a text .", ,"Benefiting from the rapid development of deep learning techniques and large - scale benchmarks , the end - to - end neural methods have achieved promising results on MRC task .",0.0421052631578947,0.0512820512820512,0.0512820512820512,research-problem,ablation-analysis
natural_language_inference,26,introduction,introduction,13,3,3,"Benefiting from the rapid development of deep learning techniques and large - scale benchmarks , the end - to - end neural methods have achieved promising results on MRC task .","Machine reading comprehension ( MRC ) is a challenging task in natural language processing , which requires that machine can read , understand , and answer questions about a text .","The best systems have even surpassed human performance on the Stanford Question Answering Dataset ( SQuAD ) , one of the most widely used MRC benchmarks .",0.0456140350877193,0.0769230769230769,0.0769230769230769,research-problem,experimental-setup
natural_language_inference,26,introduction,introduction,37,27,27,"In this paper , we decompose the problem of MRC with unanswerable questions into three sub - tasks : answer pointer , no - answer pointer , and answer verifier .","Intuitively , it is unnecessary since the underlying comprehension and reasoning of language for these components is the same .","Since these three sub - tasks are highly related , we regard the MRC with unanswerable questions as a multi-task learning problem ( Caruana 1997 ) by sharing some meta-knowledge .",0.1298245614035087,0.6923076923076923,0.6923076923076923,model,experimental-setup
natural_language_inference,26,introduction,introduction,39,29,29,We propose the U - Net to incorporate these three sub - tasks into a unified model :,"Since these three sub - tasks are highly related , we regard the MRC with unanswerable questions as a multi-task learning problem ( Caruana 1997 ) by sharing some meta-knowledge .","1 ) an answer pointer to predict a can - didate answer span for a question ; 2 ) a no -answer pointer to avoid selecting any text span when a question has no answer ; and 3 ) an answer verifier to determine the probability of the "" unanswerability "" of a question with candidate answer information .",0.1368421052631579,0.7435897435897436,0.7435897435897436,model,approach
natural_language_inference,26,introduction,introduction,40,30,30,"1 ) an answer pointer to predict a can - didate answer span for a question ; 2 ) a no -answer pointer to avoid selecting any text span when a question has no answer ; and 3 ) an answer verifier to determine the probability of the "" unanswerability "" of a question with candidate answer information .",We propose the U - Net to incorporate these three sub - tasks into a unified model :,"Additionally , we also introduce a universal node and process the question and its context passage as a single contiguous sequence of tokens , which greatly improves the conciseness of U - Net .",0.1403508771929824,0.7692307692307693,0.7692307692307693,model,model
natural_language_inference,26,introduction,introduction,41,31,31,"Additionally , we also introduce a universal node and process the question and its context passage as a single contiguous sequence of tokens , which greatly improves the conciseness of U - Net .","1 ) an answer pointer to predict a can - didate answer span for a question ; 2 ) a no -answer pointer to avoid selecting any text span when a question has no answer ; and 3 ) an answer verifier to determine the probability of the "" unanswerability "" of a question with candidate answer information .",The universal node acts on both question and passage to learn whether the question is answerable .,0.143859649122807,0.7948717948717948,0.7948717948717948,model,approach
natural_language_inference,26,introduction,introduction,42,32,32,The universal node acts on both question and passage to learn whether the question is answerable .,"Additionally , we also introduce a universal node and process the question and its context passage as a single contiguous sequence of tokens , which greatly improves the conciseness of U - Net .","Different from the previous pipeline models , U - Net can be learned in an end - to - end fashion .",0.1473684210526315,0.8205128205128205,0.8205128205128205,model,experimental-setup
natural_language_inference,26,training,training,180,19,19,"We use Spacy to process each question and passage to obtain tokens , POS tags , NER tags and lemmas tags of each text .",Implementation Details,"We use 12 dimensions to embed POS tags , 8 for NER tags .",0.631578947368421,0.2134831460674156,0.6785714285714286,hyperparameters,approach
natural_language_inference,26,training,training,181,20,20,"We use 12 dimensions to embed POS tags , 8 for NER tags .","We use Spacy to process each question and passage to obtain tokens , POS tags , NER tags and lemmas tags of each text .","We use 3 binary features : exact match , lower - case match and lemma match between the question and passage .",0.6350877192982456,0.2247191011235955,0.7142857142857143,hyperparameters,approach
natural_language_inference,26,training,training,182,21,21,"We use 3 binary features : exact match , lower - case match and lemma match between the question and passage .","We use 12 dimensions to embed POS tags , 8 for NER tags .",We use 100 - dim Glove pretrained word embeddings and 1024 - dim Elmo embeddings .,0.6385964912280702,0.2359550561797753,0.75,hyperparameters,approach
natural_language_inference,26,training,training,183,22,22,We use 100 - dim Glove pretrained word embeddings and 1024 - dim Elmo embeddings .,"We use 3 binary features : exact match , lower - case match and lemma match between the question and passage .",All the LSTM blocks are bi-directional with one single layer .,0.6421052631578947,0.247191011235955,0.7857142857142857,hyperparameters,experiments
natural_language_inference,26,training,training,184,23,23,All the LSTM blocks are bi-directional with one single layer .,We use 100 - dim Glove pretrained word embeddings and 1024 - dim Elmo embeddings .,"We set the hidden layer dimension as 125 , attention layer dimension as 250 .",0.6456140350877193,0.2584269662921348,0.8214285714285714,hyperparameters,baselines
natural_language_inference,26,training,training,185,24,24,"We set the hidden layer dimension as 125 , attention layer dimension as 250 .",All the LSTM blocks are bi-directional with one single layer .,"We added a dropout layer overall the modeling layers , including the embedding layer , at a dropout rate of 0.3 .",0.6491228070175439,0.2696629213483146,0.8571428571428571,hyperparameters,approach
natural_language_inference,26,training,training,186,25,25,"We added a dropout layer overall the modeling layers , including the embedding layer , at a dropout rate of 0.3 .","We set the hidden layer dimension as 125 , attention layer dimension as 250 .",We use Adam optimizer with a learning rate of 0.002 ( Kingma and Ba 2014 ) .,0.6526315789473685,0.2808988764044944,0.8928571428571429,hyperparameters,experimental-setup
natural_language_inference,26,training,training,187,26,26,We use Adam optimizer with a learning rate of 0.002 ( Kingma and Ba 2014 ) .,"We added a dropout layer overall the modeling layers , including the embedding layer , at a dropout rate of 0.3 .","During training , we omit passage with over 400 words and question with more than 50 words .",0.656140350877193,0.2921348314606741,0.9285714285714286,hyperparameters,baselines
natural_language_inference,26,training,main results,191,30,2,"Our model achieves an F 1 score of 74.0 and an EM score of 70.3 on the development set , and an F 1 score of 72.6 and an EM score of 69.2 on Test set 1 , as shown in .", ,Our model outperforms most of the previous approaches .,0.6701754385964912,0.3370786516853932,0.3333333333333333,results,baselines
natural_language_inference,26,training,main results,192,31,3,Our model outperforms most of the previous approaches .,"Our model achieves an F 1 score of 74.0 and an EM score of 70.3 on the development set , and an F 1 score of 72.6 and an EM score of 69.2 on Test set 1 , as shown in .","Comparing to the best - performing systems , our model has a simple architecture and is an end - to - end model .",0.6736842105263158,0.348314606741573,0.5,results,approach
natural_language_inference,26,training,main results,193,32,4,"Comparing to the best - performing systems , our model has a simple architecture and is an end - to - end model .",Our model outperforms most of the previous approaches .,"In fact , among all the end - to - end models , we achieve the best F1 scores .",0.6771929824561403,0.3595505617977528,0.6666666666666666,results,approach
natural_language_inference,26,training,main results,194,33,5,"In fact , among all the end - to - end models , we achieve the best F1 scores .","Comparing to the best - performing systems , our model has a simple architecture and is an end - to - end model .",We believe that the performance of the U - Net can be boosted with an additional post -processing step to verify answers using approaches such as .,0.6807017543859649,0.3707865168539326,0.8333333333333334,results,approach
natural_language_inference,26,training,ablation study,203,42,8,"Our results showed that when node U is shared , as it is called ' universal ' , it learns information interaction between the question and passage , and when it is not shared , the performance slightly degraded .",We also tried to make the universal node U only attached to the passage representation when passing the attention layer .,"As for the approaches to encode the representations , we pass both the question and passage through a shared BiL - STM .",0.712280701754386,0.4719101123595506,0.3636363636363637,ablation-analysis,approach
natural_language_inference,26,training,ablation study,206,45,11,"Results show that the performance dropped slightly , suggesting sharing BiLSTM is an effective method to improve the quality of the encoder .","To test the effectiveness of this , we ran the experiment using separate BiLSTMs on embedded question and passage representations .","After removing the plausible answer pointer , the performance also dropped , indicating the plausible answers are useful to improve the model even though they are incorrect .",0.7228070175438597,0.5056179775280899,0.5,ablation-analysis,baselines
natural_language_inference,26,training,ablation study,207,46,12,"After removing the plausible answer pointer , the performance also dropped , indicating the plausible answers are useful to improve the model even though they are incorrect .","Results show that the performance dropped slightly , suggesting sharing BiLSTM is an effective method to improve the quality of the encoder .","After removing the answer verifier , the performance dropped greatly , indicating it is vital for our model .",0.7263157894736842,0.5168539325842697,0.5454545454545454,ablation-analysis,baselines
natural_language_inference,26,training,ablation study,208,47,13,"After removing the answer verifier , the performance dropped greatly , indicating it is vital for our model .","After removing the plausible answer pointer , the performance also dropped , indicating the plausible answers are useful to improve the model even though they are incorrect .","Lastly , we run a test using a more concise configuration .",0.7298245614035088,0.5280898876404494,0.5909090909090909,ablation-analysis,approach
natural_language_inference,26,training,ablation study,210,49,15,"In the second block ( multi - level attention ) of the U - Net , we do not split the output of the encoded presentation and let it pass through a self - attention layer .","Lastly , we run a test using a more concise configuration .",The bidirectional attention is removed .,0.7368421052631579,0.5505617977528089,0.6818181818181818,ablation-analysis,experimental-setup
natural_language_inference,27,title,title,2,2,2,SDNET : CONTEXTUALIZED ATTENTION - BASED DEEP NETWORK FOR CONVERSATIONAL QUESTION AN - SWERING, , ,0.0131578947368421,1.0,1.0,research-problem,approach
natural_language_inference,27,abstract,abstract,4,2,2,Conversational question answering ( CQA ) is a novel QA task that requires understanding of dialogue context ., ,"Different from traditional single - turn machine reading comprehension ( MRC ) tasks , CQA includes passage comprehension , coreference resolution , and contextual understanding .",0.0263157894736842,0.25,0.25,research-problem,ablation-analysis
natural_language_inference,27,abstract,abstract,5,3,3,"Different from traditional single - turn machine reading comprehension ( MRC ) tasks , CQA includes passage comprehension , coreference resolution , and contextual understanding .",Conversational question answering ( CQA ) is a novel QA task that requires understanding of dialogue context .,"In this paper , we propose an innovated contextualized attention - based deep neural network , SDNet , to fuse context into traditional MRC models .",0.0328947368421052,0.375,0.375,research-problem,ablation-analysis
natural_language_inference,27,introduction,introduction,19,9,9,"In this paper , we propose SDNet , a contextual attention - based deep neural network for the task of conversational question answering .","Existing approaches to conversational QA tasks include BiDAF + + , , DrQA + PGNet , which all try to find the optimal answer span given the passage and dialogue history .","Our network stems from machine reading comprehension models , but has several unique characteristics to tackle contextual understanding during conversation .",0.125,0.5,0.5,model,experimental-setup
natural_language_inference,27,introduction,introduction,21,11,11,"Firstly , we apply both inter-attention and self - attention on passage and question to obtain a more effective understanding of the passage and dialogue history .","Our network stems from machine reading comprehension models , but has several unique characteristics to tackle contextual understanding during conversation .","Secondly , SDNet leverages the latest breakthrough in NLP : BERT contextual embedding .",0.1381578947368421,0.6111111111111112,0.6111111111111112,model,ablation-analysis
natural_language_inference,27,introduction,introduction,22,12,12,"Secondly , SDNet leverages the latest breakthrough in NLP : BERT contextual embedding .","Firstly , we apply both inter-attention and self - attention on passage and question to obtain a more effective understanding of the passage and dialogue history .","Different from the canonical way of appending a thin layer after BERT structure according to , we innovatively employed a weighted sum of BERT layer outputs , with locked BERT parameters .",0.1447368421052631,0.6666666666666666,0.6666666666666666,model,approach
natural_language_inference,27,introduction,introduction,23,13,13,"Different from the canonical way of appending a thin layer after BERT structure according to , we innovatively employed a weighted sum of BERT layer outputs , with locked BERT parameters .","Secondly , SDNet leverages the latest breakthrough in NLP : BERT contextual embedding .","Thirdly , we prepend previous rounds of questions and answers to the current question to incorporate contextual information .",0.1513157894736842,0.7222222222222222,0.7222222222222222,model,model
natural_language_inference,27,introduction,introduction,24,14,14,"Thirdly , we prepend previous rounds of questions and answers to the current question to incorporate contextual information .","Different from the canonical way of appending a thin layer after BERT structure according to , we innovatively employed a weighted sum of BERT layer outputs , with locked BERT parameters .",Empirical results show that each of these components has substantial gains in prediction accuracy .,0.1578947368421052,0.7777777777777778,0.7777777777777778,model,model
natural_language_inference,27,experiments,experiments,128,13,13,"As shown , SDNet achieves significantly better results than baseline models .",2,"In detail , the single SDNet model improves overall F 1 by 1.6 % , compared with previous state - of - art model on CoQA , Flow QA .",0.8421052631578947,0.4482758620689655,0.4482758620689655,results,baselines
natural_language_inference,27,experiments,experiments,129,14,14,"In detail , the single SDNet model improves overall F 1 by 1.6 % , compared with previous state - of - art model on CoQA , Flow QA .","As shown , SDNet achieves significantly better results than baseline models .","Ensemble SDNet model further improves overall F 1 score by 2.7 % , and it 's the first model to achieve over 80 % F 1 score on in - domain datasets ( 80.7 % ) .",0.8486842105263158,0.4827586206896552,0.4827586206896552,results,baselines
natural_language_inference,27,experiments,experiments,130,15,15,"Ensemble SDNet model further improves overall F 1 score by 2.7 % , and it 's the first model to achieve over 80 % F 1 score on in - domain datasets ( 80.7 % ) .","In detail , the single SDNet model improves overall F 1 by 1.6 % , compared with previous state - of - art model on CoQA , Flow QA .",shows the F 1 score on development set over epochs .,0.8552631578947368,0.5172413793103449,0.5172413793103449,results,baselines
natural_language_inference,27,experiments,experiments,132,17,17,"As seen , SDNet overpasses all but one baseline models after the second epoch , and achieves state - of - the - art results only after 8 epochs .",shows the F 1 score on development set over epochs .,Ablation Studies .,0.8684210526315791,0.5862068965517241,0.5862068965517241,results,baselines
natural_language_inference,28,title,title,2,2,2,TRACKING THE WORLD STATE WITH RECURRENT ENTITY NETWORKS, , ,0.0081967213114754,1.0,1.0,research-problem,experimental-setup
natural_language_inference,28,introduction,introduction,23,12,12,"In this paper , we investigate this problem through a simple story understanding scenario , in which the agent is given a sequence of textual statements and events , and then given another series of statements about the final state of the world .","At any given time , an agent typically receives limited information about the state of the world , and should therefore be able to infer new information through partial observation .","If the second series of statements is given in the form of questions about the final state of the world together with their correct answers , the agent should be able to learn from them and its performance can be measured by the accuracy of its answers .",0.0942622950819672,0.4,0.4,research-problem,ablation-analysis
natural_language_inference,28,introduction,introduction,28,17,17,We propose to handle this scenario with a new kind of memory - augmented neural network that uses a distributed memory and processor architecture : the Recurrent Entity Network ( EntNet ) .,"It may also learn basic rules of approximate ( logical ) inference , such as the fact that objects belonging to the same category tend to have similar properties ( light objects can be carried over from rooms to rooms for instance ) .","The model consists of a fixed number of dynamic memory cells , each containing a vector key w j and a vector value ( or content ) h j .",0.1147540983606557,0.5666666666666667,0.5666666666666667,model,baselines
natural_language_inference,28,introduction,introduction,29,18,18,"The model consists of a fixed number of dynamic memory cells , each containing a vector key w j and a vector value ( or content ) h j .",We propose to handle this scenario with a new kind of memory - augmented neural network that uses a distributed memory and processor architecture : the Recurrent Entity Network ( EntNet ) .,"Each cell is associated with its own "" processor "" , a simple gated recurrent network that may update the cell value given an input .",0.1188524590163934,0.6,0.6,model,model
natural_language_inference,28,introduction,introduction,30,19,19,"Each cell is associated with its own "" processor "" , a simple gated recurrent network that may update the cell value given an input .","The model consists of a fixed number of dynamic memory cells , each containing a vector key w j and a vector value ( or content ) h j .","If each cell learns to represent a concept or entity in the world , one can imagine a gating mechanism that , based on the key and content of the memory cells , will only modify the cells that concern the entities mentioned in the input .",0.1229508196721311,0.6333333333333333,0.6333333333333333,model,model
natural_language_inference,28,introduction,introduction,33,22,22,"Alternatively , the EntNet can be seen as a bank of gated RNNs ( all sharing the same parameters ) , whose hidden states correspond to latent concepts and attributes , and whose parameters describe the laws of the world according to which the attributes of objects are updated .","In the current version of the model , there is no direct interaction between the memory cells , hence the system can be seen as multiple identical processors functioning in parallel , with distributed local memory .","The sharing of these parameters reflects an invariance of these laws across object instances , similarly to how the weight tying scheme in a CNN reflects an invariance of image statistics across locations .",0.1352459016393442,0.7333333333333333,0.7333333333333333,model,model
natural_language_inference,28,introduction,introduction,35,24,24,"Their hidden state is updated only when new information relevant to their concept is received , and remains otherwise unchanged .","The sharing of these parameters reflects an invariance of these laws across object instances , similarly to how the weight tying scheme in a CNN reflects an invariance of image statistics across locations .","The keys used in the addressing / gating mechanism also correspond to concepts or entities , but are modified only during learning , not during inference .",0.1434426229508196,0.8,0.8,model,model
natural_language_inference,28,introduction,introduction,36,25,25,"The keys used in the addressing / gating mechanism also correspond to concepts or entities , but are modified only during learning , not during inference .","Their hidden state is updated only when new information relevant to their concept is received , and remains otherwise unchanged .","The EntNet is able to solve all 20 bAb I question - answering tasks , a popular benchmark of story understanding , which to our knowledge sets a new state - of - the - art .",0.1475409836065573,0.8333333333333334,0.8333333333333334,model,approach
natural_language_inference,28,experiments,synthetic world model task,127,4,1,SYNTHETIC WORLD MODEL TASK, , ,0.5204918032786885,0.0533333333333333,0.05,experiments,baselines
natural_language_inference,28,experiments,synthetic world model task,134,11,8,"For the MemN2N , we set the number of hops equal to T ? 2 and the embedding dimension to d = 20 .","We compared the performance of a MemN2N , LSTM and EntNet .","The EntNet had embedding dimension d = 20 and 5 memory slots , and the LSTM had 50 hidden units which resulted in it having significantly more parameters than the other two models .",0.5491803278688525,0.1466666666666666,0.4,experiments,experiments
natural_language_inference,28,experiments,synthetic world model task,135,12,9,"The EntNet had embedding dimension d = 20 and 5 memory slots , and the LSTM had 50 hidden units which resulted in it having significantly more parameters than the other two models .","For the MemN2N , we set the number of hops equal to T ? 2 and the embedding dimension to d = 20 .","For each model , we repeated the experiment with 5 different initializations and reported the best performance .",0.5532786885245902,0.16,0.45,experiments,baselines
natural_language_inference,28,experiments,synthetic world model task,137,14,11,"All models were trained with ADAM with initial learning rates set by grid search over { 0.1 , 0.01 , 0.001 } and divided by 2 every 10,000 updates .","For each model , we repeated the experiment with 5 different initializations and reported the best performance .",shows the results .,0.5614754098360656,0.1866666666666666,0.55,experiments,baselines
natural_language_inference,28,experiments,synthetic world model task,139,16,13,"The MemN2N has the worst performance , which degrades quickly as the length of the sequence increases .",shows the results .,"The LSTM performs better , but still loses accuracy as the length of the sequence increases .",0.569672131147541,0.2133333333333333,0.65,experiments,baselines
natural_language_inference,28,experiments,synthetic world model task,140,17,14,"The LSTM performs better , but still loses accuracy as the length of the sequence increases .","The MemN2N has the worst performance , which degrades quickly as the length of the sequence increases .","In contrast , the EntNet is able to solve the task in all cases .",0.5737704918032787,0.2266666666666666,0.7,experiments,baselines
natural_language_inference,28,experiments,babi tasks,171,48,25,CHILDRE N'S BOOK TEST ( CBT ),"Note that it does not store useful or correct information in the memory slots corresponding to locations , most likely because this task does not contain questions about locations ( such as "" who is in the kitchen ? "" ) .","We next evaluated our model on the Children 's Book Test , which is a semantic language modeling ( sentence completion ) benchmark built from children 's books that are freely available from Project Gutenberg 3 .",0.7008196721311475,0.64,0.4807692307692308,experiments,model
natural_language_inference,28,experiments,babi tasks,175,52,29,"It was shown in that methods with limited memory such as LSTMs perform well on more frequent , syntax based words such as prepositions and verbs , being similar to human performance , but poorly relative to humans on more semantically meaningful words such as named entities and common nouns .","More specifically , each sample consists of a tuple ( S , q , C , a ) where S is the story consisting of 20 sentences , Q is the 21st sentence with one word replaced by a special blank token , C is a set of 10 candidate answers of the same type as the missing word ( for example , common nouns or named entities ) , and a is the true answer ( which is always contained in C ) .","Therefore , most recent methods have been evaluated on the Named Entity and Common Noun subtasks , since they better test the ability of a model to make use of wider contextual information .",0.7172131147540983,0.6933333333333334,0.5576923076923077,experiments,baselines
natural_language_inference,29,title,title,2,2,2,PHASE CONDUCTOR ON MULTI - LAYERED ATTEN - TIONS FOR MACHINE COMPREHENSION, , ,0.0130718954248366,1.0,1.0,research-problem,baselines
natural_language_inference,29,introduction,introduction,13,3,3,"Benefiting from the availability of large - scale benchmark datasets such as SQuAD , the attention - based neural networks has spread to machine comprehension and question answering tasks to allow the model to attend over past output vectors .","Attention - based neural networks have demonstrated success in a wide range of NLP tasks ranging from neural machine translation , image captioning , and speech recognition .",uses attention mechanism in Pointer Network to detect an answer boundary by predicting the start and the end indices in the passage .,0.0849673202614379,0.1363636363636363,0.1363636363636363,research-problem,ablation-analysis
natural_language_inference,29,introduction,introduction,19,9,9,"Inspired by the above - mentioned works , we are proposing to introduce a general framework PhaseCond for the use of multiple attention layers .",Both and employs the structure of multi-hops or iterative aligner to repeatedly fuse the passage representation with the question representation as well as the passage representation itself .,There are two motivations .,0.1241830065359477,0.4090909090909091,0.4090909090909091,model,experimental-setup
natural_language_inference,29,introduction,introduction,21,11,11,"First , previous research on the self - attention model is to purely capture long - distance dependencies , and therefore a multi-hops architecture is used to alternatively captures question - aware passage representations and refines the results by using a self - attention model .",There are two motivations .,"In contrast to the multi-hops and interactive architecture , our motivation of using the self - attention model for machine comprehension is to propagate answer evidence which is derived from the preceding question - passage representation layers .",0.1372549019607843,0.5,0.5,model,ablation-analysis
natural_language_inference,29,introduction,introduction,24,14,14,"Second , unlike the domains such as machine translation which jointly align and translate words , question - passage attention models for machine comprehension and question answering calculate the alignment matrix corresponding to all question and passage word pairs .","This perspective leads to a different attention - based architecture containing two sequential phases , question - aware passage representation phase and evidence propagation phase. , RNET , MReader , and PhaseCond ( our proposed model ) .","Despite the attention models ' success on the machine comprehension task , there has not been any other work exploring learning to encode multiple representations of question or passage from different perspectives for different parts of attention functions .",0.1568627450980392,0.6363636363636364,0.6363636363636364,model,model
natural_language_inference,29,experiments and analysis,main results of model comparison,126,30,17,"The EM result of our baseline Iterative Aligner is lower than RNET , confirming that the problem is not caused by our proposed model .","We draw the same conclusion for the ensemble model setting , despite that the RNET works better on the Dev EM measure .","Our explanations is that 1 ) RNET uses a different feature set ( e.g. , Glo Ve 300 dimensional word vectors are employed ) and different encoding steps ( e.g. , three GRU layers are used for encoding question and passage representations ) , and 2 ) RNET uses a different ensemble method from our implementation .",0.8235294117647058,0.6,0.7083333333333334,results,baselines
natural_language_inference,29,experiments and analysis,main results of model comparison,127,31,18,"Our explanations is that 1 ) RNET uses a different feature set ( e.g. , Glo Ve 300 dimensional word vectors are employed ) and different encoding steps ( e.g. , three GRU layers are used for encoding question and passage representations ) , and 2 ) RNET uses a different ensemble method from our implementation .","The EM result of our baseline Iterative Aligner is lower than RNET , confirming that the problem is not caused by our proposed model .",shows the performance with different number of layers for both question - passage attention phase and self - attention phase .,0.8300653594771242,0.62,0.75,results,experimental-setup
natural_language_inference,29,experiments and analysis,main results of model comparison,128,32,19,shows the performance with different number of layers for both question - passage attention phase and self - attention phase .,"Our explanations is that 1 ) RNET uses a different feature set ( e.g. , Glo Ve 300 dimensional word vectors are employed ) and different encoding steps ( e.g. , three GRU layers are used for encoding question and passage representations ) , and 2 ) RNET uses a different ensemble method from our implementation .",We change the layer number separately to compare the performance .,0.8366013071895425,0.64,0.7916666666666666,results,approach
natural_language_inference,29,experiments and analysis,main results of model comparison,130,34,21,"For the question - passage attention phase , using single layer does n't degrade the performance significantly from the default setting of two layers , resulting in a different conclusion from ; .",We change the layer number separately to compare the performance .,"Intuitively , this is largely expected because representing the passage repeatedly with the same question does n't constantly add more information .",0.8496732026143791,0.68,0.875,results,baselines
natural_language_inference,29,experiments and analysis,main results of model comparison,132,36,23,"In contrast , multiple stacking layers are needed to allow the evidence fully propagated through the passage .","Intuitively , this is largely expected because representing the passage repeatedly with the same question does n't constantly add more information .",This is exactly what we observed in two stacking layered self - attention phase .,0.8627450980392157,0.72,0.9583333333333334,results,model
natural_language_inference,3,title,title,2,2,2,Exploring Question Understanding and Adaptation in Neural - Network - Based Question Answering, , ,0.0092592592592592,1.0,1.0,research-problem,experimental-setup
natural_language_inference,3,abstract,abstract,4,2,2,The last several years have seen intensive interest in exploring neural - networkbased models for machine comprehension ( MC ) and question answering ( QA ) ., ,"In this paper , we approach the problems by closely modelling questions in a neural network framework .",0.0185185185185185,0.3333333333333333,0.3333333333333333,research-problem,ablation-analysis
natural_language_inference,3,introduction,introduction,13,5,5,"In this paper , we take a closer look at modeling questions in such an end - to - end neural network framework , since we regard question understanding is of importance for such problems .","The recent availability of relatively large training datasets ( see Section 2 for more details ) has made it more feasible to train and estimate rather complex models in an end - to - end fashion for these problems , in which a whole model is fit directly with given question - answer tuples and the resulting model has shown to be rather effective .",We first introduced syntactic information to help encode questions .,0.0601851851851851,0.0328947368421052,0.0649350649350649,model,ablation-analysis
natural_language_inference,3,introduction,introduction,14,6,6,We first introduced syntactic information to help encode questions .,"In this paper , we take a closer look at modeling questions in such an end - to - end neural network framework , since we regard question understanding is of importance for such problems .",We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them .,0.0648148148148148,0.0394736842105263,0.0779220779220779,model,approach
natural_language_inference,3,introduction,introduction,15,7,7,We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them .,We first introduced syntactic information to help encode questions .,"On the Stanford Question Answering Dataset ( SQuAD ) , we show that these approaches can help attain better results on our competitive baselines .",0.0694444444444444,0.0460526315789473,0.0909090909090909,model,approach
natural_language_inference,3,experiment results,set up,163,3,2,We test our models on Stanford Question Answering Dataset ( SQuAD ) ., ,"The SQuAD dataset consists of more than 100,000 questions annotated by crowdsourcing workers on a selected set of Wikipedia articles , and the answer to each question is a span of text in the Wikipedia articles .",0.7546296296296297,0.0588235294117647,0.0714285714285714,experimental-setup,approach
natural_language_inference,3,experiment results,set up,164,4,3,"The SQuAD dataset consists of more than 100,000 questions annotated by crowdsourcing workers on a selected set of Wikipedia articles , and the answer to each question is a span of text in the Wikipedia articles .",We test our models on Stanford Question Answering Dataset ( SQuAD ) .,"Training data includes 87,599 instances and validation set has 10,570 instances .",0.7592592592592593,0.0784313725490196,0.1071428571428571,experimental-setup,approach
natural_language_inference,3,experiment results,set up,168,8,7,We use pre-trained 300 - D Glove 840B vectors to initialize our word embeddings .,The evaluation of SQuAD is Exact Match ( EM ) and F1 score .,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,0.7777777777777778,0.1568627450980392,0.25,experimental-setup,experimental-setup
natural_language_inference,3,experiment results,set up,169,9,8,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,We use pre-trained 300 - D Glove 840B vectors to initialize our word embeddings .,"CharCNN filter length is 1 , 3 , 5 , each is 50 dimensions .",0.7824074074074074,0.1764705882352941,0.2857142857142857,experimental-setup,experimental-setup
natural_language_inference,3,experiment results,set up,170,10,9,"CharCNN filter length is 1 , 3 , 5 , each is 50 dimensions .",Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,All vectors including word embedding are updated during training .,0.7870370370370371,0.196078431372549,0.3214285714285714,experimental-setup,experimental-setup
natural_language_inference,3,experiment results,set up,172,12,11,The cluster number K in discriminative block is 100 .,All vectors including word embedding are updated during training .,The Adam method is used for optimization .,0.7962962962962963,0.2352941176470588,0.3928571428571429,experimental-setup,experimental-setup
natural_language_inference,3,experiment results,set up,173,13,12,The Adam method is used for optimization .,The cluster number K in discriminative block is 100 .,And the first momentum is set to be 0.9 and the second 0.999 .,0.8009259259259259,0.2549019607843137,0.4285714285714285,experimental-setup,approach
natural_language_inference,3,experiment results,set up,174,14,13,And the first momentum is set to be 0.9 and the second 0.999 .,The Adam method is used for optimization .,The initial learning rate is 0.0004 and the batch size is 32 .,0.8055555555555556,0.2745098039215686,0.4642857142857143,experimental-setup,experimental-setup
natural_language_inference,3,experiment results,set up,175,15,14,The initial learning rate is 0.0004 and the batch size is 32 .,And the first momentum is set to be 0.9 and the second 0.999 .,"We will half learning rate when meet a bad iteration , and the patience is 7 .",0.8101851851851852,0.2941176470588235,0.5,experimental-setup,experiments
natural_language_inference,3,experiment results,set up,178,18,17,"All hidden states of GRUs , and TreeLSTMs are 500 dimensions , while word - level embedding d w is 300 dimensions .",Our early stop evaluation is the EM and F 1 score of validation set .,"We set max length of document to 500 , and drop the question - document pairs beyond this on training set .",0.8240740740740741,0.3529411764705882,0.6071428571428571,experimental-setup,baselines
natural_language_inference,3,experiment results,set up,179,19,18,"We set max length of document to 500 , and drop the question - document pairs beyond this on training set .","All hidden states of GRUs , and TreeLSTMs are 500 dimensions , while word - level embedding d w is 300 dimensions .",Explicit question - type dimension d ET is 50 .,0.8287037037037037,0.3725490196078432,0.6428571428571429,experimental-setup,approach
natural_language_inference,3,experiment results,set up,180,20,19,Explicit question - type dimension d ET is 50 .,"We set max length of document to 500 , and drop the question - document pairs beyond this on training set .",We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5 .,0.8333333333333334,0.392156862745098,0.6785714285714286,experimental-setup,baselines
natural_language_inference,3,experiment results,set up,181,21,20,We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5 .,Explicit question - type dimension d ET is 50 .,Model,0.8379629629629629,0.4117647058823529,0.7142857142857143,experimental-setup,experimental-setup
natural_language_inference,3,experiment results,results,192,32,3,"Our model achieves a 68.73 % EM score and 77.39 % F1 score , which is ranked among the state of the art single models ( without model ensembling shows the ablation performances of various Q- code on the development set .",Overall results shows the official leaderboard on SQuAD test set when we submitted our system .,"Note that since the testset is hidden from us , we can only perform such an analysis on the development set .",0.8888888888888888,0.6274509803921569,0.1363636363636363,results,baselines
natural_language_inference,3,experiment results,results,194,34,5,"Our baseline model using no Q- code achieved a 68.00 % and 77.36 % EM and F 1 scores , respectively .","Note that since the testset is hidden from us , we can only perform such an analysis on the development set .","When we added the explicit question type T - code into the baseline model , the performance was improved slightly to 68.16 % ( EM ) and 77.58 % ( F1 ) .",0.8981481481481481,0.6666666666666666,0.2272727272727273,results,baselines
natural_language_inference,3,experiment results,results,195,35,6,"When we added the explicit question type T - code into the baseline model , the performance was improved slightly to 68.16 % ( EM ) and 77.58 % ( F1 ) .","Our baseline model using no Q- code achieved a 68.00 % and 77.36 % EM and F 1 scores , respectively .","We then used TreeLSTM introduce syntactic parses for question representation and understanding ( replacing simple question type as question understanding Q-code ) , which consistently shows further improvement .",0.9027777777777778,0.6862745098039216,0.2727272727272727,results,approach
natural_language_inference,3,experiment results,results,196,36,7,"We then used TreeLSTM introduce syntactic parses for question representation and understanding ( replacing simple question type as question understanding Q-code ) , which consistently shows further improvement .","When we added the explicit question type T - code into the baseline model , the performance was improved slightly to 68.16 % ( EM ) and 77.58 % ( F1 ) .",We further incorporated the soft adaptation .,0.9074074074074074,0.7058823529411765,0.3181818181818182,results,approach
natural_language_inference,3,experiment results,results,204,44,15,"Take our best model as an example , we observed a 78.38 % F1 score on the whole development set , which can be separated into two parts : one is where F1 score equals to 100 % , which means an exact match .",shows the composition of F1 score .,This part accounts for 69 . 10 % of the entire development set .,0.9444444444444444,0.8627450980392157,0.6818181818181818,results,baselines
natural_language_inference,30,title,title,2,2,2,Convolutional Neural Network Architectures for Matching Natural Language Sentences, , ,0.0104166666666666,1.0,1.0,research-problem,experimental-setup
natural_language_inference,30,abstract,abstract,4,2,2,"Semantic matching is of central importance to many natural language tasks [ 2,28 ] .", ,A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them .,0.0208333333333333,0.2857142857142857,0.2857142857142857,research-problem,baselines
natural_language_inference,30,introduction,introduction,11,2,2,Matching two potentially heterogenous language objects is central to many natural language applications ., ,"It generalizes the conventional notion of similarity ( e.g. , in paraphrase identification ) or relevance ( e.g. , in information retrieval ) , since it aims to model the correspondence between "" linguistic objects "" of different nature at different levels of abstractions .",0.0572916666666666,0.1176470588235294,0.1176470588235294,research-problem,approach
natural_language_inference,30,introduction,introduction,14,5,5,"Natural language sentences have complicated structures , both sequential and hierarchical , that are essential for understanding them .","Examples include top -k re-ranking in machine translation ( e.g. , comparing the meanings of a French sentence and an English sentence ) and dialogue ( e.g. , evaluating the appropriateness of a response to a given utterance ) .",A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .,0.0729166666666666,0.2941176470588235,0.2941176470588235,model,ablation-analysis
natural_language_inference,30,introduction,introduction,15,6,6,A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .,"Natural language sentences have complicated structures , both sequential and hierarchical , that are essential for understanding them .","Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .",0.078125,0.3529411764705882,0.3529411764705882,model,approach
natural_language_inference,30,introduction,introduction,16,7,7,"Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .",A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0.0833333333333333,0.4117647058823529,0.4117647058823529,model,approach
natural_language_inference,30,introduction,introduction,17,8,8,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .","Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .","Our model is generic , requiring no prior knowledge of natural language ( e.g. , parse tree ) and putting essentially no constraints on the matching tasks .",0.0885416666666666,0.4705882352941176,0.4705882352941176,model,experimental-setup
natural_language_inference,30,introduction,introduction,18,9,9,"Our model is generic , requiring no prior knowledge of natural language ( e.g. , parse tree ) and putting essentially no constraints on the matching tasks .","To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",This is part of our continuing effort 1 in understanding natural language objects and the matching between them .,0.09375,0.5294117647058824,0.5294117647058824,model,approach
natural_language_inference,30,experiments,experiment i sentence completion,148,12,1,Experiment I : Sentence Completion, , ,0.7708333333333334,0.2727272727272727,0.0666666666666666,experiments,approach
natural_language_inference,30,experiments,experiment i sentence completion,157,21,10,"ARC - II outperforms ARC - I significantly , showing the power of joint modeling of matching and sentence meaning .","The two proposed models get nearly half of the cases right 5 , with large margin over other sentence models and models without explicit sequence modeling .","As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .",0.8177083333333334,0.4772727272727273,0.6666666666666666,experiments,baselines
natural_language_inference,30,experiments,experiment i sentence completion,158,22,11,"As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .","ARC - II outperforms ARC - I significantly , showing the power of joint modeling of matching and sentence meaning .","It is a bit surprising that URAE comes last on this task , which might be caused by the facts that 1 ) the representation model ( including word - embedding ) is not trained on Reuters , and 2 ) the split - sentence setting hurts the parsing , which is vital to the quality of learned sentence representation ..",0.8229166666666666,0.5,0.7333333333333333,experiments,baselines
natural_language_inference,30,experiments,experiment iii paraphrase identification,164,28,1,Experiment III : Paraphrase Identification, , ,0.8541666666666666,0.6363636363636364,0.1428571428571428,experiments,approach
natural_language_inference,30,experiments,experiment iii paraphrase identification,170,34,7,"Nevertheless , our generic matching models still manage to perform reasonably well , achieving an accuracy and F1 score close to the best performer in 2008 based on hand - crafted features , but still significantly lower than the state - of - the - art ( 76.8%/83.6 % ) , achieved with unfolding - RAE and other features designed for this task .","As stated earlier , our model is not specially tailored for modeling synonymy , and generally requires ? 100K instances to work favorably .", ,0.8854166666666666,0.7727272727272727,1.0,experiments,baselines
natural_language_inference,31,title,title,2,2,2,Scaling Memory - Augmented Neural Networks with Sparse Reads and Writes, , ,0.0058651026392961,1.0,1.0,research-problem,baselines
natural_language_inference,31,introduction,introduction,15,5,5,We refer to this class of models as memory augmented neural networks ( MANNs ) .,"Recent approaches , such as Neural Turing Machines ( NTMs ) and Memory Networks , have addressed this issue by decoupling the memory capacity from the number of model parameters .","External memory allows MANNs to learn algorithmic solutions to problems that have eluded the capabilities of traditional LSTMs , and to generalize to longer sequence lengths .",0.0439882697947214,0.25,0.25,research-problem,approach
natural_language_inference,31,introduction,introduction,23,13,13,"In this paper , we present a MANN named SAM ( sparse access memory ) .","30 MiB physical memory ; to store 64,000 memories the overhead exceeds 29 GiB ( see ) .","By thresholding memory modifications to a sparse subset , and using efficient data structures for content - based read operations , our model is optimal in space and time with respect to memory size , while retaining end - to - end gradient based optimization .",0.0674486803519061,0.65,0.65,model,experimental-setup
natural_language_inference,31,introduction,introduction,24,14,14,"By thresholding memory modifications to a sparse subset , and using efficient data structures for content - based read operations , our model is optimal in space and time with respect to memory size , while retaining end - to - end gradient based optimization .","In this paper , we present a MANN named SAM ( sparse access memory ) .","To test whether the model is able to learn with this sparse approximation , we examined its performance on a selection of synthetic and natural tasks : algorithmic tasks from the NTM work , Babi reasoning tasks used with Memory Networks and Omniglot one - shot classification .",0.0703812316715542,0.7,0.7,model,baselines
natural_language_inference,31,results,learning with sparse memory access,137,3,1,Learning with sparse memory access, , ,0.4017595307917888,0.0491803278688524,0.0909090909090909,results,baselines
natural_language_inference,31,results,learning with sparse memory access,145,11,9,"shows that sparse models are able to learn with comparable efficiency to the dense models and , surprisingly , learn more effectively for some tasks - notably priority sort and associative recall .",We chose these tasks because the NTM is known to perform well on them .,This shows that sparse reads and writes can actually benefit early - stage learning in some cases .,0.4252199413489736,0.180327868852459,0.8181818181818182,results,baselines
natural_language_inference,31,results,scaling with a curriculum,148,14,1,Scaling with a curriculum, , ,0.4340175953079179,0.2295081967213115,0.0625,results,baselines
natural_language_inference,31,results,scaling with a curriculum,160,26,13,"For all tasks , SAM was able to advance further than the other models , and in the associative recall task , SAM was able to advance through the curriculum to sequences greater than 4000 ( ) .",These sizes were chosen to ensure all models use approximately the same amount of physical memory when trained over 100 steps .,"Note that we did not use truncated backpropagation , so this involved BPTT for over 4000 steps with a memory size in the millions of words .",0.469208211143695,0.4262295081967213,0.8125,results,baselines
natural_language_inference,31,results,question answering on the babi tasks,164,30,1,Question answering on the Babi tasks, , ,0.4809384164222874,0.4918032786885246,0.0909090909090909,results,approach
natural_language_inference,31,results,question answering on the babi tasks,169,35,6,"The MANNs , except the NTM , are able to learn solutions comparable to the previous best results , failing at only 2 of the tasks .",The full results and training details are described in Supplementary G.,"The SDNC manages to solve all but 1 of the tasks , the best reported result on Babi that we are aware of .",0.4956011730205279,0.5737704918032787,0.5454545454545454,results,approach
natural_language_inference,31,results,question answering on the babi tasks,170,36,7,"The SDNC manages to solve all but 1 of the tasks , the best reported result on Babi that we are aware of .","The MANNs , except the NTM , are able to learn solutions comparable to the previous best results , failing at only 2 of the tasks .",Notably the best prior results have been obtained by using supervising the memory retrieval ( during training the model is provided annotations which indicate which memories should be used to answer a query ) .,0.498533724340176,0.5901639344262295,0.6363636363636364,results,approach
natural_language_inference,31,results,question answering on the babi tasks,174,40,11,We believe the NTM may perform poorly since it lacks a mechanism which allows it to allocate memory effectively .,"Both the sparse and dense perform comparably at this task , again indicating the sparse approximations do not impair learning .", ,0.5102639296187683,0.6557377049180327,1.0,results,approach
natural_language_inference,31,results,learning on real world data,175,41,1,Learning on real world data, , ,0.5131964809384164,0.6721311475409836,0.0476190476190476,results,approach
natural_language_inference,31,results,learning on real world data,187,53,13,"SAM outperformed other models , presumably due to its much larger memory capacity .",4 longer than seen during training .,"Previous results on the Omniglot curriculum task are not identical , since we used 1 - hot labels throughout and the training curriculum scaled to longer sequences , but our results with the dense models are comparable (?",0.5483870967741935,0.8688524590163934,0.6190476190476191,results,approach
natural_language_inference,31,results,learning on real world data,192,58,18,All of the MANNs were able to perform much better than chance with ?,The characters used in the test set were not used in validation or training .,"500 characters ( sequence lengths of ? 5000 ) , even though they were trained , at most , on sequences of ? 130 ( chance is 0.002 for 500 characters ) .",0.5630498533724341,0.9508196721311476,0.8571428571428571,results,approach
natural_language_inference,4,title,title,2,2,2,MemoReader : Large - Scale Reading Comprehension through Neural Memory Controller, , ,0.0102564102564102,1.0,1.0,research-problem,baselines
natural_language_inference,4,abstract,abstract,4,2,2,Machine reading comprehension helps machines learn to utilize most of the human knowledge written in the form of text ., ,"Existing approaches made a significant progress comparable to human - level performance , but they are still limited in understanding , up to a few paragraphs , failing to properly comprehend lengthy document .",0.0205128205128205,0.25,0.25,research-problem,baselines
natural_language_inference,4,abstract,abstract,6,4,4,"In this paper , we propose a novel deep neural network architecture to handle a long - range dependency in RC tasks .","Existing approaches made a significant progress comparable to human - level performance , but they are still limited in understanding , up to a few paragraphs , failing to properly comprehend lengthy document .","In detail , our method has two novel aspects : ( 1 ) an advanced memory - augmented architecture and ( 2 ) an expanded gated recurrent unit with dense connections that mitigate potential information distortion occurring in the memory .",0.0307692307692307,0.5,0.5,research-problem,baselines
natural_language_inference,4,introduction,introduction,13,3,3,Reading comprehension ( RC ) to understand this knowledge is a major challenge that can vastly increase the range of knowledge available to the machines .,Most of the human knowledge has been stored in the form of text .,"Many neural networkbased methods have been proposed , pushing performance close to a human level .",0.0666666666666666,0.1304347826086956,0.1304347826086956,research-problem,ablation-analysis
natural_language_inference,4,introduction,introduction,23,13,13,"To overcome this issue , we propose two novel strategies that improve the memory - handling capability while mitigating the information distortion .","However , we found that the memory controller is susceptible to information distortion as neural networks become deeper , this distortion can hinder the performance .",We extend the memory controller with a residual connection to alleviate the information distortion occurring in it .,0.1179487179487179,0.5652173913043478,0.5652173913043478,model,baselines
natural_language_inference,4,introduction,introduction,24,14,14,We extend the memory controller with a residual connection to alleviate the information distortion occurring in it .,"To overcome this issue , we propose two novel strategies that improve the memory - handling capability while mitigating the information distortion .",We also expand the gated recurrent unit ( GRU ) with a dense connection that conveys enriched features to the next layer containing the original as well as the transformed information .,0.123076923076923,0.6086956521739131,0.6086956521739131,model,approach
natural_language_inference,4,introduction,introduction,25,15,15,We also expand the gated recurrent unit ( GRU ) with a dense connection that conveys enriched features to the next layer containing the original as well as the transformed information .,We extend the memory controller with a residual connection to alleviate the information distortion occurring in it .,"We conducted extensive experiments through several benchmark datasets such as TriviaQA , QUASAR - T , and SQ u AD .",0.1282051282051282,0.6521739130434783,0.6521739130434783,model,experimental-setup
natural_language_inference,4,experimental setup,experimental setup,120,18,18,Implementation details . to build the model and Sonnet 2 to implement the memory interface .,"In QUASAR - T , we follow the same preprocessing steps done by .",NLTK is used for tokenizing words .,0.6153846153846154,0.72,0.72,experimental-setup,approach
natural_language_inference,4,experimental setup,experimental setup,121,19,19,NLTK is used for tokenizing words .,Implementation details . to build the model and Sonnet 2 to implement the memory interface .,"In the memory controller , we use four read heads and one write head , and the memory size is set to 100 36 , with all initialized as 0 .",0.6205128205128205,0.76,0.76,experimental-setup,approach
natural_language_inference,4,experimental setup,experimental setup,122,20,20,"In the memory controller , we use four read heads and one write head , and the memory size is set to 100 36 , with all initialized as 0 .",NLTK is used for tokenizing words .,The hidden vector dimension l is set to 200 .,0.6256410256410256,0.8,0.8,experimental-setup,baselines
natural_language_inference,4,experimental setup,experimental setup,123,21,21,The hidden vector dimension l is set to 200 .,"In the memory controller , we use four read heads and one write head , and the memory size is set to 100 36 , with all initialized as 0 .","We use AdaDelta ( Zeiler , 2012 ) as an optimizer with a learning rate of 0.5 .",0.6307692307692307,0.84,0.84,experimental-setup,experimental-setup
natural_language_inference,4,experimental setup,experimental setup,124,22,22,"We use AdaDelta ( Zeiler , 2012 ) as an optimizer with a learning rate of 0.5 .",The hidden vector dimension l is set to 200 .,The batch size is set to 20 for TriviaQA and 30 for SQuAD and QUASAR - T .,0.6358974358974359,0.88,0.88,experimental-setup,experiments
natural_language_inference,4,experimental setup,experimental setup,125,23,23,The batch size is set to 20 for TriviaQA and 30 for SQuAD and QUASAR - T .,"We use AdaDelta ( Zeiler , 2012 ) as an optimizer with a learning rate of 0.5 .",We use an exponential moving average of weights with a decaying factor of 0.001 .,0.6410256410256411,0.92,0.92,experimental-setup,experimental-setup
natural_language_inference,4,experimental setup,experimental setup,126,24,24,We use an exponential moving average of weights with a decaying factor of 0.001 .,The batch size is set to 20 for TriviaQA and 30 for SQuAD and QUASAR - T .,"Our model does require more memory than existing methods , but a single GPU ( e.g. , M40 with 12 GB memory ) was enough to train model within a reasonable amount of time .",0.6461538461538462,0.96,0.96,experimental-setup,experimental-setup
natural_language_inference,4,experimental setup,experimental setup,127,25,25,"Our model does require more memory than existing methods , but a single GPU ( e.g. , M40 with 12 GB memory ) was enough to train model within a reasonable amount of time .",We use an exponential moving average of weights with a decaying factor of 0.001 ., ,0.6512820512820513,1.0,1.0,experimental-setup,approach
natural_language_inference,4,quantitative results,quantitative results,131,4,4,"Overall , in lengthy - document cases such as Trivi aQA and QUASAR - T , our model outperforms all the published results , as seen in Tables 2 and 3 , while in the short - document case such as SQuAD , we mostly achieve the best results , as seen in .","In TriviaQA and QUASAR - T dataset , we compare our model with BiDAF as well as its variant called ' BiDAF + DNC , ' which is augmented with an existing external memory architecture just before the answer prediction layer in the BiDAF .","In the following , we present detailed analyses on each dataset .",0.6717948717948717,0.0727272727272727,0.0727272727272727,results,model
natural_language_inference,4,quantitative results,quantitative results,164,37,37,We assume that the concatenation of the layer outputs in DEBS helps the memory controller store contextual representations clearly .,Ablation study with an encoder block .,"To see how DEBS affects the memory controller depending on different positions in the entire network , we conducted an ablation study by replacing the encoder block with DEBS on SQuAD .",0.8410256410256409,0.6727272727272727,0.6727272727272727,ablation-analysis,approach
natural_language_inference,4,quantitative results,quantitative results,166,39,39,"As can be seen in , using DEBS in all the places improves the performance most , and furthermore , the memory controller with DEBS gives the largest performance margin .","To see how DEBS affects the memory controller depending on different positions in the entire network , we conducted an ablation study by replacing the encoder block with DEBS on SQuAD .","This implies that DEBS can generally work as a better alternative to a BiGRU module , and DEBS is critical in maintaining the high performance of our proposed memory controller .",0.8512820512820513,0.7090909090909091,0.7090909090909091,ablation-analysis,approach
natural_language_inference,5,title,title,2,2,2,Sentence Similarity Learning by Lexical Decomposition and Composition, , ,0.0078740157480314,1.0,1.0,research-problem,approach
natural_language_inference,5,abstract,abstract,4,2,2,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences .", ,"In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences .",0.0157480314960629,0.0392156862745098,0.0392156862745098,research-problem,ablation-analysis
natural_language_inference,5,abstract,abstract,45,43,43,"In this paper , we propose a novel model to tackle all these challenges jointly by decomposing and composing lexical semantics over sentences .","attention in the past , the only related work of explored the dissimilarity between sentences in a pair for paraphrase identification task , but they require human annotations in order to train a classifier , and their performance is still below the state of the art .","Given a sentence pair , the model represents each word as a low -dimensional vector ( challenge 1 ) , and calculates a semantic matching vector for each word based on all words in the other sentence ( challenge 2 ) .",0.1771653543307086,0.8431372549019608,0.8431372549019608,model,baselines
natural_language_inference,5,abstract,abstract,46,44,44,"Given a sentence pair , the model represents each word as a low -dimensional vector ( challenge 1 ) , and calculates a semantic matching vector for each word based on all words in the other sentence ( challenge 2 ) .","In this paper , we propose a novel model to tackle all these challenges jointly by decomposing and composing lexical semantics over sentences .","Then based on the semantic matching vector , each word vector is decomposed into two components : a similar component and a dissimilar component ( challenge 3 ) .",0.1811023622047244,0.8627450980392157,0.8627450980392157,model,model
natural_language_inference,5,abstract,abstract,47,45,45,"Then based on the semantic matching vector , each word vector is decomposed into two components : a similar component and a dissimilar component ( challenge 3 ) .","Given a sentence pair , the model represents each word as a low -dimensional vector ( challenge 1 ) , and calculates a semantic matching vector for each word based on all words in the other sentence ( challenge 2 ) .","We use similar components of all the words to represent the similar parts of the sentence pair , and dissimilar components of every word to model the dissimilar parts explicitly .",0.1850393700787401,0.8823529411764706,0.8823529411764706,model,approach
natural_language_inference,5,abstract,abstract,48,46,46,"We use similar components of all the words to represent the similar parts of the sentence pair , and dissimilar components of every word to model the dissimilar parts explicitly .","Then based on the semantic matching vector , each word vector is decomposed into two components : a similar component and a dissimilar component ( challenge 3 ) .","After this , a two - channel CNN operation is performed to compose the similar and dissimilar components into a feature vector ( challenge 2 and 3 ) .",0.1889763779527559,0.9019607843137256,0.9019607843137256,model,approach
natural_language_inference,5,abstract,abstract,49,47,47,"After this , a two - channel CNN operation is performed to compose the similar and dissimilar components into a feature vector ( challenge 2 and 3 ) .","We use similar components of all the words to represent the similar parts of the sentence pair , and dissimilar components of every word to model the dissimilar parts explicitly .","Finally , the composed feature vector is utilized to predict the sentence similarity .",0.1929133858267716,0.9215686274509804,0.9215686274509804,model,baselines
natural_language_inference,5,abstract,abstract,50,48,48,"Finally , the composed feature vector is utilized to predict the sentence similarity .","After this , a two - channel CNN operation is performed to compose the similar and dissimilar components into a feature vector ( challenge 2 and 3 ) .","Experimental results on two tasks show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task .",0.1968503937007874,0.9411764705882352,0.9411764705882352,model,approach
natural_language_inference,5,experiment,comparing with state of the art models,202,43,3,QASent dataset .,"In this subsection , we evaluated our model on the test sets of QASent , WikiQA and MSRP .","presents the performances of the state - of - the - art systems and our model , where the performances were evaluated with the standard trec eval - 8.1 script",0.7952755905511811,0.5308641975308642,0.073170731707317,experiments,approach
natural_language_inference,5,experiment,comparing with state of the art models,207,48,8,"After adding some word overlap features between the two sentences , the performance was improved significantly ( the third row of ) .","Because only the sentencelevel granularity was used , the performance is much lower ( the second row of ) .","Therefore , the lower - level granularity is an indispensable factor for a good performance .",0.8149606299212598,0.5925925925925926,0.1951219512195122,experiments,approach
natural_language_inference,5,experiment,comparing with state of the art models,214,55,15,"We can see that our model ( the last row of ) got the best MAP among all previous work , and a comparable MRR than dos .",Our model makes use of all these useful factors and also considers the dissimilarities of a sentence pair .,Wiki QA dataset .,0.84251968503937,0.6790123456790124,0.3658536585365853,experiments,baselines
natural_language_inference,5,experiment,comparing with state of the art models,215,56,16,Wiki QA dataset .,"We can see that our model ( the last row of ) got the best MAP among all previous work , and a comparable MRR than dos .",presents the results of our model and several state - of - the - art models .,0.8464566929133859,0.6913580246913579,0.3902439024390244,experiments,approach
natural_language_inference,5,experiment,comparing with state of the art models,218,59,19,The best performance ( shown at the second row of ) was acquired by a bigram CNN model combining with the word overlap features .,constructed the dataset and reimplemented several baseline models .,models the sentence similarity by enriching LSTMs with a latent stochastic attention mechanism .,0.8582677165354331,0.7283950617283951,0.4634146341463415,experiments,baselines
natural_language_inference,5,experiment,comparing with state of the art models,225,66,26,MSRP dataset .,The last row of shows that our model is more effective than the other models .,granularity and modeled interaction features at each level for a pair of sentences .,0.8858267716535433,0.8148148148148148,0.6341463414634146,experiments,baselines
natural_language_inference,5,experiment,comparing with state of the art models,237,78,38,"Comparing to these neural network based models , our model obtained a comparable performance ( the last row of ) without using any sparse features , extra annotated resources and specific training strategies .","By adding a couple of sparse features and using a layerwise training strategy , they got a pretty good performance .","However , the best performance so far on this dataset is obtained by .",0.9330708661417324,0.9629629629629628,0.9268292682926828,experiments,baselines
natural_language_inference,6,title,title,2,2,2,Dynamic Self - Attention : Computing Attention over Words Dynamically for Sentence Embedding, , ,0.0144927536231884,1.0,1.0,research-problem,approach
natural_language_inference,6,abstract,abstract,4,2,2,"In this paper , we propose Dynamic Self - Attention ( DSA ) , a new self - attention mechanism for sentence embedding .", ,"We design DSA by modifying dynamic routing in capsule network ( Sabour et al. , 2017 ) for natural language processing .",0.0289855072463768,0.4,0.4,research-problem,baselines
natural_language_inference,6,abstract,abstract,5,3,3,"We design DSA by modifying dynamic routing in capsule network ( Sabour et al. , 2017 ) for natural language processing .","In this paper , we propose Dynamic Self - Attention ( DSA ) , a new self - attention mechanism for sentence embedding .",DSA attends to informative words with a dynamic weight vector .,0.036231884057971,0.6,0.6,research-problem,approach
natural_language_inference,6,introduction,introduction,23,16,16,"Motivated by dynamic routing ) , we propose a new self - attention mechanism for sentence embedding , namely Dynamic Self - Attention ( DSA ) .",We expect the dynamic weight vector to give rise to flexibility in self - attention since it can adapt to given sentences even after training .,"To this end , we modify dynamic routing such that it functions as self - attention with the dynamic weight vector .",0.1666666666666666,0.6956521739130435,0.6956521739130435,model,ablation-analysis
natural_language_inference,6,introduction,introduction,24,17,17,"To this end , we modify dynamic routing such that it functions as self - attention with the dynamic weight vector .","Motivated by dynamic routing ) , we propose a new self - attention mechanism for sentence embedding , namely Dynamic Self - Attention ( DSA ) .","DSA , which is stacked on CNN with Dense Connection , achieves new state - of - the - art results among the sentence encoding methods in Stanford Natural Language Inference ( SNLI ) dataset with the least number of parameters , while obtaining comparative results in Stanford Sentiment Treebank ( SST ) dataset .",0.1739130434782608,0.7391304347826086,0.7391304347826086,model,approach
natural_language_inference,6,introduction,introduction,25,18,18,"DSA , which is stacked on CNN with Dense Connection , achieves new state - of - the - art results among the sentence encoding methods in Stanford Natural Language Inference ( SNLI ) dataset with the least number of parameters , while obtaining comparative results in Stanford Sentiment Treebank ( SST ) dataset .","To this end , we modify dynamic routing such that it functions as self - attention with the dynamic weight vector .",It also outperforms recent models in terms of time efficiency due to its simplicity and highly parallelized computations .,0.181159420289855,0.7826086956521741,0.7826086956521741,model,baselines
natural_language_inference,6,introduction,introduction,26,19,19,It also outperforms recent models in terms of time efficiency due to its simplicity and highly parallelized computations .,"DSA , which is stacked on CNN with Dense Connection , achieves new state - of - the - art results among the sentence encoding methods in Stanford Natural Language Inference ( SNLI ) dataset with the least number of parameters , while obtaining comparative results in Stanford Sentiment Treebank ( SST ) dataset .",Our technical contributions are as follows :,0.1884057971014492,0.8260869565217391,0.8260869565217391,model,approach
natural_language_inference,6,experiments,natural language inference results,102,9,1,Natural Language Inference Results, , ,0.7391304347826086,0.25,0.0588235294117647,experiments,approach
natural_language_inference,6,experiments,natural language inference results,105,12,4,"Entailment , Contradiction and Neutral .","We conduct experiments on Stanford Natural Language Inference ( SNLI ) dataset , consisting of human- written 570 k pairs of English sentences labeled with one of three classes :","As the task considers the semantic relationship , SNLI is used as a benchmark for evaluating the performance of a sentence encoder .",0.7608695652173914,0.3333333333333333,0.2352941176470588,experiments,approach
natural_language_inference,6,experiments,natural language inference results,106,13,5,"As the task considers the semantic relationship , SNLI is used as a benchmark for evaluating the performance of a sentence encoder .","Entailment , Contradiction and Neutral .","We follow a conventional approach , called heuristic matching , to classify the relationship of two sentences .",0.7681159420289855,0.3611111111111111,0.2941176470588235,experiments,baselines
natural_language_inference,6,experiments,natural language inference results,115,22,14,"With tradeoffs in terms of parameters and learning time per epoch , multiple DSA outperforms other models by a large margin ( + 1.1 % ) .",Distance - based self - attention network 89 Model SST - 2 SST - 5 BiLSTM 87.5 49.5 CNN - non- static 87 is significantly faster than recent models because of its simple structure and highly parallelized computations .,"In comparison to the baseline , single DSA shows better performance than self - attention ( + 2.2 % ) .",0.8333333333333334,0.6111111111111112,0.8235294117647058,experiments,baselines
natural_language_inference,6,experiments,natural language inference results,116,23,15,"In comparison to the baseline , single DSA shows better performance than self - attention ( + 2.2 % ) .","With tradeoffs in terms of parameters and learning time per epoch , multiple DSA outperforms other models by a large margin ( + 1.1 % ) .",This confirms that the dynamic weight vector is more effective for sentence embedding .,0.8405797101449275,0.6388888888888888,0.8823529411764706,experiments,approach
natural_language_inference,6,experiments,natural language inference results,118,25,17,"Note that our implementation of the baseline , selfattention stacked on CNN with Dense Connection , shows better performance ( + 0.4 % ) than the one stacked on BiLSTM .",This confirms that the dynamic weight vector is more effective for sentence embedding ., ,0.855072463768116,0.6944444444444444,1.0,experiments,baselines
natural_language_inference,6,experiments,sentiment analysis results,119,26,1,Sentiment Analysis Results, , ,0.8623188405797102,0.7222222222222222,0.0909090909090909,experiments,approach
natural_language_inference,6,experiments,sentiment analysis results,127,34,9,"Single DSA outperforms all the baseline models in SST - 2 dataset , and achieves comparative results in SST - 5 , which again verifies the effectiveness of the dynamic weight vector .","BiLSTM , CNN and self - attention with BiLSTM or CNN with dense connection .","In contrast to the distinguished results in SNLI dataset ( + 2.2 % ) , in SST dataset , only marginal differences in the performance between DSA and the previous self - attentive models are found .",0.9202898550724636,0.9444444444444444,0.8181818181818182,experiments,baselines
natural_language_inference,6,experiments,sentiment analysis results,128,35,10,"In contrast to the distinguished results in SNLI dataset ( + 2.2 % ) , in SST dataset , only marginal differences in the performance between DSA and the previous self - attentive models are found .","Single DSA outperforms all the baseline models in SST - 2 dataset , and achieves comparative results in SST - 5 , which again verifies the effectiveness of the dynamic weight vector .",We conclude that DSA exhibits a more significant improvement for large and complex datasets .,0.927536231884058,0.9722222222222222,0.9090909090909092,experiments,approach
natural_language_inference,7,abstract,abstract,5,3,3,"Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen , but until now large scale training and test datasets have been missing for this type of evaluation .",Teaching machines to read natural language documents remains an elusive challenge .,In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data .,0.0217391304347826,0.6,0.6,research-problem,ablation-analysis
natural_language_inference,7,introduction,introduction,10,3,3,"Traditional approaches to machine reading and comprehension have been based on either hand engineered grammars , or information extraction methods of detecting predicate argument triples that can later be queried as a relational database .",Progress on the path from shallow bag - of - words information retrieval algorithms to machines capable of reading and understanding documents has been slow .,"Supervised machine learning approaches have largely been absent from this space due to both the lack of large scale training datasets , and the difficulty in structuring statistical models flexible enough to learn to exploit document structure .",0.0434782608695652,0.0491803278688524,0.09375,research-problem,ablation-analysis
natural_language_inference,7,introduction,introduction,16,9,9,In this work we seek to directly address the lack of real natural language training data by introducing a novel approach to building a supervised reading comprehension data set .,"Historically , however , many similar approaches in Computational Linguistics have failed to manage the transition from synthetic data to real environments , as such closed worlds inevitably fail to capture the complexity , richness , and noise of natural language .","We observe that summary and paraphrase sentences , with their associated documents , can be readily converted to context - query - answer triples using simple entity detection and anonymisation algorithms .",0.0695652173913043,0.1475409836065573,0.28125,approach,ablation-analysis
natural_language_inference,7,introduction,introduction,17,10,10,"We observe that summary and paraphrase sentences , with their associated documents , can be readily converted to context - query - answer triples using simple entity detection and anonymisation algorithms .",In this work we seek to directly address the lack of real natural language training data by introducing a novel approach to building a supervised reading comprehension data set .,Using this approach we have collected two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites .,0.0739130434782608,0.1639344262295081,0.3125,approach,baselines
natural_language_inference,7,introduction,introduction,18,11,11,Using this approach we have collected two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites .,"We observe that summary and paraphrase sentences , with their associated documents , can be readily converted to context - query - answer triples using simple entity detection and anonymisation algorithms .",We demonstrate the efficacy of our new corpora by building novel deep learning models for reading comprehension .,0.0782608695652174,0.180327868852459,0.34375,approach,baselines
natural_language_inference,7,introduction,introduction,19,12,12,We demonstrate the efficacy of our new corpora by building novel deep learning models for reading comprehension .,Using this approach we have collected two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites .,These models draw on recent developments for incorporating attention mechanisms into recurrent neural network architectures .,0.0826086956521739,0.1967213114754098,0.375,approach,approach
natural_language_inference,7,empirical evaluation,empirical evaluation,140,5,5,We expect that the attention - based models would therefore outperform the pure LSTM - based approaches .,"However , we argued that simple recurrent models such as the LSTM probably have insufficient expressive power for solving tasks that require complex inference .","Considering the second dimension of our investigation , the comparison of traditional versus neural approaches to NLP , we do not have a strong prior favouring one approach over the other .",0.6086956521739131,0.1136363636363636,0.1136363636363636,results,approach
natural_language_inference,7,empirical evaluation,empirical evaluation,156,21,21,Word distance benchmark,This was true for the majority of queries in the dataset .,"More surprising perhaps is the relatively strong performance of the word distance benchmark , particularly relative to the frame - semantic benchmark , which we had expected to perform better .",0.6782608695652174,0.4772727272727273,0.4772727272727273,results,baselines
natural_language_inference,7,empirical evaluation,empirical evaluation,157,22,22,"More surprising perhaps is the relatively strong performance of the word distance benchmark , particularly relative to the frame - semantic benchmark , which we had expected to perform better .",Word distance benchmark,"Here , again , the nature of the datasets used can explain aspects of this result .",0.6826086956521739,0.5,0.5,results,approach
natural_language_inference,7,empirical evaluation,empirical evaluation,164,29,29,Neural models,We expect that on other types of machine reading data where questions rather than Cloze queries are used this particular model would perform significantly worse .,"Within the group of neural models explored here , the results paint a clear picture with the Impatient and the Attentive Readers outperforming all other models .",0.7130434782608696,0.6590909090909091,0.6590909090909091,results,approach
natural_language_inference,7,empirical evaluation,empirical evaluation,165,30,30,"Within the group of neural models explored here , the results paint a clear picture with the Impatient and the Attentive Readers outperforming all other models .",Neural models,This is consistent with our hypothesis that attention is a key ingredient for machine reading and question answering due to the need to propagate information overlong distances .,0.7173913043478259,0.6818181818181818,0.6818181818181818,results,approach
natural_language_inference,7,empirical evaluation,empirical evaluation,167,32,32,The Deep LSTM,This is consistent with our hypothesis that attention is a key ingredient for machine reading and question answering due to the need to propagate information overlong distances .,"Reader performs surprisingly well , once again demonstrating that this simple sequential architecture can do a reasonable job of learning to abstract long sequences , even when they are up to two thousand tokens in length .",0.7260869565217392,0.7272727272727273,0.7272727272727273,results,experimental-setup
natural_language_inference,7,empirical evaluation,empirical evaluation,168,33,33,"Reader performs surprisingly well , once again demonstrating that this simple sequential architecture can do a reasonable job of learning to abstract long sequences , even when they are up to two thousand tokens in length .",The Deep LSTM,"However this model does fail to match the performance of the attention based models , even though these only use single layer LSTMs .",0.7304347826086957,0.75,0.75,results,baselines
natural_language_inference,8,title,title,2,2,2,Learning Natural Language Inference using Bidirectional LSTM model and Inner- Attention, , ,0.0166666666666666,1.0,1.0,research-problem,approach
natural_language_inference,8,abstract,abstract,4,2,2,"In this paper , we proposed a sentence encoding - based model for recognizing text entailment .", ,"In our approach , the encoding of sentence is a two - stage process .",0.0333333333333333,0.2222222222222222,0.2222222222222222,research-problem,experimental-setup
natural_language_inference,8,introduction,introduction,13,2,2,"Given a pair of sentences , the goal of recognizing text entailment ( RTE ) is to determine whether the hypothesis can reasonably be inferred from the premises .", ,"There were three types of relation in RTE , Entailment ( inferred to be true ) , Contradiction ( inferred to be false ) and Neutral ( truth unknown ) .",0.1083333333333333,0.08,0.08,research-problem,ablation-analysis
natural_language_inference,8,introduction,introduction,14,3,3,"There were three types of relation in RTE , Entailment ( inferred to be true ) , Contradiction ( inferred to be false ) and Neutral ( truth unknown ) .","Given a pair of sentences , the goal of recognizing text entailment ( RTE ) is to determine whether the hypothesis can reasonably be inferred from the premises .",A few examples were given in .,0.1166666666666666,0.12,0.12,research-problem,approach
natural_language_inference,8,introduction,introduction,30,19,19,"In this paper , we proposed a unified deep learning framework for recognizing textual entailment which dose not require any feature engineering , or external resources .",A recent work by improved the performance by applying a neural attention model that did n't yield sentence embeddings .,The basic model is based on building biL - STM models on both premises and hypothesis .,0.25,0.76,0.76,model,experimental-setup
natural_language_inference,8,introduction,introduction,31,20,20,The basic model is based on building biL - STM models on both premises and hypothesis .,"In this paper , we proposed a unified deep learning framework for recognizing textual entailment which dose not require any feature engineering , or external resources .",The basic mean pooling encoder can roughly form a intuition about what this sentence is talking about .,0.2583333333333333,0.8,0.8,model,model
natural_language_inference,8,introduction,introduction,32,21,21,The basic mean pooling encoder can roughly form a intuition about what this sentence is talking about .,The basic model is based on building biL - STM models on both premises and hypothesis .,"Obtained this representation , we extended this model by utilize an Inner - Attention mechanism on both sides .",0.2666666666666666,0.84,0.84,model,ablation-analysis
natural_language_inference,8,introduction,introduction,35,24,24,"In addition , we introduced a simple effective input strategy that get ride of same words in hypothesis and premise , which further boosts our performance .",This mechanism helps generate more accurate and focused sentence representations for classification .,"Without parameter tuning , we improved the art - of - the - state performance of sentence encodingbased model by nearly 2 % .",0.2916666666666667,0.96,0.96,model,baselines
natural_language_inference,8,experiments,parameter setting,77,10,2,"The training objective of our model is cross - entropy loss , and we use minibatch SGD with the Rmsprop ( Tieleman and Hinton , 2012 ) for optimization .", ,The batch size is 128 .,0.6416666666666667,0.2325581395348837,0.1666666666666666,hyperparameters,approach
natural_language_inference,8,experiments,parameter setting,78,11,3,The batch size is 128 .,"The training objective of our model is cross - entropy loss , and we use minibatch SGD with the Rmsprop ( Tieleman and Hinton , 2012 ) for optimization .",A dropout layer was applied in the output of the network with the dropout rate set to 0.25 .,0.65,0.2558139534883721,0.25,hyperparameters,experiments
natural_language_inference,8,experiments,parameter setting,79,12,4,A dropout layer was applied in the output of the network with the dropout rate set to 0.25 .,The batch size is 128 .,"In our model , we used pretrained 300D Glove 840B vectors to initialize the word embedding .",0.6583333333333333,0.2790697674418605,0.3333333333333333,hyperparameters,approach
natural_language_inference,8,experiments,parameter setting,80,13,5,"In our model , we used pretrained 300D Glove 840B vectors to initialize the word embedding .",A dropout layer was applied in the output of the network with the dropout rate set to 0.25 .,"Out - of - vocabulary words in the training set are randomly initialized by sampling values uniformly from ( 0.05 , 0.05 ) .",0.6666666666666666,0.3023255813953488,0.4166666666666667,hyperparameters,experimental-setup
natural_language_inference,8,experiments,parameter setting,81,14,6,"Out - of - vocabulary words in the training set are randomly initialized by sampling values uniformly from ( 0.05 , 0.05 ) .","In our model , we used pretrained 300D Glove 840B vectors to initialize the word embedding .",All of these embedding are not updated during training .,0.675,0.3255813953488372,0.5,hyperparameters,approach
natural_language_inference,8,experiments,results and qualitative analysis,107,40,7,"We observed that more attention was given to Nones , Verbs and Adjectives .",Visualizations of Inner - Attention on these examples are depicted in .,This conform to our experience that these words are more semantic richer than function words .,0.8916666666666667,0.9302325581395348,0.7,results,approach
natural_language_inference,8,experiments,results and qualitative analysis,109,42,9,"While mean pooling regarded each word of equal importance , the attention mechanism helps re-weight words according to their importance .",This conform to our experience that these words are more semantic richer than function words .,And more focused and accurate sentence representations were generated based on produced attention vectors .,0.9083333333333332,0.9767441860465116,0.9,results,model
natural_language_inference,9,title,title,2,2,2,A BERT Baseline for the Natural Questions, , ,0.0307692307692307,1.0,1.0,research-problem,approach
natural_language_inference,9,introduction,introduction,14,7,7,In this technical note we describe a BERT - based model for the Natural Questions .,"The qualities that we think make NQ more challenging than other question answering datasets are the following : ( 1 ) the questions in NQ were formulated by people out of genuine curiosity or out of need for an answer to complete another task , ( 2 ) the questions were formulated by people before they had seen the document that might contain the answer , ( 3 ) the documents in which the answer is to be found are much longer than the documents used in some of the existing question answering challenges .","BERT performs very well on this dataset , reducing the gap between the model F 1 scores reported in the original dataset paper and the human upper bound by 30 % and 50 % relative for the long and short answer tasks respectively .",0.2153846153846154,0.5833333333333334,0.5833333333333334,approach,experimental-setup
natural_language_inference,9,introduction,introduction,17,10,10,"The key insights in our approach are 1 . to jointly predict short and long answers in a single model rather than using a pipeline approach , 2 . to split each document into multiple training instances by using overlapping windows of tokens , like in the original BERT model for the SQuAD task , 3 . to aggressively downsample null instances ( i.e. instances without an answer ) at training time to create a balanced training set , 4 . to use the "" [ CLS ] "" token at training time to predict null instances and rank spans at inference time by the difference between the span score and the "" [ CLS ] "" score .","However , there is still ample room for improvement : 22.5 F1 points for the long answer task and 23 F1 points for the short answer task .",We refer to our model as BERT joint to emphasize the fact that we are modeling short and long answers in a single model rather than in a pipeline of two models .,0.2615384615384615,0.8333333333333334,0.8333333333333334,approach,model
natural_language_inference,9,experiments,experiments,54,2,2,We initialized our model from a BERT model already finetuned on SQ u AD 1.1 ., ,We then further finetuned the model on the training instances precomputed as described in Section 2 .,0.8307692307692308,0.2,0.2,experimental-setup,experimental-setup
natural_language_inference,9,experiments,experiments,56,4,4,"We trained the model by minimizing loss L from Section 3 with the Adam optimizer ( Kingma and Ba , 2014 ) with a batch size of 8 .",We then further finetuned the model on the training instances precomputed as described in Section 2 .,"As is common practice for BERT models , we only tuned the number of epochs and the initial learning rate for finetuning and found that training for 1 epoch with an initial learning rate of 3 10 ? 5 was the best setting .",0.8615384615384616,0.4,0.4,experimental-setup,experiments
natural_language_inference,9,experiments,experiments,57,5,5,"As is common practice for BERT models , we only tuned the number of epochs and the initial learning rate for finetuning and found that training for 1 epoch with an initial learning rate of 3 10 ? 5 was the best setting .","We trained the model by minimizing loss L from Section 3 with the Adam optimizer ( Kingma and Ba , 2014 ) with a batch size of 8 .",Evaluation completed in about 5 hours on the NQ dev and test set with a single Tesla P100 GPU .,0.8769230769230769,0.5,0.5,experimental-setup,approach
natural_language_inference,9,experiments,experiments,58,6,6,Evaluation completed in about 5 hours on the NQ dev and test set with a single Tesla P100 GPU .,"As is common practice for BERT models , we only tuned the number of epochs and the initial learning rate for finetuning and found that training for 1 epoch with an initial learning rate of 3 10 ? 5 was the best setting .",The results obtained by our model are shown in .,0.8923076923076924,0.6,0.6,experimental-setup,approach
natural_language_inference,9,experiments,experiments,60,8,8,Our BERT model for NQ performs dramatically better than the models presented in the original NQ paper .,The results obtained by our model are shown in .,Our model closes the gap between the F 1 score achieved by the original baseline systems and the super - annotator upper bound by 30 % for the long answer NQ task and by 50 % for the short answer NQ task .,0.9230769230769232,0.8,0.8,results,approach
natural_language_inference,9,experiments,experiments,61,9,9,Our model closes the gap between the F 1 score achieved by the original baseline systems and the super - annotator upper bound by 30 % for the long answer NQ task and by 50 % for the short answer NQ task .,Our BERT model for NQ performs dramatically better than the models presented in the original NQ paper .,"However NQ appears to be still far from being solved , with more than 20 F1 points of headroom for both the long and short answer tasks .",0.9384615384615383,0.9,0.9,results,baselines
