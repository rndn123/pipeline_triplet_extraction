{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Info_Units_Results/test_results_info_units_8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>paper_ID</th>\n",
       "      <th>main_heading</th>\n",
       "      <th>sub_heading</th>\n",
       "      <th>pos1</th>\n",
       "      <th>pos2</th>\n",
       "      <th>pos3</th>\n",
       "      <th>text</th>\n",
       "      <th>prev_text</th>\n",
       "      <th>next_text</th>\n",
       "      <th>ofs1</th>\n",
       "      <th>ofs2</th>\n",
       "      <th>ofs3</th>\n",
       "      <th>labels</th>\n",
       "      <th>label</th>\n",
       "      <th>info_units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>title</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Recurrent Neural Network Grammars</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.008969</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>research-problem</td>\n",
       "      <td>research-problem</td>\n",
       "      <td>research-problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>introduction</td>\n",
       "      <td>introduction</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>In this paper , we introduce recurrent neural ...</td>\n",
       "      <td>Despite these impressive results , sequential ...</td>\n",
       "      <td>RNNGs operate via a recursive syntactic proces...</td>\n",
       "      <td>0.053812</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>introduction</td>\n",
       "      <td>introduction</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>RNNGs operate via a recursive syntactic proces...</td>\n",
       "      <td>In this paper , we introduce recurrent neural ...</td>\n",
       "      <td>The foundation of this work is a top - down va...</td>\n",
       "      <td>0.058296</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>introduction</td>\n",
       "      <td>introduction</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>We give two variants of the algorithm , one fo...</td>\n",
       "      <td>The foundation of this work is a top - down va...</td>\n",
       "      <td>While several transition - based neural models...</td>\n",
       "      <td>0.067265</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>introduction</td>\n",
       "      <td>introduction</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>The discriminative model also lets us use ance...</td>\n",
       "      <td>time deterministic parser ( provided an upper ...</td>\n",
       "      <td>We present a simple importance sampling algori...</td>\n",
       "      <td>0.107623</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>introduction</td>\n",
       "      <td>introduction</td>\n",
       "      <td>25</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>We present a simple importance sampling algori...</td>\n",
       "      <td>The discriminative model also lets us use ance...</td>\n",
       "      <td>Experiments show that RNNGs are effective for ...</td>\n",
       "      <td>0.112108</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>experiments</td>\n",
       "      <td>experiments</td>\n",
       "      <td>160</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>For the discriminative model , we used hidden ...</td>\n",
       "      <td>Model and training parameters .</td>\n",
       "      <td>For the generative model , we used 256 dimensi...</td>\n",
       "      <td>0.717489</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>hyperparameters</td>\n",
       "      <td>hyper-setup</td>\n",
       "      <td>hyper-setup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>experiments</td>\n",
       "      <td>experiments</td>\n",
       "      <td>161</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>For the generative model , we used 256 dimensi...</td>\n",
       "      <td>For the discriminative model , we used hidden ...</td>\n",
       "      <td>For both models , we tuned the dropout rate to...</td>\n",
       "      <td>0.721973</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>hyperparameters</td>\n",
       "      <td>hyper-setup</td>\n",
       "      <td>hyper-setup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>experiments</td>\n",
       "      <td>experiments</td>\n",
       "      <td>162</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>For both models , we tuned the dropout rate to...</td>\n",
       "      <td>For the generative model , we used 256 dimensi...</td>\n",
       "      <td>For the sequential LSTM baseline for the langu...</td>\n",
       "      <td>0.726457</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>hyperparameters</td>\n",
       "      <td>hyper-setup</td>\n",
       "      <td>hyper-setup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>experiments</td>\n",
       "      <td>experiments</td>\n",
       "      <td>163</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>For the sequential LSTM baseline for the langu...</td>\n",
       "      <td>For both models , we tuned the dropout rate to...</td>\n",
       "      <td>For training we used stochastic gradient desce...</td>\n",
       "      <td>0.730942</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>hyperparameters</td>\n",
       "      <td>hyper-setup</td>\n",
       "      <td>experiments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>experiments</td>\n",
       "      <td>experiments</td>\n",
       "      <td>164</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>For training we used stochastic gradient desce...</td>\n",
       "      <td>For the sequential LSTM baseline for the langu...</td>\n",
       "      <td>All parameters were initialized according to r...</td>\n",
       "      <td>0.735426</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>hyperparameters</td>\n",
       "      <td>hyper-setup</td>\n",
       "      <td>hyper-setup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>1</td>\n",
       "      <td>title</td>\n",
       "      <td>title</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Cloze - driven Pretraining of Self - attention...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.009804</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>research-problem</td>\n",
       "      <td>research-problem</td>\n",
       "      <td>research-problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>1</td>\n",
       "      <td>abstract</td>\n",
       "      <td>abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>We present a new approach for pretraining a bi...</td>\n",
       "      <td></td>\n",
       "      <td>Our model solves a cloze - style word reconstr...</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>research-problem</td>\n",
       "      <td>research-problem</td>\n",
       "      <td>research-problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>1</td>\n",
       "      <td>introduction</td>\n",
       "      <td>introduction</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Language model pretraining has recently been s...</td>\n",
       "      <td></td>\n",
       "      <td>However , existing work has either used unidir...</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>research-problem</td>\n",
       "      <td>research-problem</td>\n",
       "      <td>research-problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>1</td>\n",
       "      <td>introduction</td>\n",
       "      <td>introduction</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>In this paper , we show that even larger perfo...</td>\n",
       "      <td>However , existing work has either used unidir...</td>\n",
       "      <td>Our bi-directional transformer architecture pr...</td>\n",
       "      <td>0.053922</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>approach</td>\n",
       "      <td>approach</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>1</td>\n",
       "      <td>introduction</td>\n",
       "      <td>introduction</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Our bi-directional transformer architecture pr...</td>\n",
       "      <td>In this paper , we show that even larger perfo...</td>\n",
       "      <td>We achieve this by introducing a cloze - style...</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>approach</td>\n",
       "      <td>approach</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>1</td>\n",
       "      <td>introduction</td>\n",
       "      <td>introduction</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>We achieve this by introducing a cloze - style...</td>\n",
       "      <td>Our bi-directional transformer architecture pr...</td>\n",
       "      <td>Our model separately computes both forward and...</td>\n",
       "      <td>0.063725</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>approach</td>\n",
       "      <td>approach</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>1</td>\n",
       "      <td>introduction</td>\n",
       "      <td>introduction</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Our model separately computes both forward and...</td>\n",
       "      <td>We achieve this by introducing a cloze - style...</td>\n",
       "      <td>Illustration of the model .</td>\n",
       "      <td>0.068627</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>approach</td>\n",
       "      <td>approach</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>1</td>\n",
       "      <td>experimental setup</td>\n",
       "      <td>experimental setup</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Experimental setup</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.539216</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>experimental-setup</td>\n",
       "      <td>hyper-setup</td>\n",
       "      <td>results</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>1</td>\n",
       "      <td>experimental setup</td>\n",
       "      <td>pretraining hyper parameters</td>\n",
       "      <td>128</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>CNN models use an adaptive softmax in the outp...</td>\n",
       "      <td>The CNN models have unconstrained input vocabu...</td>\n",
       "      <td>The learning rate is linearly warmed up from 1...</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>experimental-setup</td>\n",
       "      <td>hyper-setup</td>\n",
       "      <td>hyper-setup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   topic  paper_ID        main_heading  \\\n",
       "0   constituency_parsing         0               title   \n",
       "1   constituency_parsing         0        introduction   \n",
       "2   constituency_parsing         0        introduction   \n",
       "3   constituency_parsing         0        introduction   \n",
       "4   constituency_parsing         0        introduction   \n",
       "5   constituency_parsing         0        introduction   \n",
       "6   constituency_parsing         0         experiments   \n",
       "7   constituency_parsing         0         experiments   \n",
       "8   constituency_parsing         0         experiments   \n",
       "9   constituency_parsing         0         experiments   \n",
       "10  constituency_parsing         0         experiments   \n",
       "11  constituency_parsing         1               title   \n",
       "12  constituency_parsing         1            abstract   \n",
       "13  constituency_parsing         1        introduction   \n",
       "14  constituency_parsing         1        introduction   \n",
       "15  constituency_parsing         1        introduction   \n",
       "16  constituency_parsing         1        introduction   \n",
       "17  constituency_parsing         1        introduction   \n",
       "18  constituency_parsing         1  experimental setup   \n",
       "19  constituency_parsing         1  experimental setup   \n",
       "\n",
       "                     sub_heading  pos1  pos2  pos3  \\\n",
       "0                          title     2     2     2   \n",
       "1                   introduction    12     5     5   \n",
       "2                   introduction    13     6     6   \n",
       "3                   introduction    15     8     8   \n",
       "4                   introduction    24    17    17   \n",
       "5                   introduction    25    18    18   \n",
       "6                    experiments   160     4     4   \n",
       "7                    experiments   161     5     5   \n",
       "8                    experiments   162     6     6   \n",
       "9                    experiments   163     7     7   \n",
       "10                   experiments   164     8     8   \n",
       "11                         title     2     2     2   \n",
       "12                      abstract     4     2     2   \n",
       "13                  introduction     9     2     2   \n",
       "14                  introduction    11     4     4   \n",
       "15                  introduction    12     5     5   \n",
       "16                  introduction    13     6     6   \n",
       "17                  introduction    14     7     7   \n",
       "18            experimental setup   110     1     1   \n",
       "19  pretraining hyper parameters   128    19     6   \n",
       "\n",
       "                                                 text  \\\n",
       "0                   Recurrent Neural Network Grammars   \n",
       "1   In this paper , we introduce recurrent neural ...   \n",
       "2   RNNGs operate via a recursive syntactic proces...   \n",
       "3   We give two variants of the algorithm , one fo...   \n",
       "4   The discriminative model also lets us use ance...   \n",
       "5   We present a simple importance sampling algori...   \n",
       "6   For the discriminative model , we used hidden ...   \n",
       "7   For the generative model , we used 256 dimensi...   \n",
       "8   For both models , we tuned the dropout rate to...   \n",
       "9   For the sequential LSTM baseline for the langu...   \n",
       "10  For training we used stochastic gradient desce...   \n",
       "11  Cloze - driven Pretraining of Self - attention...   \n",
       "12  We present a new approach for pretraining a bi...   \n",
       "13  Language model pretraining has recently been s...   \n",
       "14  In this paper , we show that even larger perfo...   \n",
       "15  Our bi-directional transformer architecture pr...   \n",
       "16  We achieve this by introducing a cloze - style...   \n",
       "17  Our model separately computes both forward and...   \n",
       "18                                 Experimental setup   \n",
       "19  CNN models use an adaptive softmax in the outp...   \n",
       "\n",
       "                                            prev_text  \\\n",
       "0                                                       \n",
       "1   Despite these impressive results , sequential ...   \n",
       "2   In this paper , we introduce recurrent neural ...   \n",
       "3   The foundation of this work is a top - down va...   \n",
       "4   time deterministic parser ( provided an upper ...   \n",
       "5   The discriminative model also lets us use ance...   \n",
       "6                     Model and training parameters .   \n",
       "7   For the discriminative model , we used hidden ...   \n",
       "8   For the generative model , we used 256 dimensi...   \n",
       "9   For both models , we tuned the dropout rate to...   \n",
       "10  For the sequential LSTM baseline for the langu...   \n",
       "11                                                      \n",
       "12                                                      \n",
       "13                                                      \n",
       "14  However , existing work has either used unidir...   \n",
       "15  In this paper , we show that even larger perfo...   \n",
       "16  Our bi-directional transformer architecture pr...   \n",
       "17  We achieve this by introducing a cloze - style...   \n",
       "18                                                      \n",
       "19  The CNN models have unconstrained input vocabu...   \n",
       "\n",
       "                                            next_text      ofs1      ofs2  \\\n",
       "0                                                      0.008969  1.000000   \n",
       "1   RNNGs operate via a recursive syntactic proces...  0.053812  0.238095   \n",
       "2   The foundation of this work is a top - down va...  0.058296  0.285714   \n",
       "3   While several transition - based neural models...  0.067265  0.380952   \n",
       "4   We present a simple importance sampling algori...  0.107623  0.809524   \n",
       "5   Experiments show that RNNGs are effective for ...  0.112108  0.857143   \n",
       "6   For the generative model , we used 256 dimensi...  0.717489  0.235294   \n",
       "7   For both models , we tuned the dropout rate to...  0.721973  0.294118   \n",
       "8   For the sequential LSTM baseline for the langu...  0.726457  0.352941   \n",
       "9   For training we used stochastic gradient desce...  0.730942  0.411765   \n",
       "10  All parameters were initialized according to r...  0.735426  0.470588   \n",
       "11                                                     0.009804  1.000000   \n",
       "12  Our model solves a cloze - style word reconstr...  0.019608  0.400000   \n",
       "13  However , existing work has either used unidir...  0.044118  0.111111   \n",
       "14  Our bi-directional transformer architecture pr...  0.053922  0.222222   \n",
       "15  We achieve this by introducing a cloze - style...  0.058824  0.277778   \n",
       "16  Our model separately computes both forward and...  0.063725  0.333333   \n",
       "17                        Illustration of the model .  0.068627  0.388889   \n",
       "18                                                     0.539216  0.040000   \n",
       "19  The learning rate is linearly warmed up from 1...  0.627451  0.760000   \n",
       "\n",
       "        ofs3              labels             label        info_units  \n",
       "0   1.000000    research-problem  research-problem  research-problem  \n",
       "1   0.238095               model             model             model  \n",
       "2   0.285714               model             model             model  \n",
       "3   0.380952               model             model             model  \n",
       "4   0.809524               model             model             model  \n",
       "5   0.857143               model             model             model  \n",
       "6   0.235294     hyperparameters       hyper-setup       hyper-setup  \n",
       "7   0.294118     hyperparameters       hyper-setup       hyper-setup  \n",
       "8   0.352941     hyperparameters       hyper-setup       hyper-setup  \n",
       "9   0.411765     hyperparameters       hyper-setup       experiments  \n",
       "10  0.470588     hyperparameters       hyper-setup       hyper-setup  \n",
       "11  1.000000    research-problem  research-problem  research-problem  \n",
       "12  0.400000    research-problem  research-problem  research-problem  \n",
       "13  0.111111    research-problem  research-problem  research-problem  \n",
       "14  0.222222            approach          approach             model  \n",
       "15  0.277778            approach          approach             model  \n",
       "16  0.333333            approach          approach             model  \n",
       "17  0.388889            approach          approach             model  \n",
       "18  1.000000  experimental-setup       hyper-setup           results  \n",
       "19  0.500000  experimental-setup       hyper-setup       hyper-setup  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp = df[df['labels'].isin(['hyperparameters', 'experimental-setup'])].reset_index(drop = True)\n",
    "#df_temp = df[df['labels'].isin(['results', 'experiments'])].reset_index(drop = True)\n",
    "df_temp = df[df['info_units'] == 'hyper-setup'].reset_index()\n",
    "df_temp.drop(columns = ['label', 'info_units'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>topic</th>\n",
       "      <th>paper_ID</th>\n",
       "      <th>main_heading</th>\n",
       "      <th>sub_heading</th>\n",
       "      <th>pos1</th>\n",
       "      <th>pos2</th>\n",
       "      <th>pos3</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>experiments</td>\n",
       "      <td>experiments</td>\n",
       "      <td>160</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>For the discriminative model , we used hidden ...</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>experiments</td>\n",
       "      <td>experiments</td>\n",
       "      <td>161</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>For the generative model , we used 256 dimensi...</td>\n",
       "      <td>hyperparameters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>experiments</td>\n",
       "      <td>experiments</td>\n",
       "      <td>162</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>For both models , we tuned the dropout rate to...</td>\n",
       "      <td>hyperparameters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>experiments</td>\n",
       "      <td>experiments</td>\n",
       "      <td>164</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>For training we used stochastic gradient desce...</td>\n",
       "      <td>hyperparameters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>1</td>\n",
       "      <td>experimental setup</td>\n",
       "      <td>pretraining hyper parameters</td>\n",
       "      <td>128</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>CNN models use an adaptive softmax in the outp...</td>\n",
       "      <td>experimental-setup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>2707</td>\n",
       "      <td>natural_language_inference</td>\n",
       "      <td>8</td>\n",
       "      <td>experiments</td>\n",
       "      <td>parameter setting</td>\n",
       "      <td>80</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>In our model , we used pretrained 300D Glove 8...</td>\n",
       "      <td>hyperparameters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>2708</td>\n",
       "      <td>natural_language_inference</td>\n",
       "      <td>8</td>\n",
       "      <td>experiments</td>\n",
       "      <td>parameter setting</td>\n",
       "      <td>81</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>Out - of - vocabulary words in the training se...</td>\n",
       "      <td>hyperparameters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>2714</td>\n",
       "      <td>natural_language_inference</td>\n",
       "      <td>9</td>\n",
       "      <td>experiments</td>\n",
       "      <td>experiments</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>We initialized our model from a BERT model alr...</td>\n",
       "      <td>experimental-setup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>2715</td>\n",
       "      <td>natural_language_inference</td>\n",
       "      <td>9</td>\n",
       "      <td>experiments</td>\n",
       "      <td>experiments</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>We trained the model by minimizing loss L from...</td>\n",
       "      <td>experimental-setup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>2716</td>\n",
       "      <td>natural_language_inference</td>\n",
       "      <td>9</td>\n",
       "      <td>experiments</td>\n",
       "      <td>experiments</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>As is common practice for BERT models , we onl...</td>\n",
       "      <td>experimental-setup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                       topic  paper_ID        main_heading  \\\n",
       "0        6        constituency_parsing         0         experiments   \n",
       "1        7        constituency_parsing         0         experiments   \n",
       "2        8        constituency_parsing         0         experiments   \n",
       "3       10        constituency_parsing         0         experiments   \n",
       "4       19        constituency_parsing         1  experimental setup   \n",
       "..     ...                         ...       ...                 ...   \n",
       "444   2707  natural_language_inference         8         experiments   \n",
       "445   2708  natural_language_inference         8         experiments   \n",
       "446   2714  natural_language_inference         9         experiments   \n",
       "447   2715  natural_language_inference         9         experiments   \n",
       "448   2716  natural_language_inference         9         experiments   \n",
       "\n",
       "                      sub_heading  pos1  pos2  pos3  \\\n",
       "0                     experiments   160     4     4   \n",
       "1                     experiments   161     5     5   \n",
       "2                     experiments   162     6     6   \n",
       "3                     experiments   164     8     8   \n",
       "4    pretraining hyper parameters   128    19     6   \n",
       "..                            ...   ...   ...   ...   \n",
       "444             parameter setting    80    13     5   \n",
       "445             parameter setting    81    14     6   \n",
       "446                   experiments    54     2     2   \n",
       "447                   experiments    56     4     4   \n",
       "448                   experiments    57     5     5   \n",
       "\n",
       "                                                  text              labels  \n",
       "0    For the discriminative model , we used hidden ...                code  \n",
       "1    For the generative model , we used 256 dimensi...     hyperparameters  \n",
       "2    For both models , we tuned the dropout rate to...     hyperparameters  \n",
       "3    For training we used stochastic gradient desce...     hyperparameters  \n",
       "4    CNN models use an adaptive softmax in the outp...  experimental-setup  \n",
       "..                                                 ...                 ...  \n",
       "444  In our model , we used pretrained 300D Glove 8...     hyperparameters  \n",
       "445  Out - of - vocabulary words in the training se...     hyperparameters  \n",
       "446  We initialized our model from a BERT model alr...  experimental-setup  \n",
       "447  We trained the model by minimizing loss L from...  experimental-setup  \n",
       "448  As is common practice for BERT models , we onl...  experimental-setup  \n",
       "\n",
       "[449 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.loc[0, 'labels'] = 'code'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.drop(columns = ['prev_text', 'next_text', 'ofs1', 'ofs2', 'ofs3'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = df_temp.groupby(['topic', 'paper_ID', 'main_heading', 'sub_heading'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([6, 7, 8, 10]), list([19, 20, 21, 22]),\n",
       "       list([44, 45, 46, 47]), list([84, 85, 86, 87, 88, 89]),\n",
       "       list([140, 141]),\n",
       "       list([180, 181, 182, 183, 184, 185, 186, 187, 188, 189]),\n",
       "       list([205, 207, 209, 210]), list([223, 224, 225, 226, 227]),\n",
       "       list([252, 253, 254, 255, 256, 257, 258]),\n",
       "       list([259, 260, 261, 262, 263, 264, 265, 266]), list([282, 283]),\n",
       "       list([303, 304, 305, 306, 307, 308, 309, 310]),\n",
       "       list([320, 321, 322]), list([352, 353, 354, 355, 356, 357, 358]),\n",
       "       list([370]), list([383, 384]),\n",
       "       list([397, 398, 399, 400, 401, 402, 403]), list([414]),\n",
       "       list([432]), list([435, 436, 437, 438, 439, 440]), list([475]),\n",
       "       list([481]), list([747, 748, 749]), list([772, 774, 775, 776]),\n",
       "       list([789, 791, 792]), list([805]), list([877]),\n",
       "       list([881, 882, 883]), list([553, 554]),\n",
       "       list([587, 593, 595, 604, 605, 606]), list([583]), list([635]),\n",
       "       list([658, 659, 660, 661, 662, 663]),\n",
       "       list([677, 678, 679, 680, 681, 682]),\n",
       "       list([696, 697, 698, 699, 700, 701, 702]),\n",
       "       list([729, 730, 731, 732]),\n",
       "       list([899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909]),\n",
       "       list([1046, 1047]), list([1086]),\n",
       "       list([1112, 1113, 1114, 1115, 1117, 1118, 1119]), list([1158]),\n",
       "       list([1163, 1164, 1165, 1166]), list([951, 952]),\n",
       "       list([966, 967, 968, 969, 971, 972, 973]), list([994, 995, 996]),\n",
       "       list([1212, 1213, 1214, 1215, 1216]),\n",
       "       list([1443, 1444, 1446, 1447, 1448]),\n",
       "       list([1464, 1465, 1466, 1467, 1468]),\n",
       "       list([1483, 1484, 1485, 1486]), list([1491]), list([1501]),\n",
       "       list([1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540]),\n",
       "       list([1557, 1558, 1559]), list([1577, 1578, 1579, 1580, 1581]),\n",
       "       list([1602, 1603, 1604, 1605, 1606, 1607]),\n",
       "       list([1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1246, 1251]),\n",
       "       list([1276, 1277, 1278]), list([1304]),\n",
       "       list([1320, 1321, 1322, 1323, 1324]), list([1363, 1364, 1367]),\n",
       "       list([1402, 1403, 1404, 1405, 1406]),\n",
       "       list([1430, 1431, 1433, 1434]),\n",
       "       list([1627, 1628, 1629, 1630, 1631, 1632]), list([1644, 1645]),\n",
       "       list([1643]), list([1856, 1857, 1858, 1859, 1860]), list([1922]),\n",
       "       list([1939, 1940, 1941, 1942]),\n",
       "       list([1977, 1978, 1979, 1980, 1981, 1982]),\n",
       "       list([2028, 2029, 2030, 2032, 2034, 2035, 2036]), list([2062]),\n",
       "       list([1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670]),\n",
       "       list([1692, 1693, 1694]), list([1716]),\n",
       "       list([1718, 1719, 1720, 1721, 1722, 1723, 1724]), list([1766]),\n",
       "       list([1829]), list([1842, 1843, 1844, 1845, 1846, 1847, 1848]),\n",
       "       list([2076, 2077, 2078, 2079, 2080, 2081, 2082]),\n",
       "       list([2118, 2119]),\n",
       "       list([2185, 2186, 2187, 2188, 2189, 2190, 2191]),\n",
       "       list([2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385]),\n",
       "       list([2590, 2591, 2592, 2593, 2595, 2596, 2597, 2598, 2599, 2600]),\n",
       "       list([2641, 2642, 2643, 2644, 2645, 2646, 2647]),\n",
       "       list([2704, 2705, 2706, 2707, 2708]), list([2714, 2715, 2716]),\n",
       "       list([2201]),\n",
       "       list([2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218]),\n",
       "       list([2227, 2228, 2229, 2230, 2231, 2233, 2234]),\n",
       "       list([2257, 2258, 2259, 2260, 2261]),\n",
       "       list([2272, 2273, 2274, 2275]), list([2289, 2290, 2291, 2292]),\n",
       "       list([2304, 2305, 2306]), list([2301]),\n",
       "       list([2360, 2361, 2362, 2363, 2364]),\n",
       "       list([2417, 2418, 2419, 2420, 2421, 2422, 2424, 2425]),\n",
       "       list([2438, 2439, 2440, 2441, 2442, 2443, 2444]),\n",
       "       list([2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514]),\n",
       "       list([2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp['index'].apply(list).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = df_temp.groupby(['topic', 'paper_ID', 'main_heading', 'sub_heading'])['topic'].apply(list).values\n",
    "paper = df_temp.groupby(['topic', 'paper_ID', 'main_heading', 'sub_heading'])['paper_ID'].apply(list).values\n",
    "main = df_temp.groupby(['topic', 'paper_ID', 'main_heading', 'sub_heading'])['main_heading'].apply(list).values\n",
    "sub = df_temp.groupby(['topic', 'paper_ID', 'main_heading', 'sub_heading'])['sub_heading'].apply(list).values\n",
    "sentence = df_temp.groupby(['topic', 'paper_ID', 'main_heading', 'sub_heading'])['text'].apply(list).values\n",
    "label = df_temp.groupby(['topic', 'paper_ID', 'main_heading', 'sub_heading'])['labels'].apply(list).values\n",
    "pos11 = df_temp.groupby(['topic', 'paper_ID', 'main_heading', 'sub_heading'])['pos1'].apply(list).values\n",
    "pos22 = df_temp.groupby(['topic', 'paper_ID', 'main_heading', 'sub_heading'])['pos2'].apply(list).values\n",
    "pos33 = df_temp.groupby(['topic', 'paper_ID', 'main_heading', 'sub_heading'])['pos3'].apply(list).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, paper_ID, main_heading, sub_heading = [], [], [], []\n",
    "paragraph, labels, pos1, pos2, pos3, length = [], [], [], [], [], []\n",
    "count = 0\n",
    "for i in range(len(topic)):\n",
    "    unit = label[i][0]\n",
    "    for info in label[i]:\n",
    "        if(info != unit):\n",
    "            count += 1\n",
    "            break\n",
    "    topics.append(topic[i][0])\n",
    "    paper_ID.append(paper[i][0])\n",
    "    main_heading.append(main[i][0])\n",
    "    sub_heading.append(sub[i][0])\n",
    "    pos1.append(pos11[i])\n",
    "    pos2.append(pos22[i])\n",
    "    pos3.append(pos33[i])\n",
    "    labels.append(unit)\n",
    "    sent = \"[CLS] \"\n",
    "    for text in sentence[i]:\n",
    "        sent += text\n",
    "        sent += \" [SEP] \"\n",
    "    paragraph.append(sent)\n",
    "    length.append(len(topic[i]))\n",
    "print(count, len(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp = pd.DataFrame({\"topic\" : topics, \"paper_ID\" : paper_ID, \"main_heading\" : main_heading, \n",
    "                       \"sub_heading\" : sub_heading, \"paragraph\" : paragraph, \"labels\" : labels, \n",
    "                       \"length\" : length, \"pos1\" : pos1, \"pos2\" : pos2, \"pos3\" : pos3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp.to_csv(\"Info_Units_Results/test_results_info_units_hyper.csv\", index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
